[{"title":"aaa","url":"/2020/12/20/163913/","content":"\n在DeePL中把ppt翻译后再下载下来不能进行编辑，只能以只读方式打开，只有输入密码后才能进行编辑，这里破解密码，步骤如下：\n\n<!-- more -->\n\n\n\n1、首先将ppt的名称加上.zip，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164232.png)\n\n2、解压缩后，打开文件夹，进入ppt文件夹下并打开presentation.xml文件，如下图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164425.png)\n\n3、删除里面``cy=\"10058400\" /></p:presentation>`之间的内容，如下图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164546.png)\n\n\n\n4、全选下图这些文件压缩为zip文件文件\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164853.png)\n\n\n\n5、然后再把压缩后的zip文件后面`.zip`删除，再次打开ppt，即可进行编辑了\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164748.png)","tags":["tag1","tag2"],"categories":["Life"]},{"title":"matplotlib绘制图像","url":"/2020/11/25/125802/","content":"\n@[TOC](文章目录)\n\n<!-- more -->\n\n\n\n> 我是用cv2读取图片，最近发现了一个奇异的问题，在一个目录下用相对路径读取读片（相对路径没有英文），新建一个notebook文件可以读取，但是把别的地方的notebook文件拷贝过来就不行了，一直返回None，虽然不知道是什么原因，但以后需要注意啊！！！\n\n\n\n\n\n## 1. 基本matplotlib设置\n\n```python\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# 设置中文字体为黑体\nplt.rcParams['font.sans-serif'] = ['SimHei']\n# 用来正常显示负号\nplt.rcParams['axes.unicode_minus'] = False\ndef BGRTORGB(img):\n    return cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\ndef BGRTOGRAY(img):\n    return cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n```\n\n\n\n## 2. 同时显示多个图像\n\n因为matplotlib显示是从小到大排列，比如要显示10张图像，并分2为行显示，显示的子图序号是`251`、`252`、...、`2510`，这是不行的，因为`2510`不被识别，因此采用这种方式显示图片有个数限制。\n\n所以，用虽然用下面的方法可以同时显示多个图像，但是最多不超过10张。\n\n\n\n```python\n# 同时显示多个图像\n\"\"\"\nimgs 源图片列表，类型为list（或numpy数组）\nnames 源图片标题，类型为list（或numpy数组）\nuse_color 是否彩色显示，None为全部灰度显示，如果指定某一张图片彩色显示，也可以设置，例如[False,True,False]，即执行第二种图片彩色显示\ntfigsize 整个画布的尺寸\nrow 显示的行数，行数一定要被图片数整除\n\"\"\"\ndef show_mutli_img(imgs,names,use_color=None,tfigsize=(10,10),row=1):\n    if use_color is None :\n        use_color = [False]*len(imgs)\n    plt.figure(figsize=tfigsize)\n    show_len=len(imgs)\n    gird=int(str(row)+str(show_len//row))*10;\n    r,i,c=0,0,0;\n    for r in range(row):\n        for i in range(show_len//row):       \n            plt.subplot(gird+c+1)\n            if use_color[i]:\n                plt.imshow(imgs[c])\n            else: plt.imshow(imgs[c],cmap='gray')\n            plt.title(names[c])\n            c = c+1\n    plt.show()\n```\n\n\n\n## 3. 同时显示多个图像（无个数限制）\n\n```python\n\"\"\"\nplt.subplot2grid创建子图\nrow,col分别为行、列，类型为int\ndata_img为存储图片的数组列表（或numpy数组），必须是(row,col)大小\ndata_title为存储对应图片的标题列表（或numpy数组），大小同上\n\"\"\"\ndef show_data_img(data_img,data_title,row,col,tfigsize=(12,5)):\n    plt.figure(figsize=tfigsize)\n    plots = []\n    for i in range(row):\n        for j in range(col):\n            ax = plt.subplot2grid((row,col), (i,j))\n            ax.imshow(data_img[i][j],cmap='gray')\n            ax.set_title(data_title[i][j])\n    plt.show()   \n```\n\n\n\n## 4. 多个大小不同的图像\n\n```python\n# 显示不同级别的小波图像\n\"\"\"\n使用plt.subplot2grid来创建多小图\n- grid 表示将整个图像窗口分成几行几列，类型为tuple\n- pos 表示从第几行几列开始作图，类型为list\n- spans 表示这个图像行列跨度，类型为list\n- titles 表示子图的标题，类型为list\n\"\"\"\ndef show_levelImg(show_imgs,grid,pos,spans,titles,tfigsize=(10,10)):\n    plt.figure(figsize=tfigsize)\n    i = 0;\n    for i in range(len(pos)):\n        #行，列\n        trowspan,tcolspan = spans[i]\n        ax1 = plt.subplot2grid(grid, pos[i], rowspan=trowspan, colspan=tcolspan);\n        ax1.imshow(show_imgs[i],cmap='gray');ax1.set_title(titles[i]);ax1.axis('off')\n```\n\n可以运行一下看看效果：\n\n```python\n# len(imgs) =7\ngrid = (4,4)\npos = [(0, 2),(2,0),(2,2),(0,0),(0,1),(1,0),(1,1)]\nspans = [(2,2),(2,2),(2,2),(1,1),(1,1),(1,1),(1,1)]\ntitles = ['person1','person2','person3','person4','person5','ca1','car2']\nshow_levelImg(imgs,grid,pos,spans,titles)  \n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201125140304.png)\n\n\n\n\n\n\n\n## 5. 直方图\n\n```python\n# 读取图像\nimg = cv2.imread(\"resource/lina.jpg\")\nimg = BGRTORGB(img)\ngray_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\nplt.hist(gray_img.ravel(), 256);plt.title(\"直方图\")\nplt.show()\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201125140529.png)\n\n\n\n```python\ndef show_hist_quantized(histogram_quantized):\n    plt.figure(figsize=(7,7))\n    plt.title(u\"等分量化后的直方图\")\n    plt.xlabel(u\"等分index\")\n    plt.ylabel(u\"nums\")\n    plt.bar(x=list(range(len(histogram_quantized))), height=histogram_quantized)\n    plt.show()\nhistogram_quantized = [50,40,30,60,70]    \nshow_hist_quantized(histogram_quantized)\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201125141415.png)\n\n\n\n\n\n## 6. 坐标轴为整数的折线图\n\n```python\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nplt.title(u\"a plot\")\nplt.xlabel(u\"xxx\")\nplt.ylabel(u\"yyy\")\ndata = [13.4,4.6,7.8,9.9,5.5]\nplt.xticks(range(len(data)))\nplt.plot(data)\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201125141022.png)\n\n","tags":["matplotlib"],"categories":["matplotlib"]},{"title":"实用的代码片段","url":"/2020/11/25/125249/","content":"\n\n\n@[TOC](文章目录 )\n\n<!-- more -->\n\n## python 获取当前文件夹下所有文件名\n\n\n\n```python\nimport os\n\ndef file_name(file_dir):\n    for root, dirs, files in os.walk(file_dir):\n        print(root) #当前目录路径\n        print(dirs) #当前路径下所有子目录\n        print(files) #当前路径下所有非目录子文件\n```\n\n参考链接：https://www.cnblogs.com/strongYaYa/p/7200357.html","categories":["工具"]},{"title":"黑马C++教程笔记——函数提高","url":"/2020/11/25/105331/","content":"\n@[TOC](文章目录)\n\n<!-- more -->\n\n\n\n## 1 函数默认参数\n\n在C++中，函数的形参列表中的参数是可以有默认值大的\n语法：`返回值类型 函数名 (参数= 默认值){ }`\n\n示例：\n\n```cpp\n#include<iostream>\nusing namespace std;\n\n//函数默认参数\n//如果我们自己传入数据，就用自己的数据，如果没有，那么用默认值\nint func(int a, int b=50, int c=70) {\n\treturn a + b + c;\n}\n\n//注意事项\n//1、如果某个位置已经有了默认参数，那么从这个位置往后，从左到右都必须有默认值\n//int func2(int a, int b = 50, int c) {\n//\treturn a + b + c;\n//}\n//2、如果函数声明有默认参数，函数实现就不能有默认参数\n//准确的说，是声明和实现只能有一个有默认参数，否则，可能会二义性\nint func2(int a, int b);\n\nint func2(int a=20, int b=20) {\n\treturn a + b;\n}\n\nint  main() {\n\t//cout << func(10, 20, 30) << endl;\n\t//cout << func(10,20) << endl;\n\t//cout << func(10) << endl;\n\tcout<<func2(10, 10)<<endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n## 2 函数占位参数\nC++中函数的形参列表里可以有占位参数，用来做占位，调用函数时必须填补该位置\n\n语法：`返回值类型 函数名 (函数类型){}`\n\n在现阶段函数的占位参数存在意义不大，但是以后可能会用到该技术\n\n示例：\n```cpp\n//占位参数\n//目前阶段占位参数，我们还用不到，以后可能会用到\n\nvoid func(int a,int) {\n\tcout << \"this is func\" << endl;\n}\n//占位参数 还可以有默认参数\nvoid func1(int a, int = 10) {\n\tcout << \"this is func1\" << endl;\n}\nint  main() {\n\tfunc(10,10);\n\tfunc1(10);\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 3 函数重载\n### 3.1 函数重载概述\n\n**作用**：函数名可以相同，提高复用性\n\n函数重载满足条件：\n- 同一作用域下\n- 函数名称相同\n- 函数参数**类型不同** 或者 **个数不同** 或者 **顺序不同**\n\n**注意**：函数的返回值不可以作为函数重载的条件\n\n示例：\n\n```cpp\n//函数重载\n//可以让函数名相同，提高复用性\n\n//函数重载的满足条件1\n//1、同一作用域下\n//2、函数名称相同\n//3、函数参数类型不同，或者个数不同，或者顺序不同\n\nvoid func() {\n\tcout << \"func 调用\" << endl;\n}\nvoid func(int a) {\n\tcout << \"func(int a) 调用\" << endl;\n}\nvoid func(double a) {\n\tcout << \"func(double a) 调用\" << endl;\n}\nvoid func(int a,double b) {\n\tcout << \"func(int a,double b) 调用\" << endl;\n}\nvoid func(double a, int b ) {\n\tcout << \"func(double a, int b ) 调用\" << endl;\n}\n//注意事项\n//函数的返回追不可以作为函数重载的条件\n//int func(double a, int b) {\n//\tcout << \"func(double a, int b ) 调用\" << endl;\n//}\nint  main() {\n\n\t//func();\n\t//func(1);\n\t//func(10.0);\n\t//func(1, 10.0);\n\tfunc(10.0, 1);\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n### 3.2 函数重载注意事项\n\n- 引用作为重载条件\n- 函数重载碰到函数默认参数\n\n示例：\n\n```cpp\n//函数重载的注意事项\n//1、引用作为重载的条件\nvoid func(int &a) {// int &a=10;不合法的，因为这个字面量10放在常量区里面，不可写\n\tcout << \"fun(int &a) 调用\" << endl;\n}\nvoid func(const int &a) {//const int &a=10 只读的引用，是合法的\n\tcout << \"fun(const int &a) 调用\" << endl;\n}\n\n//2、函数重载碰到默认参数\nvoid func2(int a,int b=10) {\n\tcout << \"func2(int a,int b)的调用\" << endl;\n}\nvoid func2(int a) {\n\tcout << \"func2(int a)的调用\" << endl;\n}\n\nint  main() {\n\t//int a = 10;\n\t//const引用也可以重载\n\t//func(a);//调用无const函数\n\t//func(10);//调用有const函数\n\n\tfunc2(10);//当函数重载碰到默认参数，出现二义性，报错，尽量避免这种情况\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n\n\n","tags":["c++"],"categories":["c&c++"]},{"title":"黑马C++教程笔记——引用","url":"/2020/11/25/105203/","content":"\n\n@[TOC](文章目录)\n\n<!-- more -->\n\n> 本文仅做笔记使用，黑马C++教程从0到1入门编程链接：https://www.bilibili.com/video/BV1et411b73Z\n\n\n\n## 1 引用的基本使用\n\n- **作用**：给变量起别名\n- **语法**：`数据类型 &别名 = 原名`\n- 注意：引用的数据类型要和原类型一样\n\n原理图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20201004193426381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n示例：\n\n```cpp\n#include<iostream>\n\nusing namespace std;\n\n\nint  main() {\n\n\tint a = 10;\n\t//创建引用\n\tint &b = a;\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\tb = 100;\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 2 引用的注意事项\n- 引用必须初始化\n- 引用在初始化后，不可以改变\n\n原理图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20201004194926459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n示例：\n\n```cpp\nint  main() {\n\tint a = 10;\n\n\t//1、引用必须初始化\n\tint &b=a;//错误，必须要初始化\n\n\t//2、引用在初始化后，不可以改变\n\tint c = 20;\n\tb = c;//赋值操作，而不是更改引用,即这里只是把b指向的内置赋值成20\n\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\tcout << \"b = \" << b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 3 引用做函数参数\n- 作业：函数传参时，可以利用引用的技术让形参修饰实参\n- 优点：可以简化指针修改实参\n\n示例：\n\n```cpp\n//交换函数\n//1、值传递\nvoid mySwap01(int a, int b) {\n\tint temp = a;\n\ta = b;\n\tb = temp;\n\t//cout << \"mySwap01 a = \" << a << endl;\n\t//cout << \"mySwap01 b = \" << b << endl;\n}\n//2、地址传递\nvoid mySwap02(int *a, int *b) {\n\tint temp = *a;\n\t*a = *b;\n\t*b = temp;\n\n}\n//3、引用传递\nvoid mySwap03(int &a, int &b) {\n\tint temp = a;\n\ta = b;\n\tb = temp;\n}\n\nint  main() {\n\tint a = 10;\n\tint b = 20;\n\t//mySwap01(a, b);//值传递，实参不会改变\n\tmySwap02(&a, &b);//地址传递，实参会改变\n\tmySwap03(a, b);//地址传递，实参会改变\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n> 总结：通过引用参数产生的效果同地址传递是一样的。引用的语法更清楚简单\n\n## 4 引用做函数返回值\n\n作用：引用是可以作为函数的返回值存在的\n\n注意：**不要返回局部变量的引用**\n用法：函数调用作为左值\n\n示例：\n\n```cpp\n//引用做函数的返回值\n//1、不要返回局部变量的引用\nint& test01() {\n\tint a = 10;//局部变量存放在四区中的栈区\n\treturn a;\n}\n\n//2、函数的调用可以作为左值\nint& test02() {\n\tstatic int a = 10;//静态变量，存放在全局区，全局区上的数据在程序结束后由系统释放\n\treturn a;\n}\n\n\nint  main() {\n\n\tint &ref = test01();\n\n\t//cout << \"ref = \" << ref << endl;//第一次结果正确，是因为编译器做了保留\n\t//cout << \"ref = \" << ref << endl;//第二次结果错误，是因a的内存已经释放\n\n\tint &ref2 = test02();\n\tcout << \"ref2 = \" << ref2 << endl;\n\tcout << \"ref2 = \" << ref2 << endl;\n\ttest02() = 1000;//如果函数的返回值是引用，这个函数调用可以作为左值\n\tcout << \"ref2 = \" << ref2 << endl;\n\tcout << \"ref2 = \" << ref2 << endl;\n\tcout << \"ref2 = \" << ref2 << endl;\n\n\n\t//这个就不是变量的引用了，这里相对于直接在新的内存copy函数的返回值\n\tint ref3 = test02();\n\tcout << \"ref3 = \" << ref3 << endl;\n\tcout << \"ref3 = \" << ref3 << endl;\n\ttest02() = 999;\n\tcout << \"ref3 = \" << ref3 << endl;\n\tcout << \"ref3 = \" << ref3 << endl;\n\tcout << \"ref3 = \" << ref3 << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 5 引用的本质\n\n本质：**引用的本质在c++内部实现是一个指针常量**，即指针的指向不可以修改，指针指向的值是可以修改的\n\n\n示例：\n\n```cpp\n//发现是引用，转换为int* const ref = &a;\nvoid func(int &ref) {\n\tref = 100;//ref是引用，转换为*ref = 100;\n}\n\nint  main() {\n\n\tint a = 10;\n\n\t//自动转化为int *const ref = &a;指针常量是指针的指向不可以改，也说明为什么引用不可更改\n\tint &ref = a;\n\tref = 20; //内部发现ref是引用1，自动帮我们转换为：*ref = 20;\n\n\tcout << \"a = \" << a << endl;\n\tcout << \"ref = \" << ref << endl;\n\n\tfunc(a);\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n原理图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20201004204328249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n> 结论：C++推荐用引用技术，因为语法方便，引用本质是指针常量，但是所有的指针操作编译器都帮我们做了\n\n## 6 常量引用\n\n作用：常量引用主要用来修饰形参，防止误操作\n\n在函数形参列表中，可以加==const修饰形参==，防止形参改变实参\n\n示例：\n\n```cpp\n//打印数据\nvoid showValue(const int &val) {//加上const就不能修改了\n\t//val = 1000;\n\tcout << \"val = \" << val << endl;\n}\n\nint  main() {\n\t//常量引用\n\t//使用场景：用来修饰形参，防止误操作\n\t//int a=10;\n\t//int &ref = 10;//错误，引用必须引一块合法的内存空间，可以是栈区，也可以是堆区，所以这里字面量是不可以的\n\tconst int &ref = 10;//加上const之后，编译器将代码修改为 int temp = 10; const int &ref = temp;\n\t//ref = 20;//加入const之后变为只读，不可以修改\n\n\tint a = 100;\n\tshowValue(a);\n\tcout << \"a = \" << a << endl;\n\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n","tags":["c++"],"categories":["c&c++"]},{"title":"黑马C++教程笔记——程序的内存模型","url":"/2020/11/25/105031/","content":"\n@[TOC](文章目录)\n\n<!-- more -->\n\n\n\n## 1 内存分区模型\n\nC++程序在执行时，将内存大方向划分为4个区域\n- 代码区：存放函数体的二进制代码，由操作系统进行管理的（所有的代码及注释）\n- 全局区：存放全局变量和静态变量以及常量\n- 栈区：由编译器自动分配释放，存放函数的参数值，局部变量等\n- 堆区：有程序员分配和释放，若程序员不释放，程序结束时由操作系统回收\n\n\n**内存四区的意义**：\n不同区域存放的数据，赋予不同的生命周期，给我们更大的灵活编码\n\n### 1.1 程序运行前\n在程序编译后，生成了exe可执行程序，**未执行该程序前**分为两个区域：代码区和全局区\n\n\n**代码区：**\n- 存放CPU执行的机器指令（就是你写的代码）\n- 代码区是**共享的**，共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可\n- 代码区是**只读的**，使其只读的原因是防止程序意外地修改了它的指令\n\n**全局区：**\n- 全局变量和静态变量存放在此\n- 全局区还包含了常量区，字符串常量和其他常量（const修饰的变量即常量）也存放在此\n- ==该区域的数据在程序结束后由操作系统释放==\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020092320063968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\n//全局变量\nint g_a = 10;\nint g_b = 10;\n\n//const修饰的全局变量，即全局常量，全局常量在常量区\nconst int c_g_a = 10;\nconst int c_g_b = 10;\n\nint main() {\n\n\t// 全局区\n\t// 全局变量、静态变量、常量（字符串常量和全局常量）\n\n\t//创建普通局部变量\n\tint a = 10;\n\tint b = 10;\n\tcout << \"局部变量a的地址为：\" << (int)&a << endl;\n\tcout << \"局部变量b的地址为：\" << (int)&b << endl;\n\tcout << \"全局变量g_a的地址为：\" << (int)&g_a << endl;\n\tcout << \"全局变量g_b的地址为：\" << (int)&g_b << endl;\n\n\t// 静态变量 在普通变量前面加static，数据静态变量\n\tstatic int s_a = 10;\n\tstatic int s_b = 10;\n\tcout << \"静态变量s_a的地址为：\" << (int)&s_a << endl;\n\tcout << \"静态变量s_b的地址为：\" << (int)&s_b << endl;\n\t\n\t//常量\n\t//字符串常量\n\tcout << \"字符串常量hello地址为：\" << (int)&\"hello\" << endl;\n\tcout << \"字符串常量world地址为：\" << (int)&\"world\" << endl;\n\n\t//const修饰的变量\n\t//const修饰的全局变量，const修饰的局部变量\n\tcout << \"全局常量c_g_a的地址为：\" << (int)&c_g_a << endl;\n\tcout << \"全局常量c_g_b的地址为：\" << (int)&c_g_b << endl;\n\n\t//const修饰的局部变量，即局部常量,局部常量不在常量区，也不在全局区\n\t//局部常量存放在栈区\n\tconst int c_l_a = 10;// c - const g - global  l - local\n\tconst int c_l_b = 10;\n\tcout << \"局部常量c_l_a的地址为：\" << (int)&c_l_a << endl;\n\tcout << \"局部常量c_l_b的地址为：\" << (int)&c_l_b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n\n```\n\n总结：\n- c++中在程序运行前分为全局区和代码区\n- 代码区特点是共享和只读\n- 全局区中存放全局变量、静态变量、常量\n- 常量区中存放const修饰的全局常量和字符串常量\n- const修饰的局部常量放在栈区（同普通局部变量一个位置）\n\n### 1.2 程序运行后\n**栈区：**\n- 由编译器自行分配和释放，存放函数的参数值，局部变量等\n- 注意事项：不要返回局部变量的地址，栈区开辟的数据由编译器自行释放（在函数执行完后自行释放）\n\n```cpp\n//栈区数据注意事项 -- 不要返回局部变量的地址\n//栈区的数据由编译器管理开辟和释放\n\n\nint * func(int b) {//形参数据也会放在栈区\n\tb = 100;\n\tint a = 10;//局部变量存放在栈区，存放在栈区，栈区的数据在函数执行完后自行释放\n\treturn &a;//返回局部变量的地址\n}\n\nint main() {\n\t//接受func函数的返回值\n\tint * p = func(1);\n\n\tcout << *p << endl;//第一次可以打印正确的数字，是因为编译器给我们做了一次保留\n\tcout << *p << endl;//第二次这个数据就不再保留了\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n**堆区：**\n- 由程序员分配释放，若程序员不释放，程序运行期间，该内存不会被释放，程序结束时由操作系统回收\n- 在C++中利用new在堆区开辟内存\n\n```cpp\nint * func() {\n\t//利用new关键字，可以将数据开辟到堆区\n\t//指针 本质也是变量，放在栈上，指针保存的数据是放在堆区\n\tint * p = new int(10);//返回的是地址\n\treturn p;\n}\n\nint main() {\n\t\n\t//在堆区开辟数据\n\tint * p = func();//得到func返回的地址，保存在main函数区部变量中（也在堆区），当main函数运行结束后，这个func返回的局部变量地址才会消除\n\tcout << *p << endl;\n\tcout << *p << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923205713998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n总结：\n- 堆区数据由程序员管理开辟和释放\n- 堆区数据利用new关键字进行开辟内存\n\n### 1.3 new操作符\n- C++中利用==new==操作符在堆区开辟数据\n- 堆区开辟的数据，由程序员手动开辟，手动释放，释放利用操作符==delete==\n\n语法：`new 数据类型`\n利用new创建的数据，会返回该数据对应的类型的指针\n\n```cpp\n//1、new的基本语法\nint * func() {\n\t//在堆区创建整型数据\n\t//new返回的是 该数据类型的指针\n\tint * p = new int(10);\n\treturn p;\n}\n\nvoid test01() {\n\tint * p = func();\n\tcout << *p << endl;\n\tcout << *p << endl;\n\t//堆区的数据 由程序员管理开辟，程序员管理释放\n\t//如果释放堆区的数据，利用关键字delete\n\tdelete p;\n\tcout << *p << endl; //内存已经被释放，再次访问就是非法操作，会报错\n}\n\n//2、在堆区利用new开辟数组\nvoid test02() {\n\t//创建10整型数据的数组，在堆区\n\tint * arr = new int[10];//10代表数组有10个元素\n\tfor (int i = 0; i < 10; i++)\n\t\tarr[i] = i + 100;\n\tfor (int i = 0; i < 10; i++)\n\t\tcout << arr[i] << endl;\n\t//释放堆区数组\n\t//释放数组的时候，要加[]才可以\n\tdelete[] arr;\n}\n\nint main() {\n\t\n\t//test01();\n\ttest02();\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 2 几个重要结论\n### 2.1 结论1\n结论1：当调用函数时，其实就相当于对传入实参进行了拷贝，存放在栈中（局部变量），具体如下（见示例1）：\n- 当传入值时，拷贝值存放在栈中\n- 当传入指针时，拷贝指针的地址存放在栈中\n- 当传入引用时，和传入指针原理是一样的，因为引用本身是由指针常量实现的（即指针的指向不可以修改，指针指向的值是可以修改的）\n\n\n另外，需要注意的是：**简短的赋值操作不属于拷贝，赋值操作只是把原有内存空间换成了别的值，所以拷贝只是在函数调用时存在**，可以看下面的实例2.\n\n示例1：\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\nvoid func(int a) {\n\tcout << \"func(int a)中a的地址：\" << (int)&a << endl;\n}\nvoid func1(int *a) {\n\tcout << \"func1(int *a)中a的地址：\" << (int)a << endl;\n\tcout << \"func1(int *a)中指针a本身存放的地址：\" << (int)&a << endl;\n\n}\nvoid func2(int &a) {\n\tcout << \"func2(int &a)中a的地址：\" << (int)&a << endl;\n}\n\nint g_a = 10;\nint main() {\n\tcout << \"（全局区）全局变量g_a的地址：\" << (int)&g_a << endl;\n\tint l_a = 10;\n\tcout << \"（栈区）局部变量l_a的地址：\" <<(int)&l_a<< endl;\n\tfunc(g_a);\n\tfunc(l_a);\n\tfunc1(&l_a);\n\tfunc2(l_a);\n\t/*\n\t结论1：当调用函数时，其实就相当于对传入实参进行了拷贝，存放在栈中（局部变量），具体有以下情况：\n\t- 当传入值时，拷贝值存放在栈中\n\t- 当传入指针时，拷贝指针的地址存放在栈中\n\t- 当传入引用时，和传入指针原理是一样的，因为引用本身是由指针常量实现的（即指针的指向不可以修改，指针指向的值是可以修改的）\n\t*/\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n示例2：\n\n```cpp\nint main() {\n\n\t//下面赋值前后a的地址是一样的\n\tint a = 10;\n\tcout << \"a的地址为：\" << (int)&a << endl;\n\n\ta = 20;\n\tcout << \"值更改后a的地址为：\" << (int)&a << endl;\n\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n### 2.2 结论2\n结论2，**全局区中全局变量和静态变量在一个区域，全局常量和字符串常量在一个区域。全局变量和静态变量可以读写（即可以更改），全局常量和字符串常量只能读，不能写。在全局区的常量区（全局常量和字符串常量）中，数值只存在一份，而全局变量区（全局变量和静态变量），相同数值可以有多个（方法是多创建几个数值相同的变量）。见示例1**\n\n除了作用域外，全局变量区（全局变量和静态变量）和局部变量区（栈区）是一样的，都可以读写，多个值相同的变量拥有**不同**的内存空间。\n\n另外，**所有的字面量（包括字符串字面量和其他字面量）都是常量，存放在全局区中的常量区中，不能进行写操作，而引用是为了给变量其别名，为了对变量的内存空间进行读写操作（必须有一个合法的内存空间，可以读写的空间都可以，堆区，栈区，全局变量区），所以把变量赋给引用，而不能是字面量。如果是只读的引用，则把字面量赋给引用也是合法的，见示例2**。\n\n**要记住，创建一个局部变量（指针变量、数值变量，后面结论3会更详细说），存放在栈区，而引用只是给变量起别名，不是创建变量，所以引用的右侧位置只能放变量**\n\n```cpp\n// 全局区\n// 全局变量、静态变量、常量（字符串常量和全局常量）\n\n//全局变量\nint g_a = 10;\nint g_b = 10;\n\n//const修饰的全局变量，即全局常量\nconst int c_g_a = 10;\nconst int c_g_b = 10;\nint main() {\n\n\tcout << \"全局区变量地址为：\" << endl;\n\tcout << \"全局变量g_a的地址为：\" << (int)&g_a << endl;\n\tcout << \"全局变量g_b的地址为：\" << (int)&g_b << endl;\n\n\tstatic int s_a = 10;\n\tstatic int s_b = 10;\n\tcout << \"全局区静态变量地址为：\" << endl;\n\tcout << \"静态变量s_a的地址为：\" << (int)&s_a << endl;\n\tcout << \"静态变量s_b的地址为：\" << (int)&s_b << endl;\n\n\n\tcout << \"全局区普通常量地址为：\" << endl;\n\tcout << \"全局常量c_g_a的地址为：\" << (int)&c_g_a << endl;\n\tcout << \"全局常量c_g_b的地址为：\" << (int)&c_g_b << endl;\n\n\tcout << \"全局区字符串常量地址为：\" << endl;\n\tcout << \"字符串常量hello地址为：\" << (int)&\"hello\" << endl;\n\tcout << \"字符串常量world地址为：\" << (int)&\"world\" << endl;\n\tcout << \"字符串常量hello地址为：\" << (int)&\"hello\" << endl;\n\n\t//结论2，全局区中全局变量和静态变量在一个区域，全局常量和字符串常量在一个区域。\n\t//全局变量和静态变量可以读写（即可以更改），全局常量和字符串常量只能读，不能写\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n示例2：\n\n```cpp\nint main() {\n\t//int &p = 10;//读写的引用，不合法的，因为这个字面量10放在常量区里面，不可以进度写\n\t//只读的引用，合法的\n\tconst int &c = 10;\n\n\tint a = 10;\n\t//引用右侧放变量，合法\n\tint &b = a;\n\n\tcout << b << endl;\n\tcout << c << endl;\n\t\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n\n\n### 2.3 结论3\n**结论3（很重要）**：\n- 所有的局部变量(一般为指向一片内存空间的指针的内存地址)存放在栈区，当然数据型局部变量也在栈区\n- 所有的全局变量(一般为指向一片内存空间的指针的地址)存放在全局变量区，当然数据型全局变量也在全局变量区\n- 如果在局部作用域中new一个数据(数值或数组），new出来的数据的内存空间在堆区，而指向该内存空间的指针的地址在栈区\n- 如果在全局作用域中new一个数据(数值或数组），new出来的数据的内存空间在堆区，而指向该内存空间的指针的地址在全局变量区\n- 在局部作用域中用如`char * s = \"hello\"`创建出来的字符串，该字符串的内存空间在全局常量区，而指向该字符串内存空间的指针的地址(即s的地址)在栈区，故该字符串的内存空间是只读不可写的，而指向该字符串内存空间的指针的地址是可以被重新写入的（即重新赋值，看结论1的注意事项）\n- **在局部作用域中用如`char arr[] = \"hello\"`创建出来的数组字符串，该字符串的内存空间在栈区，该字符串内存空间的指针的地址(即arr的地址)也在栈区**，这个是需要注意的地方，所以，该字符串的内存空间是**可读可写的**，并且内存空间由编译器自动释放。\n- 在局部作用域中用如`char *newarr = new char[6]`创建的数组，该数组的内存空间在堆区，而指向数组内存空间的指针的地址在栈区，该数组的内存空间需要由程序员释放，指向数组内存空间的指针的地址(内存空间)由编译器自动释放。\n- 在全局作用域用如`int * global_newArr = new int[20]`创建的数组，该数组的内存空间在堆区，而指向数组内存空间的指针的地址在全局变量区，该数组的内存空间需要由程序员释放，指向数组内存空间的指针的内存空间由编译器自动释放。\n\n示例：\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\nint * global_newArr = new int[20];\n\nint main() {\n\n\t//int型局部变量\n\tint a = 10;\n\tcout << \"栈区的地址为：\" << (int)&a << endl;\n\n\tcout << \"全局常量区的地址为：\" << (int)&\"hello\" << endl;\n\t\n\n\tint * p = new int(10);\n\tcout << \"堆区的地址为：\" << (int)p << endl;\n\tcout << \"指向堆区内存空间的指针(变量)的地址为：\" << (int)&p << endl;\n\n\n\tchar * s = \"hello\";\n\tcout << \"字符串的地址为：\" << (int)s << endl;\n\tcout << \"指向该字符串内存空间的指针的地址为：\" << (int)&s << endl;\n\t//s[3] = 'z'; 报错，全局常量区只能读，不能写\n\n\tchar arr[] = \"hello\";\n\tcout << \"数组字符串的地址为：\" << (int)arr << endl;\n\tcout << \"指向该数组字符串内存空间的指针的地址为：\" << (int)&arr << endl;\n\t//arr[3] = 'z';不报错，数组字符串存放在堆区，可读可写\n\n\n\tchar *newarr = new char[6];\n\tcout << \"new数组的地址为：\" << (int)newarr << endl;\n\tcout << \"指向new数组内存空间的指针的地址为：\" << (int)&newarr << endl;\n\n\tcout << \"global_newArr的地址为：\" << (int)global_newArr << endl;\n\tcout << \"指向global_newArr内存空间的指针的地址为：\" << (int)&global_newArr << endl;\n\n\tdelete p;\n\tdelete[] newarr;\n\tdelete[] global_newArr;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n### 2.4 结论4(sizeof统计空间大小)\n\n**关于数组字符串和指针字符串所占空间大小**，无论指针字符串长度多少，使用`sizeof`统计的大小始终为4个字节，因为统计的是指针的大小。而对于数组字符串，使用`sizeof`统计的是整个字符串空间的大小。\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\nint main() {\n\tchar * a = \"abcefg\";\n\tchar b[] = \"abcefg\";\n\tcout << \"a:\" << sizeof(a) << endl;\n\tcout << \"b：\" << sizeof(b) << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n","tags":["c++"],"categories":["c&c++"]},{"title":"黑马C++教程笔记——结构体","url":"/2020/11/25/104732/","content":"\n@[TOC]()\n\n<!-- more -->\n\n\n\n\n\n### 1.1 结构体基本概念\n结构体属于用户==自定义的数据类型==，允许用户存储不同的数据类型\n\n\n### 1.2 结构体定义和作用\n语法：`struct 结构体名 { 结构体成员列表 };`\n通过结构体创建变量的方式有三种：\n- struct 结构体名变量名\n- struct 结构体名 = {成员1值，成员2值...}\n- 定义结构体时顺便创建变量\n\n示例：\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\n// 1、创建学生数据类型：学生包括(姓名，年龄，分数)\n// 语法 struct 类型名称 {成员列表}\nstruct Student {\n\n\t//成员列表\n\tstring name;\n\tint age;\n\tint score;\n\n} s3;//顺便常见结构体变量\n\n// 2、通过学生类型创建具体学生\n\nint main() {\n\n\t// 2.1 struct Student s1\n\t//struct 关键字可以省略\n\tStudent s1;\n\ts1.age = 20;\n\ts1.name = \"张三\";\n\ts1.score = 100;\n\tcout << \"姓名：\" << s1.name << \"\\t年龄：\" << s1.age << \"\\t分数：\" << s1.score << endl;\n\t// 2.2 struct Student s2 = {...}\n\tstruct Student s2 = { \"李四\",21,101 };\n\tcout << \"姓名：\" << s2.name << \"\\t年龄：\" << s2.age << \"\\t分数：\" << s2.score << endl;\n\t// 2.3 在定义结构体时顺便创建结构体变量 （不建议）\n\ts3.age = 22;\n\ts3.name = \"王五\";\n\ts3.score = 102;\n\tcout << \"姓名：\" << s3.name << \"\\t年龄：\" << s3.age << \"\\t分数：\" << s3.score << endl;\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n> - 总结1：定义结构体时的关键字是struct，不可省略\n> - 总结2：创建结构体变量时，关键字struct可以省略\n> - 总结3：结构体变量利用操作符\".\"访问成员\n\n### 1.3 结构体数组\n作用：将自定义的结构体放入到数组中方便维护\n语法：`struct 结构体名 数组名[元素个数]={ {} , {} , {} , ... , {}}`\n示例：\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\nstruct Student {\n\n\tstring name;\n\tint age;\n\tint score;\n\n};\n\nint main() {\n\n\tstruct Student stus[3] = { {\"张三\",20,100},{\"李四\",21,101},{\"王五\",22,102} };\n\tfor (int i = 0; i < 3; i++)\n\t\tcout << \"名字：\" << stus[i].name << \"\\t年龄：\" << stus[i].age << \"\\t分数：\" << stus[i].score << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n### 1.4 结构体指针\n作用：通过指针访问结构体中的成员\n\n- 利用操作符`->`可以通过结构体指针访问结构体属性\n\n示例：\n\n```cpp\nstruct Student {\n\n\tstring name;\n\tint age;\n\tint score;\n\n};\n\nint main() {\n\n\t//1、创建结构体变量\n\tstruct Student s = { \"张三\",18,100 };\n\n\t//2、通过指针指向结构体变量（struct可以省略）\n\tstruct Student * p = &s;\n\n\t//3、通过指针访问结构体变量中的数据\n\t\n\tcout << \"名字：\" << p->name << \"\\t年龄：\" << p->age << \"\\t分数：\" << p->score << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n### 1.5 结构体嵌套结构体\n作用：结构体中的成员可以是另一结构体\n例如：每个老师辅导一个学员，一个老师的结构体中，记录一个学生的结构体\n\n```cpp\nstruct Student {\n\tstring name;\n\tint age;\n\tint score;\n\n};\nstruct Teacher {\n\tint id;\n\tstring name;\n\tint age;\n\tstruct Student stu;\n};\n\nint main() {\n\tstruct Teacher t;\n\tt.id = 1000;\n\tt.name = \"老王\";\n\tt.age = 50;\n\tt.stu.name = \"小王\";\n\tt.stu.age = 20;\n\tt.stu.score = 60;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n### 1.6 结构体做函数参数\n作用：将结构体作为参数向函数中传递\n\n传递方式有两种：\n- 值传递\n- 地址传递\n\n```cpp\n// 定义学生结构体\nstruct Student {\n\tstring name;\n\tint age;\n\tint score;\n\n};\n\n\n//打印学生信息的函数\n//1、值传递 不会改变实参的值\nvoid printStudent1(struct Student s) {\n\ts.age = 100;\n\tcout << \"值传递函数打印 姓名：\" << s.name << \" 年龄：\" << s.age << \" 分数：\" << s.score << endl;\n}\n\n//2、地址传递 会改变实参的值\nvoid printStudent2(struct Student * p) {\n\tp->age = 200;\n\tcout << \"地址传递函数打印 姓名：\" << p->name << \" 年龄：\" << p->age << \" 分数：\" << p->score << endl;\n}\n\nint main() {\n\t//结构体做函数参数\n\t//将学生传入到一个参数中，打印学生身上的所有信息\n\t\n\t//创建结构体变量\n\tstruct Student s;\n\ts.name = \"张三\";\n\ts.age = 20;\n\ts.score = 85;\n\t\n\t//printStudent1(s);\n\tprintStudent2(&s);\n\n\tcout << \"main函数打印 姓名：\" << s.name << \" 年龄：\" << s.age << \" 分数：\" << s.score << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n### 1.7 结构体中const使用场景\n作用：用const来防止误操作\n\n```cpp\n// const使用场景\n\n// 定义学生结构体\nstruct Student {\n\tstring name;\n\tint age;\n\tint score;\n\n};\n\n// 将函数中的形参改为指针，可以减少内存空间，而且不会复制新的副本出来\n// 传入指针，只占4个字节，比值传递大幅节省空间\nvoid printStudent(const struct Student * s) {\n\t//s->age = 150; 加入const之后，只能读不能写，一旦有修改的操作就会报错，可以防止我们误操作\n\tcout << \"值传递函数打印 姓名：\" << s->name << \" 年龄：\" << s->age << \" 分数：\" << s->score << endl;\n}\n\nint main() {\n\n\t//创建结构体变量\n\tstruct Student s;\n\ts.name = \"张三\";\n\ts.age = 20;\n\ts.score = 85;\n\t\n\tprintStudent(&s);\n\n\tcout << \"main函数打印 姓名：\" << s.name << \" 年龄：\" << s.age << \" 分数：\" << s.score << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n","tags":["c++"],"categories":["c&c++"]},{"title":"黑马C++教程笔记——指针","url":"/2020/11/25/100029/","content":"\n> 本文仅做笔记使用，黑马C++教程从0到1入门编程链接：https://www.bilibili.com/video/BV1et411b73Z\n\n@[TOC]( )\n\n<!-- more -->\n\n\n\n\n### 1.1 指针的基本概念\n\n指针的作用：可以通过指针**间接访问**内存，**指针就是一个地址**\n\n- 内存编号是从0开始记录的，一般用十六进制数字表示\n- **可以利用指针变量保存地址**\n\n### 1.2 指针变量的定义和使用\n指针变量定义语法：`数据类型 * 变量名`\n\n\n实例如下：\n```cpp\n#include<iostream>\nusing namespace std;\n\nint main() {\n\t//1.定义zhizhen\n\tint a = 10;\n\t// 指针定义的语法：数据类型 * 指针变量名\n\tint * p;\n\t// 让指针记录变量a的地址\n\tp = &a;\n\tcout << \"a的地址为：\" << &a << endl;\n\tcout << \"指针p为：\" << p << endl;\n\n\t//2.使用指针\n\t//可以通过解引用的方式来找到指针指向的内存\n\t// 指针前加 * 代表解应用，找到指针指向内存中的数据，然后进行读和写的操作\n\t*p = 1000;\n\tcout << \"a = \" << a << endl;\n\tcout << \"*p = \" << *p << endl;\n\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n示意图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923150324142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n### 1.3 指针所占用内存空间\n\n- 在32位操作系统下，指针是占4个字节空间大小，不管是什么类型\n- 在64位操作系统下，指针是占8个字节空间大小\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923145719788.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n实例：\n\n```cpp\n#include<iostream>\nusing namespace std;\n\nint main() {\n\n\t//指针所占用内存空间\n\tint a = 10;\n\tint * p = &a;\n\n\t// 在32位操作系统下，指针是占4个字节空间大小，不管是什么类型\n\t// 在64位操作系统下，指针是占8个字节空间大小\n\t// sizeof(p)和sizeof(int *)一样\n\tcout << \"sizeof （int *) = \" << sizeof(int *) << endl;\n\tcout << \"sizeof （float *) = \" << sizeof(float *) << endl;\n\tcout << \"sizeof （double *) = \" << sizeof(double *) << endl;\n\tcout << \"sizeof （char *) = \" << sizeof(char *) << endl;\n\tsystem(\"pause\");\n\treturn 0;\n}\n\n```\n\n### 1.4 空指针和野指针\n- **空指针**：指针变量指向内存中编号为0的空间\n- 用途：初始化指针变量\n- 注意：空指针指向的内存是不可以访问的\n\n示例1：空指针\n\n```cpp\nint main() {\n\n\t//空指针\n\t// 1、空指针用于给指针变量进行初始化\n\tint * p = NULL;\n\t// 2、空指针是不可以进行访问的\n\t// 0~255之间的内存编号是系统占用的，因此不可以访问\n\t*p = 100;\n\t// 会报错\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n- **野指针**：指针变量指向非法的内存空间\n\n示例2：野指针\n\n```cpp\nint main() {\n\n\t//野指针\n\t// 在程序中，尽量避免出现野指针\n\n\t// 指针变量p指向内存地址编号为0x1100的空间\n\tint * p = (int *)0x1100;\n\t\n\t// 访问野指针报错\n\tcout << *p << endl;\n\t// 引发了异常: 读取访问权限冲突。因为你没有申请这个地址所指向的内存空间，却又访问它\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n> 总结：空指针和野指针都不是我们申请的空间，因此不要访问，否则会出错。\n\n### 1.5 const修饰指针\nconst修饰指针三种情况：\n1. const修饰指针 ---常量指针\n2. const修饰常量 ---指针常量\n3. const即修饰指针，又修饰常量\n\n示例1：常量指针（红线标注的是错误的方式）\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923153102951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n示例2：指针常量（红线标注的是错误的方式）\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923153331852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n示例3：const即修饰指针，又修饰常量（红线标注的是错误的方式）\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020092316462361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n示例：\n\n```cpp\nint main() {\n\t// 1、const修饰指针\n\tint a = 10;\n\tint b = 10;\n\n\tconst int * p = &a;\n\t// 指针指向的值不可以改，指针的指向可以改\n\t// *p = 20; 错误\n\tp = &b; //正确\n\n\t// 2、const修饰常量 指针常量\n\tint * const p2 = &a;\n\t*p2 = 100; //正确\n\t// p2 = &b; //错误，指针的指向不可以改\n\t// 3、const修饰指针和常量\n\tconst int * const p3 = &a;\n\t// 指针的指向和指针指向的值 都不可以改\n\t// *p3 = 100; //错误\n\t// p3 = &b; //错误\n}\n```\n\n> 技巧：看const右侧紧跟着的是指针还是常量，是指针就是常量指针，是常量就是指针常量\n\n### 1.6 指针和数组\n- 作用：利用指针访问数据数据中元素\n\n示例：\n\n```cpp\nint main() {\n\t//指针和数组\n\t//利用指针访问数组中的元素\n\n\tint arr[10] = { 1,2,3,4,5,6,7,8,9,10 };\n\tcout << \"第一个元素为：\" << arr[0] << endl;\n\tint * p = arr;//arr就是数组首地址\n\tcout << \"利用指针访问第一个元素：\" << *p << endl;\n\t//p++不是把地址加1，是指针偏移一个单位，在32位下是偏移4个字节，在64位下，偏移8个字节\n\tp++;\n\tcout << \"利用指针访问第二个元素：\" << *p << endl;\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n### 1.7 指针和函数\n作用：利用指针作函数参数，可以修改实参的值\n\n\n\n\n\n```cpp\n#include<iostream>\nusing namespace std;\n\n\nvoid swap01(int a, int b) {\n\tint temp = a;\n\ta = b;\n\tb = temp;\n}\n\nvoid swap02(int *p1, int *p2) {\n\tint temp = *p1;\n\t*p1 = *p2;\n\t*p2 = temp;\n}\n\nint main() {\n\n\t// 指针和函数\n\t//1、值传递 不会改变实参的值\n\tint a = 10;\n\tint b = 20;\n\tswap01(a, b);\n\tcout << \"值传递：\\n\";\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\n\t//2、地址传递 会改变实参的值\n\tcout << \"地址传递：\\n\";\n\tswap02(&a, &b);\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n\n```\n\n示例图如下\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923160126957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n### 1.8 指针、数组、函数组合案例\n案例描述：封装一个函数，利用冒泡排序，实现对整型数组的升序排序\n\n例如数组：int arr[10] = {4,3,6,8,1,2,10,8,7,5}\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923160823956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n","tags":["c++"],"categories":["c&c++"]},{"title":"机器学习概述","url":"/2020/11/25/095842/","content":"\n\n在这里将学到：\n- 什么是机器学习\n- 为什么需要机器学习\n- 机器学习中的基本概念：包括**样本、特征、标签、模型、学习算法**\n- 机器学习的三要素：模型、评价准则、优化算法\n- 训练集、测试集、样本集的概念\n\n<!-- more -->\n\n\n\n\n\n\n## 什么是机器学习\n\n通俗地讲， 机器学习 （Machine Learning， ML）让计算机从数据中进行自动学习，得到某种知识，而不是人为指定且明显的去编程。以手写体数字识别为例，我们需要让计算机能自动识别手写的数字，由于每个人的写法都不相同，我们很难总结每个数字的手写体特征，因此设计一套识别算法几乎是一项几乎不可能的任务。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802172618567.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70 =500x300)\n在现实生活中，很多问题都类似于手写体数字识别这类问题，比如物体识别、语音识别等。对于这类问题，我们不知道如何设计一个计算机程序来解决。因此，**人们开始尝试采用另一种思路，即让计算机“看”大量的样本，并从中学习到一些经验，然后用这些经验来识别新的样本**。要识别手写体数字，首先通过人工标注大量的手写体数字图像（即每张图像都通过人工标记了它是什么数字），这些图像作为训练数据，然后通过学习算法自动生成**模型**，并依靠它来识别新的手写体数字。这和人类学习过程也比较类似，我们教小孩子识别数字也是这样的过程。这种通过数据来学习的方法就称为**机器学习**的方法。\n\n机器学习自动生成的**模型**也称为**决策函数$f$**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802190614894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70 =500x300)\n\n\n\n## 机器学习的基本概念\n\n机器学习中的一些基本概念：包括**样本、特征、标签、模型、学习算法**等。以一个生活中的经验学习为例，假设我们要到市场上购买芒果，但是之前毫无挑选芒果的经验，那么我们如何通过学习来获取这些知识？\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802174401531.gif)\n\n\n首先，我们从市场上随机选取一些芒果，列出每个芒果的**特征 (Feature)**，包括颜色，大小，形状，产地，品牌，以及我们需要预测的**标签 (Label)**。标签可以是**连续值**（比如关于芒果的甜度、水分以及成熟度的综合打分），也可以是**离散值**（比如好、坏两类标签）。\n\n一个标记好特征以及标签的芒果可以看作是一个**样本 (Sample)**。一组样本构成的集合称为**数据集 (Data Set)**。一般将数据集分为两部分：训练集和测试集。 **训练集 (Training Set)**中的样本是用来训练模型的，也叫**训练样本**(Training Sample)，而**测试集 (Test Set)**中的样本是用来检验模型好坏的，也叫**测试样本**(Test Sample)。\n\n> 特征也可以称为属性（Attribute），样本（Sample），也叫示例（Instance）。\n\n我们用一个 d 维向量 $X = [x_1, x_2, · · · , x_d]^T$ 表示一个芒果的所有特征构成的向量，称为**特征向量** （Feature Vector），其中每一维表示一个特征。\n\n假设训练集由 N 个样本组成，其中每个样本都是独立同分布 （Identically and Independently Distributed， IID）的，即独立地从相同的数据分布中抽取的，记为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802191715788.png)\n其中，${X^{ { {\\left( 1 \\right)}^{ } } } }$表示第一个样本的特征向量，${y^{ { {\\left( 1 \\right)}^{ } } } }$第一个样本的标签\n\n给定训练集 $D$，我们希望让计算机自动寻找一个**函数**$f (X; θ)$ 来建立每个样本特性向量 X 和标签 y 之间的映射。对于一个样本 $X$，我们可以通过决策函数来预测其标签的值\n$$\\hat y=f (X; θ)$$\n或标签的条件概率\n$$p(y|X)=f_y(x;θ)$$\n其中 $θ$ 为可学习的参数\n\n通过一个**学习算法** (Learning Algorithm) ${\\rm A}$，在训练集上找到一组参数 $θ^∗$，使得函数 $f (X; θ^*)$ 可以近似真实的**映射关系**。这个过程称为学习 （Learning）或训练 （Training）过程，函数 $f (X; θ)$ 称为**模型** （Model）。\n\n> 在有些文献中，学习算法也叫做学习器 （Learner）。\n\n下次从市场上买芒果（测试样本）时，可以根据芒果的特征，使用学习到的模型 $f (X; θ^*)$ 来预测芒果的好坏。为了评价的公正性，我们还是独立同分布地抽取一组样本作为测试集 $D^′$，并在测试集中所有样本上进行测试，计算预测结果的准确率。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802193511664.png)\n其中 $I (·)$ 为指示函数， $|D^′|$ 为测试集大小。(对于分类，一般是求出总测试集样本的个数和预测正确的个数)\n\n下图给出了机器学习的基本概念。对一个预测任务，输入特征向量为 $X$，输出标签为 $y$，我们选择一个函数 $f (X; θ)$，通过学习算法 ${\\rm A}$ 和一组训练样本 $D$，找到一组最优的参数 $θ^∗$，得到最终的模型 $f (X; θ^*)$。这样就可以对新的输入 $X$ 进行预测。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802194158846.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n## 机器学习的三要素\n机器学习方法可以分为三个基本要素：模型、学习准则、优化算法。\n\n### 模型\n> 输入空间默认为样本的特征空间\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802200142159.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n> $R$为实数，$m$为参数的数量，表示有m个实数要学习\n\n**线性模型：**\n\n**线性模型**的假设空间为一个参数化的**线性函数族**，\n$$f (X; θ)=W^T*X+b$$\n\n其中，$X$是特征向量，$W$是权重向量，$b$是偏置\n\n**非线性模型：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802200931903.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n比如，卷积神经网络中的卷积运算本身就是一个可以学习**非线性基函数**\n\n\n## 学习准则\n学习准则就是找到可以评价学习成果好坏的准则，如果用函数表示，则成为**损失函数**\n\n**损失函数**是一个非负实数函数，，用$L(y, f (X; θ))$ 来表示，用来量化模型预测和真实标签之间的差异。\n\n常见回归损失函数有：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802202112854.png)\n常见分类损失函数有：**交叉熵损失函数 （Cross-Entropy Loss Function）**\n\n## 优化算法\n梯度下降\n\n## 关于验证集的一点补充\n\n将数据分为训练集和测试集对于简单的模型和样本来说就够了，但是对于复杂的模型（如CNN）和样本（如图片），就需要将数据集分类三部分：训练集、验证集、测试集\n\n- 训练集（Training Set）：用来训练模型的\n- 验证集（Validation set）：用于对模型的能力进行初步评估，可以作为调参、选择特征等算法相关的选择的依据。\n- 测试集 （Test Set）：用来评估模最终模型的泛化能力，不能作为调参、选择特征等算法相关的选择的依据\n\n就好比考试一样，我们平时做的题相当于训练集，测试集相当于最终的考试，我们通过最终的考试来检验我们最终的学习能力，将测试集信息泄露出去，相当于学生提前知道了考试题目，那最后再考这些提前知道的考试题目，当然代表不了什么，你在最后的考试中得再高的分数，也不能代表你学习能力强。所以说，如果通过**测试集**来调节模型，相当于不仅知道了考试的题目，学生还都学会怎么做这些题了（因为我们肯定会人为的让模型在测试集上的误差最小，因为这是你调整超参数的目的），那再拿这些题考试的话，人人都有可能考满分，但是并没有起到检测学生学习能力的作用。原来我们通过测试集来近似泛化误差，也就是通过考试来检验学生的学习能力，但是由于**信息泄露**，此时的测试集即考试无任何意义，现实中可能学生的能力很差。所以，我们在学习的时候，老师会准备一些小测试来帮助我们查缺补漏，**这些小测试也就是要说的验证集**。我们通过验证集来作为调整模型的依据，这样不至于将测试集中的信息泄露。\n\n- 训练集-----------学生的课本；学生 根据课本里的内容来掌握知识。\n- 验证集------------作业，通过作业可以知道 不同学生学习情况、进步的速度快慢。\n- 测试集-----------考试，考的题是平常都没有见过，考察学生举一反三的能力。\n\n也就是说我们将数据划分训练集、验证集和测试集。在训练集上训练模型，在验证集上评估模型，一旦找到的最佳的参数，就在测试集上最后测试一次，测试集上的误差作为泛化误差的近似。关于验证集的划分可以参考测试集的划分，其实都是一样的，这里不再赘述。\n\n总结\n\n- 机器学习让计算机从数据中进行自动学习，得到某种知识，而不是人为指定且明显的去编程\n- 之所以机器学习，是因为现实世界的问题都比较复杂，很难通过规则来手工实现\n- 机器学习中的基本概念：包括**样本、特征、标签、模型、学习算法**\n- 机器学习的三要素：模型、评价准则、优化算法\n- 训练集、测试集、样本集的概念\n\n> 声明：本文大部分摘录自神邱老师的《神经网络与深度学习》，加上一小部分个人的理解和其他博客的资料，因此将本文声明为**转载**，本文只做学习和交流使用，如有侵权请联系博主删除。\n\n参考文档\n【神经网络与深度学习-邱锡鹏著】\n[训练集、验证集、测试集以及交验验证的理解](https://blog.csdn.net/kieven2008/article/details/81582591)\n[训练集、验证集和测试集](https://zhuanlan.zhihu.com/p/48976706)\n\n\n\n\n","categories":["机器学习"]},{"title":"hexo复杂latex公式无法显示并报错的问题","url":"/2020/11/24/231533/","content":"\nhexo复杂latex公式无法显示并报错的问题\n\n<!-- more -->\n\n## 问题描述\n\n在写hexo博客的时候，遇到markdown中公式太长太复杂的时候，就老是显示出错，部分错误信息如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124231718.png)\n\n\n\n我报错的地方如下:\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124231907.png)\n\n\n\n## 解决方法\n\n这是一个hexo中的latex公式渲染问题。不能在公式内连续使用两个`左花括号`！，把两个左花括号中间加`空格`分开就行了，即改为：`$\\frac{ {\\partial L} }{ {\\partial W} }$`\n\n你就会看到成功的显示公式啦！！！\n\n另外，用这种方法也可以：[Nunjucks Error: 解决方案](https://blog.csdn.net/weixin_45333934/article/details/108274320)，该方法好像执行`hexo g`速度快一点，可能是心理作用，哈哈！\n\n\n\n参考文档\n\n[hexo复杂latex公式无法显示问题](https://blog.csdn.net/ALexander_Monster/article/details/106178094)","categories":["工具"]},{"title":"Google Colab的一些常用命令","url":"/2020/11/24/224504/","content":"\n在此记录一些我遇到Google Colab常用命令，以便以后查阅！该命令colab在线地址为：[colab的一些常用命令](https://drive.google.com/file/d/1JRO9afK-2PQwvudxaDLqcuawZRxZWT-c/view?usp=sharing)\n\n\n<!-- more -->\n\n\n\n\n\n# 基本操作\n（1）colab挂载drive上的数据文件\n推荐参考我的博客[Google Colab挂载drive上的数据文件](https://blog.csdn.net/qq_37555071/article/details/107544680)，这里不就过多赘述了。\n\n（2）查看当前所在路径\n```bash\n!pwd\n```\n\n（3）切换目录\n```bash\n# 后面为要切换的路径，支持相对、绝对路径\n%cd /content/drive/Colab/   \n```\n\n（4）查看当前目录的所有文件名称\n```bash\n!ls  也可以 ls\n```\n\n（5）拷贝文件\n```bash\n# 前面是要拷贝的文件名，后面是拷贝后的文件目录\n!cp -i /content/drive/cat_dog_result.csv /content/\n```\n\n（6）创建文件或文件夹\n```bash\n# 创建dirabc文件夹\nmkdir dirabc\n# 创建test1、test2、test3文件\ntouch test1.txt test2.txt test3.txt\n```\n\n（7）删除文件\n```bash\n#  删除文件夹或文件，后面跟文件夹或文件名称\n!rm -rf test3.txt\n# 也可以删除多个文件\n!rm -rf test1.txt test2.txt test3.txt\n# 删除除了drive的所有文件\nls | grep -v drive | xargs rm -rf\n```\n\n# 解压缩操作\n（1）解压rar文件\n```bash\n! apt-get install rar\n!apt-get install unrar\n# x参数是保存原来的文件架构，e参数是把里面的文件都解压到当前路径下\n# 注意压缩文件时要右键，添加到xxx.rar，不要添加到压缩文件\n! unrar x cat_dog.rar\n```\n\n（2）压缩rar文件\n```bash\n# !rar 压缩后的文件名 要压缩的文件名或文件夹名\n!rar a 123.rar  wxl.jks\n```\n\n（3）解压zip文件\n```bash\n!unzip FileName.zip \n```\n\n（4）压缩zip文件\n```bash\n# !zip 压缩后的文件名 要压缩的文件名或文件夹名\n!zip FileName.zip DirName \n```\n更多解压缩方式可参考：[Unrar, Unzip in colab](https://colab.research.google.com/drive/17Jtj0Mrs0lgWo4zi8jQoiAKunjR0GzwQ?usp=sharing)\n\n\n# 阻止Colab自动掉线\n在colab上训练代码，页面隔一段时间无操作之后就会自动掉线，之前训练的数据都会丢失。不过好在最后终于找到了一种可以让其自动保持不离线的方法，用一个js程序自动点击连接按钮。代码如下：\n\n```js\nfunction ClickConnect(){\n  console.log(\"Working\"); \n  document\n    .querySelector(\"#top-toolbar > colab-connect-button\")\n    .shadowRoot\n    .querySelector(\"#connect\")\n    .click()\n}\n \nsetInterval(ClickConnect,60000)\n```\n\n使用方式是：按快捷键`ctrl+shift+i`，并选择`Console`，然后复制粘贴上面的代码，并点击回车，该程序便可以运行了，如下所示：\n\n![](https://img-blog.csdnimg.cn/img_convert/a812f4f6edd1c69db0f38e73e2ac11d1.png)\n\n\n参考文档\n[linux下解压命令大全](https://blog.csdn.net/xsfqh/article/details/89448976)","tags":["colab"],"categories":["工具"]},{"title":"为什么MobileNet及其变体（如ShuffleNet）会变快？","url":"/2020/11/24/224323/","content":"\n> 本文是转载文章，转载自[深入剖析：为什么MobileNet及其变体（如ShuffleNet）会变快？](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247506133&idx=2&sn=1082d154907cedb3ebea028df3787d8e&chksm=ec1c352cdb6bbc3af142fe776b2c9ac73d831d946eda8cc45c6abf9cdd6c4abaef82eeb0c250&mpshare=1&scene=1&srcid=0828Av9UvPz9AbUBnyi3wnAc&sharer_sharetime=1599125590114&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=c45d238be947117fcf9d09c202668aad75e244db66c43fc3a6c9ec8bcc9fc50c2cea193203b229ce4e107aea09fe33bfdf0d8528a563618300689838ca76ab39f9ee3393d565a0a091b9713ef47b7273dd8c99b377abc7d00d8cbc662ebffd68d8fafa2b606af324cd1df8fed9a07a6d523be8a08c4323a195123680a9aa8a75&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AcmLpoaSOHY8/ugr1Vi8OdE=&pass_ticket=3cYbF3kWoNM5K54gjR4tlKnJ8nzIvZSHzY/x9JiBuxWbw3Pu9NJ1BcsIB%2byab7wA&wx_header=0)，**删除了文中冗余的部分，加入许多自己的理解，有些部分也通过pytorch进行了实现，并通过引入具体的计算更清晰的反映出轻量级神经网络的本质**。\n\n\n<!-- more -->\n\n\n\n\n\n\n## 前言\n\n从MobileNet等CNN模型的组成部分出发，概述了**高效CNN模型**（如MobileNet及其变体）中**使用的组成部分**（building blocks），并解释了**它们如此高效的原因**。特别地，我提供了关于如何在**空间和通道域**进行卷积的直观说明。\n\n## 高效CNN模型的组成部分\n在解释具体的高效CNN模型之前，我们先检查一下高效CNN模型中使用的组成部分的计算量，看看卷积在空间和通道域中是如何进行的。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903215104929.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n假设 H x W 为输出feature map的空间大小，N为输入通道数，K x K为卷积核的大小，M为输出通道数，则标准卷积的计算量为 H\\*W\\*N\\*K\\*K\\*M 。这里重要的一点是，**标准卷积的计算量与(1)输出特征图H x W的空间大小，(2)卷积核K x K的大小，(3)输入输出通道的数量N x M成正比**。当在空间域和通道域进行卷积时，需要上述计算量。通过分解这个卷积，可以加速 CNNs，如上图所示。\n\n## 卷积\n首先，我提供了一个直观的解释，关于空间和通道域的卷积是如何对进行标准卷积的，它的计算量是H\\*W\\*N\\*K\\*K\\*M。\n\n这里连接输入和输出之间的线，以可视化输入和输出之间的依赖关系。直线数量大致表示空间（spatial）和通道（channel）域中卷积的计算量。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903215447602.png#pic_center)\n例如，最常用的卷积——conv3x3，可以如上图所示。我们可以看到，输入和输出在**空间域是局部连接的，而在通道域是全连接的**。你可能看的不太明白，那我们换张图试试，下面是空间域中输入和输出的关系，可以看出空间域确实是局部连接的。**空间域可以想象为，把特征图输入和输出的神经元拉平，然后进行连接，事实上，在计算机内部就是这么做的。**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904092416818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n下面是通道域中输入和输出的关系（隐藏红线框部分），下图输入通道是3，输出通道是1，输入的3个通道都连接在了输出的1个通道上，这也证明了通道域是全连接的。如果输出通道有多个，也能想象出来吧。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731150746946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n接下来，如上所示用于改变通道数的conv1x1，或pointwise convolution。由于kernel的大小是1x1，所以这个卷积的计算量是 H\\*W\\*N\\*M，（H x W 为输出feature map的空间大小，N为输入通道数，M为输出通道数），计算量比conv3x3降低了1/9。**这种卷积被用来“混合”通道之间的信息**。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903220728682.png#pic_center)\n## 分组卷积（Grouped Convolution）\n分组卷积是卷积的一种变体，将输入的feature map的**通道分组**，对每个分组的通道独立地进行卷积。\n\n假设 G 表示组数，分组卷积的计算量为 H\\*W\\*N\\*K\\*K\\*M/G，计算量变成标准卷积的1/G。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903221031750.png#pic_center)\n举个例子，假如，卷积核大小为3x3，输入通道数为10，输出通道数为20，输出特征图大小为15x15，对于不分组情况下，计算量是`15*15*10*3*3*20=405000`，如果把输入的feature map的通道分为2组，即输入通道数为10分为2组，每组的输入通道数为5，每组输出通道数为10，输出的总通道数为20，则计算量为`15*15*5*3*3*10*2=202500`，可以看到计算量变成标准卷积的1/2。\n\n在conv3x3 而且 G=2的情况。我们可以看到，通道域中的连接数比标准卷积要小，说明计算量更小。\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903222640830.png#pic_center)\n在 conv3x3，G=3的情况下，**连接变得更加稀疏**。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090322265421.png#pic_center)\n在 conv1x1，G=2的情况下，conv1x1也可以被分组。**这种类型的卷积被用于ShuffleNet中**。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903222726588.png#pic_center)\n## 深度卷积（Depthwise Convolution）\n在**深度卷积**中，对每个输入通道分别进行卷积。**它也可以定义为分组卷积的一种特殊情况，其中输入和输出通道数相同，G等于通道数**。不得不提的是，**Depthwise Convolution在通道域上把计算量降低到了极致**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904093441110.png#pic_center)\n如上所示，depthwise convolution 通过省略通道域中的卷积，大大降低了计算量。\n\n## Channel Shuffle\nChannel shuffle是一种操作(层)，它改变 ShuffleNet 中**使用的通道的顺序**。这个操作是通过张量reshape和 transpose 来实现的。\n\n\n假设G表示分组卷积的组数，N表示输入通道的数量，首先将输入通道的维数reshape 为(G, N ')，即G\\*N '=N，然后将(G, N ')转置为(N '， G)，最后将其view成与输入相同的形状。pytorch实现如下：\n\n```python\nimport torch\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n \n    channels_per_group = num_channels // groups\n \n    # reshape\n    x = x.view(batchsize, groups,\n               channels_per_group, height, width)\n \n    # transpose\n    # - contiguous() required if transpose() is used before view().\n    #   See https://github.com/pytorch/pytorch/issues/764\n    x = torch.transpose(x, 1, 2).contiguous()\n \n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n\nx = torch.randn(1,10,224,224)\nx = channel_shuffle(x,2)\nx.size()\n# 输出：torch.Size([1, 10, 224, 224])\n```\n\n虽然Channel Shuffle的操作不能像计算卷积那样来定义计算量，但应该有一些开销。\n\nG=2时的channel shuffle 情况如下，这里无卷积操作，只是改变了通道的顺序。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904111231327.png#pic_center)\n打乱的通道数 G=3，情况如下\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904111255450.png#pic_center)\n\n\n\n\n## 高效的CNN模型\n\n下面，对于高效的CNN模型，我将直观地说明为什么它们是高效的，以及如何在空间和通道域进行卷积。\n\n### ResNet (Bottleneck Version)\nResNet 中使用的**带有bottleneck 架构的残差单元**是与其他模型进行进一步比较的良好起点。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904095643782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n如上所示，具有bottleneck架构的残差单元由conv1x1、conv3x3、conv1x1组成。第一个conv1x1减小了输入通道的维数，降低了随后的conv3x3的计算量。最后的conv1x1恢复输出通道的维数。\n\n### ResNeXt\nResNeXt是一个高效的CNN模型，可以看作是ResNet的一个特例，将conv3x3替换为**成组**的conv3x3。**通过使用有效的分组conv**，与ResNet相比，conv1x1的通道减少率变得适中（可以让conv1x1不用降维那么多，因为用分组conv已经降低了很多计算量了），从而在相同的计算代价下获得更好的精度。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904095826862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n### MobileNet (Depthwise Separable Conv)\nMobileNet是一个**深度可分离卷积**模块的堆叠，由depthwise conv和conv1x1 (pointwise conv)组成。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090410015854.png#pic_center)\n**深度可分离卷积分别在空间域和通道域独立执行卷积**。这种卷积分解显著降低了计算量，从**H\\*W\\*N\\*K\\*K\\*M** 降低到 **H\\*W\\*N\\*K\\*K**(depthwise) + **H\\*W\\*N\\*M**(conv1x1)=**HWN(K² + M)**。一般情况下，输出通道M远远大于卷积核大小K(如K=3和M≥32)，减小率约为1/8-1/9。\n\n这里你可能对于上述计算比较懵，那我们分解一下吧。\n\n- 假设 H x W 为输出feature map的空间大小，N为输入通道数，K x K为卷积核的大小，M为输出通道数，则标准卷积的计算量为 H\\*W\\*N\\*K\\*K\\*M。\n- depthwise conv可以看做分组卷积的一种特殊情况，其中输入和输出通道数相同，分组数G等于通道数，其计算量为H\\*W\\*N\\*K\\*K\\*N/N=H\\*W\\*N\\*K\\*K。\n- conv1x1的输入通道数是N，输出通道数是M，计算量为H\\*W\\*N\\*M。\n\n### ShuffleNet\nShuffleNet的动机是如上所述，**conv1x1是在空间域上已经把计算量降低到了极致**，所以在空间域上已经没有改进的空间，而**分组conv1x1可以在通道域上再次降低计算量**。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904102338403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n上图说明了用于ShuffleNet的模块。这里重要的使用的组成部分（building blocks）是channel shuffle层，它在分组卷积中对通多在组间的顺序进行“shuffles”。**如果没有channel shuffle，分组卷积的输出在组之间就不会被利用，导致精度下降**。\n\n### MobileNet-v2\nMobileNet-v2采用类似ResNet中带有bottleneck架构残差单元的模块架构；并用深度卷积（depthwise convolution）代替conv3x3，是残差单元的改进版本。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904112511274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n从上面可以看到，与标准的 bottleneck 架构相反，第一个conv1x1增加了通道维度，然后执行depthwise conv，最后一个conv1x1减少了通道维度。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904112551109.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n通过如上所述对组成部分（building blocks） 进行重新排序，将其与MobileNet-v1(Depthwise Separable Conv)进行比较，我们可以看到这个体系结构（MobileNet-v2）是如何工作的 (这种重新排序不会改变整个模型体系结构，因为MobileNet-v2是这个模块的**堆叠**，所以不会有影响的)。\n\n也就是说，上述模块可以看作是深度可分离卷积的一个改进版本，其中可分离卷积中的单个conv1x1被分解为两个conv1x1。让T表示通道维数的**扩展因子**，两个 conv1x1 的计算量为 2\\*H\\*W\\*N\\*N/T ，而深度可分离卷积下的conv1x1的计算量为 HWN²。如果T = 6，将 conv1x1 的计算成本降低了3倍(一般为T/2)。\n\n这里可能有人看的不太理解，我们来详细算一下吧（以上图为例子）。\n\n上图中第一个depthwise conv，第二个和第三个为conv1x1，假设第二个Conv1x1的输入通道为N，则第二个Conv1x1输出通道就为N/T，因为第二个Conv1x1是**经过扩展因子扩大了T倍**，则第三个Conv1x1的输入通道是N/T，则第三个Conv1x1的输出通道是N。\n\n这样就可以算出，第二个Conv1x1的计算量为`H*W*N*N/T`，第三个Conv1x1的计算量为`H*W*(N/T)*N`，两个Conv的总计算量为**2\\*H\\*W\\*N\\*N/T**，而深度可分离卷积下的conv1x1的计算量为 HWN²，如果T = 6，成本计算成本就是降低了3蓓。\n\n可能有人要问，PW升维不是增加参数量了么，你这么一算咋减小了？是的，用PW**升维**是增加了一部分参数量，不过正因为是1x1Conv，所以增加的参数量并不多。上面，我们对组成部分（building blocks） 进行重新排序并进行了计算，在**高维度下**两个conv1x1比**低维度下**深度可分离卷积下的conv1x1参数可降低了不少呢！\n\n### FD-MobileNet\n\n最后，介绍 Fast-Downsampling MobileNet (FD-MobileNet)。在这个模型中，与MobileNet相比，下采样在较早的层中执行。这个简单的技巧可以降低总的计算成本。\n\n从VGGNet开始，许多模型采用相同的下采样策略：**执行向下采样，然后将后续层的通道数增加一倍**。对于标准卷积，下采样后计算量不变，因为根据定义得 H\\*W\\*N\\*K\\*K\\*M 。而对于深度可分离卷积，下采样后其计算量减小：由 **HWN(K² + M)** 降为 **(H/2)\\*(W/2)\\* (2N)\\*(K² + 2M)** = HWN(K²/2 + M)。当M不是很大时(即较早的层)，这是相对占优势的。注意：这里的2N和2M是因为执行向下采样后，后续层的通道数了增加一倍。\n\n\n**下面是对全文的总结：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200905112522639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n","tags":["MobileNet"],"categories":["神经网络"]},{"title":"DenseNet密集卷积网络详解（附代码实现）","url":"/2020/11/24/224154/","content":"\n## 前言\nDenseNet是CVPR2017的最佳论文，由康奈尔大学黄高博士（Gao Huang）、清华大学本科生刘壮（Zhuang Liu）、Facebook 人工智能研究院研究科学家 Laurens van der Maaten 及康奈尔大学计算机系教授 Kilian Q. Weinberger 所作，有兴趣的同学可以结合[原文](https://arxiv.org/pdf/1608.06993.pdf)阅读。\n\n<!-- more -->\n\n\n\n\nResNet通过前层与后层的“**短路连接**”（Shortcuts），加强了前后层之间的信息流通，在一定程度上缓解了梯度消失现象，从而可以将神经网络搭建得很深，具体可以参考[ResNet残差网络及变体详解](https://blog.csdn.net/qq_37555071/article/details/108258862)。更进一步，这次的主角DenseNet最大化了这种前后层信息交流，**通过建立前面所有层与后面层的密集连接，实现了特征在通道维度上的复用，不但减缓了梯度消失的现象，也使其可以在参数与计算量更少的情况下实现比ResNet更优的性能**。连接方式可以看下面这张图：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903105033190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n标准的 L 层卷积网络有 $L$ 个连接，即每一层与它的前一层和后一层相连，而DenseNet将前面所有层与后面层连接，故有 $(1+2+...+L)*L=(L+1)*L/2$ 个连接。这里看完有些摸不着头脑没关系，接下来我们会具体展开。\n\n## Dense Block\nDense Block是DenseNet的一个基本模块，这里我们从一般的神经网络说起：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090311000558.png#pic_center)\n上图是标准神经网络的一个图，输入和输出的公式是$X_l = H_l(X_{l-1})$，其中$H_l$是一个组合函数，通常包括BN、ReLU、Pooling、Conv操作，$X_{l-1}$是第 $l$ 层输入的特征图，$X_{l}$是第 $l$ 层输出的特征图。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903110339759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n上图则是ResNet的示意图，我们知道ResNet是跨层相加，输入和输出的公式是$X_l = H_l(X_{l-1})+X_{l-1}$\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903110523502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n而对于DesNet，则是采用跨通道concat的形式来连接，用公式来说则是$X_l = H_l(X_0,X_1,...,X_{l-1}$)，这里要注意所有的层的输入都来源于前面所有层在channel维度的concat，我们用一张动图体会一下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090311071451.gif#pic_center)\n特征传递方式是**直接将前面所有层的特征concat后传到下一层，而不是前面层都要有一个箭头指向后面的所有层**，这与具体代码实现是一致的，后面会具体的实现。\n\n这里要注意，**因为我们是直接跨通道直接做concat，所以这里要求不同层concat之前他们的特征图大小应当是相同的**，所以DenseNet分为了好几个Dense Block，**每个Dense Block内部的feature map的大小相同**，而每个Dense Block之间使用一个Transition模块来进行下采样过渡连接，这个后文会介绍。\n\n## Growth rate\n假如输入特征图的channel为$K_0$，那么第 $l$ 层的channel数就为 $K_0+(l-1)K$，我们将其称之为网络的增长率（growth rate）。因为每一层都接受前面所有层的特征图，即特征传递方式是**直接将前面所有层的特征concat后传到下一层**，所以这个$K$不能很大，要注意这个K的实际含义就是这层新提取出的特征。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903111854807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n## Bottleneck\n\n在刚才Dense Block中的非线性组合函数是指**BN+ReLU+3x3 Conv**的组合，尽管每前进一层，只产生K张新特征图，但还是嫌多，于是**在进行3×3卷积之前先用一个 1×1卷积将输入的特征图个数降低到 4\\*k**，我们发现这个设计对于DenseNet来说特别有效。所以我们的非线性组合函数就变成了**BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv**的结构，由此形成的网络结构我们称之为**DenseNet-B**。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903164547925.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n**增加了1x1的卷积的Dense Block也称为Bottleneck结构**，实现细节如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903114842520.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n有以下几个细节需要注意：\n\n1. 每一个Bottleneck输出的特征通道数是相同的，例如这里的K=32。同时可以看到，经过concat操作后的通道数是按K的增长量增加的，因此这个K也被称为GrowthRate。\n2. 这里1×1卷积的作用是**固定输出通道数，达到降维的作用**，1×1卷积输出的通道数通常是GrowthRate的4倍。当几十个Bottleneck相连接时，concat后的通道数会增加到上千，如果不增加1×1的卷积来降维，后续3×3卷积所需的参数量会急剧增加。比如，输入通道数64，增长率K=32，经过15个Bottleneck，通道数输出为`64+15*32=544`，再经过第16个Bottleneck时，如果不使用1×1卷积，第16个Bottleneck层参数量是`3*3*544*32=156672`，如果使用1×1卷积，第16个Bottleneck层参数量是`1*1*544*128+3*3*128*32=106496`，可以看到参数量大大降低。\n3. Dense Block采用了激活函数在前、卷积层在后的顺序，即BN-ReLU-Conv的顺序，这种方式也被称为**pre-activation**。通常的模型relu等激活函数处于卷积conv、批归一化batchnorm之后，即Conv-BN-ReLU，也被称为post-activation。作者证明，如果采用post-activation设计，性能会变差。想要更清晰的了解pre-activition，可以参考我的博客[ResNet残差网络及变体详解](https://blog.csdn.net/qq_37555071/article/details/108258862)中的Pre Activation ResNet。\n\n\n\n\n\n\n\n## Transition layer\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903113233261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n两个相邻的Dense Block之间的部分被称为**Transition层**，具体包括BN、ReLU、1×1卷积、2×2平均池化操作。**通过1×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。**\n\n## 压缩因子\n为进一步提高网络的紧密度，我们可以在转换层(transition layers)减少feature-maps的数量。我们引入一个压缩因子$\\theta$，假定上一层得到的feature map的channel大小为$m$，那经过Transition层就可以产生 $\\theta m$ 个特征，其中$\\theta$在0和1之间。在**DenseNet-C**中，我们令$\\theta$=0.5。当模型结构即含瓶颈层，又含压缩层时，我们记模型为DenseNet-BC。\n\n## DenseNet网络结构\nDenseNet网络构成如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903151310711.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n上图中，增长率K=32，采用pre-activation，即BN-ReLU-Conv的顺序。\n\n以DenseNet-121为例，看下其网络构成：\n1. DenseNet-121由121层权重层组成，其中4个Dense block，共计2×(6+12+24+16) = 116层权重，加上初始输入的1卷积层+3过渡层+最后输出的全连接层，共计121层；\n2. 训练时采用了DenseNet-BC结构，压缩因子0.5，增长率k = 32；\n3. 初始卷积层有2k个通道数，经过7×7卷积将224×224的输入图片缩减至112×112；Denseblock块由layer堆叠而成，layer的尺寸都相同：1×1+3×3的两层conv（每层conv = BN+ReLU+Conv）；Denseblock间由过渡层构成，过渡层通过1×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽。最后经过全局平均池化 + 全连接层的1000路softmax得到输出。\n\n\n## DenseNet优缺点\nDenseNet的优点主要有3个：\n\n1. **更强的梯度流动**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903151844516.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\nDenseNet可以说是一种隐式的**强监督模式**，因为每一层都建立起了与前面层的连接，误差信号可以很容易地传播到较早的层，所以较早的层可以从最终分类层获得直接监管（监督）。\n2. **能够减少参数总量**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903152017839.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n3.**保存了低维度的特征**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903152107501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n在标准的卷积网络中，最终输出只会利用提取最高层次的特征。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903152120395.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n而**在DenseNet中，它使用了不同层次的特征，倾向于给出更平滑的决策边界**。这也解释了为什么训练数据不足时DenseNet表现依旧良好。\n\n\n\n**DenseNet的不足在于由于需要进行多次Concatnate操作，数据需要被复制多次，显存容易增加得很快，需要一定的显存优化技术。另外，DenseNet是一种更为特殊的网络，ResNet则相对一般化一些，因此ResNet的应用范围更广泛。**\n\n\n## 实验效果\n这里给出DenseNet在CIFAR-100和ImageNet数据集上与ResNet的对比结果，首先来看下**DenseNet与ResNet在CIFAR-100数据集上实验结果**，如下图所示，可以看出，只有0.8M大小的DenseNet-100性能已经超越ResNet-1001，并且后者参数大小为10.2M。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903181215171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n下面是**DenseNet与ResNet在ImageNet数据集上的比较**，可以看出，同等参数大小时，DenseNet也优于ResNet网络。其它实验结果见原论文。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090318171124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n## Pytorch实现DenseNet\n首先实现DenseBlock中的内部结构，这里是BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv结构，最后也加入dropout层以用于训练过程。\n\n```python\nclass _DenseLayer(nn.Sequential):\n    \"\"\"Basic unit of DenseBlock (using bottleneck layer) \"\"\"\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\"norm1\", nn.BatchNorm2d(num_input_features))\n        self.add_module(\"relu1\", nn.ReLU(inplace=True))\n        self.add_module(\"conv1\", nn.Conv2d(num_input_features, bn_size*growth_rate,\n                                           kernel_size=1, stride=1, bias=False))\n        self.add_module(\"norm2\", nn.BatchNorm2d(bn_size*growth_rate))\n        self.add_module(\"relu2\", nn.ReLU(inplace=True))\n        self.add_module(\"conv2\", nn.Conv2d(bn_size*growth_rate, growth_rate,\n                                           kernel_size=3, stride=1, padding=1, bias=False))\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate)\n        # 在通道维上将输入和输出连结\n        return torch.cat([x, new_features], 1)\n```\n据此，实现DenseBlock模块，内部是密集连接方式（输入特征数线性增长）：\n\n```python\nclass _DenseBlock(nn.Sequential):\n    \"\"\"DenseBlock\"\"\"\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features+i*growth_rate, growth_rate, bn_size,\n                                drop_rate)\n            self.add_module(\"denselayer%d\" % (i+1), layer)\n```\n此外，实现Transition层，它主要是一个卷积层和一个池化层：\n\n```python\nclass _Transition(nn.Sequential):\n    \"\"\"Transition layer between two adjacent DenseBlock\"\"\"\n    def __init__(self, num_input_feature, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\"norm\", nn.BatchNorm2d(num_input_feature))\n        self.add_module(\"relu\", nn.ReLU(inplace=True))\n        self.add_module(\"conv\", nn.Conv2d(num_input_feature, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\"pool\", nn.AvgPool2d(2, stride=2))\n```\n最后我们实现DenseNet网络：\n\n```python\nclass DenseNet(nn.Module):\n    \"DenseNet-BC model\"\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64,\n                 bn_size=4, compression_rate=0.5, drop_rate=0, num_classes=1000):\n        \"\"\"        \n        :param growth_rate: 增长率，即K=32\n        :param block_config: 每一个DenseBlock的layers数量，这里实现的是DenseNet-121\n        :param num_init_features: 第一个卷积的通道数一般为2*K=64\n        :param bn_size: bottleneck中1*1conv的factor=4，1*1conv输出的通道数一般为factor*K=128\n        :param compression_rate: 压缩因子\n        :param drop_rate: dropout层将神经元置0的概率，为0时表示不使用dropout层\n        :param num_classes: 分类数\n        \"\"\"\n        super(DenseNet, self).__init__()\n        # first Conv2d\n        self.features = nn.Sequential(OrderedDict([\n            (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\"norm0\", nn.BatchNorm2d(num_init_features)),\n            (\"relu0\", nn.ReLU(inplace=True)),\n            (\"pool0\", nn.MaxPool2d(3, stride=2, padding=1))\n        ]))\n\n        # DenseBlock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers, num_features, bn_size, growth_rate, drop_rate)\n            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n            num_features += num_layers*growth_rate\n            if i != len(block_config) - 1:\n                transition = _Transition(num_features, int(num_features*compression_rate))\n                self.features.add_module(\"transition%d\" % (i + 1), transition)\n                num_features = int(num_features * compression_rate)\n\n        # final bn+ReLU\n        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n        self.features.add_module(\"relu5\", nn.ReLU(inplace=True))\n\n        # classification layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # params initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.avg_pool2d(features, 7, stride=1).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n```\n\n【参考文档】\n[深入解析DenseNet(含大量可视化及计算)](https://mp.weixin.qq.com/s?__biz=MzUzNzk2ODUyNQ==&mid=2247484110&idx=1&sn=0f415bac7335342fd151cffcd1dc502d&chksm=fadfa82ccda8213aedac938b58dca11ec0e8e0d3a9205f4efca844f77c92f12225920bef741d&mpshare=1&scene=1&srcid=0903APUhFluNU7752uehJ7pM&sharer_sharetime=1599098864114&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=af1f462991b3ae2f73d8edd600c4378b767aff3e954a4656a4eeb460f16939d828393da9cba16ed045c655c8fb207f7dbd13c2de88325f562ff92a6fcd2d95cf6ecbc5a4bccc2a8c973ffd995c75134146c311897ad6086c17c304e1fa2484da63fb4df1767131ed3f1605e2a6917c79b142a52b6737422fdebf716e9939e540&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AW14wywO1LP2LC9IpvKu9xI=&pass_ticket=mWKUrpjSxXDNNq6Wxeq9z%2bpdzjoRwb%2bsOY1LPtVaBwUd%2bbTos55%2bq/sa%2b%2bkvdHSF&wx_header=0)\n[来聊聊DenseNet及其变体PeleeNet、VoVNet](https://mp.weixin.qq.com/s?__biz=MzI0NDYxODM5NA==&mid=2247484785&idx=1&sn=fcfd18eff932853b4d3f7e7cec22d3d6&chksm=e95a4084de2dc992791d204a1232d237800c5e9f8166d000a16d54360bb60aba0fb6f9645f50&mpshare=1&scene=1&srcid=0829OeTUIOQO2urTTN8ArxN1&sharer_sharetime=1598666372095&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=029d4162e347a3ad8256a33675540963e9543dda8391a5f64379beeee4b8e75a04d1c796a2cac2066bb9a0384e2dc86527c3af81fcf09176d74d582e0a638e3d1dd6e86cb094342d867957c69fc56d0eb8f99f62d3df3f0491f44d396211630f7eda98e0645c1a23ee20847187d31d745b19eac23f4bd9d92a2fe5295788d935&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AQTz6sAlVMxA9IAp9ltR15Q=&pass_ticket=VOgSQ7cGMld09vU35rB0cjJS6QZUBmypkhw5kxq/X%2bFElM8Ehh63InCpsqo8aHzC)\n[稠密连接网络（DenseNet）](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.12_densenet)\n[深度学习网络篇——DenseNet](https://blog.csdn.net/weixin_43624538/article/details/85227041?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159866029119724825707695%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=159866029119724825707695&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v1~rank_blog_v1-1-85227041.pc_v1_rank_blog_v1&utm_term=densenet&spm=1018.2118.3001.4187)\n[论文笔记DenseNet](https://blog.csdn.net/ChuiGeDaQiQiu/article/details/79512257)\n[DenseNet：比ResNet更优的CNN模型](https://zhuanlan.zhihu.com/p/37189203)\nDensely Connected Convolutional Networks\n","tags":["DenseNet"],"categories":["神经网络"]},{"title":"细粒度分析与Bilinear CNN model（附代码实现）","url":"/2020/11/24/224009/","content":"\n## 前言\n\n\n有时，我们逛街时看到不同的狗，却不知道其具体品种，看到路边开满鲜花，却傻傻分不清具体是什么花。实际上，类似的问题在实际生活中屡见不鲜，人类尚且如此，更别说人工智能了。为了解决这一问题，研究者们提出了**细粒度分析**（fine-grained image analysis）这一专门研究物体精细差别的方向。\n\n<!-- more -->\n\n\n\n## 细粒度分析\n\n**细粒度分析**任务相较于**通用图像**（general/generic images）任务的区别和难点在于**其图像所属类别的粒度更为精细**。下图为例，通用图像分类其任务诉求是将“袋鼠”和“狗”这两个物体大类（蓝色框和红色框中物体）分开，可见无论从样貌、形态等方面，二者还是能很容易被区分；而细粒度图像分类任务则要求对“狗”类别下细粒度的子类，即分别对“哈士奇”和“爱斯基摩犬”的图像分辨出来。正因同类别物种的不同子类往往仅在耳朵形状、毛色等细微处存在差异，可谓“差之毫厘，谬以千里”。不止对计算机，对普通人来说，细粒度图像任务的难度和挑战无疑也更为巨大。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902104417284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n近年来，随着AI的发展，深度学习方面的细粒度图像分析任务可分为**细粒度图像分类**（fine-grained image classification）和 **细粒度图像检索**（fine-grained image retrieval）两大经典图像研究方向。\n\n\n**细粒度图像分类**\n\n由于细粒度物体的差异仅体现在细微之处，**如何有效地对图像进行分析检测，并从中发现重要的局部区域信息，成为了细粒度图像分类算法要解决的关键问题**。对细粒度分类模型，可以按照其使用的监督信息的多少，分为“基于强监督信息的分类模型”和“基于弱监督信息的分类模型”两大类。\n\n\n**基于强监督信息的细粒度图像分类模型**\n\n所谓“强监督细粒度图像分类模型”是指，在模型训练时，为了获得更好的分类精度，除了图像的类别标签（label）外，还使用了物体标注框（object bounding box）和 部位标注点（part annotation）等额外的人工标注信息，这点与目标检测（Detection）与图像分割（Segmentation）的含义是相同的，如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902105325386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n常见的强监督信息细粒度分类的经典模型有Part-based R-CNN、Pose Normalized CNN、Mask-CNN，这里不再详细赘述。\n\n\n**基于弱监督信息的细粒度图像分类模型**\n\n基于强监督信息的分类模型虽然取得了较满意的分类精度，但由于标注信息的获取代价十分昂贵，在一定程度上也局限了这类算法的实际应用。因此，目前细粒度图像分类的一个明显趋势是，希望在模型训练时仅使用**图像级别标注信息**（即图片的label），而不再使用额外的object bounding box和part annotation信息，也能取得与强监督分类模型可比的分类精度，这便是“基于弱监督信息的细粒度分类模型”。思路同强监督分类模型类似，基于弱监督信息的细粒度分类模型也需要借助**全局和局部信息来做细粒度级别的分类**。而区别在于，弱监督细粒度分类希望在不借助object bounding box和part annotation的情况下，**也可以做到较好的局部信息的捕捉**。当然，在分类精度方面，目前最好的弱监督分类模型仍与最好的强监督分类模型存在差距（分类准确度相差约1～2%）。常见的基于弱监督信息的细粒度图像分类模型有Two Level Attention Model、Constellations、Bilinear CNN model。\n\n\n**细粒度图像检索**\n\n图像分析中除监督环境下的分类任务，还有另一大类经典任务——无监督环境下的图像检索。图像检索（image retrieval）按检索信息的形式，分为“以文搜图”（text-based）和“以图搜图”（image-based），这里具体就不介绍了。\n\n\n\n## Bilinear CNN model\n\n**双线性模型**（Bilinear CNN model）是基于弱监督信息的细粒度图像分类模型，在2015与Bilinear CNN Models for Fine-grained Visual Recognition》被提出来用于fine-grained分类。\n\n我们知道，深度学习成功的一个重要精髓，就是将原本分散的处理过程，如特征提取，模型训练与决策等，整合进了一个完整的系统，进行端到端的整体优化训练，不需要人来干预了。并且，对于图像的不同特征，我们常用的方法是进行**连接**（特征图尺寸不变，通道数增加），或者进行**加和**（特征图同一位置像素值相加，通道数不变），或者max-pooling（通道数不变，特征图尺寸变小）。\n\n\n研究者们通过研究人类的大脑发现，人类的视觉处理主要有两条pathway（通路），一条是the ventral stream，对物体进行识别，另一条是the dorsal stream，为了发现物体的位置。作者基于这样的思想，希望能够将两个不同特征进行融合来共同发挥作用，提高细粒度图像的分类效果，即希望两个特征能分别表示图像的位置和对目标进行识别，模型框架如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902112409503.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n> 一种对Bilinear CNN模型的解释是，**网络A的作用是对物体／部件进行定位，即完成物体与局部区域检测工作，而网络B则是用来对网络A检测到的物体位置进行特征提取**。两个网络相互协调作用，完成了细粒度图像分类过程中两个最重要的任务：**物体、局部区域的检测与特征提取**。\n\n两个不同的stream代表着通过CNN得到的不同特征，然后将两个特征进行bilinear操作。一个 bilinear CNN model 由四元组构成，$\\beta=(f_A,f_B,P,C)$，其中$f_A$ 和 $f_B$ 代表特征提取函数，即网络中的A、B，$P$ 是一个池化函数（pooling function），C表示分类器。图像 $I$ 的特征的每一个位置 $l$，进行如下计算：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902160557998.png#pic_center)\n**具体来讲，过程如下：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902113008719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n就是先把在特征图同一位置上的特征进行矩阵相乘，得到矩阵 $b$ ，对所有位置的 $b$ 进行sum pooling（也可以是 max pooling，但一般采用 sum pooling 以方便进行矩阵运算）得到矩阵 $\\xi$。比如，对于一个CNN来讲，输入的特征图有c个通道数，那么在位置 $I$ 上的特征就是1\\*c 的大小，然后与同一位置上，不用CNN得到的 1\\*c 的矩阵进行乘积，得到c*c矩阵，然后将所有位置上的 c\\*c 的矩阵进行求和（就得到了 $\\xi$ ），再转换成向量的形式就可以得到Bilinear vector，即上图中的$x$  。对 $x$ 进行 $y=sign(x)\\sqrt{|x|}$ 操作，再对得到的 $y$ 进行 L2归一化操作 $z=y/||y||_2$，然后我们就可以把特征 $z$ 用于**细粒度图像分类**（fine-grained image classification）了。\n\n>L2归一化操作具体参考[L2范数归一化](https://blog.csdn.net/geekmanong/article/details/51344732)。\n\n\n上面的解释可能看的很懵，举一个例子来说明吧。如使用VGG Conv5_3 输出特征图维度为12\\*12\\*512（特征图大小12\\*12，有512个通道），则特征图共有12\\*12=144个位置，每个位置的特征维度为1\\*512，将两个特征图同一位置的512\\*1与1\\*512的矩阵相乘，得到该位置特征向量维度为512\\*512。文章中使用**所有位置特征向量之和**对其进行池化，故将144个512\\*512的特征向量相加，最终得到512\\*512的双线性特征。 该过程可以使用矩阵乘法实现，将特征图变形为144\\*512的特征矩阵，之后其转置与其相乘，得到512*512的双线性特征向量。\n\n\n```python\n# 实现方式：pytorch \nx = torch.randn(1,512,12,12)\nbatch_size = x.size(0)\nfeature_size = x.size(2)*x.size(3)\nx = x.view(batch_size , 512, feature_size)\nx = (torch.bmm(x, torch.transpose(x, 1, 2)) / feature_size).view(batch_size, -1)\nx = torch.nn.functional.normalize(torch.sign(x) * torch.sqrt(torch.abs(x) + 1e-10))\n```\n\n>关于torch.bmm参考[torch.bmm()函数解读](https://blog.csdn.net/qq_40178291/article/details/100302375)\n\n\nBilinear CNN model的形式简单，便于梯度反向传播，进而实现端到端的训练。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902154836140.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n另外，值得一提的是，bilinear模型由于其优异的泛化性能，不仅在细粒度图像分类上取得了优异效果，还被用于其他图像分类任务，如行人重检测（person Re-ID）。\n\n\n## 代码实现\n\n通过引用resnet18的特征提取部分，实现Bilinear CNN model，如下所示：\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net,self).__init__()\n        self.features = nn.Sequential(resnet18().conv1,\n                                     resnet18().bn1,\n                                     resnet18().relu,\n                                     resnet18().maxpool,\n                                     resnet18().layer1,\n                                     resnet18().layer2,\n                                     resnet18().layer3,\n                                     resnet18().layer4)\n        self.classifiers = nn.Sequential(nn.Linear(512**2,14))\n        \n    def forward(self,x):\n        x=self.features(x)\n        batch_size = x.size(0)\n        feature_size = x.size(2)*x.size(3)\n        x = x.view(batch_size , 512, feature_size)\n        x = (torch.bmm(x, torch.transpose(x, 1, 2)) / feature_size).view(batch_size, -1)\n        x = torch.nn.functional.normalize(torch.sign(x)*torch.sqrt(torch.abs(x)+1e-10))\n        x = self.classifiers(x)\n        return x\n```\n\n\n【参考文档】\n1. [「见微知著」——细粒度图像分析进展综述](https://zhuanlan.zhihu.com/p/24738319)\n2. [双线性池化（Bilinear Pooling）详解、改进及应用](https://zhuanlan.zhihu.com/p/62532887)\n3. [bilinear model && bilinear pooling（一）](https://zhuanlan.zhihu.com/p/87650330)\n4. [学习笔记之——Bilinear CNN model](https://blog.csdn.net/gwplovekimi/article/details/91891439)\n5. [Bilinear CNN](https://blog.csdn.net/u013841196/article/details/102730183)\n6. BilinearCNNModelsforFine-grainedVisualRecognition","tags":["细粒度分析"],"categories":["神经网络"]},{"title":"RuntimeError CUDA out of memory（已解决）","url":"/2020/11/24/223828/","content":"\n今天用pytorch训练神经网络时，出现如下错误：\n\n**RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 2.00 GiB total capacity; 1.29 GiB already allocated; 79.00 MiB free; 1.30 GiB reserved in total by PyTorch)**\n\n明明 GPU 0 有2G容量，为什么只有 79M 可用？ 并且 1.30G已经被PyTorch占用了。**这就说明PyTorch占用的GPU空间没有释放，导致下次运行时，出现CUDA out of memory**。\n\n<!-- more -->\n\n\n\n解决方法如下：\n\n（1）新建一个终端\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200901171405531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（2）输入 `nvidia-smi`，会显示GPU的使用情况，以及占用GPU的应用程序\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200901171522884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（3）输入`taskkill -PID 进程号 -F ` 结束占用的进程，比如 `taskkill -PID 7392 -F`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200901171704466.png)\n（4）再次输入 `nvidia-smi` 查看GPU使用情况，会发现GPU被占用的空间大大降低，这样我们就可以愉快地使用GPU运行程序了\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200901171827184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n【参考文档】\n\n[CUDA out of memory.(已解决）](https://blog.csdn.net/weixin_43398590/article/details/105383173?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight)\n\n","tags":["pytorch"],"categories":["pytorch"]},{"title":"ResNet残差网络及变体详解（符代码实现）","url":"/2020/11/24/223553/","content":"\n本文通过分析深度网络模型的缺点引出ResNet残差网络，并介绍了几种变体，最后用代码实现ResNet18。\n\n<!-- more -->\n\n\n## 前言\n\nResNet（Residual Network， ResNet）是微软团队开发的网络，它的特征在于具有比以前的网络更深的结构，在2015年的ILSVRC大赛中获得分类任务的第1名。\n\n\n网络的深度对于学习表达能力更强的特征至关重要的。网络的层数越多，意味着能够提取的特征越丰富，表示能力就越强。（越深的网络提取的特征越抽象，越具有语义信息，特征的表示能力就越强）。\n\n\n但是，随着网络深度的增加，所带来的的问题也是显而易见的，主要有以下几个方面：\n1. 增加深度带来的首个问题就是**梯度爆炸/消散**的问题，这是由于随着层数的增多，在网络中反向传播的梯度会随着连乘变得不稳定，变得特别大或者特别小。这其中经常出现的是梯度消散的问题。\n2. 为了克服梯度消散也想出了许多的解决办法，如使用BatchNorm，将激活函数换为ReLu等，但是改善问题的能力有限。\n3. 增加深度的另一个问题就是网络的**degradation**（退化）问题，即随着深度的增加，网络的性能会越来越差。如下所示：![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTYwMzkyMC8yMDE5MDMvMTYwMzkyMC0yMDE5MDMyMjA5MTgwNjc4Ny02NDg1NTk0NDUuanBn?x-oss-process=image/format,png#pic_center)\n\n为了让更深的网络也能训练出好的效果，何凯明大神提出了一个新的网络结构——ResNet（Residual Network，残差网络）。通过使用残差网络结构，深层次的卷积神经网络模型不仅避免了出现模型性能退化的问题，并取得了更好的性能。\n\n需要注意的是，**Residual Network不是为了解决过拟合的问题，因为过拟合只是在测试集上错误率很高，而在训练集上错误率很低**，通过上图可以出，随着深度的加深而引起的 **model degradation**（模型退化）不仅在训练集上错误率高，在测试集上错误率也很高。所以说 Residual Network 主要是为了解决因网络加深而导致的模型退化问题（也有效避免了梯度消散问题，下面会讲）。\n\n\n## 模型退化\n\n通常，当我们堆叠一个模型时，会认为效果会越堆越好。因为，网络的层数越多，意味着能够提取到的特征越丰富，特征的表示能力就越强，假设一个比较浅的网络已经可以达到不错的效果，那么再进行叠加的网络如果什么也不做，效果不会变差。\n\n事实上，这是问题所在，**因为“什么都不做”是之前神经网络最难做到的事情之一**。这时因为由于非线性激活函数（Relu）的存在，每次输入到输出的过程都几乎是不可逆的（信息损失），所以很难从输出反推回完整的输入。所以随着深度的加深而引起的 **model degradation**（模型退化）仅通过普通网络模型是无法避免的。\n\nResidual Learning（残差学习）设计的本质，是让**模型的内部结构具有恒等映射的能力，至少让深度网络实现和浅层网络一样的性能，即让深度网络后面的层至少实现恒等映射的作用，这样在堆叠网络的过程中，不会因为继续堆叠而产生退化。**\n\n\n在mobileNetV2论文中，作者说明了使用ReLU的问题，即当使用ReLU等激活函数时，会导致信息丢失，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200827170000864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n低维（2维）的信息嵌入到n维的空间中（即Input的特征经过高维空间进行变换），并通过随机矩阵$T$对特征进行变换，之后再加上ReLU激活函数，之后在通过 $T^{−1}$ （T的逆矩阵）进行反变换。当n=2，3时，会导致比较严重的信息丢失，部分特征重叠到一起了；当n=15到30时，信息丢失程度降低。\n\n\n\n\n## 残差结构\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200827170620973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n在上图中，我们可以使用一个非线性变化函数来描述一个网络的输入输出，即深层的输入为X（X也为浅层的输出），深层的输出为F(x)+x，F通常包括了卷积，激活等操作。\n\n这里需要注意附加的恒等映射关系具有两种不同的使用情况：残差结构的输入数据若和输出结果的维度一致，则直接相加；若维度不一致，必须对x进行升维操作，让它俩的维度相同时才能计算。升维的方法有两种：\n- 直接通过zero padding 来增加维度（channel）；\n- 用1x1卷积实现，直接改变1x1卷积的filters数目，这种会增加参数。\n\n\n\n\n令H(x)=F(x)+x，即H(x)为深层的输出，则 F(x)=H(x)−x。此时残差结构如下图所示，虚线框中的部分就是F(x)，即H(x)−x。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200827172706533.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n\n\n当浅层的x代表的特征已经足够成熟（即浅层网络输出的特征x已经达到了最优），再经过任何深层的变换改变特征x都会让loss变大的话，**F(x)会自动趋向于学习成为0**，x则从恒等映射的路径继续传递。这样就在不增加计算成本的情况下实现了目的：**在前向过程中，当浅层的输出已经足够成熟，让深层网络后面的层能够实现恒等映射的作用（即让后面的层从恒等映射的路径继续传递）**，这样就解决了由于网络过深而导致的模型退化的问题。\n\n从另一方面讲，**残差结构可以让网络反向传播时信号可以更好的地传递**，以一个例子来解释。\n\n\n假设非残差网络输出为G(x)，残差网络输出为H(x)，其中H=F(x)+x，输入的样本特征 **x=1**。（注意：这里G和H中的F是一样的，为了区分，用不同的符号）\n\n（1）在某一时刻：\n\n非残差网络G(1)=1.1， 把G简化为线性运算$G(x)=W_g*x$，可以明显看出$W_g=1.1$。\n残差网络H(1)=1.1， H(1)=F(1)+1, F(1)=0.1，把F简化为线性运算$F(x)=W_f*x$，$W_f=0.1$。\n\n（2）经过一次反向传播并更新G和F中的$W_g$与$W_f$后（输入的样本特征x不变，仍为1）：\n\n非残差网络G’(1)=1.2， 此时$W_g=1.2$\n残差网络H’(1)=1.2, H’(1)=F’(1)+1, F’(1)=0.2，$W_f=0.2$\n\n可以明显的看出，F的参数$W_f$就从0.1更新到0.2，而G的参数$W_g$就从1.1更新到1.2，这一点变化对F的影响远远大于G，**说明引入残差后的映射对输出的变化更敏感，对权重的调整作用更大，所以效果更好**。\n\n从另外一方面来说，对于残差网络H=F(x)+x，每一个导数就加上了一个恒等项1，dh/dx=d(f+x)/dx=1+df/dx，此时就算原来的导数df/dx很小，这时候误差仍然能够有效的反向传播，有效避免了非残差网络链式求导连乘而引发的**梯度消散**。\n\n>这里可能要有人问，反向传播不应该是对权重求偏导吗，这里怎么是对x求偏导？\n反向传播的目的是为了更新权重，但是反向传播的过程是用链式法则实现，在这个过程中，网络中间层的x和w都会参与回传。乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，加法节点的反向传播将上游的值原封不动地输出到下游。\n\n\n\n\n\n\n\n因此，从上面的分析可以看出，残差模块最重要的作用就是改变了前向和后向信息传递的方式从而很大程度上促进了网络的优化。\n- 前向：当浅层的输出已经足够成熟，让深层网络后面的层能够实现恒等映射的作用（即让后面的层从恒等映射的路径继续传递），解决了由于网络过深而导致的模型退化的问题。\n- 后向：引入残差后的映射对输出的变化更敏感，对权重的调整作用更大，效果更好。\n\n至于为何 shortcut（即附加的恒等映射关系）的输入时X，而不是X/2或是其他形式。kaiming大神的另一篇文章 Identity Mappings in Deep Residual Networks 中探讨了这个问题，对以下6种结构的残差结构进行实验比较，shortcut 是X/2的就是第二种，结果发现还是第一种效果好。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828200641837.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n\n\nResNet的研究者还提出了**能够让网络结构更深的残差模块**，如下图所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828203656416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n将原来的building block(残差结构)改为bottleneck（瓶颈结构），很好地减少了参数数量，即先用第一个1x1的卷积把256维channel降到64维，第三个1x1卷积又升到256维，总共用参数：1x1x256x64+3x3x64x64+1x1x64x256=69632，如果不使用 bottleneck，参数将是 3x3x256x256x2=1179648，差了16.94倍。\n\n\n总的来说，由于将原来的building block(残差结构)改为bottleneck（瓶颈结构）减少了模型训练的参数量，同时减少整个模型的计算量，并且网络深度得以增加，这使得拓展更深的模型结构成为可能，于是出现了拥有50层、101层、152层的ResNet模型，这不仅没有出现模型性能退化的问题，而且错误率和计算复杂度都保持在很低的程度。\n\n作者最后在Cifar-10上尝试了1202层的网络，结果在训练误差上与一个较浅的110层的相近，但是测试误差要比110层大1.5%。作者认为是采用了太深的网络，发生了过拟合。所以现在的残差结构最多到100多层，不能再深了，后面会讲到如何把残差网络扩展到1000层。\n\n## ResNet网络结构\n我们以VGG作对比介绍ResNet网络。看下图：\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTYwMzkyMC8yMDE5MDMvMTYwMzkyMC0yMDE5MDMyMjExNTY1MjA2Ni0xMTE1OTA2MzA0LnBuZw?x-oss-process=image/format,png#pic_center)\n左边为基本的VGGNet，中间为基于VGG作出的扩增至34层的普通网络，右边为34层的残差网络，不同的是每隔两层就会有一个residual模块。对于残差网络（右图），维度匹配的shortcut连接为实线（输入和输出有相同的通道数），反之为虚线。维度不匹配时，同等映射有两种可选方案：全0填充和1x1卷积。\n\n\n\n常用的ResNet有5种常用深度：18，34，50，101，152层。网络分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x。如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200827205445989.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n根据上图，ResNet101的层数为3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，再加上第一层的卷积conv1，以及最后的fc层（用于分类），一共是99+1+1=101层。\n\n\n以往模型大多在ImageNet上作测试，所以这里只给出在ImageNet上的成绩，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20181217193440759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70#pic_center)\n可以看到，由于使用1×1的卷积层来减少模型训练的参数量，同时减少整个模型的计算量，增加了网络的深度，152层的ResNet相比于其他网络有提高了一些精度。\n\n\n\n## Pre Activation ResNet\n由于ResNet引入了残差模块，很好的解决了网络模型degradation的问题，从而提高了网络深度。由于将原来的building block（残差结构）改为bottleneck（瓶颈结构）减少了模型训练的参数量，同时减少整个模型的计算量，这使得拓展更深的模型结构成为可能，于是出现了拥有50层、101层、152层的ResNet模型，那么，**我们还能不能加深一些呢？100层可以，1000层呢？**\n\n答案是不可以，至少目前的残差模型是不行的，**因为目前的残差块在加和之后会经过一个relu，由于这个激活函数Relu的位置带来的影响**，使得增加的操作虽然在100层中不会有很大的影响，但是在1000层的超深网络里面还是会阻碍整个网络的前向反向传播（具体原因接着往下看），我们需要接着改进。\n\n\n\n当前卷积神经网络的基本模块通常为卷积+归一化+激活函数（conv+bn+relu）的顺序，对于普通的按照这些模块进行顺序堆叠的网络来说，各个模块的顺序没有什么影响，但是对于残差网络来说却不一样。\n\npre activation 和 post activation ，也就是**激活函数Relu的位置，是放在卷积前还是卷积后**。对于一般的网络来说，这是没什么区别的，因为网络是各个模块进行叠加，这个卷积块的激活函数的输出就是另一卷积块的激活函数的输入，总体来看无所谓先后。\n\n但是，残差网络不一样，它的残差分支包含着完整的卷积，归一化，激活函数层，而后这一分支要和原始信号分支进行相加，因此就有了多种方案，如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828155213685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n我们最常见的是图a的形式，即原始信号和残差信号相加之和再经过Relu输出到下一个block，但实际上还有b、c、d、e等形式，它们的性能如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828155347392.png#pic_center)\n\n\n\n图b，将BN拿出来，放到加和函数之后，结果最差，分析原因可能是不应该把原始信号和残差信号一起加和后归一化，改变了原始信号的分布。\n\n图c，将ReLU提到残差分支，结果也不行，因为这样一来，残差分支的信号就是非负的，当然会影响表达能力，结果也就变得更差了。\n\n图a是原始的ResNet模块，我们可以看到原始信号和残差信号相加后需要进入Relu做一个非线性激活，这样一来，相加后的结果永远是非负的，这就约束了模型的表达能力（和图c原理类似），因此需要做一个调整。图d和图e都是讲ReLU提到了卷积之前，但是BN的顺序有所不同。图d在临近输出放了BN，然后再和原始信号相加，**本来BN就是为了给数据一个固定的分布，一旦经过别的操作就会改变数据的分布，会削减BN的作用，在原版本的resnet中就是这么使用的BN，所以，图d效果与原始的ResNet（图a）性能大致相当**。图e在临近输入放了BN，效果大大提升，这应该是来自于BN层的功劳，本来BN层就应该放置在卷积层之前提升网络泛化能力。\n\n我们来看下这两种结构在CIFAR-10和CIFAR-100上的效果吧：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828162419741.png#pic_center)\n原始的ResNet结构增加到1000层错误率反而提高，但是用上Pre Activation unit后把网络增加到1000层，错误率显著降低，值的注意的是**这里所有精度提升都是出自于深度的增加**。\n\n1000层的残差网络和100层的网络之间的计算复杂度基本是线性的，因此从时间和算力的角度而言还是100层的网络更加实用，但是改进后的残差模块证明了1000层的网络的可实现性，实际上现在各大厂每次开会都要拿超深的网络出来吓人，原理就是这个模块。\n\n\n**那么，我们有没有什么其他不是靠深度的办法来增加特征的表征能力呢？如果有的话结合上ResNet的深度，会不会产生很好的效果呢？**\n\n## 其它的ResNet变体\n### Wide ResNet\n\nWide ResNet，就是比普通的残差网络更宽（也就是通道数量更大）的结构，那么它与ResNet有什么不同呢？\n\n首先，看一下几个不同的残差模块的对比，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828164655354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n(a)是普通的残差结构，(b)是使用1*1卷积进行升维和降维的结构，(c) 是直接增加网络的宽度，图中方块的宽度就表示它的残差块的通道数更大。(d)是文章中提出，可以看到相比于基础模块在中间加上了dropout层，这是因为增加了宽度后参数的数量会显著增加，为了防止过拟合使用了卷积层中的dropout，并取得了相比于不用dropout更好的效果。\n\n作者们实验结果表明：16层的改进残差网络就达到了1000层残差网络的性能，而且计算代价更低。\n\n\n\n\nWide ResNet网络结构如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20181217193732589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70#pic_center)\n\n作者通过实验发现每个残差内部由两个3\\*3卷积层组成可以有最好的效果，上图是改进后模型的基本架构，与ResNet唯一不同的是多了一个k，代表了原始模块中卷积核数量的k倍（也就是通道数量更大），B(3,3) 代表每一个残差模块内由两个3\\*3的卷积层组成。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828170839561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n上图是164层原始残差网络和28层十倍宽度残差网络在CIFAR-10和CIFAR-100数据集上的比较，实线是在测试集上的错误率，虚线是在训练集上的错误率，可以看到，改进后的宽网络无论在测试集上还是在训练集上都有更低的错误率。\n\n\n\n下图是不同宽度的模型之间纵向比较，同深度下，随着宽度增加，准确率增加。深度为22层，宽度为原始8倍的模型比深度为40层的同宽的模型准确率还要高\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20181217193750658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70#pic_center)\n我们可以得到如下结论：\n\n1. 在同深度情况下，增大宽度可以增加各种深度的残差模型的性能\n2. 宽度和深度的增加就可以使性能提升。\n3. 深度的增加对于模型的提升是有限的，在一定范围内，增加深度可以使模型性能提升，但是一定程度以后，在增加模型的深度，反而有更高的错误率\n4. 从某种意义上来说，宽度比深度增加对于模型的提升更重要。\n\n### Inception v4\n这里推荐看一下我的博客[深入解读GoogLeNet网络结构](https://blog.csdn.net/qq_37555071/article/details/108214680)，可以更好的理解Inception模块。作者在Inceptionv4论文中共提出了3个新的网络：Inceptionv4、Inception-ResNetv1、Inception-ResNetv2，并拿这三个网络加上Inceptionv3一起进行比较。\n\n作者认为，**对于训练非常深的卷积模型，残差连接本质上是必需的**。但似乎发现并**不支持这种观点**，至少对于图像识别来说是的。但是，它可能需要更多的测试数据和更深的模型**来了解残差连接提供的有益方面的真实程度**。在实验部分，作者证明了**在不利用残差连接的情况下训练非常深的网络并不是很困难**。然而，**使用残差连接似乎大大提高了训练速度**，这仅仅是它们使用的一个很好的论据。也就是说**Residual connection并不是必要条件，只是使用了Residual connection会加快训练速度。**\n\n\n\n我们直接来看Inception-ResNet的网络结构吧（下图）。值得注意的是，Inception-ResNetv1计算代价跟Inception-v3大致相同，Inception-ResNetv2的计算代价跟Inception-v4网络基本相同。\n\n![在这里插入图片描述](https://img-blog.csdn.net/20180613075024302?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70#pic_center)\n这里其他模块不介绍了，现在只重点关注Inception的ResNet模块部分。\n\nInception-ResNet-v1和Inception-ResNet-v2对应的Inception-resnet-A模块为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828173646805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n如上面的图片所示，改进主要有两点。1. 将residual模块加入inception，2. 将各分支的输出通过聚合后通过同一个1*1的卷积层来进行通道的扩增。\n\nInceptionv4比Inceptionv3层次更深、结构更复杂，并且IncpetionV4对于Inception块的每个网格大小进行了统一。Inception-ResNet在Inception块上加了残差连接加快训练速度。Inception-ResNet-v2的整体框架和Inception-ResNet-v1是一致的，只不过v2的计算量更加expensive些（输出的channel数量更多）。它们训练精度如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828174215491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n从上图可以看出，加了Residual模块的模型训练速度明显比正常的Inception模型快一些，而且也有稍微高一些的准确率。最后，Inception-ResNet-v2的Top-5准确率达到了3.1%，如下图所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828175612572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n想要更详细的了解Inceptionv4，可以参考这两篇博客[Inceptionv4论文详解](https://blog.csdn.net/qq_38807688/article/details/84590291) 和 [卷积神经网络的网络结构——Inception V4](https://blog.csdn.net/u013841196/article/details/80673688)\n\n\n\n### ResNext\nResNeXt基于wide residual和inception，提出了将残差模块中的所有通道分组进行汇合操作会有更好的效果。同时也给inception提出了一个简化的表示方式。\n\n简化的inception如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828191338471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n与原始的Inception相比，简化的Inception将不同尺寸的卷积核和池化等归一化为3\\*3的卷积核，并且每个分支都用 1\\*1 的卷积核去扩展通道后相加就得到了上面的结构，再这个基础上加上shortcut就得到了ResNext模块：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828191427486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\nResNext包含了32个分支的基模块，每个分支一模一样。每一个框中的参数分别表示输入维度，卷积核大小，输出维度，如256,1x1,64表示当前网络层的输入为256个通道，使用1x1的卷积，输出为64个通道。ResNext通过1x1的网络层，控制通道数先进行降维，再进行升维，然后保证和ResNet模块一样，输入输出的通道数都是256。因此，可以总结如下：\n\n1.  相对于Inception-Resnet，ResNext的每个分支都是相同的结构，相对于Inception，网络架构更加简洁。\n2. Inception-Resnet中的先各分支输出并concat，然后用1 \\* 1卷积变换深度的方式被先变换通道数后单位加的形式替代。\n3. 因为每个分支的相同结构，**可以提出除了深度和宽度之外的第三维度cardinality，即分支的数量**。\n\n\nResNext结构对比普通的ResNet结构，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020082818095390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n从另外一个角度，ResNext是也可以看做一个增加了分组的残差模块，对于上图的ResNet结构的第一个卷积模块，输入为256维，输出为64维。右侧包含了32个同样的支路，每一个支路的输入为256维，输出为4维。不过两者的参数量是相当的，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828191955481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n**类比Wide Residual的配置图，二者一个是改变了卷积核的倍数，一个增加了分组，但都是在残差模块做工作。** 不同深度、不同宽度、不同分组的网络对比如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828192913658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n可以看到，相对于100层的残差网络，用深度，宽度，和cardinality三种方式增大了两倍的复杂度，相同复杂度下，**分组更多即C更大的模型性能更好，这说明cardinality是一个比深度和宽度更加有效的维度**。而且，ResNext的计算速度更快，因为ResNext的结构本来就非常适合硬件并行处理。\n\n\n\n## ResNet18的实现\n\n残差块的实现如下。它可以设定输出通道数、是否使用额外的 1×1 卷积层来修改通道数以及卷积层的步幅。\n\n```python\nimport time\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\n\nclass Residual(nn.Module):  \n    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n        super(Residual, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        # 1x1conv来升维\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        # 1x1conv对浅层输入的X升维\n        if self.conv3:\n            X = self.conv3(X)\n        return F.relu(Y + X)\n```\n下面我们来查看输入和输出形状一致的情况：\n\n```python\nblk = Residual(3, 3)\nX = torch.rand((4, 3, 6, 6))\nblk(X).shape # torch.Size([4, 3, 6, 6])\n```\n我们也可以在增加输出通道数的同时减半输出的高和宽。\n\n```python\nX = torch.rand((4, 3, 6, 6))\nblk = Residual(3, 6, use_1x1conv=True, stride=2)\nblk(X).shape # torch.Size([4, 6, 3, 3])\n```\n\nResNet的**前两层**跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。\n\n```python\nresnet_18 = nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n        nn.BatchNorm2d(64), \n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n```\n\nGoogLeNet在后面接了4个由Inception块组成的模块。**ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致**。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在**第一个残差块**里将上一个模块的通道数翻倍，并将高和宽减半。\n\n下面我们来实现这个模块。注意，这里对第一个模块做了特别处理。\n\n```python\ndef resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n    if first_block:\n        assert in_channels == out_channels # 第一个模块的通道数同输入通道数一致\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n        else:\n            blk.append(Residual(out_channels, out_channels))\n    # 解包迭代器，从而传入多个模块\n    return nn.Sequential(*blk)\n```\n接着我们为ResNet加入所有残差块。这里每个模块使用两个残差块。\n\n```python\nresnet_18.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\nresnet_18.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\nresnet_18.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\nresnet_18.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n```\n最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。\n\n```python\nclass GlobalAvgPool2d(nn.Module):\n    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n    def forward(self, x):\n        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n    \nclass FlattenLayer(torch.nn.Module):\n    def __init__(self):\n        super(FlattenLayer, self).__init__()\n    def forward(self, x): # x shape: (batch, *, *, ...)\n        return x.view(x.shape[0], -1)\n        \nresnet_18.add_module(\"global_avg_pool\", GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, 512, 1, 1)\nresnet_18.add_module(\"fc\", nn.Sequential(FlattenLayer(), nn.Linear(512, 10))) \n```\n这里每个模块里有4个卷积层（不计算1×1卷积层），加上最开始的卷积层和最后的全连接层，共计18层。这个模型通常也被称为ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。虽然ResNet的主体架构跟GoogLeNet的类似，但ResNet结构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。\n\n我们来观察一下输入形状在ResNet不同模块之间的变化。\n\n```python\nX = torch.rand((1, 1, 224, 224))\nfor name, layer in resnet_18.named_children():\n    X = layer(X)\n    print(name, ' output shape:\\t', X.shape)\n\"\"\"\n# 前面四层是 7x7conv、BN、nn.ReLU、MaxPool2d\n# 输出\n0  output shape:\t torch.Size([1, 64, 112, 112])\n1  output shape:\t torch.Size([1, 64, 112, 112])\n2  output shape:\t torch.Size([1, 64, 112, 112])\n3  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block1  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block2  output shape:\t torch.Size([1, 128, 28, 28])\nresnet_block3  output shape:\t torch.Size([1, 256, 14, 14])\nresnet_block4  output shape:\t torch.Size([1, 512, 7, 7])\nglobal_avg_pool  output shape:\t torch.Size([1, 512, 1, 1])\nfc  output shape:\t torch.Size([1, 10])\n\"\"\"\n```\n\n另外，本文用到的论文我上传到百度云上了，有需要的请自提，链接：https://pan.baidu.com/s/1cParM5EEOz3QOQgjAZt9jA 提取码：ngvb 。包含如下三个论文：\n- Identity Mappings in Deep Residual Networks\n- Wide Residual Networks\n- AggregatedResidualTransformationsforDeepNeuralNetworks\n\n\n【参考文档】\n[深度学习网络篇——ResNet](https://blog.csdn.net/weixin_43624538/article/details/85049699)\n[resnet中的残差连接，你确定真的看懂了？](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029645&idx=1&sn=75b494ec181fee3e8756bb0fa119e7ce&chksm=87134270b064cb66aea66e73b4a6dc283d5750cfa9d331015424f075ba117e38f857d2f25d07&mpshare=1&scene=1&srcid=0826sj1X99iidGol2vOYXoyL&sharer_sharetime=1598412745177&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a4858933403615d2b1a972f5118c4a09c232aef7f191fe77f564ecd8fcee532326e67d272b0451ccad5d44c0586df1184bd3170c632e98c86f33bc664bca4603775cab14cca0fe4739d5170a3f0b4fa71418da0597ca52322b9008de5cda1b2746ea23f0d96b8a0720c48fc50cf26ae7c16982bfd6f2aad9addf8&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AS/my8QJbD1iVlhaLq6mPl4=&pass_ticket=YcCsKQUoka0nj/P%2b0pwrfYVeXAi9wdtnOBln8h11m8ftsr1WLiJ5a6KMsypN6fsD)\n[残差网络（ResNet）](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.11_resnet)\n[残差网络ResNet笔记](https://www.cnblogs.com/alanma/p/6877166.html)\n[CNN 经典网络之-ResNet](https://www.cnblogs.com/yanshw/p/10576354.html)\n深度学习之Pytorch实战计算机视觉-唐进民著\n\n","tags":["ResNet"],"categories":["神经网络"]},{"title":"深入解读GoogLeNet网络结构（附代码实现）","url":"/2020/11/24/223407/","content":"\n\n\n## 前言\n\n> 七夕了，看着你们秀恩爱，单身狗的我还是做俺该做的事吧！\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vd3hsZXIvYmxvZ2ltZy9yYXcvbWFzdGVyL2ltZ3MvMjAyMDA4MDMxMDQ1MzcuZ2lm)\n\n\n在上一篇文章中介绍了[VGG网络结构](https://blog.csdn.net/qq_37555071/article/details/108199352)，VGG在2014年ImageNet 中获得了定位任务第1名和分类任务第2名的好成绩，而同年分类任务的第一名则是**GoogleNet** 。GoogleNet是Google研发的深度网络结构，之所以叫“GoogLeNet”，是为了向“LeNet”致敬，有兴趣的同学可以看下原文[Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842.pdf)。\n\n\n\n<!-- more -->\n\n\n\n\n与VGGNet模型相比较，GoogleNet模型的网络深度已经达到了22层（ 如果只计算有参数的层，GoogleNet网络有22层深 ，算上池化层有27层），而且在网络架构中引入了Inception单元，从而进一步提升模型整体的性能。虽然深度达到了22层，但大小却比AlexNet和VGG小很多，GoogleNet参数为500万个（ 5M ），VGG16参数是138M，是GoogleNet的27倍多，而VGG16参数量则是AlexNet的两倍多。\n\n## Inception单元结构\n\n我们先来看一下模型中的Inception单元结构，然后在此基础上详细分析GoogleNet网络结构，这里推荐看一下我的这篇博客[从Inception到Xception，卷积方式的成长之路](https://blog.csdn.net/qq_37555071/article/details/107835402)，可以对下面的内容有更好的理解。\n\nInception 最初提出的版本主要思想是**利用不同大小的卷积核实现不同尺度的感知**，网络结构图如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806113406524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nInception Module基本组成结构有四个成分。1\\*1卷积，3\\*3卷积，5\\*5卷积，3\\*3最大池化。最后对四个成分运算结果进行通道上组合，这就是Naive Inception的核心思想：利用不同大小的卷积核实现不同尺度的感知，最后进行融合，可以得到图像更好的表征。\n\n下面通过一个具体的实例来看看整个Naive Inception单元的详细工作过程，假设在上图中Naive  Inception单元的前一层输入的数据是一个32×32×256的特征图，**该特征图先被复制成4份并分别被传至接下来的4个部分**。我们假设这4个部分对应的滑动窗口的步长均为1，其中，1×1卷积层的Padding为0，滑动窗口维度为1×1×256，要求输出的特征图深度为128；3×3卷积层的Padding为1，滑动窗口维度为3×3×256，要求输出的特征图深度为192；5×5卷积层的Padding为2，滑动窗口维度为5×5×256，要求输出的特征图深度为96；3×3最大池化层的  Padding为1，滑动窗口维度为3×3×256。**这里对每个卷积层要求输出的特征图深度没有特殊意义，仅仅举例用**，之后通过计算，分别得到这4部分输出的特征图为32×32×128、32×32×192、32×32×96  和  32×32×256，最后在合并层进行合并，得到32×32×672的特征图，合并的方法是将各个部分输出的特征图相加，最后这个Naive  Inception单元输出的特征图维度是32×32×672，总的参数量就是`1*1*256*128+3*3*256*192+5*5*256*96=1089536`。\n\n但是Naive  Inception有两个非常严重的问题：**首先，所有卷积层直接和前一层输入的数据对接，所以卷积层中的计算量会很大；其次，在这个单元中使用的最大池化层保留了输入数据的特征图的深度，所以在最后进行合并时，总的输出的特征图的深度只会增加，这样增加了该单元之后的网络结构的计算量**。于是人们就要想办法减少参数量来减少计算量，在受到了模型 “Network in Network”的启发，开发出了在GoogleNet模型中使用的Inception单元（Inception V1），这种方法可以看做是一个额外的1\\*1卷积层再加上一个ReLU层。如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806113747780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n这里使用1x1 卷积核主要目的是进行**压缩降维，减少参数量**，从而让网络更深、更宽，更好的提取特征，这种思想也称为**Pointwise Conv**，简称PW。\n\n\n举个例子来论证下吧。假设新增加的  1×1 的卷积的输出深度为64，步长为1，Padding为0，其他卷积和池化的输出深度、步长都和之前在Naive  Inception单元中定义的一样（即上面例子中定义的一样），前一层输入的数据仍然使用同之前一样的维度为32×32×256的特征图，通过计算，分别得到这  4  部分输出的特征图维度为32×32×128、32×32×192、32×32×96  和32×32×64，将其合并后得到维度为32×32×480的特征图，将这4部分输出的特征图进行相加，最后Inception单元输出的特征图维度是32×32×480。新增加的3个  1×1 的卷积参数量是`3*1*1*256*64=49152`，原来的卷积核参数量是`1*1*256*128+3*3*64*192+5*5*64*96=296960`，总的参数量就是`49152+296960=346112`。\n\n在输出的结果中，32×32×128、32×32×192、32×32×96  和之前的Naive  Inception  单元是一样的，**但其实这三部分因为1×1卷积层的加入，总的卷积参数数量已经大大低于之前的Naive  Inception单元**，**而且因为在最大池化层之前也加入了1×1的卷积层，所以最终输出的特征图的深度也降低了，这样也降低了该单元之后的网络结构的计算量**。\n\n## GoogLeNet模型解读\n\nGoogleNet网络结构（Inception V1）的网络结构如下：\n\n\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X2pwZy94bkJGSGNSNDE4cUM4TE9zN1R0dnJOTDRLRllpYkRIMmtpYzluQjhweXowbXFIdU0yUVk4WjZtNXNJQzlwZDRJRmJ4MlZLbWttYm1wVlg0N2lidE1VaWMwdXcvNjQw?x-oss-process=image/format,png)\n\nGoogLeNet网络有22层深（包括pool层，有27层深），在分类器之前，采用Network in Network中用Averagepool（平均池化）来代替全连接层的思想，而在avg pool之后，还是添加了一个全连接层，是为了大家做finetune（微调）。而无论是VGG还是LeNet、AlexNet，在输出层方面均是采用连续三个全连接层，全连接层的输入是前面卷积层的输出经过reshape得到。**据发现，GoogLeNet将fully-connected layer用avg pooling layer代替后，top-1 accuracy 提高了大约0.6%；然而即使在去除了fully-connected layer后，依然必须dropout。**\n\n由于全连接网络参数多，计算量大，容易过拟合，所以GoogLeNet没有采用VGG、LeNet、AlexNet三层全连接结构，直接在Inception模块之后使用Average Pool和Dropout方法，不仅起到降维作用，还在一定程度上防止过拟合。\n\n**在Dropout层之前添加了一个7×7的Average Pool，一方面是降维，另一方面也是对低层特征的组合。我们希望网络在高层可以抽象出图像全局的特征，那么应该在网络的高层增加卷积核的大小或者增加池化区域的大小，GoogLeNet将这种操作放到了最后的池化过程，前面的Inception模块中卷积核大小都是固定的，而且比较小，主要是为了卷积时的计算方便。**\n\n\n\nGoogLeNet在网络模型方面与AlexNet、VGG还是有一些相通之处的，它们的主要相通之处就体现在卷积部分，\n\n- AlexNet采用5个卷积层\n- VGG把5个卷积层替换成5个卷积块\n- GoogLeNet采用5个不同的模块组成主体卷积部分\n\n用表格的形式表示GoogLeNet的网络结构如下所示：\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9zYnphQnhDRXJManBpYzBtaWNaOVFkZ3FTcjhXZ01WdEV2NGhvaFh2bWpxdHBxZlVKWGxIVWliSDZXaWFxWjNicW85NEJrazFCanBGaWNTd28yNkd6dDRLRlhBLzY0MA?x-oss-process=image/format,png#pic_center)\n上述就是GoogLeNet的结构，可以看出，和AlexNet统一使用5个卷积层、VGG统一使用5个卷积块不同，GoogLeNet在主体卷积部分是**卷积层与Inception块混合使用**。另外，需要注意一下，在输出层GoogleNet采用**全局平均池化**，得到的是高和宽均为1的卷积层，而不是通过reshape得到的全连接层。\n\n需要注意的是，上图中 “＃3×3reduce” 和 “＃5×5reduce” 表示在3×3和5×5卷积之前，使用的**降维层**中的1×1滤波器的数量。pool proj代表max-pooling后的投影数量（即先max-pooling，再PW降维），所有的reductions（降维）和projections（投影）也都使用激活函数ReLU。\n\n\n下面就来详细介绍一下GoogLeNet的模型结构。\n\n**输入**\n\n原始输入图像为224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）\n\n**第一模块**\n\n第一模块采用的是一个单纯的卷积层紧跟一个最大池化层。\n\n卷积层：卷积核大小7*7，步长为2，padding为3，输出通道数64，输出特征图尺寸为`(224-7+3*2)/2+1=112.5(向下取整)=112`，输出特征图维度为112x112x64，卷积后进行ReLU操作。\n\n池化层：窗口大小3*3，步长为2，输出特征图尺寸为`((112 -3)/2)+1=55.5(向上取整)=56`，输出特征图维度为56x56x64。\n\n关于卷积和池化中的特征图大小计算方式，可以参考我的博客[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)\n\n**第二模块**\n\n第二模块采用**2个卷积层**，后面跟一个最大池化层。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825150804453.png#pic_center)\n\n**卷积层：**\n1. 先用64个1x1的卷积核（3x3卷积核之前的降维）将输入的特征图（56x56x64）变为56x56x64，然后进行ReLU操作。参数量是`1*1*64*64=4096`\n2. 再用卷积核大小3*3，步长为1，padding为1，输出通道数192，进行卷积运算，输出特征图尺寸为`(56-3+1*2)/1+1=56`，输出特征图维度为56x56x192，然后进行ReLU操作。参数量是`3*3*64*192=110592`\n\n第二模块卷积运算总的参数量是`110592+4096=114688`，即`114688/1024=112K`。\n\n\n**池化层：** 窗口大小3*3，步长为2，输出通道数192，输出为`((56 - 3)/2)+1=27.5(向上取整)=28`，输出特征图维度为28x28x192。\n\n**第三模块(Inception 3a层)**\n\nInception 3a层，分为四个分支，采用不同尺度，图示如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/202008251231252.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n再看下表格结构，来分析和计算吧：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825151846422.png#pic_center)\n\n1. 使用64个1x1的卷积核，运算后特征图输出为28x28x64，然后RuLU操作。参数量`1*1*192*64=12288`\n2. 96个1x1的卷积核（3x3卷积核之前的降维）运算后特征图输出为28x28x96，进行ReLU计算，再进行128个3x3的卷积，输出28x28x128。参数量`1*1*192*96+3*3*96*128=129024`\n3. 16个1x1的卷积核（5x5卷积核之前的降维）将特征图变成28x28x16，进行ReLU计算，再进行32个5x5的卷积，输出28x28x32。参数量`1*1*192*16+5*5*16*32=15872`\n4. pool层，使用3x3的核，输出28x28x192，然后进行32个1x1的卷积，输出28x28x32.。总参数量`1*1*192*32=6144`\n\n将四个结果进行连接，对这四部分输出结果的第三维并联，即64+128+32+32=256，最终输出28x28x256。总的参数量是`12288+129024+15872+6144=163328`，即`163328/1024=159.5K`，约等于159K。\n\n**第三模块(Inception 3b层)**\n\nInception 3b层，分为四个分支，采用不同尺度。\n\n1. 128个1x1的卷积核，然后RuLU，输出28x28x128\n2. 128个1x1的卷积核（3x3卷积核之前的降维）变成28x28x128，进行ReLU，再进行192个3x3的卷积，输出28x28x192\n3. 32个1x1的卷积核（5x5卷积核之前的降维）变成28x28x32，进行ReLU，再进行96个5x5的卷积，输出28x28x96\n4. pool层，使用3x3的核，输出28x28x256，然后进行64个1x1的卷积，输出28x28x64\n\n将四个结果进行连接，对这四部分输出结果的第三维并联，即128+192+96+64=480，最终输出输出为28x28x480。\n\n**第四模块(Inception 4a、4b、4c、4e)**\n\n与Inception3a，3b类似\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825153012306.png#pic_center)\n**第五模块(Inception 5a、5b)**\n\n与Inception3a，3b类似\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825153237754.png#pic_center)\n\n\n**输出层**\n\n前面已经多次提到，在输出层GoogLeNet与AlexNet、VGG采用3个连续的全连接层不同，GoogLeNet采用的是全局平均池化层，得到的是高和宽均为1的卷积层，然后添加丢弃概率为40%的Dropout，输出层激活函数采用的是softmax。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825153225724.png#pic_center)\n\n**激活函数**\n\nGoogLeNet每层使用的激活函数为ReLU激活函数。\n\n**辅助分类器**\n\n根据实验数据，发现**神经网络的中间层也具有很强的识别能力，为了利用中间层抽象的特征，在某些中间层中添加含有多层的分类器**。如下图所示，红色边框内部代表添加的辅助分类器。**GoogLeNet中共增加了两个辅助的softmax分支，作用有两点，一是为了避免梯度消失，用于向前传导梯度。反向传播时如果有一层求导为0，链式求导结果则为0。二是将中间某一层输出用作分类，起到模型融合作用**。最后的loss=loss_2 + 0.3 \\* loss_1 + 0.3 \\* loss_0。实际测试时，这两个辅助softmax分支会被去掉。\n\n\n\n\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9QbjRTbTBSc0F1aWEwRUlYWE5zb1h1SHZqN241YmVTNHpWMmNLSXVZTWlibmliTHB2bWYxY3ZodkJuOUppYm51YnBjc04xeUZ2cDJuVThaRjY1WWljN0NBN3FBLzY0MA?x-oss-process=image/format,png#pic_center)\n\n## GoogLeNet其他版本\n上面介绍的GoogLeNet模型是Inception v1版本，还有Inception v2，v3，v4版本\n\n**Inception V2**\n\n1. 学习VGGNet的特点，用两个3*3卷积代替5*5卷积，可以降低参数量。\n2. 提出BN算法。BN算法是一个正则化方法，可以提高大网络的收敛速度。就是每一batch的输入分布标准化处理，使得规范化为N(0,1)的高斯分布，收敛速度大大提高。详情可以参考我的博客[Batch Normalization：批量归一化详解](https://blog.csdn.net/qq_37555071/article/details/107549047)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806130828579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n**Inception V3**\n\n学习Factorization into small convolutions的思想，在Inception V2的基础上，将一个二维卷积拆分成两个较小卷积，例如将7\\*7卷积拆成1\\*7卷积和7\\*1卷积，这样做的好处是降低参数量。该paper中指出，通过这种非对称的卷积拆分比对称的拆分为几个相同的小卷积效果更好，可以处理更多，更丰富的空间特征，这就是Inception V3网络结构。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080613330515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n**Inception V4**\n\n借鉴了微软的ResNet网络结构思想，后面写到Resnet再介绍吧。\n\n\n## GoogLeNet测试样本处理\n1. 对于一个测试样本，将图像的短边缩放成4种尺寸，分别为256，288，320，352。\n2. 从每种尺寸的图像的左边，中间，右边（或者上面，中间，下面）分别截取一个方形区域，共三块方形区域。\n3. 对于每一个方形区域，我们取其四角和中心，裁切出5个区域，再将方形区域缩小到224×224，共6快区域，加上它们的镜像版本（将图像水平翻转），一共得到4×3×6×2=144张图像。这样的方法在实际应用中是不必要的，可能存在更合理的修剪方法。下图展示了不同修剪方法和不同模型数量的组合结果：\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825155701819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n*上表中，通过改变模型数量以及切分数量，展示几种测试策略对于图片进行预测的效果，所有数据报告基于验证数据集,以避免测试集上的过拟合。*\n\n\n使用多个模型时，每个模型的Softmax分类器在多个修剪图片作为输入时都得到多个输出值，然后再对所有分类器的softmax概率值求平均。\n\n\n**效果如下所示：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825155830735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n可以看到，GoogLeNet在验证集和测试集上top-5的错误率都降到了6.67%，在当年参赛者中排名第一。\n\n\n  \t\n\n\n\n\n## GoogleNet代码实现\n\n**Inception实现**\n\n```python\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nclass Inception(nn.Module):\n    # c1 - c4为每条线路里的层的输出通道数\n    def __init__(self, in_c, c1, c2, c3, c4):\n        super(Inception, self).__init__()\n        # 线路1，单1 x 1卷积层\n        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)\n        # 线路2，1 x 1卷积层后接3 x 3卷积层\n        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # 线路3，1 x 1卷积层后接5 x 5卷积层\n        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)\n \n    def forward(self, x):\n        p1 = F.relu(self.p1_1(x))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n        p4 = F.relu(self.p4_2(self.p4_1(x)))\n        return torch.cat((p1, p2, p3, p4), dim=1)  # 在通道维上连结输出\n```\n\n**GlobalAvgPool2d与FlattenLayer**\n\n```python\nclass GlobalAvgPool2d(nn.Module):\n    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n    def forward(self, x):\n        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n    \nclass FlattenLayer(torch.nn.Module):\n    def __init__(self):\n        super(FlattenLayer, self).__init__()\n    def forward(self, x): # x shape: (batch, *, *, ...)\n        return x.view(x.shape[0], -1)\n```\n\n**GoogLeNet实现**\n\n```python\nclass GoogLeNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(GoogLeNet, self).__init__()\n        \n        self.b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n                           nn.ReLU(),\n                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        self.b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n                           nn.Conv2d(64, 192, kernel_size=3, padding=1),\n                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        self.b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n                           Inception(256, 128, (128, 192), (32, 96), 64),\n                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        self.b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n                           Inception(512, 160, (112, 224), (24, 64), 64),\n                           Inception(512, 128, (128, 256), (24, 64), 64),\n                           Inception(512, 112, (144, 288), (32, 64), 64),\n                           Inception(528, 256, (160, 320), (32, 128), 128),\n                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        self.b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n                           Inception(832, 384, (192, 384), (48, 128), 128),\n                           GlobalAvgPool2d())\n        self.output=nn.Sequential(FlattenLayer(),\n                                  nn.Dropout(p=0.4),\n                                  nn.Linear(1024, 1000))\n        \n        def forward(self, x):\n            x=b1(x)\n            x=b2(x)\n            x=b3(x)\n            x=b4(x)\n            x=b5(x)\n            x=output(x)\n            return x\n```\n**测试输出**\n\n```python\nnet = GoogLeNet()\nX = torch.rand(1, 3, 224, 224)\n# 可以对照表格看一下各层输出的尺寸\nfor blk in net.children(): \n    X = blk(X)\n    print('output shape: ', X.shape)\n\"\"\"\n# 输出：\noutput shape:  torch.Size([1, 64, 56, 56])\noutput shape:  torch.Size([1, 192, 28, 28])\noutput shape:  torch.Size([1, 480, 14, 14])\noutput shape:  torch.Size([1, 832, 7, 7])\noutput shape:  torch.Size([1, 1024, 1, 1])\noutput shape:  torch.Size([1, 1000])\n\"\"\"\n```\n\n\n\n【参考文档】\n1. [GoogLeNet中的inception结构，你看懂了吗](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029565&idx=1&sn=330e398a4007b7b24fdf5203a5bf5d91&chksm=871345c0b064ccd6dd7d954c90d63f1f3b883c7d487844cbe3424bec3c9abb66625f1837edbd&scene=21#wechat_redirect)\n2. [GoogleNet 论文解析及代码实现](https://mp.weixin.qq.com/s?__biz=MzI1ODEzMDQ3OQ==&mid=2247484578&idx=1&sn=4e41b5304664aa3f8020faec9454747a&chksm=ea0d93e2dd7a1af4556b3852a00961615d0df3605d98dcd8c23acb37b22626455d11e6a90adf&mpshare=1&scene=1&srcid=08253WfHcIkpm3iCO1ICf58x&sharer_sharetime=1598319287371&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=19019e2f1359b2de084edfb643f43e91654d68635120cbf885413ba7afc22639d7757ea0d526f323ce679a5040343d9eb6f254294fee1ea2f991310ee5e25bbe884660b311ce10d0b531a684dd82ac14da5d51f7d6d5a73fc4ca5ec88189890b2fa1c86131d2bd1ec69398222799856c8b6c9890d64acdf0fe0410292c1a2d83&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AVxFjJhoCwO8UI1HylAlShI=&pass_ticket=vFUwXThz6nwuqDXBu8RFZMPKZjXMC0vxTeL29D9KIAj11S9RDOH70u14fPZeWLU0)\n3. [带你快速学会 GoogLeNet 神经网络](https://mp.weixin.qq.com/s?__biz=MjM5MjAwODM4MA==&mid=2650725770&idx=4&sn=34c148f1c5dd80115fea9c38215973cb&chksm=bea6ac5989d1254f8c8015ec27e0647f03099d5989d296fecf0fa5de78f205d3e3170f77e9ef&mpshare=1&scene=1&srcid=0825UnHhIih8vnQgNvChrWBQ&sharer_sharetime=1598319794600&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=b6de4a213a64b7292bca947a43a1bd9735df46270820810bf2af4d69d20c297f9f436989b6c839ca238cf07eb05b11f019b360e19c055a4b568a103b9d929516059f43875a73dd44f83eef760bb9e85c6573653e1a2a81124fcf925747d30ff106c8bdde5229391fea5028c415b8c87debb59134f8fce4f621ea9467cd5419dc&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=Afibu8%2bcZCDn182XsPwKq8g=&pass_ticket=vFUwXThz6nwuqDXBu8RFZMPKZjXMC0vxTeL29D9KIAj11S9RDOH70u14fPZeWLU0)\n4. [卷积神经网络之GoogLeNet](https://mp.weixin.qq.com/s?__biz=MzI0NTM1MzA2Mw==&mid=2247484704&idx=1&sn=6b75e56ee317922b536b9eca119dc06d&chksm=e94e9a28de39133ed340f38e332f064a3c0fe42d81842d93d7bec309222bc3270f49482b3513&mpshare=1&scene=1&srcid=082568MV7VsEDOmvSikHDyM6&sharer_sharetime=1598319942128&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a48589b9f7f2db79c9d66ee5708a8ce35f046f5ea2617868dda3dd5719d48310f524f62e09bd966f8f620da4a78c6b2d2a77c933ce7ea10befa64250913ac4bb5c84423d846f996f3c11eb5e10ca6830c919daaaca0e77e2060d642d0bb739722a44aa9e27ea068df75fb26003f530bda9c9eadfd72210c0fca94&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=ASSsmXagqLOuBFd%2byPqfM6s=&pass_ticket=vFUwXThz6nwuqDXBu8RFZMPKZjXMC0vxTeL29D9KIAj11S9RDOH70u14fPZeWLU0)\n5. [GoogLeNet学习笔记](https://zhuanlan.zhihu.com/p/27124535)\n6. [GoogLeNet模型](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.9_googlenet?id=_592-googlenet%E6%A8%A1%E5%9E%8B)\n7. 深度学习之pytorch计算机视觉-唐进民著\n8. Going Deeper with Convolutions, CVPR 2014\n\n我把GoogLeNet的论文上传到百度云上了，有需要请自提，链接：https://pan.baidu.com/s/1Tcg6-s1pHCE2WZ9TBaQ90A  提取码：ivft","tags":["GoogLeNet"],"categories":["神经网络"]},{"title":"深入解读VGG网络结构（附代码实现）","url":"/2020/11/24/223242/","content":"\nVGGNet由牛津大学的视觉几何组（Visual  Geometry  Group）提出，并在2014年举办的ILSVRC（ImageNet 2014比赛）中获得了定位任务第1名和分类任务第2名的好成绩，（GoogleNet 是2014 年的分类任务第1 名）。虽然VGGNet在性能上不及GoogleNet，但因为VGG结构简单，应用性强，所以很多技术人员都喜欢使用基于VGG 的网络。VGG论文[Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556.pdf)，有兴趣的同学可以看下。\n\n<!-- more -->\n\n\n\n\n**VGG 最大的特点就是通过比较彻底地采用 3x3 尺寸的卷积核来堆叠神经网络，这样也加深整个神经网络的深度。这两个重要的改变对于人们重新定义卷积神经网络模型架构也有不小的帮助，至少证明使用更小的卷积核并且增加卷积神经网络的深度，可以更有效地提升模型的性能。**\n\n\n\nVGG 选择的是在 AlexNet 的基础上加深它的层数，但是它有个很显著的特征就是持续性的添加 3x3 的卷积核。而AlexNet 有 5 层卷积层，从下面的网络结构图我们可以看出来，VGG 就是针对这 5 层卷积层进行改造，共进行了 6 种配置。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824151316119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n这六种配置的参数量：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152012157.png#pic_center)\n\n这六种配置的效果展示图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020082415194940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n从上面的效果图中，我们发现VGG19是最好的。但是，VGG-19 的参数比 VGG-16 的参数多了好多。由于VGG-19需要消耗更大的资源，因此实际中VGG-16使用得更多。而且VGG-16网络结构十分简单，并且很适合迁移学习，因此至今VGG-16仍在广泛使用，下面我们主要来讨论一下VGG16的网络结构，也就是图中的类型D。（VGG19即上面的E类型）\n\nVGG16相比AlexNet的一个改进是采用连续的3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5），如下图中所示，共有13个卷积层，3个全连接层。其**全部采用3\\*3卷积核，步长统一为1，Padding统一为1，和2\\*2最大池化核，步长为2，Padding统一为0**。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152348459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n那么，就让我们一层一层来分析吧！\n\n（1）INPUT层：VGG16卷积神经网络默认的输入数据必须是维度为224×224×3的图像，和  AlexNet一样，其输入图像的高度和宽度均为224，而且拥有的色彩通道是R、G、B这三个。\n（2）**CONV3-64**：使用的卷积核为`(3*3*3)*64`（卷积核大小为3\\*3，输入通道为3，输出通道为64），步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为224，即$224=\\frac{224-3+2}{1}+1$   ，最后输出的特征图的维度为224×224×64。卷积通用公式参考我的博客[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中卷积中的特征图大小计算方式。\n（3）**CONV3-64**：使用的卷积核为`(3*3*64)*64`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为224，即$224=\\frac{224-3+2}{1}+1$   ，最后输出的特征图的维度为224×224×64。\n（4）Max pool：池化核大小为2×2，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为112，即 $112=\\frac{224-2}{2}+1$   ，最后得到的输出的特征图的维度为112×112×64。\n（5）**CONV3-128**：使用的卷积核为`(3*3*64)*128`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为112，即$112=\\frac{112-3+2}{1}+1$   ，最后输出的特征图的维度为112×112×128。\n（6）**CONV3-128**：使用的卷积核为`(3*3*128)*128`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为112，即$112=\\frac{112-3+2}{1}+1$   ，最后输出的特征图的维度为112×112×128。\n（7）Max pool：池化核大小为2×2，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为56，即 $56=\\frac{112-2}{2}+1$   ，最后得到的输出的特征图的维度为56×56×128。\n（8）**CONV3-256**：使用的卷积核为`(3*3*128)*256`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为56，即$56=\\frac{56-3+2}{1}+1$   ，最后输出的特征图的维度为56×56×256。\n（9）**CONV3-256**：使用的卷积核为`(3*3*256)*256`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为56，即$256=\\frac{256-3+2}{1}+1$   ，最后输出的特征图的维度为56×56×256。\n（10）**CONV3-256**：经过`(3*3*256)*256`卷积核，生成featuremap为`56*56*256`。\n（11）Max pool：  经过`（2*2）`maxpool，生成featuremap为`28*28*256`。\n（12）**CONV3-512**：经过`（3*3*256）*512`卷积核，生成featuremap为`28*28*512`。\n（13）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`28*28*512`。\n（14）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`28*28*512`。\n（15）Max pool：经过`（2*2）`maxpool,生成featuremap为`14*14*512`。\n（16）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`14*14*512`。\n（17）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`14*14*512`。\n（18）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`14*14*512`。\n（19）Max pool：经过`2*2`卷积，生成featuremap为`7*7*512`。\n（20）**FC-4096**：输入为`7*7*512`，和AlexNet模型一样，都需要对输入特征图进行扁平化处理以得到1×25088的数据，输出数据的维度要求是1×4096，所以需要一个维度为25088×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×4096。\n（21）**FC-4096**：输入数据的维度为1×4096，输出数据的维度要求是1×4096，所以需要一个维度为4096×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×4096。\n（22）**FC-1000**：输入数据的维度为1×4096，输出数据的维度要求是1×1000，所以需要一个维度为4096×1000的矩阵完成输入数据和输出数据的全连接，最后得到输入数据的维度为1×1000。\n\n上面加粗的层即带有可训练参数的层，共16 weight layers。\n\nVGG16的参数一共是多少呢，现在来计算吧！\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824164009624.png#pic_center)\nVGG16（即上图D）总参数量是138M，具体如下：\n第1层：`1792 = 3*3*3*64+64`\n第2层：`36928 = 3*3*64*64+64`\n第3层：`73856 = 3*3*64*128+128`\n第4层：`147584 = 3*3*128*128+128`\n第5层：`295168 = 3*3*128*256+256`\n第6层：`590080 = 3*3*256*256+256`\n第7层：`590080 = 3*3*256*256+256`\n第8层：`1180160 = 3*3*256*512+512`\n第9层：`2359808 = 3*3*512*512+512`\n第10层：`2359808 = 3*3*512*512+512`\n第11层：`2359808 = 3*3*512*512+512`\n第12层：`2359808 = 3*3*512*512+512`\n第13层：`2359808 = 3*3*512*512+512`\n第14层：`102764544 = 7*7*512*4096+4096`\n第15层：`16781312 = 4096*4096+4096`\n第16层：`4097000 = 4096*1000+1000`\n\n总计：138357544个  （138M）\n\n\n\n\n**总结：**\n\n1. VGG16相比AlexNet的一个改进是采用连续的3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）\n2. 加深结构都使用ReLU激活函数：提升非线性变化的能力\n3. VGG16 **全部采用3\\*3卷积核，步长统一为1，Padding统一为1，和2\\*2最大池化核，步长为2，Padding统一为0**\n4. VGG19比VGG16的区别在于多了3个卷积层，其它完全一样\n5. VGG16基本是AlexNet（AlexNet是8层，包括5个卷积层和3个全连接层）的加强版，深度上是其2倍，参数量大小也是两倍多。\n\n我们已经知道，VGG16相比AlexNet的一个改进是采用连续的3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5），现在，来思考几个问题吧。\n\n\n**Thinking1：使用3x3卷积核替代7x7卷积核的好处？**\n- 2 个 3x3 的卷积核叠加，它们的感受野等同于 1 个 5x5 的卷积核，3 个叠加后，它们的感受野等同于 1 个 7x7 的效果。用2个3x3的卷积核代替原来的 5x5卷积核如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152851501.png#pic_center)\n- 由于感受野相同，3个3x3的卷积，使用了3个非线性激活函数，增加了非线性表达能力，从而可以提供更复杂的模式学习。\n- 使用3x3卷积核可以减少参数，假设现在有 3 层 3x3 卷积核堆叠的卷积层，输出和输出通道数都是C，那么它的参数总数是 3x(3x3xCxC)=27xCxC 。同样和它感受野大小一样的一个卷积层，卷积核是 7x7 的尺寸，假如输出和输出通道数都是C，那么它的参数总数就是 7x7xCxC=49xCxC。而且通过上述方法网络层数还加深了。三层3x3的卷积核堆叠参数量比一层7x7的卷积核参数链还要少。\n- 总的来说，使用3x3卷积核堆叠的形式，既增加了网络层数又减少了参数量。\n\n\n**Thinking2：多少个3x3的卷积核可以替代原来11x11的卷积核？**\n\n(11-1)/2=5，故5个3x3的卷积核可以替代原来11x11的卷积核，即n-11+1=n+(-3+1)\\*5\n\n\n**Thinking3：VGG的C网络结构使用了1x1卷积核，1x1卷积核的主要好处?**\n\n- 使用多个1x1卷积核，在保持feature map 尺寸不变（即不损失分辨率）的前提下，可以大幅增加非线性表达能力，把网络做得很deep。\n- 进行卷积核通道数的降维和升维。\n- 1x1卷积相当于线性变换，非线性激活函数起到非线性作用。\n- 总结就是：1x1 卷积核的好处是不改变感受野的情况下，进行升维和降维，同时也加深了网络的深度。\n\n\n\nVGG16和VGG19都在pytorch封装好了，如下所示：\n\n```python\ntorchvision.models.vgg16(pretrained=False)\ntorchvision.models.vgg19(pretrained=False)\n```\n\n\n代码实现：\n\n```python\nimport torch\nimport torch.nn as nn\n# 带BN层的vgg16\nclass VGG16_bn(torch.nn.Module):\n\n    def __init__(self, num_classes):\n        super(VGG16_bn, self).__init__()\n        \n        self.block_1 = nn.Sequential(\n                nn.Conv2d(in_channels=3,\n                          out_channels=64,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=64,\n                          out_channels=64,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n            \n                nn.MaxPool2d(kernel_size=2,\n                             stride=2)\n        )\n        \n        self.block_2 = nn.Sequential(\n                nn.Conv2d(in_channels=64,\n                          out_channels=128,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(128),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=128,\n                          out_channels=128,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(128),\n                nn.ReLU(inplace=True),\n            \n                nn.MaxPool2d(kernel_size=2,\n                             stride=2)\n        )\n        \n        self.block_3 = nn.Sequential(\n                nn.Conv2d(in_channels=128,\n                          out_channels=256,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=256,\n                          out_channels=256,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=256,\n                          out_channels=256,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(inplace=True),\n            \n                nn.MaxPool2d(kernel_size=2,\n                             stride=2)\n        )\n        \n          \n        self.block_4 = nn.Sequential(\n                nn.Conv2d(in_channels=256,\n                          out_channels=512,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n\n                nn.MaxPool2d(kernel_size=2,\n                             stride=2)\n        )\n        \n        self.block_5 = nn.Sequential(\n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=(3, 3),\n                          stride=(1, 1),\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=(3, 3),\n                          stride=(1, 1),\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=(3, 3),\n                          stride=(1, 1),\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n\n                nn.MaxPool2d(kernel_size=(2, 2),\n                             stride=(2, 2))\n        )\n        \n        #自适应平均池化，见https://www.zhihu.com/question/282046628\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        \n        self.classifier = nn.Sequential(\n                nn.Linear(512 * 7 * 7, 4096),\n                nn.ReLU(True),\n                nn.Dropout(),\n                nn.Linear(4096, 4096),\n                nn.ReLU(True),\n                nn.Dropout(),\n                nn.Linear(4096, num_classes),\n        )\n        \n         \n        # 初始化权重\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # VGG采用了Kaiming initialization\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)            \n    \n        \n    def forward(self, x):\n\n        x = self.block_1(x)\n        x = self.block_2(x)\n        x = self.block_3(x)\n        x = self.block_4(x)\n        x = self.block_5(x) \n        x = self.avgpool(x)\n        # 拉平     \n        x = torch.flatten(x, 1)      \n        x = self.classifier(x)\n        # 一般不用softmax\n        # x = F.softmax(x, dim=1)\n        return x\n```\n\n```python\n# 拿一个数据测试下吧！\nif __name__ == \"__main__\":\n    a=torch.randn(3,224,224)\n    a=a.unsqueeze(0)\n    print(a.size())\n    net = VGG16_bn(10)\n    x=net(a)\n    print(x.size())\n\"\"\"\n# 输出\ntorch.Size([1, 3, 224, 224])\ntorch.Size([1, 10])\n\"\"\"\n```\n\n【参考文档】\n1. [【卷积神经网络结构专题】经典网络结构之VGG(附代码实现)](https://mp.weixin.qq.com/s?__biz=MzU2NDExMzE5Nw==&mid=2247487630&idx=2&sn=77d0bcef8452741823081ac8a0e4ed44&chksm=fc4eaacccb3923da09e61e26f4950a860881b07fb619f8f9d3b0ee61f17a3a76c2cba7aa75e9&scene=158#rd)\n2. [【模型解读】从LeNet到VGG，看卷积+池化串联的网络结构](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029512&idx=1&sn=a46fc10de7daba25694bda75a916aa91&chksm=871345f5b064cce3c16ab3b7c671f9e93c838836e20d0aa91bc83f7879915d0c8318bcd9d187&mpshare=1&scene=1&srcid=0804y5ewJsTSJKBSVaMNmvrm&sharer_sharetime=1596532567452&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a4858b2c5b0a38954321b0f8c3dc80d539d1fc2a0778dc107c45ce2aca8614afc433452f4fd83aba02ddb6a1be7e244027038c09196c4dc62cca07dafbdb87d527756f0a49d5105425e85&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AZ6oMa1fen0omFBd4MdcdrU=&pass_ticket=bfbIoM7yRmV2MGkwcfISFD0R1Uc2EfrGFv2CzbEqH1kNmM/wiobhHOk806C/dvoE)\n3.    [深度学习之基础模型-VGG](https://blog.csdn.net/whz1861/article/details/78111606)\n4. 【深度学习之pytorch计算机视觉】-唐进民著\n\n\n","tags":["VGG"],"categories":["神经网络"]},{"title":"array，list，tensor，Dataframe，Series之间互相转换总结","url":"/2020/11/24/223046/","content":"\n\n本文转载自：[【串讲总结】array, list, tensor，Dataframe，Series之间互相转换总结](https://blog.csdn.net/qq_33431368/article/details/107581604?utm_medium=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight)\n\n## 一、前言\n对于在Deep Learning的学习中总会有几个数据类型的转换，这次想把这些常用的转换做一个总结，方便以后看。\n\n<!-- more -->\n\n\n\n\n\n这些主要包括：`Dataframe、Series(pandas), array(numpy), list, tensor(torch)`\n\n## 二、定义\n\n### 2.1 Dataframe和Series\n\n这里简单介绍一下这两个结构。Dataframe创建的方式有很多种，这里不赘述了。以下举个例子，因为我们这里要讲的是和array等的转换，这里全都用数字型的元素。\n\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9GSXpPRWliOFZRVXBPczdPeU9INkxtSjRrcTdIcksxVVdtV3lRenpleUFQQzRXSDBkc2xDQjdOcDdobldxbllpYk50aWJmYzBSajNMb1pWR0N6cXNEekY1QS82NDA?x-oss-process=image/format,png)\n对于dataframe来说，我们打印出来，结构类似于一个二维矩阵格式，只是每一列和每一个行都有个index，这并且这些结构之间有很多方便的操作，在读入结构化数据的时候尤为方便，所以平时做偏结构化数据的时候， 比如excel、pickle等等，pandas的使用是绕不开的。\n\n而其中的series相当于dataframe的一个元素，如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081320193186.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nSeries只有row index，**有点类似于一个一维向量**。\n\n\n而DataFrame既有行索引也有列索引，它也可以被看做由Series组成的字典（共同用一个索引）\n\n\n### 2.2 array\n\n数组结构是由不同维度的list转换来的，用array的原因主要在于有更多的矩阵操作，数据使用起来更方便，比如转置、矩阵相乘、reshape等等。\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813204916515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n### 2.3 tensor\n张量是在深度学习框架中的一个数据结构，把数据喂进模型中需要把数据转换为tensor结构，等我们再取出来做框架以外的操作，比如保存成文件，用plot画图，都需要重新转换为array或list结构。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813204952507.png)\n## 三、互相转换\n\n先用一个例子直观举例下\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081320501214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813205228508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n## 四、详细总结\n### 4.1 Dataframe到Series\n挑一列的index取出\n\n```python\nSeries = Dataframe['column']\n```\n\n### 4.2 Series到list\n\n```python\nlist = Series.to_list()\n```\n\n### 4.3 list 转 array\n\n```python\narray = np.array(list)\n```\n\n### 4.4 array 转 torch.Tensor\n\n```python\ntensor = torch.from_numpy(array)\n```\n\n### 4.5 torch.Tensor 转 array\n\n```python\narray = tensor.numpy()\n# gpu情况下需要如下的操作\narray = tensor.cpu().numpy()\n```\n\n### 4.6 torch.Tensor 转 list\n\n```python\n# 先转numpy，后转list\nlist = tensor.numpy().tolist()\n```\n\n### 4.7 array 转 list\n\n```python\nlist = array.tolist()\n```\n\n### 4.8 list 转 torch.Tensor\n\n```python\ntensor=torch.Tensor(list)\n```\n\n### 4.9 array或者list转Series\n\n```python\nseries = pd.Series({'a': array})\nseries2 = pd.Series({'a': list})\n```\n\n之后这里的操作就多了，看你具体需求了，也可以多个series拼成一个dataframe, 如下， 其他操作不一一赘述了\n\n```python\ndf = pd.DataFrame({'aa': series, 'bb': series2})\n```\n\n原文链接：[https://blog.csdn.net/qq_33431368/article/details/107581604?utm_medium=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight](https://blog.csdn.net/qq_33431368/article/details/107581604?utm_medium=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight)\n","tags":["python"],"categories":["python"]},{"title":"【超详细】对比10种优化函数BGD、SGD、mini-batch GD、Momentum、NAG、Adagrad、RMSProp、Adadelta、Adam、AMSgrad","url":"/2020/11/24/222922/","content":"\n在实践中常用到一阶优化函数，典型的一阶优化函数包括 BGD、SGD、mini-batch GD、Momentum、Adagrad、RMSProp、Adadelta、Adam 等等，**一阶优化函数在优化过程中求解的是参数的一阶导数**，这些一阶导数的值就是模型中参数的微调值。另外，近年来二阶优化函数也开始慢慢被研究起来，二阶方法因为计算量的问题，现在还没有被广泛地使用。\n\n<!-- more -->\n\n\n\n深度学习模型的优化是一个**非凸函数优化**问题，这是与**凸函数优化**问题对应的。对于凸函数优化，任何局部最优解即为全局最优解。几乎所有用梯度下降的优化方法都能收敛到全局最优解，损失曲面如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811103153430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n而非凸函数优化问题则可能存在无数个局部最优点，损失曲面如下，可以看出有非常多的极值点，有极大值也有极小值。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811103348878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n本文将从原理、公式、代码、loss曲线图、优缺点等方面详解论述：\n@[TOC]( )\n\n\n## 一、BGD/SGD/mini-batch GD\n梯度下降算法主要有BGD、SGD、mini-batch GD，后面还有梯度下降算法的改进，即Momentum、Adagrad 等方法\n### 1.1 BGD\n\n**BGD**（Batch gradient descent，批量梯度下降），是拿所有样本的loss计算梯度来更新参数的，更新公式如下：\n$$\\theta=\\theta-\\eta· \\nabla_\\theta J(\\theta)$$\n\n> 在有的文献中，称GD是拿所有样本的loss计算梯度来更新参数，也就是全局梯度下降，和这里的BGD是一个意思\n\n其中，$\\theta$为要更新的参数，即weight、bias；$\\eta$ 为学习率；$J$为损失函数，即 loss function ；$\\nabla_\\theta J(\\theta)$ 是指对 loss function 的 $\\theta$ 求梯度。\n\n令$\\Delta\\theta_t=-\\eta· \\nabla_\\theta J(\\theta)$，则$\\theta_{t+1}=\\theta_{t}+\\Delta\\theta_t$，$\\theta_{1}$ 即初始化的weight、bias。\n\n写成伪代码如下：\n\n```python\n# all_input和all_target是所有样本的特征向量和label\nfor i in range(epochs):\n    optimizer.zero_grad()\n    output = model(all_input)\n    loss = loss_fn(output,all_target)\n    loss.backward()\n    optimizer.step()\n```\n\n\n\n在loss的等值线图中，随着 weight 的变化loss降低的曲线走向（红线）如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811100747627.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n其中 x1是纵轴，x2是横轴 ；$weight=[x1,x2]$ ，即两个坐标轴对应的点；$X_i=[x1_i,x2_i]$，即weight不同时刻的取值；$X_0=[x1_0,x2_0]$是weight的初始化值。\n\n从上图中可以看出，**BGD的loss曲线走向相对平滑，每一次优化都是朝着最优点走**。\n\n\n由于BGD在每次计算损失值时都是针对整个参与训练的样本而言的，所以会出现**内存装不下，速度也很慢的情况**。能不能一次取一个样本呢？于是就有了随机梯度下降（Stochastic gradient descent），简称 sgd。\n\n### 1.2 SGD\n\n**SGD**（Stochastic gradient descent，随机梯度下降）是**一次拿一个样本的loss计算梯度来更新参数**，其更新公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811105800214.png)\n其中，$x^{(i)}$ 是第一个样本的特征向量，$y^{(i)}$ 是第i个样本的真实值。\n\n也可以写成如下形式：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811110014943.png)\n其中，$g_{t,i}$ 是第i个样本的梯度。\n\n写成伪代码如下：\n\n```python\nfor i in range(epochs):\n    # batch=1,每次从dataset取出一个样本\n    for input_i,target_i in dataset:\n        optimizer.zero_grad()\n        output = model(all_input)\n        loss = loss_fn(output,all_target)\n        loss.backward()\n        optimizer.step()\n```\n\n在loss的等值线图中，随着 weight 的变化loss降低的曲线走向如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811111504701.jpg)\n从上图中可以看出，**SGD的loss曲线走向是破浪式的，相对于BGD的方式，波动大，在非凸函数优化问题中，SGD可能使梯度下降到更好的另一个局部最优解，但从另一方面来讲，SGD的更新可能导致梯度一直在局部最优解附近波动**。\n\n> SGD的不确定性较大，可能跳出一个局部最优解到另一个更好的局部最优解，也可能跳不出局部最优解，一直在局部最优解附近波动\n\n由于同一类别样本的特征是相似的，因此某一个样本的特征能在一定程度代表该类样本，所以SGD最终也能够达到一个不错的结果，但是，SGD的更新方式的波动大，更新方向前后有抵消，存在浪费算力的情况。于是，就有了后来大家常用的小批量梯度下降算法（Mini-batch gradient descent）。\n\n### 1.3 Mini-batch GD\nMini-batch GD（Mini-batch gradient descent，小批量梯度下降）是**一次拿一个batch的样本的loss计算梯度来更新参数**，其更新公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812152612762.png)\n其中，batch_size=n。\n\n写成伪代码如下：\n\n```python\nfor i in range(epochs):\n    # batch_size=n,每次从dataset取n个样本\n    for input,target in dataset:\n        optimizer.zero_grad()\n        output = model(all_input)\n        loss = loss_fn(output,all_target)\n        loss.backward()\n        optimizer.step()\n```\n在loss的等值线图中，随着 weight 的变化loss降低的曲线走向如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812153352680.png)\n\n从上图可以看出，mini-batch GD 的loss走向曲线在BGD和SGD之间，mini-batch GD 既解决了SGD更新方式波动大的问题，又可以尽量去计算多个样本的loss，提高参数的更新效率。\n\n### 1.4 GD法的缺点\n梯度下降算法虽然取得了一定的效果，但是仍然有以下缺点：\n- 学习率大小比较难缺难确定，需要反复去试\n- 学习率不够智能，对每个参数的各个维度一视同仁\n- mini-batch GD算法中，虽然一定程度上提高参数的更新效率，并没有完全解决SGD中的问题，即更新方向仍然前后有抵消，仍然有浪费算力的情况\n- SGD和mini-batch GD由于每次参数训练的样本是随机选取的，模型会受到随机训练样本中噪声数据的影响，又因为有随机的因素，所以也容易导致模型最终得到局部最优解。\n\n\n\n## 二、Momentum/NAG\n知道了GD法的缺点，动量法通过之前积累梯度来替代真正的梯度从而避免GD法浪费算力的缺点，加快更新速度，我们现在来看**动量法**（momentum）和其改进方法NAG吧。\n### 2.1 Momentum\n在使用梯度下降算法的时，刚开始的时候梯度不稳定，波动性大，导致做了很多无用的迭代，浪费了算力，**动量法**（momentum）解决SGD/mini-batch GD中参数更新震荡的问题，**通过之前积累梯度来替代真正的梯度，从而加快更新速度**，其更新公式如下：\n$$\\begin{array}{l}\n\\upsilon_t=\\gamma\\upsilon_{t-1}+\\eta· \\nabla_\\theta J(\\theta)\\\\        \n\\theta=\\theta-\\upsilon_t\n\\end{array}$$\n其中，$\\theta$是要更新的参数即weight，$\\nabla_\\theta J(\\theta)$是损失函数关于weight的梯度，$\\gamma$为动量因子，通常设为0.9；$\\eta$ 为学习率。这里新出现了一个变量 $\\upsilon$，对应物理上的速度。\n也可以写成下面的形式（不常用）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812161157101.png)\n其中，$\\rho$为动量因子，通常设为0.9；$\\alpha$为学习率。\n\n只看公式可能不好理解，我们来代入计算下吧。\n\n假设 $\\eta$ 学习率为0.1，用 g 表示损失函数关于weight的梯度，则可计算如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812164126220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n**动量法**（momentum）更新示意图如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812161702116.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n这样， 每个参数的实际更新差值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内的梯度方向不一致时， 其真实的参数更新幅度变小，增加稳定性； 相反， 当在最近一段时间内的梯度方向都一致时， 其真实的参数更新幅度变大， 起到加速作用。\n\n一般而言， 在迭代初期， 梯度方向都比较一致， 动量法会起到加速作用， 可以更快地到达最优点。**在迭代后期， 梯度方向会不一致， 在收敛值附近振荡， 动量法会起到减速作用， 增加稳定性**。动量法也能解决稀疏梯度和噪声问题，这个到Adam那里会有详细解释。\n\n\n\n### 2.2 NAG\n**NAG**（Nesterov Accelerated Gradient，Nesterov加速梯度）是一种对动量法的改进方法， 也称为 Nesterov 动量法（Nesterov Momentum）。其公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812162552828.png)\n其中，$\\nabla_\\theta J(\\theta-\\gamma\\upsilon_{t-1})$是损失函数关于下一次（提前点）weight的梯度。\n\n也可以写为（不常用）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812164820951.png)\n\n**NAG**更新示意图如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812165251355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n由于momentum刚开始时梯度方向都比较一致，收敛较快，但是到后期，由于momentum惯性的存在，很可能导致在loss极值点的附近来回震荡，而**NAG向前计算了一次梯度，当梯度方向快要改变的时候，它提前获得了该信息，从而减弱了这个过程，再次减少了无用的迭代，并保证能顺利更新到loss的极小值点**。\n\n\n\n\n\n\n## 三、Adagrad/RMSProp/Adadelta\n在神经网络的学习中，学习率$\\eta$的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。\n\n在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”学，然后逐渐“少”学的方法，**在神经网络的学习中经常被使用。逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低**。而AdaGrad进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值，现在我们来看Adagrad/RMSProp/Adadelta吧。\n\n\n\n### 3.1 Adagrad\nAdaGrad（Adaptive Grad，自适应梯度）为参数的每个参数自适应地调整学习率，让不同的参数具有不同的学习率，其公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812173130536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n其中，W表示要更新的权重参数，$\\frac{ {\\partial L} }{ {\\partial W} }$表示损失函数关于W的梯度，$\\eta$表示学习率，这里新出现了变量$h$，它保存了以前的所有梯度值的平方和，$\\odot$表示对应矩阵元素的乘法。然后，在更新参数时，通过乘以 $\\frac{1}{\\sqrt h}$，就可以调整学习的尺度。这意味着，**参数的元素中变动较大（被大幅更新）的元素的学习率将变小。也就是说，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小**。\n\nAdagrad公式也可以写为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812192420580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n其中， $g_\\tau \\in R$ 是第$\\tau$次迭代时的梯度，$\\alpha$为初始的学习率，$\\varepsilon$是为了保持数值稳定性而设置的非常小的常数， 一般取值 $e^{−7}$ 到 $e^{−10}$，此外， 这里的开平方、 除、 加运算都是按元素进行的操作。\n\n由于Adagrad学习率衰减用了所有的梯度，如果在经过一定次数的迭代依然没有找到最优点时，累加的梯度幅值是越来越大的，导致学习率越来越小， 很难再继续找到最优点，为了改善这个问题，从而提出RMSProp算法。\n\n\n### 3.2 RMSProp\n与AdaGrad不同，**RMSProp（Root Mean Square Propagation，均方根传播） 方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度**。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812192541768.png)\n\n\n其中，$\\beta$ 为衰减率， 一般取值为 0.9，$\\alpha$为初始的学习率， 比如 0.001。\n\n从上式可以看出， RMSProp 算法和 AdaGrad 算法的区别在于 $G_t$ 的计算由累积方式变成了指数衰减移动平均。在迭代过程中， 并且，**每个参数的学习率并不是呈衰减趋势， 既可以变小也可以变大**（把$\\beta$设的更小些，每个参数的学习率就呈变大趋势）。\n\n> 这里不得不提一下RProp，Rprop可以看做 RMSProp的简单版，它是依据符号来改变学习率的大小：当最后两次梯度符号一样，增大学习率，当最后两次梯度符号不同，减小学习率。\n\n\n\n### 3.3 AdaDelta \nAdaDelta 与 RMSprop 算法类似， AdaDelta 算法也是通过**梯度平方的指数衰减移动平均来调整学习率**。此外， **AdaDelta 算法还引入了每次参数更新差值Δ𝜃 的平方的指数衰减权移动平均**。\n\n第 t 次迭代时， 参数更新差值 Δ𝜃 的平方的指数衰减权移动平均为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081218193921.png)\nAdaDelta 算法的参数更新公式为：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812192717500.png)\n\n\n其中 $G_t$ 的计算方式和 RMSprop 算法一样 ， $\\Delta X_{t - 1}^2$ 为参数更新差值 Δ𝜃 的指数衰减权移动平均。\n\n从上式可以看出， **AdaDelta 算法将 RMSprop 算法中的初始学习率 𝛽 改为动态计算的 $\\sqrt {\\Delta X_{t - 1}^2}$ ，在一定程度上平抑了学习率的波动**。除此之外，AdaDelta连初始的学习率都不要设置了，提升了参数变化量的自适应能力。\n\n除此之外，AdaDelta公式还有一个常用的表示方法：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812193957164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n## 四、Adam\\AMSgrad\n现在来介绍比较好用的方法Adam和其改进方法AMSgrad。\n### 4.1 Adam\n**Adam** 算法 （Adaptive Moment Estimation Algorithm）可以看作动量法和 RMSprop 算法的结合， **不但使用动量作为参数更新方向， 而且可以自适应调整学习率**。\n\n\nAdam 算法一方面计算梯度平方 $g^2_t$ 的指数衰减移动平均（和 RMSprop 算法类似）, 另一方面计算梯度 $g_t$ 的指数衰减移动平均 （和动量法类似），如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812194548127.png)\n\n\n其中 $\\beta_1$ 和 $\\beta_2$ 分别为两个移动平均的衰减率， 通常取值为 $\\beta_1$ = 0.9，$\\beta_2$ = 0.999。 **我们可以把 $M_t$ 和 $G_t$ 分别看作梯度的均值（一阶矩估计）和未减去均值的方差（二阶矩估计）**。其中，$M_t$ 来自momentum，用来稳定梯度，$G_t$ 来自RMSProp，用来是梯度自适应化。\n\n对$M_t$ 和 $G_t$偏差进行修正：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812210354974.png)\n\n\nAdam 算法的参数更新公式为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081220485436.png)\n\n\n\n其中，其中学习率 $\\alpha $ 通常设为 0.001， 并且也可以进行衰减， 比如 $\\alpha_t=\\frac{\\alpha_{0}}{\\sqrt t}$ 。\n\n\nAdam， 结合了 动量法 和 RMSProp 算法最优的性能，它还是能提供解决**稀疏梯度和噪声问题**的优化方法，在深度学习中使用较多。这里解释一下为什么Adam能够解决**稀疏梯度和噪声问题**：稀疏梯度是指梯度较多为0的情况，由于adam引入了动量法，在梯度是0的时候，还有之前更新时的梯度存在（道理跟momentum一样），还能继续更新；噪声问题是对于梯度来说有一个小波折（类似于下山时路不平有个小坑）即多个小极值点，可以跨过去，不至于陷在里面。\n\n\nAdam 算法是 RMSProp 算法与动量法的结合， 因此一种自然的 Adam 算法的改进方法是引入 Nesterov 加速梯度， 称为**Nadam** 算法\n\n**这里思考一个问题：为什么对Adam偏差进行修正？**\n\n网上没找到好的解释，那来看一下原文吧！\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812211001933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n大致意思是说：刚开始，我们任意初始化了一个 $m_0$（注意：一般$m_0$初始化为0，在Momentum算法中也是），并且根据公式$m_t=\\beta m_{t-1}+(1-\\beta)g_t$更新公式，得到 $m_1$，可以明显的看到，第一步更新严重依赖初始化的$m_0$，这样可能会造成严重的偏差。\n\n为了纠正这个，我们需要移除这个初始化$m_0$（偏置）的影响，例如，可以把$m_1=\\beta m_{0}+(1-\\beta)g_1$中的$\\beta m_{0}$从$m_1$移除（即$m_1-\\beta m_0$），并且除以（$1-\\beta$），这样公式就变为了 $\\hat{m}=\\frac{m_1-\\beta m_0}{1-\\beta}$，当$m_0$=0时，$\\hat{m_t}=\\frac{m_t}{1-\\beta^t}$。同理，对于$G_t$也是如此。\n\n总的来说，在迭代初期，$M_t$ 和 $G_t$的更新严重依赖于初始化的$M_0=0$、$G0=0$，当初始化$M_t$ 和 $G_t$都为0时，$M_t$ 和 $G_t$都会接近于0，这个估计是有问题的，可能会造成严重的偏差（即可能使学习率和方向与真正需要优化的学习率和方向严重偏离），所以，我需要移除这个初始化的$M_0$ 和 $G0$（偏置）的影响， 故需要对偏差进行修正。\n\n\n**上面说了这么多理论，分析起来头头是道，各种改进版本似乎各个碾压 SGD 算法。但是否真的如呢？此外，Adam看起来都这么厉害了，以后的优化函数都要使用Adam吗？**\n\n来看一下下面的实验：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813163800385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n所有方法都采用作者们（原文）的默认配置，并且进行了参数调优，不好的结果就不拿出来了。\n- nesterov 方法，与 sgd 算法同样的配置。\n- adam 算法，m1=0.9，m2=0.999，lr=0.001。\n- rms 算法，rms_decay=0.9，lr=0.001。\n- adagrad，adadelta 学习率不敏感。\n\n\n看起来好像都不如 SGD 算法，实际上这是一个很普遍的现象，各类开源项目和论文都能够印证这个结论。总体上来说，改进方法降低了调参工作量，只要能够达到与**精细调参的 SGD** 相当的性能，就很有意义了，这也是 Adam 流行的原因。但是，**改进策略带来的学习率和步长的不稳定还是有可能影响算法的性能**，因此这也是一个研究的方向，不然哪来这么多Adam 的变种呢。\n\n这里引用一位清华博士举的例子：很多年以前，摄影离普罗大众非常遥远。十年前，傻瓜相机开始风靡，游客几乎人手一个。智能手机出现以后，摄影更是走进千家万户，手机随手一拍，前后两千万，照亮你的美（咦，这是什么乱七八糟的）。但是专业摄影师还是喜欢用单反，孜孜不倦地调光圈、快门、ISO、白平衡……一堆自拍党从不care的名词。技术的进步，使得傻瓜式操作就可以得到不错的效果，但是在特定的场景下，要拍出最好的效果，依然需要深入地理解光线、理解结构、理解器材。优化算法大抵也如此，大家都是殊途同归，只是相当于在SGD基础上增加了各类学习率的主动控制。如果不想做精细的调优，那么Adam显然最便于直接拿来上手。\n\n原文在[Adam那么棒，为什么还对SGD念念不忘 (2)—— Adam的两宗罪](https://zhuanlan.zhihu.com/p/32262540)\n\nAdam那么棒，也不是没有缺点，继续往下面看！\n\n### 4.2 AMSgrad\nICLR 2018 最佳论文提出了 AMSgrad 方法，研究人员观察到 Adam 类的方法之所以会不能收敛到很好的结果，是因为在优化算法中广泛使用的指数衰减方法会使得梯度的记忆时间太短。\n\n在深度学习中，每一个 mini-batch 对结果的优化贡献是不一样的，有的产生的梯度特别有效，但是也一视同仁地被时间所遗忘。\n\n具体的做法是使用过去平方梯度的最大值来更新参数，而不是指数平均。其公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813161546652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n## 五、不同优化函数比较\n\n（1）不同优化函数的loss下降曲线如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812201908590.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（2）不同优化函数在 MNIST 数据集上收敛性的比较（学习率为0.001， 批量大小为 128）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812220337887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（3）不同优化函数配对比较\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS84MC92Mi0zOWM1ZGYxNjczZDk4MzFlOWJjMTBjOWU5YzRmMTJhOF8xNDQwdy5qcGc?x-oss-process=image/format,png)\n横纵坐标表示降维后的特征空间，区域颜色则表示目标函数值的变化，红色是高原，蓝色是洼地。他们做的是配对儿实验，让两个算法从同一个初始化位置开始出发，然后对比优化的结果。可以看到，几乎任何两个算法都走到了不同的洼地，他们中间往往隔了一个很高的高原。这就说明，不同算法在高原的时候，选择了不同的下降方向。\n\n这里参考 [Adam那么棒，为什么还对SGD念念不忘 (3)—— 优化算法的选择与使用策略述](https://zhuanlan.zhihu.com/p/32338983)\n\n## 六、pytorch不同优化函数的定义\n\n这里介绍几种常用优化函数的参数定义和解释\n\n（1）**torch.optim.SGD**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813093551803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n重要参数解释：\n- lr 学习率，用于控制梯度更新的快慢，如果学习速率过快，参数的更新跨步就会变大，极易出现震荡；如果学习速率过慢，梯度更新的迭代次数就会增加，参数更新、优化的时间也会变长，所以选择一个合理的学习速率是非常关键的。\n- momentum 动量因子，介于[0,1]之间，默认为0，一般设为0.9，当某个参数在最近一段时间内的梯度方向（即与动量$\\upsilon$方向）不一致时， 其真实的参数更新幅度变小，增加稳定性； 相反， 当在最近一段时间内的梯度方向都一致时， 其真实的参数更新幅度变大， 起到加速作用。\n- weight_decay 权重衰减（L2惩罚），默认为0，来看下原文吧：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813101331216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n大致意思是说：为了避免过拟合，在这里增加了一个L2惩罚项，推到如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813101959248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n其中，$\\eta$为学习率，而$\\eta\\ \\lambda$ 就是weight_decay，不过pytorch让用户可以自由设置，有了更大的自由度\n- nesterov，布尔类型，默认为False，设为True，可使用NAG动量\n\n**使用方式：**\n```python\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\nfor i in range(epochs):\n    # batch_size=n,每次从dataset取n个样本\n    for input,target in dataset:\n        optimizer.zero_grad()\n        output = model(all_input)\n        loss = loss_fn(output,all_target)\n        loss.backward()\n        optimizer.step()\n```\n（2）**torch.optim.Adagrad**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081310322668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n主要参数详解：\n- lr-decay 学习率衰减，学习率衰减公式如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813103611835.png)\n其中，$lr_i$为第i次迭代时的学习率，$lr_start$为原始学习率，decay为一个介于[0.0, 1.0]的小数。可以看到，decay越小，学习率衰减地越慢，当decay = 0时，学习率保持不变。decay越大，学习率衰减地越快，当decay = 1时，学习率衰减最快。\n- eps，相当于$\\varepsilon$，是为了保持数值稳定性而设置的非常小的常数， 一般取值 $e^{−7}$ 到 $e^{−10}$\n\n（3）**torch.optim.Adam**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813160557807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n- betas，默认值（0.9,0.999），即Adam里的$\\beta_1$、$\\beta_2$\n- amsgrad，是否启用Adam的改进方法AMSgrad，默认为False\n\n这里只介绍了需要注意的几个优化函数，更多函数定义，请查阅[pytorch官方文档](https://pytorch.org/docs/stable/index.html)\n\n\n【参考文档】\n神经网络与深度学习-邱锡鹏著\n有三AI-深度学习视觉算法工程师指导手册\n深度学习入门：基于pytorch的理论与实现-陆宇杰译\n深度学习之pytorch实战计算机视觉-唐进民著\n[optim.sgd学习参数](https://blog.csdn.net/apsvvfb/article/details/72536495?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight)\n\n","tags":["优化函数"],"categories":["神经网络"]},{"title":"在Windows10中使用 Jupyter Notebook 运行C++","url":"/2020/11/24/222744/","content":"\n在Windows10中使用 Jupyter Notebook 运行C++ ！\n\n<!-- more -->\n\n\n\n## 一、安装Linux子系统\n（1）**开启Subsystem-Linux服务**\n\n鼠标右键开始，选中Windows PowerShell(管理员)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080719424473.png)\n输入 `Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux` 开启服务，**需要重启电脑**\n\n（2）**安装Linux 发行版本**\n鼠标右键开始，选中设置->系统->关于，查看当前Windows10版本，需要 16215 之后的版本才能安装Linux子系统，\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807194800357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**打开 Microsoft Store 搜索 Linux，选中Ubuntu**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080719495371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**点击获取：**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807195043302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**大约432M：**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807195111220.png)\n\n（3）**开始初始化Linux**\n\n下载完成后，点击启动，首次启动需要几分钟\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807195315132.png)\n输入账号和密码\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807195530167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n你也可以右键开始，点击运行，输入 `bash` 运行Linux：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807200039210.png)\n**两种方法都可以进入Linux，但是进入的根目录不一样，输入 `pwd` 查看当前的目录**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080720022272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n`/homw/wxl` 是Linux 系统的用户路径，`/mnt/c/Users/wang1` 是Linux 系统挂载的 Windows 盘符，即C盘。\n\n（4）**更新和升级发行版的软件包**\nLinux默认源是国外的站，所以访问速度可能比较慢，换成阿里的源，步骤如下：\n\n1. 备份之前的源 `sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup`\n2. 打开源文件 `sudo vim /etc/apt/sources.list`\n3. 复制中科大源或阿里云源：\n\n**中科大源：**\n```bash\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n```\n**阿里云源：**\n```bash\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n注意：vim，`i` 是编辑模式，可以添加和删除内容，`Esc键` 命令模式（刚进入Vim就是命令模式，命令模式下才可以输入各种命令），`:wq` 是保存并退出。`%d` 删除文件中所有内容的方法：(要切换到命令模式输入)\n\n\n**如下所示**：![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080720164568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n4.输入如下命令更新软件包\n```bash\nsudo apt update\nsudo apt upgrade\n```\n到这里Linux子系统就安装完成了\n\n## 二、安装 Miniconda\n在Linux命令行输入如下命令安装 Miniconda：\n\n```bash\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n```\n也可以复制`https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`到浏览器里下载，除非你能科学上网，否则这里下载特别慢，我把下载好的Miniconda3-latest-Linux-x86_64.sh上传到百度云上了，请自行提取，链接：https://pan.baidu.com/s/16US_Jt-UjYC-ox4Siql5vw 提取码：cc0b\n\n先将下载好的Miniconda3-latest-Linux-x86_64.sh文件手动拷贝到windows  `C:\\Users\\wang1` 目录下（wang1是你的用户名），执行`cp /mnt/c/Users/wang1/Miniconda3-latest-Linux-x86_64.sh  /homw/wxl`就拷贝到了Linux 系统的用户路径下，然后执行 `bash Miniconda3-latest-Linux-x86_64.sh`，就可以安装了，中间需要输入两个`yes`，成功界面如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807212151891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n推荐用Ubuntu启动的方式安装，我用 `bash` 启动的方式安装失败了，然后用这种方式就成功了。\n\n**到这一步，关闭 Ubuntu 重新打开，会发现原来的用户名前面有一个（base），说明安装成功。**\n\n## 三、安装 Jupyter notebook\n输入如下命令安装Jupyter notebook\n\n```bash\npip install jupyter -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com\n```\n注意：我用 `conda install jupyter` 安装之后不能用，我也搞不清楚为什么，大家要留意这个坑。\n\n安装之后，输入 `jupyter notebook` 就可以启动notebook，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808132036301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n复制划线的地址即可访问，可以看到，报错了，但是不影响访问，如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808132158195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n点击右上角new，只有python核，按`Ctrl + Z或Ctrl + C`关闭notebook，现在我们就安装C++内核吧。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808132305885.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n## 四、安装C++内核\n安装c++d的内核插件 `xeus-cling`，这里要用conda安装，pip无法安装。\n\n先设置conda国内源\n\n```bash\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\nconda config --set show_channel_urls yes\n```\n**下面的步骤一定要注意（此处很重要），我就因为这里的坑浪费了一天的时间，没错，就是一天的时间**\n\n\n**本人安装了很多次xeus-cling都因为下载速度太慢而以失败告终，后来发现是因为安装xeus-cling时，我们想通过指定channel加快访问速度时，conda反而会优先访问默认源而非镜像。**\n\n可以通过 `conda config --show` 看到 默认情况下的 `channel_alias` 值是 `https://conda.anaconda.org/` ，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080821212859.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**说明，在conda还是优先访问默认源而非镜像，输入如下命令修改默认源：**\n\n```bash\nconda config --set channel_alias https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n```\n可以看到，修改之后默认源就变了\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212254993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n然后输入下面命令愉快的安装c++内核插件吧，中间需要输入y确认\n\n```bash\nconda install -c conda-forge xeus-cling\n```\n看到这里就安装成功啦\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212454653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n## 五、使用notebook写C++程序\n\n安装c++内核插件后，再次输入`jupyter notebook` 命令，可以看到右上角多了`C++11、C++14、C++17`，这是不同版本的C++。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212624883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n新建一个 C++14 notebook，输入一些 C++ 代码，Shift + Enter 可以得到运行结果，没有报错就大功告成了！\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212749224.png)\n也可以输入`?cout 或 ?std::cout` 查看cout函数的帮助文档，其他的一些用法可以在 [xeus-cling 的文档](https://xeus-cling.readthedocs.io/en/latest/) 中查看\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212933561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n**如果我们需要在某个文件夹中保存下 notebook，在要保存的文件夹下按住 shift 单击右键，选择“在此处打开 Linux shell”，这样打开的 Jupyter notebook 的目录就是该目录，新建的 notebook 也自动保存在了当前目录，而不会在 Linux 系统里。**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808213342832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n按`Ctrl+z`关闭notebook，为notebook安装左侧导航插件吧：\n\n```bash\npip install jupyter_nbextensions_configurator jupyter_contrib_nbextensions -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com \njupyter contrib nbextension install --user\njupyter nbextensions_configurator enable –user\n```\n这样左侧就出现导航啦\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808222228505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n可以再安装 Jupyter lab，比 Jupyter notebook 好看，文件管理也方便，直接安装就可以和 Jupyter notebook 一样使用：\n\n```bash\nconda install -c conda-forge jupyterlab\n\n安装完成后...\n\njupyter lab\n```\n打开 jupyter lab 界面如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808225255441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n**到这里就都大功告成了，留下一个问题，这里为什么报错（如下所示），我还没解决，虽然不影响使用，但是作为强迫症，总想把它去掉，拜托请留言，谢谢了**：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808235538521.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n## 六、Linux常用命令\n\n在linux下，必然要经常和linux打交道，一些简单的linxu命令\n\n1、vim 命令：\n`vim 文件名` 用vim打开文件。vim中，`i` 是编辑模式，可以添加和删除内容，`Esc键` 命令模式（刚进入Vim就是命令模式，命令模式下才可以输入各种命令），`:wq` 是保存并退出。`%d` 删除文件中所有内容的方法：(要切换到命令模式输入)\n\n2、删除文件：\n- `rm 文件名`  \n\n3、删除整个文件夹及文件夹里的文件：\n`rm -rf /var/log/httpd/access` 将会删除/var/log/httpd/access目录以及其下所有文件、文件夹\n说明：\n-r 就是向下递归，不管有多少级目录，一并删除\n-f 就是直接强行删除，不作任何提示的意思\n\n4、拷贝文件：\n\n（1）`cp -rv A B` 拷贝A文件夹到B目录\n（2）如果你正在B目录下,可以这样:  `cp -rv A ./`\n（3）`cp -v A/A1 ./` 拷贝A文件下的A1文件\n\n\n5、赋予权限\n`sudo chmod -R 777` 文件夹\n参数-R是递归的意思\n777表示开放所有权限\n\n`chmod +x 某文件`\n\n如果给所有人添加可执行权限：chmod a+x 文件名；\n如果给文件所有者添加可执行权限：chmod u+x 文件名；\n如果给所在组添加可执行权限：chmod g+x 文件名；\n如果给所在组以外的人添加可执行权限：chmod o+x 文件名；\n\n6、`cd ~` 进入用户根目录\n\n7、查看历史命令\n`history`\n\n8、卸载软件\n`sudo apt-get remove nodejs` 卸载nodejs\n\n9、安装nodejs\n参考 https://blog.csdn.net/qq_41204927/article/details/83537207\n（1）去官网下载最新版nodejs https://nodejs.org/en/download/current/\n（2）卸载已安装的Node和npm，这一点很重要，要不你装好了 node -v 还是原来的版本\n```bash\nsudo apt remove npm  //卸载npm\nsudo apt remove node //卸载node\n\ncd /usr/local/bin   //进入该目录中，若有node或者npm文件，将他删除删除\n```\n\n（3）下载完成后通过XFtp 或者其他类似软件传到服务器上，然后解压到opt目录下\n`tar -xJf /mnt/c/Users/wang1/node-v14.7.0-linux-x64.tar.xz  -C /opt`\n（4）建立链接到 /usr/local/bin/ 目录（但是我试了/usr/bin/成功）\n`sudo ln -s /opt/node-v8.5.0-linux-x64/bin/node /usr/local/bin/node`\n输入`node -v`成功\n`sudo ln -s /opt/node-v14.7.0-linux-x64/bin/node /usr/local/bin/node`或`sudo ln -s /opt/node-v14.7.0-linux-x64/bin/node /usr/bin/node`\n不知道为什么，输入`npm -v`均不成功\n（5）设置淘宝镜像\n```bash\nsudo npm config set registry https://registry.npm.taobao.org   //设置淘宝镜像\nsource ~/.bashrc       //使修改立即生效\n```\n\n【参考文档】\n[在 Win10 中使用 Jupyter notebook 运行 C++ 详细教程](https://blog.csdn.net/qq_20084101/article/details/89494474)\n[Windows 10 安装 Linux 子系统（Windows Subsystem for Linux）](https://blog.csdn.net/qq_20084101/article/details/82316263)\n[windows下使用 Jupyter notebook 运行 C++](https://zhuanlan.zhihu.com/p/84753836)","categories":["工具"]},{"title":"从Inception到Xception，卷积方式的成长之路！","url":"/2020/11/24/222612/","content":"\n2014年Google提出了多尺度、更宽的**Inception**网络结构，不仅比同期的VGG更新小，而且速度更快。Xception则将Inception的思想发挥到了极致，解开了**分组卷积**和大规模应用的序幕。\n\n本文将详细讲述\n- Inception v1的多尺度卷积和Pointwise Conv\n- Inception v2的小卷积核替代大卷积核方法\n- Inception v3的卷积核非对称拆分\n- Bottleneck卷积结构\n- Xception的Depthwise Separable Conv深度可分离卷积\n\n<!-- more -->\n\n\n\n\n\n## 多尺度卷积\nInception 最初提出的版本主要思想是**利用不同大小的卷积核实现不同尺度的感知**，网络结构图如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806113406524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nInception Module基本组成结构有四个成分。1\\*1卷积，3\\*3卷积，5\\*5卷积，3\\*3最大池化。最后对四个成分运算结果进行通道上组合，这就是Inception Module的核心思想：利用不同大小的卷积核实现不同尺度的感知，最后进行融合，可以得到图像更好的表征。\n\n\n使用了多尺度卷积后，我们的网络更宽了，同时也提高了对于不同尺度的适应程度。\n\n## Pointwise Conv\n使用了多尺度卷积后，我们的网络更宽了，虽然提高了对于不同尺度的适应程度，但是计算量也变大了，所以我们就要想办法减少参数量来减少计算量，于是在 **Inception v1** 中的**最终版本**加上了 1x1 卷积核，网络结构图如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806113747780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*图a与图b的区别就是是否使用了 1x1 卷积进行压缩降维*\n\n使用1x1 卷积核主要目的是进行**压缩降维，减少参数量**，这就是**Pointwise Conv**，简称PW。\n\n\n\n举个例子，假如输入的维度是 96 维，要求输出的维度是 32 维，二种计算方式：\n- 第一种：用3x3的卷积核计算，参数量是`3*3*96*32=27648`（为了方便计算，这里忽略偏置bias，后面的计算均如此）\n- 第二种：先用1x1卷积核将输出通道降维到32，参数量是`1*1*96*32=3072`，再用3x3卷积计算输出，参数量是`3*3*32*32=9216`，总的参数量是`3072+9216=12288`\n\n从结果`12288/27648=0.44`可以看到，第二种方式的参数量是第一种方式的0.44倍，大大减少了参数量，加快训练速度。\n\n\n由Inception Module组成的GoogLeNet（Inception V1）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806125450803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n> Inception V1 的主要思想是利用不同大小的卷积核实现了不同尺度的感知，再加上1x1 卷积的大量运用，模型比较精简，比VGG更深但是却更小\n\n也有用**Pointwise Conv**做升维的，在 MobileNet v2 中就使用 Pointwise Conv 将 3 个特征图变成 6 个特征图，丰富输入数据的特征。\n\n## 卷积核替换\n\n就算有了Pointwise Conv，**由于 5x5 卷积核直接计算参数量还是非常大，训练时间还是比较长**，于是Google学习VGGNet的特点，提出了**使用多个小卷积核替代大卷积核的方法**，这就是 **Inception V2**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806130828579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n在Inception V2中，使用两个 3x3 卷积核来代替 5x5 卷积，不仅使参数量少了，深度也变深了，提升了神经网络的效果，可谓一举多得。\n> 为什么提升了网络深度，可以提升神经网络效果？\n> 因为多层非线性层(每一层都加了relu)可以提供更复杂的模式学习，而且参数量更少 => 采用堆积的小卷积核优于采用大卷积核（相同感受野的情况下）\n\n现在来计算一下参数量感受下吧！\n\n假设输入 256 维，输出 512 维，计算参数量：\n- 使用 **5x5 卷积核**，参数量为`5*5*256*512=3276800`\n- 使用**两个 3x3 卷积核**，参数量为`3*3*256*256+3*3*256*512=1769472`\n\n\n从结果`1769472/3276800=0.54`可以看到，第二种方式的参数量是第一种方式0.54倍，大大的减少了参数量，加快训练速度。\n\n## 卷积核拆分\n\n在使用多个小卷积核替代大卷积核的方法后，参数量还是比较大，于是Google学习Factorization into small convolutions的思想，在Inception V2的基础上，将一个二维卷积拆分成两个较小卷积，例如将7\\*7卷积拆成1\\*7卷积和7\\*1卷积，这样做的好处是降低参数量。该paper中指出，**通过这种非对称的卷积拆分比对称的拆分为几个相同的小卷积效果更好**，可以处理更多，更丰富的空间特征。这就是**Inception V3**网络结构：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080613330515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n让我们计算下参数量感受下吧！\n\n假设输入 256 维，输出 512 维，计算参数量：\n- 使用 **5x5 卷积核**，参数量为`5*5*256*512=3276800`\n- 先使用**两个 1x5和5x1 卷积核**，参数`1*5*256*256+5*1*256*512=983040`\n\n从结果`983040/3276800=0.3`可以看到，第二种方式的参数量是第一种方式0.3倍，比使用多个小卷积核替代大卷积核的方法减少还多。\n\nInception V4考虑到借鉴了微软的ResNet网络结构思想，等以后再做详细介绍。\n\n## Bottleneck\n我们发现使用上面的结构和方法，参数量还是较大，于是人们提出了 **Bottleneck** 的结构降低参数量。\n\n Bottleneck结构分三步走，首先用Pointwise Conv进行降维，再用常规卷积核进行卷积，最后使用Pointwise Conv进行进行升维，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806135210339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n来吧，又到了计算参数量的时刻！\n\n假设输入 256 维，输出 512 维，计算参数量：\n- 使用 **3x3 卷积核**，参数量为`3*3*256*256=589824`\n- 使用 **Bottleneck** 的方式，先使用**1x1卷积核**将输入的256维讲到64维，再使用**3x3卷积核**进行卷积，最后用**1x1卷积核**将64升到256维，参数量为`1*1*256*64+3*3*64*64+1*1*64*256=69632`\n\n从结果`69632/3276800=0.12`可以看到，第二种方式的参数量是第一种方式0.12倍，参数量降得令人惊叹！\n\n\n## Depthwise Separable Conv\n人们发现上面的方法参数量还是不少啊，于是又提出了**Depthwise Separable Conv**（深度可分离卷积），这就是大名鼎鼎的**Xception**的网络结构。\n\n\n\nDepthwise Separable Conv的核心思想是**首先经过1\\*1卷积，即Pointwise Convolution（逐点卷积），然后对每一个通道分别进行卷积，即Depthwise Conv（深度卷积）**，这就是**Xception**，即**Extreme Inception**。\n\n\n\n\n我们来回顾一下从Inception到Xception的过程：\n\n（1）典型的Inception结构（Inception V2）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806211303167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（2）简单的Inception结构：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806211439767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（3）对简单Inception结构进行严格等价变形的Inception结构：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806211608765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（4）极端的Inception结构（Extreme Inception），即Xception（Depthwise Separable Conv）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806211840236.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\nXception Net的主题结构是以Separable Conv+relu为**基本模块**，再加上1x1卷积作为跳层连接，结构如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806151901810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n现在我们来对比一下，计算参数量吧！\n\n一般的卷积如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806142140252.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n上图，输入通道2，输出通道3，卷积核大小3x3，参数量为`3*3*2*3=54`\n\nDepthwise Separable Conv如下：\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904164732940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n输入通道2，先进行经过1\\*1卷积，输出通道为3，参数量：`1*1*2*3=6`，再对这三个通道分别进行卷积，即进行Depthwise Conv（深度卷积），参数量：`3*3*3=27`，总的参数量为`6+27=33`\n\n\n\n从结果`33/54=0.61`可以看到，第二种方式的参数量是第一种方式0.61倍，如果有更多卷积核对不同通道进行卷积，则参数量降低的效果更明显。\n\n> 需要注意的是，Xception里面的Depthwise Separable Convolution是先PW，后DW。而MobileNet里面的Depthwise Separable Convolution是先DW，后PW，这个在我后面的博客MobileNet里面会有详细介绍，并计算量这两种方式的参数量和性能。\n\n## Suummary\n- Inception v1的多尺度卷积利用不同大小的卷积核实现不同尺度的感知，可以得到图像更好的表征。\n- Inception v1的Pointwise Conv利用1x1卷积核进行压缩降维，减少参数量，使模型更加精简。\n- Inception v2使用多个小卷积核替代大卷积核的方法，不仅使参数量少了，深度也变深了，提升了神经网络的效果。\n- Inception v3的卷积核非对称拆分不仅可以降低参数量，而且可以处理更多，更丰富的空间特征。\n- Bottleneck卷积结构分三步走，参数量降得令人惊叹！\n- Xception的Depthwise Separable Conv首先经过PW，然后DW，再度减少参数量，使分组卷积这样的思想被广泛用于设计性能高效的网络。\n\n基于Xception的网络结构MobileNets构建了轻量级的28层神经网络，成为了移动端上的高性能优秀基准模型；Resnet的残差连接直接skip connect，解决了深层网络的训练问题；可变形卷积 deformable convolution network 通过可变的感受野提升了CNN对具有不同几何形变物体识别能力的模型；DenseNet密集连接网络，把残差做到了极致，提高了特征的利用率；非局部神经网络转换一种思维，采用Non-Local连接，让神经网络具有更大的感受视野；多输入网络可以输入多张图片来完成一些任务；3D卷积虽然带来了暴涨的计算量，但是可以用于视频分类和分割；RNN和LSTM用于处理非固定长度或者大小的视频，语音等，更加适合用来处理这些时序信号；非生成对抗网络GAN已从刚开始的一个生成器一个判别器发展到了多个生成器多个判别器等各种各样的结构......人类的探索历程永无止境，未来必然有更加优秀的卷积方式和CNN架构出现，加油吧，后浪！\n\n\n【参考文档】\n[1] [GoogLeNet中的inception结构，你看懂了吗](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029565&idx=1&sn=330e398a4007b7b24fdf5203a5bf5d91&chksm=871345c0b064ccd6dd7d954c90d63f1f3b883c7d487844cbe3424bec3c9abb66625f1837edbd&scene=21#wechat_redirect)\n[2] [总结12大CNN主流模型架构设计思想](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649031450&idx=1&sn=3f7f159458e5f1621531ee107be11a98&chksm=8712bd67b0653471c052e32b9d18a26f4d6a07852f56b50f6589b3baa7a46e1e44f302204a4e&mpshare=1&scene=1&srcid=0804G8KDpyT32uItY7tg5ELC&sharer_sharetime=1596532639265&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a4858a01a73460ab9243614f9e6d48f8d78ffc0a9c2f26b5f667bf5a4c95bda3770b77eac3a83af0df071fc66fe56a156ae26d89306dd2ae948ff43fc09a2489c9bc7df445f1a3adb3bbe&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AaY04ggBFXJJbqFKz4uc9W0=&pass_ticket=bfbIoM7yRmV2MGkwcfISFD0R1Uc2EfrGFv2CzbEqH1kNmM/wiobhHOk806C/dvoE)\n[3] [对于xception非常好的理解](https://www.jianshu.com/p/4708a09c4352)\n[4] Chollet F.  Xception: DeepLearningwithDepthwiseSeparableConvolutions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Xception"],"categories":["神经网络"]},{"title":"Alexnet网络结构逐层详细分析+代码实现","url":"/2020/11/24/222439/","content":"\n在2012年Imagenet比赛冠军—Alexnet （以第一作者Alex命名）直接刷新了ImageNet的识别率，奠定了深度学习在图像识别领域的优势地位。网络结构如下图：\n\n<!-- more -->\n\n\n\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080609545260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n下面分别对每层进行介绍：\n（1）**Input** 层：为输入层，AlexNet卷积神经网络默认的输入数据必须是维度为224×224×3的图像，即输入图像的高度和宽度均为224，\n色彩通道是R、G、B三个。\n（2）**Conv1** 层：为AlexNet的第1个卷积层，使用的卷积核为`(11*11*3)*96`（卷积核大小为11\\*11，输入通道为3，输出通道为96），步长为4，Padding为2。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为55，即$55=\\frac{224-11+4}{4}+1$   ，最后输出的特征图的维度为55×55×96。卷积通用公式参考我的博客[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中卷积中的特征图大小计算方式。\n（3）**MaxPool1** 层  ：为AlexNet的第1个最大池化层，池化核大小为3×3，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为27，即 $27=\\frac{55-3}{2}+1$   ，最后得到的输出的特征图的维度为27×27×96。\n（4）**Norm1** 层：为AlexNet的第1个归一化层，即**LRN1**层，local_size=5（相邻卷积核个数设为5），输出的特征图的维度为27*27*96。\n（5）**Conv2** 层  ：为AlexNet的第 2个卷积层，使用的卷积核为`(5*5*96)*256`，步长为1，Padding为2。通过套用卷积通用公式，可以得\n到最后输出的特征图的高度和宽度均为27，即 $27=\\frac{27-5+4}{1}+1$   ，最后得到输出的特征图的维度为27×27×256。\n（6）**MaxPool2** 层  ：为AlexNet的第2个最大池化层，池化核大小为为3×3，步长为2。通过套用池化通用公式，可以得到最后\n输出的特征图的高度和宽度均为13，即  $13=\\frac{27-3}{2}+1$ ，最后得到输出的特征图的维度为13×13×256。\n（7）**Norm2** 层：为AlexNet的第2个归一化层，即**LRN2**层，local_size=5（相邻卷积核个数设为5），输出的特征图的维度为13×13×256。\n（8）**Conv3** 层  ：为AlexNet的第3个卷积层，使用的卷积核`(3*3*256)*384`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为13，即   $13=\\frac{13-3+2}{1}+1$，最后得到特征图的维度为13×13×384。\n（9）**Conv4** 层  ：为AlexNet的第4个卷积层，使用的卷积核为`(3*3*384)*384`，步长为1，Padding为1。通过套用卷积通用公式，可以得\n到最后输出的特征图的高度和宽度均为13，即 $13=\\frac{13-3+2}{1}+1$   ，最后得到输出的特征图的维度为13×13×384。\n（10）**Conv5** 层  ：为AlexNet的第5个卷积层，使用的卷积核为`(3*3*384)*256`，步长为1，Padding为1。通过套用卷积通用公式，可以得\n到最后输出的特征图的高度和宽度均为13，即 $13=\\frac{13-3+2}{1}+1$，最后得到输出的特征图的维度为13×13×256。\n（11）**MaxPool3** 层  ：为AlexNet的第3个最大池化层，池化核大小为为3×3，步长为2。通过套用池化通用公式，可以得到最后\n输出的特征图的高度和宽度均6，即 $6=\\frac{13-3}{2}+1$，最后得到输出的特征图的维度为6×6×256。\n（12）**FC6** 层  ：为AlexNet的第1个全连接层，输入的特征图的维度为6×6×256，首先要对输入的特征图进行扁平化处理，将其变成维度为1×9216的输入特征图，因为本层要求输出数据的维度是1×4096，所以需要一个维度为9216×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×4096。\n（13）**Dropout6** 层：在训练的时候以1/2概率使得隐藏层的某些神经元的输出为0，这样就丢掉了一半节点的输出，反向传播的时候也不更新这些节点，输出的特征图的维度为1×4096。\n（14）**FC7** 层  ：为AlexNet的第2个全连接层，输入数据的维度为1×4096，输出数据的维度仍然是1×4096，所以需要一个维度为4096×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度依旧为1×4096。\n（15）**Dropout7** 层：在训练的时候以1/2概率使得隐藏层的某些神经元的输出为0，这样就丢掉了一半节点的输出，反向传播的时候也不更新这些节点，输出的特征图的维度为1×4096(这些神经元还存在，只是置为0了，因此输出维度不变)。\n（16）**FC8** 层  ：为AlexNet的第3个全连接层，输入数据的维度为1×4096，输出数据的维度要求是1×1000，所以需要一个维度为4096×1000的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×1000。\n\n**总结**：\n1. 网络比LeNet更深，包括5个卷积层和3个全连接层。\n2. 使用relu激活函数，收敛很快，解决了Sigmoid在网络较深时容易出现梯度消失(或梯度弥散)的问题。\n3. 加入了dropout层，防止过拟合。\n4. 使用了LRN归一化层，通过在相邻卷积核生成的feature map之间引入竞争，从而有些本来在feature map中显著的特征在A中更显著，而在相邻的其他feature map中被抑制，这样让不同卷积核产生的feature map之间的相关性变小，从而增强了模型的泛化能力。\n5. 使用裁剪翻转等操作做数据增强，增强了模型的泛化能力。预测时使用提取图片四个角加中间五个位置并进行左右翻转一共十幅图片的方法求取平均值，这也是后面刷比赛的基本使用技巧。\n6. .分块训练，当年的GPU没有这么强大，Alexnet创新地将图像分为上下两块分别训练，然后在全连接层合并在一起。\n7. 总体的数据参数大概为240M。\n\n\n**代码实现：**\n\n```python\nclass AlexNet(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n```\n\n参考文档\n深度学习之Pytorch实战计算机视觉[唐进民著]\n[从LeNet到VGG，看卷积+池化串联的网络结构](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029512&idx=1&sn=a46fc10de7daba25694bda75a916aa91&chksm=871345f5b064cce3c16ab3b7c671f9e93c838836e20d0aa91bc83f7879915d0c8318bcd9d187&mpshare=1&scene=1&srcid=0804y5ewJsTSJKBSVaMNmvrm&sharer_sharetime=1596532567452&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a4858b2c5b0a38954321b0f8c3dc80d539d1fc2a0778dc107c45ce2aca8614afc433452f4fd83aba02ddb6a1be7e244027038c09196c4dc62cca07dafbdb87d527756f0a49d5105425e85&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AZ6oMa1fen0omFBd4MdcdrU=&pass_ticket=bfbIoM7yRmV2MGkwcfISFD0R1Uc2EfrGFv2CzbEqH1kNmM/wiobhHOk806C/dvoE)","tags":["Alexnet"],"categories":["神经网络"]},{"title":"深入理解ReLU、Leaky ReLU、 PReLU、ELU、Softplus","url":"/2020/11/24/222236/","content":"\n## ReLU\n**ReLU**（Rectified Linear Unit，修正线性单元），也叫Rectifier 函数，它的定义如下：\n\n<!-- more -->\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805112618602.png)\n\nRelu可以实现**单侧抑制**（即把一部分神经元置0），能够稀疏模型， Sigmoid 型活tanh激活函数会导致一个非稀疏的神经网络，而Relu大约 50% 的神经元会处于激活状态，具有很好的稀疏性。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805144836975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\nRelu函数右侧线性部分梯度始终为1，具有 **宽兴奋边界的特性** （即兴奋程度可以非常高），不会发生神经网络的梯度消失问题， 能够加速梯度下降的收敛速度。而tanh和sigmoid在离0点近的时候梯度大，在远离0点的时候梯度小，容易出现梯度消失。\n\n>  在生物神经网络中， 同时处于兴奋状态的神经元非常稀疏． 人脑中在同一时刻大概只有 1% ∼ 4% 的神经元处于活跃状态\n\n**Relu的缺点**：ReLU 函数不是在0周围， 相当于给后一层的神经网络引入**偏置偏移**，会影响梯度下降的效率。另外，在训练时， 如果参数在一次不恰当的更新后， 某个 ReLU 神经元输出为0，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活， 这种现象称为**死亡 ReLU 问题** （Dying ReLU Problem）\n\n>ReLU 神经元指采用 ReLU 作为激活函数的神经元。\n\n\n\n## Leaky ReLU\n**Leaky ReLU**（带泄露的 ReLU ）在输入 $x < 0$ 时， 保持一个很小的梯度 $\\gamma$． 这样当**神经元输出值为负数**也能有一个非零的梯度可以更新参数， 避免永远不能被激活，Leaky ReLU定义如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805151056568.png)\n其中 $\\gamma$ 是一个很小的常数， 比如 0.01． 当 $\\gamma$ < 1 时， Leaky ReLU 也可以写为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805151239226.png)\n$max(0.1x,x)$的图像如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805152201645.png)\n\n##  PReLU\n**PReLU**（Parametric ReLU， PReLU，即带参数的 ReLU）引入一个可学习的参数， **不同神经元可以有不同的参数**。 对于第 𝑖 个神经元，ReLU 的定义为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805151431635.png)\n$\\gamma_i$是$x \\le 0$时的梯度， 如果$\\gamma_i$ = 0， 那么PReLU 就退化为 ReLU． 如果 $\\gamma_i$ 为一个很小的常数， 则 PReLU 可以看作Leaky ReLU。 PReLU 可以允许不同神经元具有不同的参数， 也可以一组神经元共享一个参数。\n\n## ELU \n**ELU**（Exponential Linear Unit， 指数线性单元）定义为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805152305703.png)\n其中 $\\gamma$ ≥ 0 是一个超参数，图像如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805152447816.png)\n## Softplus \n**Softplus** 函数 可以看作 Relu 函数的平滑版本，其定义为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805152555701.png)\n**Softplus 函数其导数刚好是 Logistic 函数。 Softplus 函数虽然也具有单侧抑制、 宽兴奋边界的特性， 却没有稀疏激活性，不会稀疏模型。** 图像如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805154409790.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n这几个函数的图像如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805154737405.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n","tags":["激活函数"],"categories":["神经网络"]},{"title":"深入理解dropout(原理+手动实现+实战)","url":"/2020/11/24/222059/","content":"\n在这篇博客中你可以学到\n\n- 什么是dropout\n- dropout为什么有用\n- dropout中的多模型原理\n- 手动实现dropout\n- 在pytorch中使用dropout\n\n\n<!-- more -->\n\n\n\n\n\n当训练一个深度神经网络时， 我们可以随机丢弃一部分神经元（同时丢弃其对应的连接边）来避免过拟合，这种方法就称为**dropout**(丢弃法)。\n\n每次选择丢弃的神经元是随机的．最简单的方法是设置一个固定的概率 p．对每一个神经元都以概率 p 随机丢弃（即置0），其原理如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804220653389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*上图，Bernoulli是概率p的伯努利分布，取值为0或1，即该分布中的每一个元素取值为0的概率是p，$y_{}^{\\left( l \\right)}$表示第 $l$ 个全连接层（或卷积层），x是特征向量*\n\n\n\n在没有使用dropout的情况下，第 $l$ 层的神经元的值经过线性（或卷积）运算后，通过激活函数输出。\n\n如果使用了dropout，第 $l$ 层的神经元的值乘上概率为p的Bernoulli分布，假如第 $l$ 层有10个神经元，那么产生的Bernoulli分布可能是$[0,1,1,0,0,1,0,0,0,1]^T$（相当于以概率p=0.6随机将$l$ 层的神经元置0），然后第 $l$ 层神经元的输出在第 $l+1$ 层经过线性（或卷积）运算后，再通过激活函数输出\n\n\n\n在train阶段，**dropout的本质通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。比如，以概率 p=0.6 随机将神经元置0，就相当于在10个神经元选4个神经元输出(4个神经元在工作，另外6神经元置0)，这时我们就相当于训练了 $C_{10}^4$ 个模型，只是每个模型的参数量更少了**（这也就是集成学习的思想）。使用了dropout的神经网络如下图所示：\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804225113464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*左图是一般的神经网络，右图是应用了Dropout的网络，Dropout通过随机选择并删除神经元，停止向前传递信号*\n\n\n>机器学习中经常使用集成学习。所谓集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值。用神经网络的语境来说，比如，准备5个结构相同（或者类似）的网络，分别进行学习，测试时，以这 5 个网络的输出的平均值作为答案。实验告诉们，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，这个集成学习与Dropout 有密切的关系。这是因为可以将Dropout理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例（比如，0.5 等），可以取得模型的平均值。\n\n\n**由于在测试时， 所有的神经元都是可以激活的， 这会造成训练和测试时网络的输出不一致，那么，测试的时候该怎么办呢？**\n\n答案是**测试的时候让每个模型投票来得到结果**。\n\n比如，训练的时候，10个神经元置中6个置为0(p=0.6，相当于训练时随机删除了6个神经元，**只有4个神经元在工作**)，**测试**的时候是用10个神经元来投票，那么每个神经元的权重是0.4（$1- p = 0.4$），操作的方法是将dropout层这10个神经元的值加起来乘以0.4，即每个神经元的值都乘以0.4。\n\n\n\n\n注意，是**所有神经元输出的值**乘以0.4，比如 **10个神经元每次只选一个神经元工作(以概率p=0.9将神经元置0)，就相当于训练了10个模型**，最后这10个神经元的结果都要输出，做法是把这10个神经元的值加起来乘以0.1（测试时），即相当于投票得出了结果。但是输出的个数不能少，该输出几个数还是几个数。\n\n> 有的教材设定保留神经元的概率为p（保留率），即神经元有效的概率是p，那么，测试的时候将dropout层的神经元乘以 p 输出，而这里神经元无效的概率是p（废弃率），因此，测试的时候将dropout层的神经元乘以 1-p 输出\n\n总的来说，对于一个神经网络层  $y = f\\left( {W \\cdot X + b} \\right)$，我们可以引入一个**掩蔽函数mask**使得 $y = f\\left( {W \\cdot ma{\\rm{s}}k(X) + b} \\right)$ ，设神经元**废弃率**为 p ，**掩蔽函数mask**可表示为：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804231847285.png)\n\n\n\n\n\n\n 一般来讲， 对于隐藏层的神经元， 其**废弃率** p = 0.5 时效果最好， 这对大部分的网络和任务都比较有效． 当 p = 0.5 时， 在训练时有一半的神经元被丢弃， 只剩余一半的神经元是可以激活的， 随机生成的网络结构**最具多样性**，比如10个神经元随机删除5个，则可训练的模型有$C_{10}^5$，相当于取了最大值。 对于输入层的神经元， 其保留率通常设为更接近 1 的数， 使得输入变化不会太大。 **对输入层神经元进行丢弃时， 相当于给数据增加噪声， 以此来提高网络的鲁棒性**。\n\n下面我们就来实现dropout吧\n\n**手动实现dropout：**\n\n```python\nimport numpy as np\nclass Dropout:\n    def __init__(self, dropout_ratio=0.5):\n        self.dropout_ratio = dropout_ratio\n        self.mask = None\n    def forward(self, x, train_flg=True):\n        if train_flg:\n            # *为序列解包\n            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n            return x * self.mask\n        else:\n            return x * (1.0 - self.dropout_ratio)\n    def backward(self, dout):\n        return dout * self.mask\n```\n\n\n\n这里的要点是，每次正向传播时，**掩蔽函数**`self.mask`中都会以False的形式保存要删除的神经元(相当于概率p的伯努利分布)。`self.mask`会随机生成和x形状相同的数组，并将值比dropout_ratio大的元素设为True。反向传播时的行为和ReLU相同。也就是说，**正向传播时传递了信号的神经元，反向传播时按原样传递信号；正向传播时没有传递信号的神经元，反向传播时信号将停在那里**。\n\n\n在pytorch中使用dropout，只需要一行`torch.nn.Dropout(p=0.5)`即可，如下所示\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805094501511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)","tags":["dropout"],"categories":["神经网络"]},{"title":"Pytorch离线下载并使用torchvision.models预训练模型","url":"/2020/11/24/221921/","content":"\nPytorch离线下载并使用torchvision.models预训练模型\n\n<!-- more -->\n\n原本直接在IDE中执行`models.alexnet(pretrained=True)`就行了，**但是一直报错，搞得我好不难受**！\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804162456348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n不用说，肯定是**由于网络不好导致模型下载失败**，能不能离线下载完之后在本地直接使用？答案是肯定的\n\n其实步骤很简单(但是对于新手要命)，就和我们下载软件一样，详细步骤如下：\n\n1. 复制需要下载的模型地址，粘贴到浏览器地址栏中下载，各种模型的下载地址如下：\n\n```python\n1. Resnet:\n  model_urls = {\n      'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n      'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n      'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n      'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n      'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n  }\n\n2. inception:\n model_urls = {\n      Inception v3 ported from TensorFlow\n     'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n }\n\n3. Densenet: \n model_urls = {\n     'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n     'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n     'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n     'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n }\n\n4. Alexnet:\n model_urls = {\n     'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n}\n\n5. vggnet:\n model_urls = {\n     'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n     'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n     'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n     'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n     'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n     'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n     'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n     'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n }\n```\n2. 这里以`Alexnet`为例，复制`https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth`浏览器地址栏中下载\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080416345458.png)\n\n大约233M，慢慢下吧，友情提示一下，**在手机上下载每秒几M，在电脑下载每秒几kb，推荐在手机上下载好之后，再发送到电脑上**\n\n\n\n3. 将下载好的文件剪切到**torch缓存文件夹下即可**，windos和linux的torch缓存文件夹分别如下：\n- windows：**C:\\Users\\wang1\\\\.cache\\torch\\checkpoints**\t(wang1是你的电脑用户名)\n- linux：**/root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth**\n\n如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804163926693.png)\n4. 然后执行`models.alexnet(pretrained=True)`，发现OK，大功告成！\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804164034307.png)","tags":["pytorch"],"categories":["pytorch"]},{"title":"python中超级实用的30个内置函数","url":"/2020/11/24/221744/","content":"\n>**python的内置函数是指不需要任何导入，就可以使用的函数**\n\n<!-- more -->\n\n\n\n\n\n\n\n\n\n# max(iterable, *[, default=obj, key=func])\n内置函数max的作用是找到最大值，**对所有容器型数据都可以使用**\n\n- 找到容器的最大值\n\n```python\nIn [311]: a = [1,2,3,3,3,4,2,3]\nIn [312]: max(a)\nOut[312]: 4\n\n# 对于集合，默认是找出键的最大值\nIn [320]: di = {'a':3,'b1':1,'c':4}\nIn [321]: max(di)\nOut[321]: 'c'\n\nIn [316]: max('63447')\nOut[316]: '7'\n# 这里要注意，对于字符串，max是按照第一个字符的Ascii码比较的\nIn [318]: max({'3','10','7'})\nOut[318]: '7'\nIn [319]: max({'3','9','7'})\nOut[319]: '9'\n```\n\n- 找到列表最多重复的元素\n\n```python\nIn [313]: max(a,key=lambda x: a.count(x))\nOut[313]: 3\n```\n\n- 如果容器为0，设置默认值为0\n\n```python\nIn [315]: max([],default=0)\nOut[315]: 0\n```\n\n\n\n\n- 当内置函数max使用`lambda`时，lambda里的x就是容器里面的一个个元素\n\n\n\n```python\n# 找到年龄最大的人\nIn [323]: a = [{'name':'a','age':18,'gender':'male'},{'name':'b','age':20,'gender':'female'}]\n\nIn [324]: max(a,key=lambda x: x['age'])\nOut[324]: {'name': 'b', 'age': 20, 'gender': 'female'}\n```\n\n- 找到每一个列表中第二个元素最大的列表\n\n```python\nIn [325]: lst = [[1,3],[0,6],[4,5]]\nIn [326]: max(lst,key=lambda x: x[1])\nOut[326]: [0, 6]\n```\n\n\n\n# sum(iterable, start=0, /)\n\n- 列表(所有容器型数据均可)求和\n\n\n```python\nIn [327]: a = [1,3,2,1,4,2]\n\nIn [328]: sum(a)\nOut[328]: 13\n\nIn [329]: sum(a,2) # start=2 表示求和的初始值为 2\nOut[329]: 15\n```\n\n\n\n\n\n# sorted(iterable, /, *, key=None, reverse=False)\n\n- 列表排序\n\n```python\nIn [330]: a = [1,2,3,3,4,2,3]\n# 默认从小到大排序\nIn [331]: sorted(a)\nOut[331]: [1, 2, 2, 3, 3, 3, 4]\n# 从大到小排序\nIn [332]: sorted(a,reverse=True)\nOut[332]: [4, 3, 3, 3, 2, 2, 1]\n```\n\n- 按照每一个列表的第二个元素排序\n\n```python\nIn [333]: lst = [[1,4],[3,2]]\nIn [334]: sorted(lst,key = lambda x:x[1])\nOut[334]: [[3, 2], [1, 4]]\n```\n\n\n- 字典排序\n\n```python\n# 对字典直接使用sorted是对键排序\nIn [335]: d = {'a':5,'c':7,'d':6}\nIn [336]: sorted(d)\nOut[336]: ['a', 'c', 'd']\n# 按照字典的值排序，注意是d.items()\nIn [337]: sorted(d.items(),key = lambda x:x[1])\nOut[337]: [('a', 5), ('d', 6), ('c', 7)]\n```\n\n\n\n\n# len(obj, /)\n- 求容器中元素的个数\n\n```python\nIn [340]: dic = {'a':1,'b':3}\nIn [341]: len(dic)\nOut[341]: 2\n\nIn [342]: s = 'abc'\nIn [343]: len(s)\nOut[343]: 3\n```\n\n\n\n\n\n\n\n# pow(x, y, z=None, /)\n\n- x 为底的 y 次幂，如果 z 给出，取余\n\n\n```python\nIn [344]: pow(3, 2)\nOut[344]: 9\n\nIn [345]: pow(3, 2, 4)\nOut[345]: 1\n```\n\n\n\n\n# round(number, ndigits=None)\n\n- 四舍五入，ndigits 代表小数点后保留几位\n\n\n```python\nIn [346]: round(10.0222222, 3)\nOut[346]: 10.022\n\nIn [347]: round(10.02252222, 3)\nOut[347]: 10.023\n\nIn [349]: round(10.0222222)\nOut[349]: 10\n# 默认ndigits为0，返回int\nIn [348]: type(round(10.0222222))\nOut[348]: int\n```\n\n\n\n\n# abs(x, /)\n\n\n```python\nIn [350]: abs(-6)\nOut[350]: 6\n\nIn [351]: abs(-10.223555)\nOut[351]: 10.223555\n```\n\n\n# all(iterable, /)\n- 接受一个迭代器，如果迭代器的所有元素都为真，返回 True，否则返回 False\n\n\n```python\nIn [352]: all([1,0,3,6])\nOut[352]: False\n\nIn [353]: all([1,2,3])\nOut[353]: True\n```\n\n\n#  any(iterable, /)\n\n- 接受一个迭代器，如果迭代器里有一个元素为真，返回 True，否则返回 False\n\n\n```python\nIn [354]: any([0,0,0,[]])\nOut[354]: False\n\nIn [355]: any([0,0,1])\nOut[355]: True\n```\n\n\n\n# bin(number, /)\n\n- 将十进制转换为二进制，返回二进制的字符串\n\n\n```python\nIn [356]: bin(10)\nOut[356]: '0b1010'\n# 0b代表二进制\nIn [357]: 0b1010\nOut[357]: 10\n```\n\n\n\n\n# oct(number, /)\n\n- 将十进制转换为八进制，返回八进制的字符串\n\n\n```python\nIn [358]: oct(12)\nOut[358]: '0o14'\n# 0o代表八进制\nIn [359]: 0o14\nOut[359]: 12\n```\n\n\n\n# hex(number, /)\n\n- 将十进制转换为十六进制，返回十六进制的字符串\n\n\n```python\nIn [360]: hex(15)\nOut[360]: '0xf'\n# 0x代表16进制\nIn [361]: 0xf\nOut[361]: 15\n```\n\n\n# bool(x)\n\n- 如果是一个值，如果为0返回False，如果为1返回True，如果是一个容器，容器为空返回Flase，容器不为空返回True\n\n\n```python\nIn [362]: bool(0)\nOut[362]: False\n\nIn [363]: bool(1)\nOut[363]: True\n\nIn [364]: bool([1,0,0,0])\nOut[364]: True\n\nIn [365]: bool([0])\nOut[365]: True\n\nIn [366]: bool([])\nOut[366]: False\n# 容器中如果是空元素，也返回True\nIn [367]: bool([[],[]])\nOut[367]: True\n\nIn [368]: bool([[]])\nOut[368]: True\n```\n\n\n\n# str(object='')\n\n- 将字符类型、数值类型等转换为字符串类型\n\n\n```python\n# 默认为空\nIn [383]: str()\nOut[383]: ''\nIn [369]: i =123\nIn [370]: str(i)\nOut[370]: '123'\nIn [371]: str(10.235)\nOut[371]: '10.235'\n```\n\n\n\n# ord(c, /)\n\n- 查看某个字符对应的ASCII码，必须传入单个字符串\n\n\n```python\nIn [372]: ord('A')\nOut[372]: 65\n\nIn [374]: ord('0')\nOut[374]: 48\n```\n\n\n\n# dict()、dict(mapping)、dict(iterable)\n\n- 创建数据字典\n- `dict()`，通过构造函数常见字典\n- `dict(mapping)`，通过映射创建字典\n- `dict(iterable)`，通过迭代器创建字典\n\n\n```python\nIn [92]: dict()\nOut[92]: {}\n\nIn [93]: dict(a='a',b='b')\nOut[93]: {'a': 'a', 'b': 'b'}\n\nIn [94]: dict(zip(['a','b'],[1,2]))\nOut[94]: {'a': 1, 'b': 2}\n\nIn [95]: dict([('a',1),('b',2)])\nOut[95]: {'a': 1, 'b': 2}\n```\n\n# object()\n\n- 返回一个根对象，它是所有类的基类\n\n\n```python\nIn [137]: o = object()\nIn [138]: type(o)\nOut[138]: object\n```\n\n- 使用python内置函数dir，返回object的属性、方法列表\n\n\n```python\nIn [379]: dir(o)\nOut[379]: \n['__class__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__']\n```\n\n# int(x, base=10)\n\n`int(x, base =10)`，x 可能为字符串或数值，将 x 转换为一个整数，base是基数\n\n\n\n```python\nIn [380]: int('12')\nOut[380]: 12\n# int没有四舍五入的功能\nIn [382]: int(13.56)\nOut[382]: 13\n```\n\n- `int`的基数base有很大的作用，**可以将任何一个其它进制的数据转化为十进制**\n\n\n```python\n# 将八进制转化为十进制\n# 1*8+2\nIn [384]: int('12',8)\nOut[384]: 10\n# 将16进制转化为10进制\n# 1*16+2\nIn [386]: int('12',16)\nOut[386]: 18\n# 二进制转十进制\nint('0100',2)\n```\n\n\n- 将其它进制的数据转化为十进制的时候，传入的x必须是字符串，否则报错\n\n```python\n# x必须是字符串\nIn [387]: int(12,16)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-387-2153b26001c8> in <module>\n----> 1 int(12,16)\n\nTypeError: int() can't convert non-string with explicit base\n```\n\n\n\n# float(x=0, /)\n\n- 将一个字符串或整数转换为浮点数\n\n\n```python\nIn [390]: float()\nOut[390]: 0.0\n\nIn [388]: float('30.01')\nOut[388]: 30.01\n\nIn [389]: float(30)\nOut[389]: 30.0\n```\n\n\n\n# list(iterable=(), /)\n\n- 将一个容器转化为列表类型\n\n\n```python\nIn [391]: a = {1,2,3}\nIn [392]: list(a)\nOut[392]: [1, 2, 3]\n\nIn [393]: d = {'a':1,'b':2,'c':3}\nIn [394]: list(d)\nOut[394]: ['a', 'b', 'c']\n\nIn [395]: list(d.items())\nOut[395]: [('a', 1), ('b', 2), ('c', 3)]\n```\n\n\n\n# set()、set(iterable)\n\n- `set()`创建一个空的集合对象\n- `set(iterable)`返回一个集合对象，并允许创建后再增加、删除元素。\n\n**集合的一大优点，容器里不允许有重复元素，因此可对列表内的元素去重。**\n\n\n```python\nIn [397]: a=set()\nIn [399]: a.add('a')\nIn [400]: a\nOut[400]: {'a'}\n\nIn [401]: a = [1,2,1,3,4]\nIn [402]: set(a)\nOut[402]: {1, 2, 3, 4}\n```\n\n\n\n\n# slice(stop)、slice(start, stop[, step])\n\n返回一个由 `range(start, stop, step)` 所指定索引集的 slice 对象\n\n\n```python\nIn [403]: a = [1,2,1,3,4,6,7]\n\nIn [404]: a[slice(0,7,2)] #等价于a[0:7:2]\nOut[404]: [1, 1, 4, 7]\nIn [405]: a[0:7:2]\nOut[405]: [1, 1, 4, 7]\n```\n\n\n\n\n# tuple(iterable=(), /)\n- 将一个迭代器转化为元祖\n\n```python\nIn [409]: a = [1,2,3]\nIn [410]: tuple(a)\nOut[410]: (1, 2, 3)\n\nIn [411]: t = tuple(range(1,10,2))\nIn [412]: t\nOut[412]: (1, 3, 5, 7, 9)\n```\n\n\n# type(object)\n\n- 查看对象的类型\n\n\n```python\nIn [413]: type([1,2,3])\nOut[413]: list\n\nIn [414]: type((1,2,3))\nOut[414]: tuple\n\nIn [415]: type({1,2,3})\nOut[415]: set\n\nIn [416]: type({'a':1,'b':2})\nOut[416]: dict\n```\n\n\n\n# zip(*iterables)\n\n- 创建一个迭代器，聚合每个可迭代对象的元素的`元祖`。\n\n- 参数前带 *，意味着是可变序列参数，可传入 1 个，2 个或多个参数。\n\n\n```python\na = [1,2,3]\nfor i in zip(a):\n    print(i)\n\"\"\"\n    (1,)\n    (2,)\n    (3,)\n\"\"\"\nb = ['a','b','c']\nfor m,n in zip(a,b):\n    print(m,n)\n\"\"\"\n    1 a\n    2 b\n    3 c\n\"\"\"\n```\n\n\n# dir([object])　\n\n- 不带参数时，返回当前范围内的变量、方法和定义的类型列表；带参数时返回参数的属性、方法列表\n\n\n```python\nIn [417]: lst = [1,2,3]\nIn [418]: dir(lst)\nOut[418]: \n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n```\n\n\n\n# isinstance(object, classinfo)\n- 判断 object 是否为类 classinfo 的实例，若是，返回 true\n\n\n```python\nclass Node():\n    def __init__(self,value):\n        self.value = value\nnode = Node('node')\nisinstance(node,Node)\n# 输出 True\n```\n\n\n# map(func, *iterables)\n内置函数`map()` 会根据提供的函数对指定序列做映射。\n\n第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表\n\n\n```python\n# 计算平方\ndef square(x) :            \n    return x ** 2\n \nIn [426]: list(map(square, [1,2,3,4,5])) # 计算列表各个元素的平方\nOut[426]: [1, 4, 9, 16, 25]\n\nIn [427]: list(map(lambda x: x ** 2, [1, 2, 3, 4, 5]))  # 使用 lambda 匿名函数\nOut[427]: [1, 4, 9, 16, 25]\n# 提供了两个列表，对相同位置的列表数据进行相加\nIn [428]: list(map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10]))\nOut[428]: [3, 7, 11, 15, 19]\n```\n\n\n# reversed(sequence, /)\n- 逆置序列，返回一个迭代器\n\n```python\nIn [421]: ''.join(reversed('123'))\nOut[421]: '321'\nIn [422]: list(reversed([1,2,3]))\nOut[422]: [3, 2, 1]\n```\n\n\n","tags":["python"],"categories":["python"]},{"title":"python中dict和set的30种操作方法","url":"/2020/11/24/221600/","content":"\n# 字典dict\n\n字典（dict），一种映射对象（mapping）类型，键值对的容器\n\n<!-- more -->\n\n\n\n\n## 创建字典(五种方法)\n\n### 手动创建\n\n\n```python\ndic1 = {} # 创建空字典\ndic2 = {'a':1,'c':3,'e':5}\n```\n\n### 使用 dict() 构造函数\n\n\n```python\ndict() #创建空字典\ndict(a=1,b=2,c=3) # {'a': 1, 'b': 2, 'c': 3}\ndict({'a':1,'b':2},c=3,d=4) \n# {'a': 1, 'b': 2, 'c': 3, 'd': 4}\n```\n\n### 使用可迭代对象\n\n```python\ndict([('a',1),('b',2)],c=3) #  {'a': 1, 'b': 2, 'c': 3}\ndict([('c',3),('d',4),('e',5)]) # {'c': 3, 'd': 4, 'e': 5}\ndict([['c',3],['d',4],['e',5]]) # {'c': 3, 'd': 4, 'e': 5}\ndict((('c',3),('d',4),('e',5))) #  {'c': 3, 'd': 4, 'e': 5}\n```\n\n\n\n\n### 使用fromkeys() 方法\n\n已知键集合（keys），values 为初始值\n\n\n```python\n{}.fromkeys(['k1','k2','k3'],[1,2,3])\n# {'k1': [1, 2, 3], 'k2': [1, 2, 3], 'k3': [1, 2, 3]}\n{'a':1,'b':2}.fromkeys(['c','d'],[1,2])\n# {'c': [1, 2], 'd': [1, 2]}\n```\n\n\n### 使用内置函数zip\n\n\n```python\nlist1 = ['a','b','c','d','e']\nlist2 = [1,3,2,5,6]\ndict(zip(list1,list2))\n# {'a': 1, 'b': 3, 'c': 2, 'd': 5, 'e': 6}\n```\n\n\n\n## 遍历字典\n- d.items()将字典转换为可遍历的列表\n\n```python\nd = {'a':1,'b':2,'c':3}\nfor key, val in d.items():\n    print(key,val)\n\"\"\"\n输出：\na 1\nb 2\nc 3\n\"\"\"\nd.items()\n# dict_items([('a', 1), ('b', 2), ('c', 3)])\n```\n\n下面的写法会报错\n\n```python\nIn [180]: d = {'a':1,'b':2,'c':3}\n\nIn [181]: for key, val in d:\n     ...:     print(key,val)\n     ...: \n---------------------------------------------------------------------------\nValueError: not enough values to unpack (expected 2, got 1)\n```\n\n可以这样写，直接遍历所有的键\n\n```python\nIn [180]: d = {'a':1,'b':2,'c':3}\nIn [183]: for key in d:\n     ...:     print(key)\n     ...: \na\nb\nc\n```\n\n\n## 获取字典所有键集合\n\n\n```python\nd = {'a':1,'b':2,'c':3,'d':4}\n# 方法1\nset(d.keys())\n# 方法2\nset(d)\n# {'a', 'b', 'c', 'd'}\n```\n\n\n## 获取字典所有值集合\n\n\n```python\nlist(d.values())\n# [1, 2, 3, 4]\n```\n\n\n\n## 判断键是否在字典中\n\n```python\nIn [184]: d = {'a':1,'b':2,'c':3,'d':4}\nIn [185]: 'c' in d\nOut[185]: True\n\nIn [186]: 'e' in d\nOut[186]: False\n\nIn [187]: 'e' not in d\nOut[187]: True\n```\n\n\n## 获取某键对应的值\n\n\n```python\nd = {'a':1,'b':2,'c':3,'d':4}\nd.get('c') # 3\n```\n\n\n## 添加或修改一个键值对\n\n\n```python\nIn [189]: d = {'a':1,'b':2,'c':3,'d':4}\n# 添加键值对\nIn [190]: d['e'] = 4\nIn [191]: d\nOut[191]: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 4}\n# 修改键值对\nIn [192]: d['e'] = 'aa'\nIn [193]: d\nOut[193]: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 'aa'}\n```\n\n\n\n## 删除一个键值对\n\n\n```python\nd = {'a':1,'b':2,'c':3,'d':4}\ndel d['d']\nd # {'a': 1, 'b': 2, 'c': 3}\n```\n\n\n## 获取字典视图\n\n字典自带的三个方法 `d.items()、d.keys()、d.values()`，分别返回如下对象：\n\n\n```python\nIn [194]: d = {'a': 1, 'b': 2, 'c': 3}\nIn [195]: d.keys()\nOut[195]: dict_keys(['a', 'b', 'c'])\n\nIn [196]: d.values()\nOut[196]: dict_values([1, 2, 3])\n\nIn [197]: d.items()\nOut[197]: dict_items([('a', 1), ('b', 2), ('c', 3)])\n```\n\n它们都是原字典的视图，修改原字典对象，视图对象的值也会发生改变。\n\n## 键必须是可哈希的对象\n\n**可哈希的对象才能作为字典的键，不可哈希的对象不能作为字典的键，字典的哈希表实现使用从键值计算的哈希值来查找键。**\n\n简要的说可哈希的数据类型，即不可变的数据结构(数字类型（int，float，bool）字符串str、元组tuple、自定义类的对象)。如果一个对象是可哈希的,那么在它的生存期内必须不可变(而且该对象需要一个哈希函数)\n\n**对于不可变类型而言，不同的值意味着不同的内存，相同的值存储在相同的内存**。详情可参考[详解Python中的可哈希对象与不可哈希对象](https://blog.csdn.net/qq_27825451/article/details/102822506)\n\n*同理，不可哈希的数据类型，即可变的数据结构 (字典dict，列表list，集合set)*\n\n\n\n\n\n\n**dict中键必须是可哈希的对象**\n\n\n- list是不可哈希对象\n\n```python\n# list是不可哈希对象\nIn [198]: lst = [1,2]\nIn [199]: d = {lst:'ok'}\n---------------------------------------------------------------------------\nTypeError: unhashable type: 'list'\n```\n\n\n- 元祖是可哈希对象\n\n\n```python\n# 元祖是可哈希对象\nIn [200]: tup = (1,2)\nIn [201]: d = {tup:'ok'}\nIn [202]: d[tup]\nOut[202]: 'ok'\n```\n\n- 自定义类的对象也是可哈希的\n\n```python\nclass Animal:\n    def __init__(self, name):\n        self.name=name\n    def eat(self):\n        print(\"i love eat !\")\n\nan = Animal('123')\nd = {an:'ok'}   \nd[an]\n'''\n'ok'\n'''\n```\n\n\n\n\n## 批量插入键值对\n\n-  使用字典的update方法批量插入键值对\n\n```python\nIn [203]: d = {'a': 1, 'b': 2}\nIn [204]: d.update({'c':3,'d':4,'e':5})\nIn [205]: d\nOut[205]: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n```\n\n\n\n## 字典键值对存在不插入，不存在则插入\n\n如果仅当字典中不存在某个键值对时，才插入到字典中；如果存在，不必插入（也就不会修改键值对），这种情况下，使用字典自带方法 setdefault\n\n```python\nIn [206]: d = {'a':1,'b':2}\n# setdefault返回插入的value值\nIn [207]: r = d.setdefault('c',3) # r: 3\nIn [208]: r\nOut[208]: 3\nIn [209]: d\nOut[209]: {'a': 1, 'b': 2, 'c': 3}\n# 已经存在 'c':3 的键值对，所以 setdefault 时 d 无改变\nIn [210]: r = d.setdefault('c',33)\nIn [211]: r\nOut[211]: 3\nIn [212]: d\nOut[212]: {'a': 1, 'b': 2, 'c': 3}\n```\n\n\n\n## 将两个字典合并\n\n\n```python\nIn [213]: d1 = {'a':1,'b':2}\n\nIn [214]: {*d1}\nOut[214]: {'a', 'b'}\n\nIn [215]: {**d1}\nOut[215]: {'a': 1, 'b': 2}\n\nIn [216]: d2 = {'c':3,'a':2}\n# {**d1,**d2} 实现合并 d1 和 d2，返回一个新字典\nIn [217]: {**d1,**d2} # 后面的key会覆盖前面的\nOut[217]: {'a': 2, 'b': 2, 'c': 3}\n# {*d1,*d2} 只能合并 d1 和 d2 的键\nIn [218]: {*d1,*d2}\nOut[218]: {'a', 'b', 'c'}\n```\n\n\n\n\n## 求两个字典的差集\n\n\n```python\nd1 = {'a':1,'b':2,'c':3}\nd2 = {'b':2}\nd3 = dict([(k,v) for k,v in d1.items() if k not in d2])\nd3\n#  {'a': 1, 'c': 3}\n```\n\n\n\n## 按字典的key排序\n\n如果直接使用python内置函数sorted，则只是返回key排序后的集合，可以使用`lambda`按字典的key排序\n\n```python\nIn [220]: d = {'a':5,'d':7,'c':6}\n\nIn [221]: sorted(d)\nOut[221]: ['a', 'c', 'd']\n# 注意是d.items()\nIn [222]: sorted(d.items(),key = lambda x:x[0])\nOut[222]: [('a', 5), ('c', 6), ('d', 7)]\n```\n\n\n\n## 按字典的value排序\n\n\n```python\nIn [220]: d = {'a':5,'d':7,'c':6}\nIn [224]: sorted(d.items(),key = lambda x:x[1])\nOut[224]: [('a', 5), ('c', 6), ('d', 7)]\n```\n\n\n## 获取字典最大key的value\n\n\n```python\nIn [225]: d = {'a':3,'c':1,'b':2}\nIn [226]: max_key = max(d.keys())\nIn [227]: max_key\nOut[227]: 'c'\n\nIn [228]: d[max_key]\nOut[228]: 1\n\nIn [229]: (max_key,d[max_key])\nOut[229]: ('c', 1)\n```\n\n\n\n## 获取字典最大value的key\n\n- 获取字典最大value的key可能有很多\n\n```python\n# 获取字典最大value的key可能有很多\nIn [230]: d = {'a':3,'c':3,'b':2}\nIn [231]: max_val = max(d.values())\nIn [232]: max_vals = [(key,val) for key,val in d.items() if d[key]==max_val ]\n\nIn [233]: max_vals\nOut[233]: [('a', 3), ('c', 3)]\n```\n\n\n\n# 集合\n**集合（set）是一个无序的不重复元素序列，不能用列表直接索引的方式获取集合中的元素**\n\n## 创建集合\n可以使用set() 函数创建集合，注意：**创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典**\n\n```python\nIn [234]: s_1 = set()\nIn [235]: s_1\nOut[235]: set()\n\nIn [236]: s_2 = { 'orange',  'pear', 'orange', 'banana'}\nIn [237]: s_2\nOut[237]: {'banana', 'orange', 'pear'}\n```\n\n\n\n## 集合并集\n**集合并集就是将两个集合中的元素合并**\n\n- 方法1，集合转化为列表，使用列表的特性\n\n```python\nset1 = {1,3,5,7} \nset2 = {4,5,6}\nset(list(set1)+list(set2))\n#  {1, 3, 4, 5, 6, 7}\n```\n\n- 方法2，使用集合extend方法\n\n```python\n# list添加多个元素的操作是extend\nIn [279]: s_1.update([1,2,3]) #添加多个元素\nIn [280]: s_1.update({3,4}) # 添加集合\n\nIn [281]: s_1\nOut[281]: {1, 2, 3, 4, 'a'}\n```\n- 方法3，使用`|`运算符\n\n```python\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [268]: a|b\nOut[268]: {'a', 'b', 'c', 'd'}\n```\n\n\n## 集合交集\n\n\n```python\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [267]: a & b\nOut[267]: {'c'}\n```\n\n## 集合差集\n\n```python\n# 集合a与b的差集：在集合a存在，不在集合b存在的元素\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [269]: a-b\nOut[269]: {'a', 'b'}\n```\n\n\n## 集合异或\n\n```python\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [270]: a^b\nOut[270]: {'a', 'b', 'd'}\n```\n\n集合a与b的异或,可以理解为先求出只存在于a的元素集合，在求出只存在于b的元素集合，然后取并集\n\n异或是指相同为1，不同为0\n\n\n\n## 添加元素\n集合添加元素的方法是add，list添加元素的方法是append,insert，**由于集合中的元素是无序的，因此不支持在指定位置添加元素，故没有insert方法**\n\n```python\nIn [274]: s_1={'banana', 'orange', 'pear'}\nIn [275]: s_1.add('apple') #添加一个元素\n\n\nIn [277]: s_1\nOut[277]: {'apple', 'banana', 'orange', 'pear'}\n```\n\n## 移除元素\n- 集合的remove和discard方法都可以移除元素\n\n```python\nIn [282]: s_1 = {'apple', 'banana', 'orange', 'pear'}\nIn [283]: s_1.remove('apple')\n\nInIn [284]: \nIn [284]: s_1\nOut[284]: {'banana', 'orange', 'pear'}\n\nIn [285]: s_1.discard('pear')\nIn [286]: s_1\nOut[286]: {'banana', 'orange'}\n\n```\n注意：***区别*** 来了\n\n\n```python\nIn [287]: 'apple' in s_1\nOut[287]: False\nIn [288]: s_1.remove('apple')\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-288-e8c4186ba334> in <module>\n----> 1 s_1.remove('apple')\n\nKeyError: 'apple'\n\nIn [289]: s_1.discard('apple')\n```\n当删除的元素不存在时\n- remove会报错\n- discard不会报错\n\n**集合的pop方法也可以移除元素，由于set是无序的，故set.pop()是随机删除元素**,也正因此，set没有提供排序函数\n\n所以pop的索引是不能用的，即pop里面不能有参数\n\n```python\nIn [290]: print(s_1)\n{'orange', 'banana'}\n\nIn [291]: s_pop = s_1.pop()\nIn [292]: s_pop\nOut[292]: 'orange'\nIn [293]: s_1\nOut[293]: {'banana'}\n```\n\n**set的pop()是随机删除元素**\n\n\n\n\n\n## 集合元素限制\n\n可哈希的数据类型，即不可变的数据结构(数字类型（int，float，bool）字符串str、元组tuple、自定义类的对象)。如果一个对象是可哈希的,那么在它的生存期内必须不可变(而且该对象需要一个哈希函数)\n\n**对于不可变类型而言，不同的值意味着不同的内存，相同的值存储在相同的内存**。详情可参考[详解Python中的可哈希对象与不可哈希对象](https://blog.csdn.net/qq_27825451/article/details/102822506)\n\n同理，不可哈希的数据类型，即可变的数据结构 (字典dict，列表list，集合set)\n\n**集合中的元素必须是可哈希的数据类型**\n\n- 列表是不可哈希的，将不可哈希的列表放入集合中就会报错，如下：\n\n\n```python\nIn [294]: {[1,2]}\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-294-4a5722e7ef02> in <module>\n----> 1 {[1,2]}\n\nTypeError: unhashable type: 'list'\n```\n\n- 将可哈希的元祖放入列表中，则不会报错：\n\n\n```python\nIn [295]: {(1,2),(1,2,3),(1,2)}\nOut[295]: {(1, 2), (1, 2, 3)}\n```\n\n- bool类型也是可哈希的数据类型\n\n```python\nIn [296]: {True,False,True}\nOut[296]: {False, True}\n```\n\n- 字符串也是可哈希的数据类型\n\n```python\nIn [297]: set1 = {'1','2','1'}\nIn [298]: set1\nOut[298]: {'1', '2'}\n```\n\n- 自定义类的对象也是可哈希的\n\n```python\nclass Animal:\n    def __init__(self, name):\n        self.name=name\n    def eat(self):\n        print(\"i love eat !\")\n        \nIn [306]: an = Animal('狗')\nIn [307]: set1 = {an}\nIn [308]: an.name='猫'\n\nIn [309]: a = set1.pop()\nIn [310]: a.name\nOut[310]: '猫'\n```\n\n\n**set集合也可以用python内置函数list()将其转化为列表，从而使用list的所有操作方法**\n\n\n","tags":["python"],"categories":["python"]},{"title":"Python 中 （&，|）和（and，or）之间的区别","url":"/2020/11/24/221418/","content":"\n`（&，|）` 和 `（and，or）`是两组比较相似的运算符。他们的用法如下：\n\n<!-- more -->\n\n\n\n```text\na & b\na | b\na and b\na or b\n```\n（1）如果a，b是数值变量， 则`&`、`|`表示位运算，`and`、`or`则依据是否非0来决定输出，\n\n- **`&`、`|`**\n\n` 1 & 2`，2在二进制里面是`10`，1在进制中是`01`，那么`01`与运算`10`得到是0\n\n```python\nIn [239]: 1 & 2\nOut[239]: 0\n```\n\n- **`and`、`or`**\n\n`and`运算时，`and`中含0，则返回0，均为非0时，则返回后一个值\n\n```python\nIn [240]: 2 and 0\nOut[240]: 0\n\nIn [241]: 2 and 1\nOut[241]: 1\n\nIn [242]: 1 and 2\nOut[242]: 2\n```\n`or`运算时，至少一个非0时，则返回第一个非0\n\n\n```python\nIn [243]: 2 or 0\nOut[243]: 2\n\nIn [244]: 2 or 1\nOut[244]: 2\n\nIn [245]: 0 or 1\nOut[245]: 1\n```\n\n（2）如何a, b是逻辑变量， 则两类的用法基本一致\n\n```python\nIn [247]: True | False\nOut[247]: True\n\nIn [248]: True & False\nOut[248]: False\n\nIn [249]: True and False\nOut[249]: False\n\nIn [250]: True or False\nOut[250]: True\n```\n\n（3）`&、|` 支持set集合运算\n\n如果a与b是两个set集合，则可以做如下运算：\n\n- a与b的交集\n\n```python\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [267]: a & b\nOut[267]: {'c'}\n```\n\n- a与b的并集\n\n```python\nIn [268]: a|b\nOut[268]: {'a', 'b', 'c', 'd'}\n```\n\n\n除了 `&`、`|` 之外，set集合也支持 `-`、`^` 运算\n\n- a与b的差集：在集合a存在，不在集合b存在的元素\n \n\n```python\nIn [269]: a-b\nOut[269]: {'a', 'b'}\n```\n\n\n- a与b的异或\n\n```python\nIn [270]: a^b\nOut[270]: {'a', 'b', 'd'}\n```\n\na与b的异或,可以理解为先求出只存在于a的元素集合，在求出只存在于b的元素集合，然后取并集\n\n异或是指相同为1，不同为0\n\n\n\n\n【参考文档】\nhttps://www.cnblogs.com/danjiu/p/11332278.html\n","tags":["python"],"categories":["python"]},{"title":"Python中字符串与正则表达式的25个常用操作","url":"/2020/11/24/221218/","content":"\nPython 中没有像 C++ 表示的字符类型（char），所有的字符或串都被统一为 `str` 对象。如单个字符 `c` 的类型也为 `str`，因此在python中**字符也就是字符串**\n\nstr 类型会被经常使用，一些高频用法如下\n\n<!-- more -->\n\n\n\n\n\n# 字符串去空格\n\n- 使用字符串的strip方法去除字符串**开头的结尾**的空格，也可以用字符串的replace方法替换掉所有的空格\n\n```python\nIn [57]: '  I love python  '.strip()\nOut[57]: 'I love python'\nIn [58]: '  I love python  '.replace(' ','')\nOut[58]: 'Ilovepython'\n```\n\n\n\n# 字符串替换\n\n- 使用字符串的replce进行字符替换\n```python\n# 替换所有的字符\nIn [59]: 'i love python'.replace(' ','_')\nOut[59]: 'i_love_python'\n```\n\n# 字符串串联\n- 使用字符串的join方法将**一个容器型中的字符串**联为一个字符串\n\n```python\nIn [60]: '_'.join(['book', 'store','count'])\nOut[60]: 'book_store_count'\nIn [66]: '_'.join(('1','2','3'))\nOut[66]: '1_2_3'\n```\n\n\n# 查找子串位置\n- 使用字符串的find方法返回匹配字符串的**起始位置索引**\n\n```python\nIn [96]: 'i love python'.find('python')\nOut[96]: 7\n```\n\n\n\n# 反转字符串\n- 方法1，使用字符串的join方法和python内置函数将字符串反转\n\n```python\n\nIn [99]: s = \"python\"\nIn [100]: rs = ''.join(reversed(s))\nIn [101]: rs\nOut[101]: 'nohtyp'\n```\n\n- 方法2，利用字符串的切片，只要列表可以使用的切片功能，字符串都可以用\n\n\n\n\n\n```python\nIn [104]: s = \"python\"\nIn [105]: s[::-1]\nOut[105]: 'nohtyp'\n```\n\n\n\n# 字符串切片\n\n- 只要列表可以使用的切片功能，字符串都能用\n\n\n```python\nIn [110]: s = '123456'\nIn [111]: s[1:5]\nOut[111]: '2345'\n\nIn [112]: s[1:5:2]\nOut[112]: '24'\n\nIn [113]: s[::-2]\nOut[113]: '642'\n```\n\n\n\n# 分割字符串\n\n根据指定字符或字符串，分割一个字符串时，使用方法 split。\n\n**join是字符串串联， 和可以把join和split 可看做一对互逆操作**\n\n```python\nIn [114]: 'I_love_python'.split('_')\nOut[114]: ['I', 'love', 'python']\n```\n\n\n# 子串判断\n\n判断 a 串是否为 b 串的子串。\n\n- 方法1，使用成员运算符`in`\n\n```python\nIn [115]: a = 'abc'\nIn [116]: b = 'mnabcdf'\nIn [117]: a in b\nOut[117]: True\n```\n\n- 方法2，使用方法 find，返回字符串 b 中匹配子串 a 的最小索引\n\n  注意：str的find方法与list的index方法用途一样\n\n\n```python\nIn [118]: b.find(a)\nOut[118]: 2\n```\n\n## 去除重复元素\n\n```python\nIn [257]: s = 'abcadcba'\nIn [258]: s = set(s)\nIn [259]: s\nOut[259]: {'a', 'b', 'c', 'd'}\n\nIn [260]: ''.join(s)\nOut[260]: 'bdac'\n```\n\n**得到的结果是乱序的，如果要求和原来的循序一样，勿用此方法**\n\n\n# 正则表达式\n\n字符串封装的方法，处理一般的字符串操作，还能应付。但是，稍微复杂点的字符串处理任务，需要靠正则表达式，简洁且强大。\n\n首先，导入所需要的模块 re\n\n\n```python\nimport re\n```\n\n认识常用的**元字符**：\n\n- `.` 匹配除 \"\\n\" 和 \"\\r\" 之外的任何单个字符。\n- `^` 匹配字符串开始位置\n- `$` 匹配字符串中结束的位置\n- `*` 前面的原子重复 0 次、1 次、多次\n- `?` 前面的原子重复 0 次或者 1 次\n- `+` 前面的原子重复 1 次或多次\n- `{n}` 前面的原子出现了 n 次\n- `{n,}` 前面的原子至少出现 n 次\n- `{n,m}` 前面的原子出现次数介于 n-m 之间\n- `( )` 分组，输出需要的部分\n\n再认识常用的**通用字符**：\n\n- `\\s` 匹配空白字符\n- `\\w` 匹配任意字母/数字/下划线\n- `\\W` 和小写 w 相反，匹配任意字母/数字/下划线以外的字符\n- `\\d` 匹配十进制数字\n- `\\D` 匹配除了十进制数以外的值\n- `[0-9]` 匹配一个 0~9 之间的数字\n- `[a-z]` 匹配小写英文字母\n- `[A-Z]` 匹配大写英文字母\n\n正则表达式，常会涉及到以上这些元字符或通用字符，下面是一些使用方式和小技巧\n\n## search 第一个匹配串\n\n\n```python\nimport re\ns = 'I am a good student'\npat = 'good'\nr = re.search(pat,s)\n# 返回子串的起始位置和终止位置\nr.span()\n# 输出  (7, 11)\n```\n\n\n## match匹配开始位置\n\n正则模块中，match、search 方法匹配字符串不同\n\n- match 在原字符串的开始位置匹配\n- search 在字符串的任意位置匹配\n\n使用match方法的时候，如果子串不是在开始位置则报错\n\n\n```python\nimport re\ns = 'helloworld'\npat = 'wor'\nr = re.match(pat,s)\nr.span()\n# 报错  AttributeError: 'NoneType' object has no attribute 'span'\n```\n把子串`wor`放在最前面，再使用match匹配就不会报错了\n\n\n```python\ns = 'world'\npat = 'wor'\nr = re.match(pat,s)\nr.span()\n# 输出(0, 3)\n```\nmatch场景用的不多，如果只是匹配一个子串的位置，大多数情况下，使用search匹配\n\n\n## finditer匹配迭代器\n\n使用正则模块，finditer 方法，**返回所有子串匹配位置的迭代器**\n\n通过返回的对象 `re.Match`，使用它的方法 span 找出匹配位置\n\n\n```python\nIn [119]: s = 'I am a good student'\nIn [120]: pat = 'a'\nIn [121]: r = re.finditer(pat,s)\nIn [124]: for i in r:\n     ...:     print(i)\n     ...: \n<re.Match object; span=(2, 3), match='a'>\n<re.Match object; span=(5, 6), match='a'>\n```\n\n\n## findall 所有匹配\n\n`findall` 方法能查找出子串的所有匹配。\n\n注意：\n - `findall`是返回所有的匹配\n - `finditer`是返回所有匹配的位置\n\n\n\n```python\ns = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\n```\n\n目标查找出所有所有数字：通用字符 `\\d` 匹配一位数字 [0-9]，`+` 表示匹配数字前面的一个字符 1 次或多次。\n\n不带元字符`+`情况下，输出的是匹配到的一个个数字，这肯定不符合要求\n\n\n```python\nIn [125]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [126]: pat = r'\\d'\nIn [127]: r = re.findall(pat,s)\nIn [128]: print(r)\n['8', '3', '7', '1', '9', '5', '6', '3']\n```\n\n带上元字符`+`\n\n\n```python\nIn [125]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [129]: pat = r'\\d+'\nIn [130]: r = re.findall(pat,s)\nIn [131]: print(r)\n['8371', '9', '56', '3']\n```\n\n`re.findall`返回一个列表，里面包含四个数字，可以看到里面没有小数点，如果我们要找到`9.56`，应该怎么做呢？\n\n## 匹配浮点数和整数\n\n- 元字符`?` 表示前一个字符匹配 0 或 1 次\n- 元字符`.?` 表示匹配小数点（`.`）0 次或 1 次。\n\n匹配浮点数和整数，我们先来看正则表达式：`r'\\d+\\.?\\d+'`，该正则表达式可以理解为：先匹配1个或多个数字，再匹配小数点（`.`）0 次或 1 次，最后匹配1个或多个数字，分解演示如下：\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731172504960.jpg)\n\n```python\ns = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [132]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [133]: pat = r'\\d+\\.?\\d+'\nIn [134]: r = re.findall(pat,s)\nIn [135]: r\nOut[135]: ['8371', '9.56']\n```\n\n\n可以看到，**没有匹配到3**，哪里出错了呢？\n\n出现问题原因：`r'\\d+\\.?\\d+'`， 前面的`\\d+` 表示至少有一位数字，后面的`\\d+`也表示至少有一位数字，因此，整个表达式至少会匹配两位数。\n\n现在将最后的 `+` 后修改为 `*`，表示匹配前面字符 0 次、1 次或多次。如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731172652929.jpg)\n\n\n```python\nIn [132]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [136]: pat = r'\\d+\\.?\\d*'\nIn [137]: r = re.findall(pat,s)\nIn [138]: r\nOut[138]: ['8371', '9.56', '3']\n```\n\n到这里就大功完成了，到这里我们再思考一下，如果把`.?`前面的`\\`去掉以后会怎么样？如下：\n\n\n```python\nIn [132]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [139]: pat = r'\\d+.?\\d*'\nIn [140]: r = re.findall(pat,s)\nIn [141]: r\nOut[141]: ['8371,', '9.56', '3号']\n```\n\n可以看到，匹配到了一个汉字，为什么会出现这种情况呢？因为这里`.?`的含义就变了，之前是把`\\.?`作为一个整体，现在`.?`的含义是附加到`\\d.?`上。\n\n\n`\\d+.?\\d`中`.?`含义如下：\n- `.` 匹配除 `\\n` 和 `\\r` 之外的任何单个字符。\n- 那`.?`就表示匹配`\\n` 和 `\\r` 之外的任何单个字符0 次或 1 次。\n\n我们看下演示图就知道了，`\\d+.?\\d`的演示图如下\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020073117291416.jpg)\n\n\n## 匹配正整数\n\n案例：**写出匹配所有正整数的正则表达式**\n\n我们先来看几个正则表达式即它们的含义\n\n\n前面讲过\n\n- `\\d+` 表示数字出现1次或多次\n- `\\d*` 表示数字出现0次或多次\n\n1. `\\d*` 表示数字出现0次或多次，也就是会匹配所有的字符\n\n\n```python\nIn [142]: s = [2,'我是1','1是我','123',0, 3,'已经9.56秒了',10,-1]\nIn [143]: pat = r'\\d*'\nIn [144]: [i for i in s if re.match(pat,str(i))]\nOut[144]: [2, '我是1', '1是我', '123', 0, 3, '已经9.56秒了', 10, -1]\n```\n\n2. `^\\d*$`,在数字出现0次或多次前加上了开始位置，再后加上了结束位置。即必须是以数字的开头，以数字结尾，才能匹配到\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731173222954.jpg)\n\n```python\nIn [142]: s = [2,'我是1','1是我','123',0, 3,'已经9.56秒了',10,-1]\nIn [145]: pat =r'^\\d*$'\nIn [146]: [i for i in s if re.match(pat,str(i))]\nOut[146]: [2, '123', 0, 3, 10]\n```\n\n\n可以看到，匹配到了0，我们的目的是匹配所有正整数，所以不正确\n\n\n3. `^[1-9]*$` 表示匹配一个 0~9 之间的数字\n\n\n```python\nIn [147]: s = [2,'我是1','1是我','123',0, 3,'已经9.56秒了',10,-1]\nIn [148]: pat =r'^[1-9]*$'\nIn [149]: [i for i in s if re.match(pat,str(i))]\nOut[149]: [2, '123', 3]\n```\n\n可以看到，不能匹配10，因此这个也不正确\n\n4. `^[1-9]\\d*$` 表示匹配一个 0~9 之间的数字并且匹配的数字出现0次或多次\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731173331745.jpg)\n\n\n```python\nIn [150]: s = [2,'我是1','1是我','123',0, 3,'已经9.56秒了',10,-1]\nIn [151]: pat =r'^[1-9]\\d*$'\nIn [152]: [i for i in s if re.match(pat,str(i))]\nOut[152]: [2, '123', 3, 10]\n```\n\n\n\n## re.I 忽略大小写\n\n找出字符串中所有字符 t 或 T 的位置，不区分大小写。\n\n\n```python\ns = 'HELLO，World'\npat = 'L'\nr = re.finditer(pat,s,re.I)\nfor i in r:\n    print(i.span())\n\"\"\"\n输出：\n    (2, 3)\n    (3, 4)\n    (9, 10)\n\"\"\"\n```\n\n​    \n\n\n```python\ns = 'HELLO，World'\npat = 'L'\nr = re.findall(pat,s,re.I)\nr\n# 输出 ['L', 'L', 'l']\n```\n\n\n## re.split分割单词\n\n**正则模块中 split 函数强大，能够处理复杂的字符串分割任务**\n\n对于简单的分割，直接用分隔符\n\n\n```python\nIn [153]: s = 'I-am-a-good-student'\nIn [154]: s.split('-')\nOut[154]: ['I', 'am', 'a', 'good', 'student']\n```\n\n\n但是，对于分隔符复杂的字符串，split 函数就无能为力\n\n如下字符串，可能的分隔符有`, ; - . |`和空格\n\n\n```python\nIn [155]: s = 'I,,,am |  ;a - good.. student'\nIn [156]: s.split('[,;\\-.|]')\nOut[156]: ['I,,,am |  ;a - good.. student']\n```\n\n可以看到，直接使用是区分不开的，所有可以用**正则模块中的split**\n\n`\\s` 匹配空白字符\n\n\n```python\nIn [157]: s = 'I,,,am |  ;a - good.. student'\nIn [158]: w = re.split(r'[,.\\s;\\-|]+',s)\nIn [159]: w\nOut[159]: ['I', 'am', 'a', 'good', 'student']\n```\n\n注意：**re.split默认匹配所有的候选字符**，因此不需要元字符`+`了\n\n但是，如果在字符串前面和后面加多个空格，`\\s`就区分不开了，如下：\n\n```python\nIn [163]: In [157]: s = '  I,,,am |  ;a -     good.. student    '\nIn [164]: w = re.split(r'[,.\\s;\\-|]+',s)\nIn [165]: w\nOut[165]: ['', 'I', 'am', 'a', 'good', 'student', '']\n```\n\n这是，可以用字符串strip方法去除字符串的前后空格\n\n```python\nIn [163]: In [157]: s = '  I,,,am |  ;a -     good.. student    '\nIn [172]: s=s.strip()\nIn [173]: w = re.split(r'[,.\\s+;\\-|]+',s)\nIn [174]: w\nOut[174]: ['I', 'am', 'a', 'good', 'student']\n```\n\n\n## sub 替换匹配串\n\n正则模块，sub 方法，替换匹配到的子串\n\n\n```python\ns = '你好，我是12306'\npat = r'\\d+'\nw = re.sub(pat,'hello',s)\nw\n# 输出 '你好，我是hello'\n```\n\n\n## compile 预编译\n\n如果要用同一匹配模式，做很多次匹配，可以使用 compile 预先编译串\n\n如果我们要：从一系列字符串中，挑选出所有正浮点数\n\n正则表达式为：`^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*$`，字符 `a|b `表示 `a` 串匹配失败后，才执行 `b` 串\n\n\n```python\ns = [-1,10,0,7.21,0.5,'123','你好','3.25',11.0,9.]\nrec = re.compile(r'^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*$')\n[ i for i in s if rec.match(str(i))]\n# 输出 [7.21, 0.5, '3.25', 11.0, 9.0]\n```\n\n\n","tags":["python"],"categories":["python"]},{"title":"python中list和tuple的22种操作方法","url":"/2020/11/24/221035/","content":"\n# 列表list\n\n列表（list）作为 Python 中最常用的数据类型之一，是一个可增加、删除元素的可变（mutable）容器。\n\n<!-- more -->\n\n\n\n\n## 创建列表\n\n\n```python\nlst = [] #创建空列表\nlst1 = [1,'xiaoming',29.5,'17312662388']\nlst2 = [['hello','world'],[12,'abc'],['冰箱','空调']]\n```\n\n## 查看列表长度\n\n\n```python\nlen(lst) # 0\nlen(lst1) # 4\nlen(lst2) # 3\n```\n\n\n## 遍历列表\n\n\n```python\nIn [16]: for t in lst1:\n    ...:     print(t)\n    ...: \n1\nxiaoming\n29.5\n17312662388\n```\n\n\n```python\nIn [17]: for t1,t2 in lst2:\n    ...:     print(t1,t2)\n    ...: \nhello world\n12 abc\n冰箱 空调\n```\n\n​    \n\n## 元素在列表中的位置\n\n\n```python\nIn [18]: ls_3=[1, 3, 12.3, 'apple', 0.001, 'apple', 'orange', 1, 2, 3, 4]\n# 从列表ls_3 中找出\"apple\" 第一个匹配项的索引位置\nIn [19]: ls_3.index(\"apple\")\nOut[19]: 3\n\nIn [20]: ls_3.index(3)\nOut[20]: 1\n```\n\n\n\n\n\n## 列表中增加元素\n- 方法1 ，使用列表的append方法在列表尾部添加元素\n\n```python\nIn [21]: lst = ['abc','123']\nIn [22]: lst.append('efg')\nIn [23]: lst\nOut[23]: ['abc', '123', 'efg']\n```\n- 方法2，使用列表的insert方法插入元素\n```python\nIn [24]: lst = ['abc','123']\nIn [25]: lst.insert(1,'hello')\nIn [26]: lst\nOut[26]: ['abc', 'hello', '123']\n```\n\n\n## 合并列表\n- 方法1，使用列表的extend方法添加另一列表\n\n```python\nls_3 = ['abc','123']\nls_3.extend([1, 2, 3]) \nls_3.extend([4,5])\nprint (ls_3)\n# ['abc', '123', 1, 2, 3, 4, 5]\n```\n- 方法2，直接使用运算符+合并列表\n\n```python\nIn [27]: ls_3 = ['abc','123']\nIn [28]: ls_4 = [4,5]\nIn [29]: ls_3+ls_4\nOut[29]: ['abc', '123', 4, 5]\n```\n\n- 方法3，使用`*`合并列表\n\n```python\nIn [27]: ls_3 = ['abc','123']\nIn [28]: ls_4 = [4,5]\nIn [31]: [*ls_3]\nOut[31]: ['abc', '123']\nIn [32]: [ls_3]\nOut[32]: [['abc', '123']]\n\nIn [33]: [*ls_3,*ls_4]\nOut[33]: ['abc', '123', 4, 5]\n```\n\n## 移除列表元素\n- 方法1，使用列表的pop方法移除元素\n\n列表的pop默认移除最后一个元素，一个移除指定元素\n\n```python\nIn [35]: lst = ['abc', '123', 'efg','苹果','香蕉']\n# 列表的pop方法返回移除的元素\nIn [36]: a = lst.pop()\nIn [37]: a\nOut[37]: '香蕉'\nIn [38]: lst\nOut[38]: ['abc', '123', 'efg', '苹果']\nIn [39]: b = lst.pop(1)\nIn [40]: b\nOut[40]: '123'\nIn [41]: lst\nOut[41]: ['abc', 'efg', '苹果']\n```\n\n- 方法2，使用列表的remove方法移除元素\n\n```python\nlst = ['abc', '123', 'efg']\n# 列表的remove方法没有返回值\na = lst.remove('efg')\nprint(a) # None\nprint(lst) # ['abc', '123']\n```\n\n- 方法3，使用关键字del删除列表中的元素\n\n```python\na = ['苹果','香蕉','橘子','香蕉']\ndel a[2]\na # ['苹果', '香蕉', '香蕉']\n```\n\n\n\n\n## 列表的深浅拷贝\n\n\n```python\nlst = ['你好',['hello','world'],[12,'abc']]\n# 列表的copy方法只能实现浅copy\ncopy1 = lst.copy()\ncopy1[1][0] = 3\nprint(lst) # ['你好', [3, 'world'], [12, 'abc']]\nprint(copy1) # ['你好', [3, 'world'], [12, 'abc']]\n```\n\n\n要想实现深度拷贝，需要使用 `copy` 模块的 `deepcopy` 函数\n\n\n```python\nfrom copy import deepcopy\nlst = ['你好',['hello','world'],[12,'abc']]\n# 列表的copy方法只能实现浅copy\ncopy1 = deepcopy(lst)\ncopy1[1][0] = 3\nprint(lst) # ['你好', ['hello', 'world'], [12, 'abc']]\nprint(copy1) # ['你好', [3, 'world'], [12, 'abc']]\n```\n\n\n## 判断元素是否在列表中\n\n列表所有数据类型的元素，都可以用该方法判断\n\n\n```python\na = [1,2,3]\n1 in a # True\n5 in a # False\nb = ['你','好','啊']\n'你' in b #True\nc = [(1,2),(3,4)]\n(1,2) in c # True\nc = [([1,2],2),(3,4)]\n([1,2],2) in c #True\nd = [[1,2],[3,4]]\n[1,2] in d # True\n[1,3] in d # False\n'ab' in 'abc' # True\n```\n\n\n## 列表的切片\n\n\n```python\na = list(range(1,20,3))\nprint(a) # [1, 4, 7, 10, 13, 16, 19]\n```\n\n切片的返回结果也是一个列表\n\n- 使用 `a[:3]` 获取列表 a 的前三个元素\n- 使用 `a[-1]` 获取 a 的最后一个元素，返回 int 型，值为 19\n- 使用 `a[:-1]` 获取除最后一个元素的切片 `[1, 4, 7, 10, 13, 16]`\n- 使用 `a[1:5]` 生成索引为 `[1,5)`（不包括索引 5）的切片 `[4, 7, 10, 13]`\n- 使用 `a[1:5:2]` 生成索引 `[1,5)` 但步长为 2 的切片 `[4,10]`\n- 使用 `a[::3]` 生成索引 `[0,len(a))` 步长为 3 的切片 `[1,10,19]`\n- 使用 `a[::-1]` 生成逆向索引 `[len(a),0)` 的切片 `[19, 16, 13, 10, 7, 4, 1]`\n- 使用 `a[::-3]` 生成逆向索引 `[len(a),0)` 步长为 3 的切片 `[19,10,1]`\n\n需要注意的是，`a[1:5:-1]`执行会返回空列表，不是生成`[1,5)`的逆向索引，如果要生成`[1,5)`的逆向索引，可以用如下方式：\n\n\n```python\na[1:5:-1]  # []\nb = a[1:5]\nb[::-1] # [13, 10, 7, 4]\n```\n\n\n## 列表的操作符\n\n列表的操作符只支持`+、*`\n\n```python\nIn [44]: [1, 2,]+[2, 3] #列表合并\nOut[44]: [1, 2, 2, 3]\n\nIn [42]: [1]*3  #列表元素重复\nOut[42]: [1, 1, 1]\n\nIn [43]: [1,2]*3\nOut[43]: [1, 2, 1, 2, 1, 2]\n\n```\n\n\n\n## 列表中某个元素的数量\n\n\n```python\na = ['苹果','香蕉','橘子','香蕉']\nprint(a.count('苹果')) # 1\nprint(a.count('香蕉')) # 2\n```\n\n\n## 列表转化为集合\n\n\n```python\na = ['苹果','香蕉','橘子','香蕉']\nset(a) # {'橘子', '苹果', '香蕉'}\n\n# 集合里面必须是不可哈希对象(不可变对象)\na = [[1,2],[1,2],[1,2,3]]\nset(a) # 报错 unhashable type: 'list'\n```\n\n\n## 判断中有无重复元素\n\n\n```python\n# 方法1\ndef judge_duplicated(lst):\n    for x in lst:\n        if lst.count(x) > 1: # 判断 x 元素在 lst 中的出现次数\n            return True # 重复返回ture\n    return False  # 无重复返回false\n# 方法2\ndef judge_duplicated(lst):\n    return len(lst) != len(set(lst)) # 重复返回ture，不重复返回False\n```\n\n## 列表使用python内置函数\n\n**python内置函数无需import**\n\n\n```python\nlen([1,2,3,4,5]) # 列表长度 5\nmax([1, 5, 65, 5]) #返回列表最大值 65\nmin([1, -1, 89]) #返回列表最小值 -1\nsorted(['10','3','2']) # 从小到大 ['10', '2', '3']\n# 注意：max，min，sorted只能用在全数值型列表中\n```\n\n\n```python\n# 也可以判断字符串大小，判断字符串大小是根据字符串首字符ASCII编码判断的\nmax(['10','3','2']) # 返回`3`\n# 判断字符串直接用运算符\n# '10'>'3' False\n# '10'<'3' True\n```\n\n\n\n\n\n## 列表数据重洗\n\n\n```python\nfrom random import shuffle\na = [1,3,2,5,6]\nshuffle(a) # 重洗数据\na # [3, 2, 6, 1, 5]\n```\n\n## 列表逆置\n\n- 方法1，使用列表的reverse方法对列表元素逆置\n\n```python\n# 方法一\nIn [52]: a = [1,3,2,5,6]\nIn [53]: a.reverse()\nIn [54]: a\nOut[54]: [6, 5, 2, 3, 1]\n\n```\n\n- 方法2，使用python内置函数reversed方法对列表元素逆置\n\n```python\n# 方法2\nIn [52]: a = [1,3,2,5,6]\nIn [56]: list(reversed(a))\nOut[56]: [6, 5, 2, 3, 1]\n```\n- 方法3，利用列表的切片\n\n```python\nIn [102]: a = [1,3,2,5,6]\nIn [103]: a[::-1]\nOut[103]: [6, 5, 2, 3, 1]\n```\n\n## 列表排序规则\n\n- 一般的列表元素排序\n\n```python\n# sorted(a) a要求必须是数值型\na = [1,3,2,5,6]\nsorted(a)\n```\n\n- 按照列表的某个位置排序\n\n如下所示，按照每一个列表的第二个位置排序：\n\n```python\n# 方法一\nIn [45]: a=[[1,2],[4,1],[9,10],[13,-3]]\nIn [46]: a.sort(key=lambda x: x[1])\nIn [47]: a\nOut[47]: [[13, -3], [4, 1], [1, 2], [9, 10]]\n# 方法二\nIn [48]: a=[[1,2],[4,1],[9,10],[13,-3]]\nIn [49]: sorted(a,key = lambda x: x[1] )\nOut[49]: [[13, -3], [4, 1], [1, 2], [9, 10]]\n```\n\n\n## 列表重复最多的元素\n- 找到列表重复最多的元素\n\n```python\nIn [50]: a = [1,2,3,3,4,2,3]\nIn [51]: max(a,key=lambda x: a.count(x), default=1)\nOut[51]: 3\n```\n\n\n\n\n\n# 元组tuple\n\n元祖与列表很相似，只不过元组是不可变（immutable）对象(即不可哈希对象)，元组中的元素**不能修改**，没有增加、删除元素的方法。**元组一旦创建后，长度就被唯一确定**\n\n**创建元素**\n\n```python\ntup_0 = tuple() #创建空元组\ntup_1 = () #创建空元组\ntup_2 = (1, ) #创建只包含一个元素的元组，注意要有逗号,否则会被认为元素本身\ntup_3 = (1, 3, \"apple\", 66) #创建多元素元组\n```\n\n\n**元组除了没有增加、删除元素的方法，不能修改，其他用法与list完全一样，如果我们想要高效的操作元祖，大可用python内置函数list将元祖转化为列表**\n\n**上述列表中的操作，除了增加、删除元素的操作之外，其他的列表操作方法完全可以用在元组上，这里就不一一介绍了**\n\n这里仅说明一些需要注意的点\n\n对于元组，里面的元素可以是任意数据类型，这就意味着也可以是列表、字典等可哈希类型，如下所示：\n\n```python\nIn [82]: a = ([1,2],)\nIn [83]: a[0][0]=3\nIn [84]: a\nOut[84]: ([3, 2],)\nIn [79]: type(a)\nOut[79]: tuple\n```\n**刚刚明明说好元组中的元素不能改变，为什么这里变了？**\n\n这里元组中存的列表的地址，换句话说列表的地址没有改变，只是地址对应的内容变了，如下\n\n```python\nIn [85]: lst = [1,2]\nIn [91]: id(lst) # id获取地址\nOut[91]: 2763791008392\n\nIn [89]: tup=(lst,)\nIn [90]: tup\nOut[90]: ([1, 2],)\n\nIn [92]: lst[0]=3\nIn [93]: lst\nOut[93]: [3, 2]\nIn [94]: tup\nOut[94]: ([3, 2],)\n\nIn [95]: id(lst)\nOut[95]: 2763791008392\n```\n可以看到，列表的地址并没有改变\n\n","tags":["python"],"categories":["python"]},{"title":"Python入门之运算符与基本数据类型详解","url":"/2020/11/24/220821/","content":"\n# Python 的运算符\n\n## 算数运算符\n\n算数运算符 `+ - * / // % **`\n\n`//` 用于两个数值相除且向下取整\n\n<!-- more -->\n\n\n\n\n\n\n```python\nIn [1]: 5//2\nOut[1]: 2\n\nIn [2]: 5//3.0\nOut[2]: 1.0\n```\n\n`**` 用于幂运算\n\n```python\nIn [3]: 2**3\nOut[3]: 8\n```\n\n\n## 比较运算符\n\nPython比较运算符 `== != > < >= <=`，返回值：True/False\n\nPython 比较运算符还支持链式比较，应用起来更加方便\n\n\n```python\ni = 3\nprint(1 < i < 3) # False\nprint(1 < i <= 3) # True\n```\n\n\n关于`==`的详细的用法见[Python中is、in、==之间的区别](https://blog.csdn.net/qq_37555071/article/details/107664916)\n\n## 赋值运算符\n\nPython赋值运算符 `= 、+=、 -=、 \\*=、 /=、 //=、 %=、 **=、 :=`\n\n\n```python\n# 幂赋值运算符 c **= a 等效于 c = c ** a\n#2的5次方\nIn [5]: a = 2\n   ...: a**=5\n   ...: a\nOut[5]: 32\n```\n\n\n## 逻辑运算符\n\nPython逻辑运算符 `and or not`,是指 `与`  `或`  `非` 的意思\n\n\n```python\nIn [6]: True and True\nOut[6]: True\n\nIn [7]: True and False\nOut[7]: False\n\nIn [8]: True or False\nOut[8]: True\n# not优先级最高，使用时一定要注意\nIn [9]: not True and False\nOut[9]: False\n\nIn [10]: not (True and False)\nOut[10]: True\n```\n\n\n\n## 成员运算符\n\nPython成员运算符 `in 、not in `\n\n- 如果元素 i 是 s 的成员，则 `i in s` 为 True\n- 若不是 s 的成员，则返回 False，也就是`i not in s` 为 True\n\n关于`in 、not in `的详细的用法见[Python中is、in、==之间的区别](https://blog.csdn.net/qq_37555071/article/details/107664916)\n\n## 身份运算符\n\nPython身份运算符 `is 、is not`\n\n`is`是Python身份运算符，用于判断两个对象的标识符是否相等(python中万物皆对象)，实质是用于**比较两个对象是否指向同一存储单元**\n\n关于`is 、is not`的详细的用法见[Python中is、in、==之间的区别](https://blog.csdn.net/qq_37555071/article/details/107664916)\n\n# Python 四大数据类型总结\n\n## 数值型\n\n`int` 整型对象、`float` 双精度浮点型、`bool` 逻辑对象，它们都是单个元素，称为数据型\n\n前缀加 `0x`，创建一个十六进制的整数\n\n\n```python\nIn [11]: 0xa5 # 等于十进制的 165\nOut[11]: 165\n```\n\n\n使用 `e` 创建科学计数法表示的浮点数：\n\n\n```python\nIn [12]: 1.05e3 # 1050.0\nOut[12]: 1050.0\n\nIn [13]: 1e3 # 1000.0\nOut[13]: 1000.0\n```\n\n\n## 容器型\n\n可容纳多个元素的容器对象，常用的比如：list 列表对象、 tuple 元组对象、dict 字典对象、set 集合对象\n\n使用一对中括号 `[]`，创建一个 list 型变量\n\n\n```python\nlst = [1,3,5] # list 变量\n```\n\n使用一对括号 `()`，创建一个 `tuple` 型对象\n\n\n```python\ntup = (1,3,5) # tuple 变量\n```\n\n但需要注意，含单个元素的元组后面必须保留一个逗号，才被解释为元组\n\n\n```python\ntup = (1,) # 必须保留逗号\n```\n\n否则会被认为元素本身\n\n\n```python\ntup=(1)\nprint(type(tup))\n# <class 'int'>\n```\n\n仅使用一对花括号 `{}`，创建一个 set 对象\n\n```python\ns = {1,3,5} # 集合变量\n```\n\n**字符串**\n\nPython 中没有像 C++ 表示的字符类型（char），所有的字符或串都被统一为 `str` 对象。如单个字符 `c` 的类型也为 `str`\n\nstr 类型会被经常使用，一些高频用法如下\n\nstrip 用于去除字符串前后的空格\n\n\n```python\n'  I love python  '.strip()\n# 'I love python'\n```\n\n\nreplace 用于字符串的替换\n\n\n```python\n'i love python'.replace(' ','_')\n# 'i_love_python'\n```\n\n\njoin 用于合并字符\n\n\n```python\n'_'.join(['book', 'store','count'])\n# 'book_store_count'\n```\n\nfind 用于返回匹配字符串的起始位置索引\n\n```python\n'i love python'.find('python')\n# 7\n```\n\n\n","tags":["python"],"categories":["python"]},{"title":"Python中is、in、==之间的区别","url":"/2020/11/24/220637/","content":"\nPython 中，对象相等性比较相关关键字包括` is、in`，比较运算符有 `==`\n\n- `is`判断两个对象的标识符是否相等\n- `in`用于成员检测\n- `==`用于判断值或内容是否相等，默认是基于两个对象的标识号比较\n\n也就是说，如果 `a is b` 为 True 且如果按照默认行为，意味着 `a==b` 也为 True\n\n<!-- more -->\n\nPython 中，对象相等性比较相关关键字包括` is、in`，比较运算符有 `==`\n\n- `is`判断两个对象的标识符是否相等\n- `in`用于成员检测\n- `==`用于判断值或内容是否相等，默认是基于两个对象的标识号比较\n\n也就是说，如果 `a is b` 为 True 且如果按照默认行为，意味着 `a==b` 也为 True\n\n## is 判断标识号是否相等\n\n`is`是Python身份运算符，用于判断两个对象的标识符是否相等(python中万物皆对象)，实质是**用于比较两个对象是否指向同一存储单元**，以下有几点`is`的使用方法：\n\n(1) Python 中使用 `id()` 函数获取对象的标识号，可以理解为内存地址\n\n```python\nIn [49]: a = 1\nIn [50]: b = 1\nIn [51]: id(a)\nOut[51]: 140708412432784\nIn [52]: id(b)\nOut[52]: 140708412432784\nIn [53]: a is b\nOut[53]: True\n\nIn [54]: s1 = 'abc'\nIn [55]: s2 = 'abc'\nIn [56]: id(s1)\nOut[56]: 1554773534064\nIn [57]: id(s2)\nOut[57]: 1554773534064\nIn [58]: s1 is s2\nOut[58]: True\n```\n\n\n(2) 由于创建的两个列表实例位于不同的内存地址，所以它们的标识号不等，即便对于两个空列表实例也一样\n\n```python\nIn [77]: a = [1,2]\nIn [78]: b = [1,2]\nIn [79]: id(a)\nOut[79]: 1552668915144\nIn [80]: id(b)\nOut[80]: 1552668915208\nIn [81]: a is b\nOut[81]: False\nIn [83]: a is not b\nOut[83]: True\nIn [84]: not  a is b\nOut[84]: True\n\nIn [85]: a = []\nIn [86]: b = []\nIn [87]: id(a)\nOut[87]: 1552668699208\nIn [88]: id(b)\nOut[88]: 1552669048456\nIn [89]: a is b\nOut[89]: False\n```\n(3) 对于序列型、字典型、集合型对象，一个对象实例指向另一个对象实例，is 比较才返回真值。\n\n```python\nIn [95]: a = [1,2]\nIn [96]: b = a\nIn [97]: a is b\nOut[97]: True\n```\n\n\n(4) 需要注意的是，Python 解释器，对位于区间 `[-5,256]` 内的小整数，会进行缓存，不在该范围内的不会缓存，因此会出现如下现象：\n\n```python\nIn [90]: a = 12345\nIn [91]: b = 12345\nIn [92]: id(a)\nOut[92]: 1552668913040\nIn [93]: id(b)\nOut[93]: 1552668911312\nIn [94]: a is b\nOut[94]: False\n```\n\n(5) Python 中 None 对象是一个单例类的实例，具有唯一的标识号，在判断某个对象是否为 None 时，最便捷的做法：`variable is None`，如下：\n\n```python\nIn [98]: id(None)\nOut[98]: 140708411956448\nIn [99]: a = None\nIn [100]: a is None\nOut[100]: True\nIn [101]: id(a)\nOut[101]: 140708411956448\n```\n\n## in 用于成员检测\n- 如果元素 i 是 s 的成员，则 `i in s` 为 True；\n- 若不是 s 的成员，则返回 False，也就是` i not in s` 为 True\n\nin 是判断成员是否属于某个序列，in 后即可以跟字符串，也可以跟列表,实际上各种集合类型都可以。\n\n（1） 对于字符串类型，`i in s` 为 True，意味着 i 是 s 的子串。\n\n```python\nIn [104]: '234' in '12345'\nOut[104]: True\n# 当然。也可以用字符串的find()方法判断\nIn [105]: '12345'.find('234')\nOut[105]: 1\n```\n（2）列表中任何数据类型的元素都可以用in判断，同理，元祖和集合也可以这样做\n\n```python\nIn [106]: [1,2] in [[1,2],3,'abc']\nOut[106]: True\nIn [107]: lst = [[1,2],3,'abc']\nIn [109]: [1,2] in lst\nOut[109]: True\nIn [110]: 'abc' in lst\nOut[110]: True\n```\n\n（3）对于字典类型，in 操作判断 i 是否是字典的键，但不能直接判断值，如果要判断一个值是否在字典中，可以用字典的values()方法\n\n```python\nIn [111]: dct = {'a':12,'b':35,'key':'abc'}\nIn [112]: 'a' in dct\nOut[112]: True\nIn [113]: 'key' in dct\nOut[113]: True\n\nIn [114]: 12 in dct\nOut[114]: False\nIn [115]: 12 in dct.values()\nOut[115]: True\nIn [116]: 'abc' in dct.values()\nOut[116]: True\n```\n\n（4）对于自定义类型，判断是否位于序列类型中，需要重写序列类型的 魔法方法 __contains__。\n\n```python\nclass Node():\n    def __init__(self,value):\n        self.value = value\n        \nclass Nodes(list):\n        def __contains__(self,node):\n            for s in self:\n                if s.value == node.value:\n                    return True\n            return False    \n\nn1 = Node('linear')\nn2 = Node('sigmoid')\na = Nodes()\na.extend([n1,n2])\nn3 = Node('linear')\nprint(n3 in a) # True\nn4 = Node('tanh')\nprint(n4 in a) # False\n```\n\n## == 判断值是否相等\n（1） **对于数值型、字符串、列表、字典、集合，默认只要元素值相等，== 比较结果是 True**\n\n```python\nIn [117]: str1 = 'abc'\nIn [118]: str2 = 'abc'\nIn [119]: str1 == str2\nOut[119]: True\n\nIn [120]: a = [1,2]\nIn [123]: b = [1,2]\nIn [124]: a == b\nOut[124]: True\n\nIn [125]: a = [[1,2],3]\nIn [126]: b = [[1,2],3]\nIn [127]: a == b\nOut[127]: True\n\nIn [128]: a = {'a':1,'b':2}\nIn [129]: b = {'a':1,'b':2}\nIn [130]: a == b\nOut[130]: True\n\nIn [131]: a = {1,2}\nIn [132]: b = {1,2}\nIn [133]: a == b\nOut[133]: True\n```\n\n（2）对于自定义类型，当所有属性取值完全相同的两个实例，判断 == 时，返回 False。如果需要用`==`判断两个对象是否相等，需要重写方法 `__eq__`，使用` __dict__ `获取实例的所有属性。\n\n```python\nclass Node():\n    def __init__(self,value):\n        self.value = value\n    def __eq__(self,node):\n        return self.__dict__ == node.__dict__\n    \nn1 = Node('linear')\nn2 = Node('sigmoid')\nn3 = Node('linear')\n\nprint(n1.__dict__) # {'value': 'linear'}\nprint(n1 == n2) # False\nprint(n1 == n3) # True\n```\n","tags":["python"],"categories":["python"]},{"title":"python中print函数的所有输出形式","url":"/2020/11/24/220508/","content":"\npython中print函数的所有输出形式\n\n<!-- more -->\n\n\n## 普通输出\n\n```python\nIn [11]: print('b')\nb\n# end为不换行输出\nIn [14]: for i in range(10):\n    ...:     print(i,end='')\n0123456789\n```\n\n## format输出\n\n```python\nIn [16]: a, b = 1, 2\nIn [17]: print('{},{}'.format(a,b))\n1,2\nIn [18]: print('{1},{0}'.format(a,b))\n2,1\n```\n\n## f输出\n\n```python\nIn [16]: a, b = 1, 2\nIn [19]: print(f'{a},{b}')\n1,2\n```\n## %输出\n- %s --- str 字符串\n- %x --- hex 十六进制\n- %d --- dec 十进制\n- %o --- oct 八进制\n\n```python\nIn [16]: a, b = 1, 2\nIn [20]: print('%s,%s'%(a,b))\n1,2\nIn [29]: nHex = 0x20\nIn [30]: print(\"nHex = %x,nDec = %d,nOct = %o\" %(nHex,nHex,nHex))\nnHex = 20,nDec = 32,nOct = 40\n```\n\n## float格式化输出\n- %f --- float 浮点数\n\n\n```python\nIn [32]: import math\nIn [33]: print(\"PI = %f\"%(math.pi))\nPI = 3.141593\n# 保留三维有效数字\nIn [35]: print(\"PI = %.3f\" % math.pi)\nPI = 3.142\n# width = 10,precise = 3,align = right\nIn [36]: print(\"PI = %10.3f\" % math.pi)\nPI =      3.142\n# width = 10,precise = 3,align = left\nIn [37]: print(\"PI = %-10.3f\" % math.pi)\nPI = 3.142\n# 前面填充字符\nIn [38]: print(\"PI = %06d\" % int(math.pi))\nPI = 000003\n```\n\n","tags":["python"],"categories":["python"]},{"title":"卷积神经网络CNN与LeNet5详解(可训练参数量、计算量、连接数的计算+项目实战)","url":"/2020/11/24/220330/","content":"\n## 神经网络\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728094437615.png)\n神经网络可以看成一个端到端的黑盒，中间是隐藏层(可以很深)，两边是输入与输出层，完整的神经网络学习过程如下：\n1. 定义网络结构（指定输入层、隐藏层、输出层的大小）\n2. 初始化模型参数\n3. 循环操作：\n\t\t3.1.  执行前向传播（输入参数，计算一个个结点的值得到y’，即预测值）\n\t\t3.2.  计算损失函数（拿y’-y计算Loss）\n\t\t3.3.  执行后向传播（求梯度，为了更新参数）\n\t\t3.4.  权值更新\n\n<!-- more -->\n\n\n\n## CNN卷积神经网络\n### CNN的由来\n卷积神经网络（CNN）是人工神经网络的一种，是多层感知机（MLP）的一个变种模型，它是从生物学概念中演化而来的。\n\n> Hubel和Wiesel早期对猫的视觉皮层的研究中得知在视觉皮层存在一种细胞的复杂分布，这些细胞对于外界的输入局部是很敏感的，它们被称为“感受野”（细胞），它们以某种方法来覆盖整个视觉域。这些细胞就像一些滤波器一样，够更好地挖掘出自然图像中的目标的空间关系信息。\n视觉皮层存在两类相关的细胞，S细胞（Simple Cell）和C（Complex Cell）细胞。S细胞在自身的感受野内最大限度地对图像中类似边缘模式的刺激做出响应，而C细胞具有更大的感受野，它可以对图像中产生刺激的模式的空间位置进行精准地定位。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728095934616.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70 =500x300)\n卷积神经网络已成为语音和图像识别的研究热点，80年代末，Yann LeCun就作为贝尔实验室的研究员提出了卷积网络技术，并展示如何使用它来大幅度提高手写识别能力。在图像识别领域，CNN已经成为一种高效的识别方法\n\nCNN的应用广泛，包括图像分类，目标检测，目标识别，目标跟踪，文本检测和识别以及位置估计等。\n\n\nCNN的基本概念：\n- 局部感受野（local receptive fields）\n- 共享权重（shared weights）\n- 池化（pooling）\n\n### 局部感受野\n**局部感受野（local receptive fields）**：图像的空间联系是局部的，就像人通过**局部的感受野**去感受外界图像一样，每个神经元只感受局部的图像区域，然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728104951198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**CNN中相邻层之间是部分连接，即某个神经单元的感知区域来自于上层的部分神经单元。这个与MLP多层感知机不同，MLP是全连接，某个神经单元的感知区域来自于上层的所有神经单元。**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728105134988.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n### 共享权重\n**共享权重（shared weights）**：共享权重即参数共享，隐藏层的参数个数和隐藏层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关。也就是说，对于一个特征图，它的每一部分的卷积核的参数都是一样的。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728105613313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*左图是没有参数共享的情况，右图进行了参数共享*\n\n如果要提取不同的特征，就需要多个滤波器。每种滤波器的参数不一样，表示它提出输入图像的不同特征。**这样每种滤波器进行卷积图像就得到对图像的不同特征的反映，我们称之为Feature Map**；100种卷积核就有100个Feature Map，这100个Feature Map就组成了一层神经元。\n\n### 池化\n池化（pooling）原理：根据图像局部相关的原理，图像某个邻域内只需要一个像素点就能表达整个区域的信息， 池化也称为混合、下采样，目的是减少参数量。分为最大池化，最小池化，平均池化。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072811065160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n### CNN的结构\n\n\nCNN的网络结构如下：\n- 输入层，Input，输入可以是灰度图像或RGB彩色图像（三通道）。对于输入的图像像素分量为 [0, 255]，为了计算方便一般需要归一化（如果使用sigmoid激活函数，会归一化到[0, 1]，如果使用tanh激活函数，则归一化到[-1, 1]）\n- 卷积层，C*，特征提取层，得到特征图，目的是使原信号特征增强，并且降低噪音；\n- 池化层，S*，特征映射层，将C*层多个像素变为一个像素，目的是在保留有用信息的同时，尽可能减少数据量\n- 光栅化：为了与传统的多层感知器MLP全连接，就是把池化层得到的特征图拉平\n- 多层感知器(MLP)：最后一层为分类器，多分类使用Softmax，二分类可以使用Logistic Regression\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072811090764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**卷积过程包括**：用一个可训练的滤波器fx去卷积一个输入的图像（第一阶段是输入的图像，后面的阶段就是卷积特征图），然后加一个偏置bx，得到卷积层Cx\n**下采样(池化)过程包括**：每邻域四个像素求和变为一个像素，然后通过标量Wx+1加权，再增加偏置bx+1，然后通过一个sigmoid激活函数，产生一个缩小四倍的特征映射图Sx+1。(**下图很重要，下面可训练参数量的计算就可以从这里看出**)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728111236668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n卷积神经网络就是让权重在不同位置共享的神经网络，在下图中，局部区域圈起来的所有节点会被连接到下一层的一个节点上(卷积核，称为 kernel 或 filter 或 feature detector，filter的范围叫做filter size，比如 2x2的卷积核)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728112632366.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n上图的运算过程可用如下公式表示(**上图有重要作用，下面的连接数和神经元个数就可以从这里看出来**)：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728112703711.png)\nCNN学习可以帮我们进行特征提取，比如我们想要区分人脸和狗头，那么通过CNN学习，背景部位的激活度基本很少。\nCNN layer越多，学习到的特征越高阶，如下图所示：\n- layer 1、layer 2学习到的特征基本上是颜色、边缘等低层特征\n- layer 3开始稍微变得复杂，学习到的是纹理特征，比如网格纹理\n- layer 4学习到的是比较有区别性的特征，比如狗头\n- layer 5学习到的则是完整的，具有辨别性关键特征\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728113651972.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n### 光栅化\n**光栅化（Rasterization）**：为了与传统的MLP（多层感知机）全连接，把上一层的所有Feature Map的每个像素依次展开，排成一列。\n图像经过下采样后，得到的是一系列的特征图，而多层感知器接受的输入是一个向量，所以需要将这些特征图中的像素依次取出，排列成一个向量。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072811181585.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n## LeNet5详解\n>1990年，LeCun发表了一篇奠定现在CNN结构的重要文章，他们构建了一个叫做LeNet-5的多层前馈神经网络，并将其用于手写体识别。就像其他前馈神经网络，它也可以使用反向传播算法来训练。它之所以有效，是因为它能从原始图像学习到有效的特征，几乎不用对图像进行预处理。\n\nLeNet5共有7层（不包含输入层）:\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728114959612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n### LeNet5-C1层\nLeNet5的第一层是卷积层\n- 输入图片大小：`32*32`                \n- 卷积窗大小：`5*5`(作者定义)                        \n- 卷积窗种类：`6`(作者定义)\n- 输出特征图数量：`6`  。\n\t由于卷积窗种类是6，故输出的输出特征图数量也是6\n- 输出特征图大小：`(32-5+1)*(32-5+1)=28*28`   \n\t输出特征图大小计算可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中的特征图大小计算方式\n- 可训练参数量：`(5*5+1)*6`或`(5*5)*6+6`\n\t参数量就是卷积核中元素的个数+偏置bias，总共6种卷积核\n- 计算量：`(5*5+1)*6*28*28`\n\t每一个像素的计算量是`(5*5+1)`，总共`6*28*28`个像素，所以总的计算量是`(5*5+1)*6*28*28`\n- 神经元数量：`(28*28)*6)`\n\t参考上面卷积的计算方式和图像就可以看出，计算出的一个结果(像素点)就是一个神经元\n- 连接数：`(5*5+1)*6*28*28`\n\t参考上面卷积的计算方式和图像就可以看出，输入特征图的每个像素是一个神经元，输出特征图的每个像素也是一个神经元，**只要参与了计算，两个神经元之间就算做一个连接**，**因此卷积中连接数与计算数是一样的**\n\t\n\n\n### LeNet5-S2层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)LeNet5的第二层是池化层（池化核大小`2*2`，步长为2）\n- 输入大小：`(28*28)*6 `                 \n- 采样区域(池化核大小)：`2*2` (作者定义)\n- 下采样数量：`6`\n\t这6个下采样的方式其实是一样的\n- 输出特征图大小：`14*14`\n\t输出特征图大小计算可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中的特征图大小计算方式，卷积和池化输出特征图大小的计算方式是通用的\n- 可训练参数量：`(1+1)*6`\n 池化层的可训练参数量不是池化核元素的个数，而是每一个池化都有一个权重w和偏置b，总共6个下采样，故可训练参数是`(1+1)*6`\n- 计算量 ：`(2*2+1)*14*14*6`\n\t对于池化层的计算量，可能不同地方的描述不一样，我的理解是，采样区域是`2*2` ，要找到最大的采样点，需要比较3次，也就是3次计算，才能找到最大值(如果采样区域是`3*3`，那么需要比较8次才能找到最大值)，然后这个最大值再乘以权重w，加上偏置b，因此得到一个像素点的计算量是`3+1+1=(2*2-1+1+1)=(2*2+1)`，总共`14*14*6`个像素点，可得总共的计算量为`(2*2+1)*14*14*6`\n- 神经元数量：`(14*14)*6`\n\t计算出的一个结果(像素点)就是一个神经元\n- 连接数：`(2*2+1)*[(14*14)*6]`\n\t每四个输入的像素点(输入四个神经元)，输出的像素点为1个(输出一个神经元)，再加上一个偏置的点(偏置神经元)，可得每计算一个像素点的连接数是`(2*2+1)`，总共`(14*14)*6`个像素点，故总的连接数为`(2*2+1)*[(14*14)*6]`，可以看到，**池化中计算量和连接数也是一样的**\n\n### LeNet5-C3层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nLeNet5的第3层是卷积层\n- 输入图片大小：`(14*14)*6`                \n- 卷积窗大小：`5*5`(作者定义)                        \n- 卷积窗种类：`16`(作者定义)              \n- 输出特征图数量：`16`  \n- 输出特征图大小：`(14-5+1)*(14-5+1)=10*10`   \n- 可训练参数量：`(6*5*5+1)*16`\n\t每张图片的输入通道是6，我们需要用对应维度的卷积核做卷积运算，也就是要用`6*5*5`的卷积做运算，输出通道是16，因此要有16个这样的卷积核，每个卷积核再加上一个偏置bias，所以参数量是`(6*5*5+1)*16`，可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)\n- 计算量：`(6*5*5+1)*16*10*10`\n\t一个像素点的计算量为`(6*5*5+1)`，总的输出像素个数为`16*10*10`，故总的计算量为`(6*5*5+1)*16*10*10`，这里要注意：**多维卷积计算后需要把每个卷积的计算结果相加，这个相加的结果才是一个像素点，这个计算就忽略不计了**\n- 神经元数量：`10*10*16`\n\t参考上面卷积的计算方式和图像就可以看出，计算出的一个结果(像素点)就是一个神经元\n- 连接数：`(6*5*5+1)*16*10*10`\n\n\t\n\n\n### LeNet5-S4层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\nLeNet5的第四层是池化层（池化核大小`2*2`，步长为2）\n- 输入大小：`(10*10)*16 `                 \n- 采样区域(池化核大小)：`2*2` (作者定义)\n- 下采样数量：`16`\n- 输出特征图大小：`5*5`\n- 可训练参数量：`(1+1)*16`\n- 计算量 ：`(2*2+1)*5*5*16`\n- 神经元数量：`5*5*16`\n- 连接数：`(2*2+1)*5*5*16`\n\t\n\n### LeNet5-C5层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nLeNet5的第5层是卷积层\n- 输入图片大小：`(5*5)*16`                \n- 卷积窗大小：`5*5`(作者定义)                        \n- 卷积窗种类：`120`(作者定义)              \n- 输出特征图数量：`120`  \n- 输出特征图大小：`(5-5+1)*(5-5+1)=1*1`   \n- 可训练参数量：`(16*5*5+1)*120`\n- 计算量：`(16*5*5+1)*1*1*120`\n- 神经元数量：`1*1*120`\n- 连接数：`(16*5*5+1)*1*1*120`\n\n\n### LeNet5-F6层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nLeNet5的第六层是全连接层\n- 输入图片大小：`(1*1)*120`                \n- 卷积窗大小：`1*1`(作者定义)                        \n- 卷积窗种类：`84`(作者定义)              \n- 输出特征图数量：`84`  \n- 输出特征图大小：`1*1`   \n- 可训练参数量：`(120+1)*84`(全连接层)\n\t这里相当于84个线性模型，即MLP多层感知机\n- 计算量：`(120+1)*84`\n- 神经元数量：`84`\n- 连接数：`(120+1)*84`(全连接层)\n\n### LeNet5-OUTPUT层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072818211542.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\nLeNet5的第7层是输出层\n- 输入图片大小：1*84\n- 输出特征图数量：1*10\n输出1000000000，则表明是数字0的分类\n\n\n### 计算公式\n**卷积层：设卷积核大小为`k*k`，步长为1，输入特征图(输入图片)大小为`n*n`，输入通道是`a`，输出通道是`b`(输出通道就是卷积核的种类数)**\n- **输出**特征图大小：`(n-k+1)*(n-k+1)=m*m`\n- 可训练参数量：`(a*k*k+1)*b`\n- 计算量：`(a*k*k+1)*b*m*m`\n- 神经元数量：`m*m*b`\n- 连接数：`(a*k*k+1)*b*m*m`\n\n卷积步长为n的输出特征图大小计算方式可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)\n\n**池化层：设池化核大小为`k*k`，步长是stride，输入特征图(输入图片)大小为`n*n`，输入通道是`a`，输出通道是`a`(池化层输入通道和输出通道是样的)**\n- 输出特征图大小：$\\frac{ n-k}{stride} + 1=m$\n\t可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中的特征图大小计算方式\n- 可训练参数量：`(1+1)*a`\n\t池化层可训练的参数量和池化核大小没有关系\n- 计算量：`(k*k+1)*(m*m*a)`\n- 神经元数量：`m*m*a`\n- 连接数：`(k*k+1)*(m*m*a)`\n\n\n\n## LeNet5实战\n### 定义网络模型\n\n```python\nimport torch\nfrom torch import nn,optim\nimport torchvision\nimport torch.nn.functional as F\n#没参数可以用nn，也可以用F，有参数的只能用nn\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #定义卷定义层卷积层,1个输入通道，6个输出通道，5*5的filter,28+2+2=32\n        #左右、上下填充padding\n        #MNIST图像大小28，LeNet大小是32\n        self.conv1 = nn.Conv2d(1,6,5,padding=2)\n        #定义第二层卷积层\n        self.conv2 = nn.Conv2d(6,16,5)\n        \n        #定义3个全连接层\n        self.fc1 = nn.Linear(16*5*5,120)\n        self.fc2 = nn.Linear(120,84)\n        self.fc3 = nn.Linear(84,10)\n        \n    #前向传播\n    def forward(self,x):\n        #先卷积，再调用relue激活函数，然后再最大化池化\n        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n        x=F.max_pool2d(F.relu(self.conv2(x)),(2,2))        \n        #num_flat_features=16*5*5\n        x=x.view(-1,self.num_flat_features(x))\n\n        #第一个全连接\n        x=F.relu(self.fc1(x))\n        x=F.relu(self.fc2(x))\n        x=self.fc3(x)\n        return x\n    def num_flat_features(self,x):\n        size=x.size()[1:]\n        num_features=1\n        for s in size:\n            num_features=num_features*s\n        return num_features \n```\n### 初始化模型参数\n\n```python\nimport torchvision.datasets as datasets\n#import torchvision.transforms as transforms \nfrom torchvision import transforms\n\nfrom torch.utils.data import DataLoader\n\n#超参数定义\n#人为定义的参数是超参数，训练的是参数\nEPOCH = 10               # 训练epoch次数\nBATCH_SIZE = 64     # 批训练的数量\nLR = 0.001                # 学习率\n\n#首次执行download=True，下载数据集\n#mnist存在的路径一定不要出现 -.？等非法字符\ntrain_data=datasets.MNIST(root='./dataset',train=True,transform=transforms.ToTensor(),download=False)\ntest_data=datasets.MNIST(root='./dataset',train=False,transform=transforms.ToTensor(),download=False)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint('训练集大小',train_data.train_data.size())\nprint('训练集标签个数',train_data.train_labels.size())\nplt.imshow(train_data.train_data[0].numpy(),cmap='gray')\nplt.show()\n```\n输出：\n\n\t\t训练集大小 torch.Size([60000, 28, 28])\n\t\t训练集标签个数 torch.Size([60000])\n\n\n​\t\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728182811711.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n​\t\n\n```python\n#如果有dataloader 的话一般都是在dataset里，dataloader和dataset 这俩一般一起用的\n#使用DataLoader进行分批\n#shuffle=True是设置随机数种子\ntrain_loader=DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\ntest_loader=DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=True)\n\n#创建model\nmodel=LeNet5()\n#定义损失函数\ncriterion=nn.CrossEntropyLoss()\n#定义优化器\noptimizer=optim.Adam(model.parameters(),lr=1e-3)\n\n#device  cuda:0是指使用第一个gpu\ndevice=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# device=torch.device('cpu')\nprint(device,type(device))\nmodel.to(device)\n```\n### 训练\n\n```python\n\n# 训练\nfor epoch in range(EPOCH):\n    for i,data in enumerate(train_loader):\n        inputs,labels=data\n        #可能会使用GPU，注意掉就不会在gpu里运行了\n        inputs,labels=inputs.to(device),labels.to(device)\n        # print(type(inputs),inputs.size(),'\\n',inputs)\n        #前向传播\n        outpus=model(inputs)\n        \n        #计算损失函数\n        loss=criterion(outpus,labels)\n        #清空上一轮梯度\n        optimizer.zero_grad()\n        #反向传播\n        loss.backward()\n        #参数更新\n        optimizer.step()\n \n    print('epoch {} loss:{:.4f}'.format(epoch+1,loss.item()))\n```\n输出：\n\n\tepoch 1 loss:0.1051\n\tepoch 2 loss:0.0102\n\tepoch 3 loss:0.1055\n\tepoch 4 loss:0.0070\n\tepoch 5 loss:0.2846\n\tepoch 6 loss:0.1184\n\tepoch 7 loss:0.0104\n\tepoch 8 loss:0.0014\n\tepoch 9 loss:0.0141\n\tepoch 10 loss:0.0126\n\n\n​\t\n### 测试准确率\n\n```python\n#保存训练模型\ntorch.save(model,'dataset/mnist_lenet.pt')\nmodel=torch.load('dataset/mnist_lenet.pt')\n\n#测试\n\"\"\"\n训练完train_datasets之后，model要来测试样本了。\n在model(test_datasets)之前，需要加上model.eval(). \n否则的话，有输入数据，即使不训练，它也会改变权值。\n\"\"\"\nmodel.eval()\ncorrect=0\ntotal=0\n\nfor data in test_loader:\n    images,labels=data\n    images,labels=images.to(device),labels.to(device)\n    #前向传播  model(images)  和 model.forward(x)一样的\n    out=model(images)\n    \"\"\"\n    首先得到每行最大值所在的索引（比图第7个分类是最大值，则索引是7）\n    然后，与真实结果比较（如果一个样本是图片7，那么该labels是7），如果相等，则加和\n    \"\"\"\n    _,predicted=torch.max(out.data,1)\n    total=total+labels.size(0)\n    correct=correct+(predicted==labels).sum().item()\n    \n#输出测试的准确率\nprint('10000张测试图像 准确率：{:.4f}%'.format(100*correct/total))\n```\n输出：\n\n\t\t10000张测试图像 准确率：98.9400%\n### 预测结果\n\n```python\n#记住，输入源一定要转化为torch.FloatTensor，否则无法预测，并且一定要加model.eval()，否则会更新权重\ndef predict_Result(img):\n    \"\"\"\n    预测结果，返回预测的值\n    img，numpy类型，二值图像\n    \"\"\"\n    model.eval()\n    img=torch.from_numpy(img).type(torch.FloatTensor).unsqueeze(0).unsqueeze(1)\n    img=img.to(device)\n    out=model(img)\n    _,predicted=torch.max(out.data,1)\n    return predicted.item()\n    \nimg2=train_data.train_data[167].numpy()\nplt.imshow(img2,cmap='gray')\nprint('预测结果：',predict_Result(img2))\n```\n\n输出：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728183140187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n","tags":["LeNet5"],"categories":["神经网络"]},{"title":"Python函数参数之*与**用法详解","url":"/2020/11/24/220157/","content":"\n首先，我们来看一个函数定义：\n\n```python\ndef f(a,*b,**c):\n    print(f'a:{a},b:{b},c:{c}')\n```\n`*b与**c`都是可变参数\n- 出现带一个星号的参数 b，**这是可变位置参数**\n- 带两个星号的参数 c，**这是可变关键字参数**\n\n<!-- more -->\n\n\n\n首先，我们来看一个函数定义：\n\n```python\ndef f(a,*b,**c):\n    print(f'a:{a},b:{b},c:{c}')\n```\n`*b与**c`都是可变参数\n- 出现带一个星号的参数 b，**这是可变位置参数**\n- 带两个星号的参数 c，**这是可变关键字参数**\n\n现在执行如下代码：\n\n```python\nIn [2]: f(1,2,3,w=4,h=5)\na:1,b:(2, 3),c:{'w': 4, 'h': 5}\n```\n可以看到，参数 b 被传递 2 个值，参数 c 也被传递 2 个值。**可变位置参数 b 被解析为元组，可变关键字参数 c 被解析为字典**。\n\n下面有几个重要规则：\n\n（1）**可变位置参数不能传入关键字参数**\n\n```python\nIn [41]: def f(*a):\n    ...:   print(a)\nIn [42]: f(1)\n(1,)\nIn [43]: f(1,2,3)\n(1, 2, 3)\nIn [44]: f(a=1)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-44-6a0ab2c303a9> in <module>\n----> 1 f(a=1)\nTypeError: f() got an unexpected keyword argument 'a'\n```\n（2） **可变关键字参数不能传入位置参数**\n\n```python\nIn [45]: def f(**a):\n    ...:   print(a)\nIn [46]: f(a=1)\n{'a': 1}\nIn [47]: f(a=1,b=2,width=3)\n{'a': 1, 'b': 2, 'width': 3}\nIn [48]: f(1)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-48-281ab0a37d7d> in <module>\n----> 1 f(1)\nTypeError: f() takes 0 positional arguments but 1 was given\n```\n","tags":["python"],"categories":["python"]},{"title":"Anaconda与jupyter安装、操作及插件安装","url":"/2020/11/24/215911/","content":"# 一、anaconda简介\n\n1. 进入官网https://www.anaconda.com\n2. 点解download进入下载页面\n3. 按步骤安装(默认，下一步)\n4. 启动jupyterlab应用\n\n![\\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-pELNUSl3-1595855913557)(attachment:launch.jpg)\\]](https://img-blog.csdnimg.cn/20200727212201931.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n<!-- more -->\n\n\n\n# 二、jupyter lab 基本操作\n\n## 1. jupyter lab 简介\n\n- JupyterLab包含了Jupyter Notebook所有功能。\n- JupyterLab作为一种基于web的集成开发环境，你可以使用它编写notebook、操作终端、编辑markdown文本、打开交互模式、查看csv文件及图片等功能\n\n## 2. jupyter lab 基本操作\n\n### 2.1 操作模式\n\n两种操作模式 `command  mode`  和  `edit  mode`\n\n在一个`cell`中按下`enter`就进入edit  mode，按下`Esc`进入command  mode\n\n### 2.2 cell转换类型\n\n- code:代码环境\n- markdown: 带有latex公式输入的增强markdown\n- raw：纯文本\n\n**cell类型转换方式**\n\n- 鼠标操作\n- 快捷键\n\ncommand模式下：m:markdown r:raw text y:code\n\n### 2.3 运行cell\n\n- shift+enter:运行并跳转到下一个cell\n- control+enter：运行并停留在当前cell\n\n### 2.4 增加/删除cell\n\n- 鼠标操作：两种模式均可\n- 快捷键：a/d command模式\n\n\n1. A 在当前cell的上面添加cell\n2. B 在当前cell的下面添加cell\n3. 双击D  快速删除当前cell\n\n### 2.5 代码提示功能\n\n- tab 代码补全或缩进\n- Shift-Tab 提示\n- 更多快捷键参照 [Jupyter Notebook骚操作](https://mp.weixin.qq.com/s?__biz=MzU5MjI3NzIxMw==&mid=2247488276&idx=2&sn=696900f0744474486eece7dee706e31e&chksm=fe2368a6c954e1b01e7e5943b697c1aec840aba995446230d9c6dc385f8471750002539a5921&mpshare=1&scene=24&srcid=&sharer_sharetime=1591664539198&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&ascene=14&devicetype=android-29&version=27000f3d&nettype=WIFI&abtest_cookie=AAACAA%3D%3D&lang=zh_CN&exportkey=AX%2BZM%2BDxUN6bs5k3X3SQbQY%3D&pass_ticket=njbcV9rUgIhNR7K4XdDWqfGsAPAcyPs1nE8Eoab4vyjmTWneMPO%2FDwraZRwPmQxP&wx_header=1)\n\n### 2.6 注释多行代码\n- ctrl / 注释多行代码\n\n### 2.6 运行py文件\n```python\n% run path/filename \n```\n例如：\n```python\n%run C:\\Users\\wang1\\Desktop\\go.py\n```\n\n\n### 2.7 Markdown标题级别\n\n- 命令行模式下，1 2 3 4 5 6分别是1-6级标题\n\n### 2.8 cell加行号\n\n- 命令行模式下，shilt+L为所有cell加上行号，L不持支给单个cell加行号\n- 鼠标点击view显示行号\n\n### 2.9 cell合并分割\n\n- 编辑模式，`crtl shift - 在光标处分割cell`\n- 命令行模式 `shift 鼠标 选中多个cell`  \n`shift m 合并选中给的cell`\n\n# 三、插件安装\n- 更新插件：`jupyter labextensio update 插件名`\n- 更新所有插件：`jupyter labextension update --all`\n- 卸载插件：`jupyter labextensio uninstall 插件名`\n- 安装插件：`jupyter labextensio install 插件名`\n- 远程仓库安装插件：`jupyter labextension install 参考地址`\n- 安装制定版本插件：`jupyter labextensio install 插件名=版本号`\n- 查看已安装插件：`jupyter labextension list`\n\n安装目录插件，可以执行如下命令：\n```powershell\njupyter labextension install @jupyterlab/toc\n或者\njupyter labextension install https://github.com/jupyterlab/jupyterlab-toc.git\n```\n收藏了几个很实用的jupyter插件网站，如下：\n\n## 1.Jupyter Notebook目录插件\n\nhttps://www.jianshu.com/p/f314e9868cae\n\n## 2.Jupyter lab目录插件\n\nhttps://www.pinggu.com/post/details/5eccf61a7fba3d625f75771d\n\n## 3.jupyterlab_code_formatter–代码pep8\n\nhttps://www.brothereye.cn/python/362/\n\n**jupyterlab_code_formatter官网**\n- https://github.com/ryantam626/jupyterlab_code_formatter\n\n## 4.15个好用到爆炸的Jupyter Lab插件\n\nhttps://zhuanlan.zhihu.com/p/101070029\n\n## 5.工具篇-Jupyter Lab效率提升插件\n\nhttps://zhuanlan.zhihu.com/p/73374773\n\n## 6.解决build失败问题\n\n终止JupyterLab后，在命令行下输入`jupyter-lab build`\n\n","tags":["jupyter"],"categories":["工具"]},{"title":"万字详解决策树之ID3、CART、C4.5【原理+实例+代码实现】","url":"/2020/11/24/215647/","content":"\n## 决策树的含义\n首先我们先来了解下什么是决策树。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727111117277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n上图就是一颗带有决策的树，其中天气、温度等称为特征，后面问号的位置称为阈值，如温度=35°，则阈值为35。\n\n\n<!-- more -->\n\n\n\n\n- 决策树是一种基本的分类与回归方法。这里主要讨论决策树用于分类。\n- 决策树的结点和有向边分别表示\n\t1. 内部结点表示一个特征或者属性。\n\t2. 叶子结点表示一个分类。\n\t3. 有向边代表了一个划分规则。\n- 决策树从根结点到子结点的的有向边代表了一条路径。\n- 决策树的路径是互斥并且是完备的。\n- 用决策树分类时，是对样本的某个特征进行测试，根据测试结果将样本分配\n- 如果将样本分配到了树的子结点上，每个子结点对应该特征的一个取值。\n- 决策树的优点：可读性强，分类速度快。\n- 决策树学习通常包括3个步骤：\n\t1. 数据标注\n\t2. 特征选择。\n\t3. 决策树生成。\n\t4. 分类和识别。\n\n决策树模型可以认为是if-then规则的集合。决策树学习的算法通常是遍历选择最优特征和特征值，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类，这一过程对应着特征空间的划分，也对应着决策树的构建。\n\n## (信息熵+信息增益)&ID3\n\n**信息熵(Entropy)用来度量不确定性的，当熵越大，信息的不确定性越大，对于机器学习中的分类问题，那么，当前类别的熵越大，它的不确定性就越大**\n\nH(D)代表一个决策树的熵，熵的数学公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727112506781.png)\n*n是分类的数目，$p_i$是当前分类发生的概率。*\n\n细想一下，如果10个硬币，分类结果是10个正面，没有反面，那么信息熵为0，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727144929893.png)\n信息熵为0，就意味着信息确定，就意味着分类完成了。\n\n\n\n在原有树的熵 H(D) 增加了一个分裂节点，使得熵变成了H(D|A)，则**信息增益(Information Gain，IG)**为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727114424888.png)\n*A是选择作为分裂依据的特征，g(D,A)也称为条件熵*\n\n**即信息增益 = 分裂前的信息熵-分裂后的信息熵**\n\n也有更加准确的定义方法：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727154914395.png)\n*V表示根据特征a对样本集D划分(分裂)后，获得的总共类别数量(一般是二分类)；$D^v$表示每一个新类别中样本数量；Ent(D)和H(D)的含义相同，表示信息熵*\n\n\n我们以判断学生好坏的案例，计算信息熵和信息增益：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725210419125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n（1）在初始状态下，有10个学生，7个是好学生，3个不是好学生，计算树的信息熵(此时只有一个根节点) `H(D)=0.88`，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725202826602.png)\n\n（2）我们根据分数这个特征对树进行分裂，设置分数的阈值为70，计算分裂后树的信息熵`H(D|A1)=0.4344`，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727140142378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n（3）把(2)的特征有分数改为出勤率，设置出勤率阈值为75%，计算分裂后树的信息熵`H(D|A2)=0.79`，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727140432832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n从上面的计算中，我们得到了3个熵：\n- 在初始状态下，信息熵为0.88，\n- 设置分数阈值为70进行分裂，信息熵为0.4344\n- 设置出勤率阈值为75%进行分裂，信息熵为0.79\n\n只要增加分裂节点后的熵比之前的熵小，那么就可以认为本次分裂是有效的，现在(2)和(3)的分裂都有效，但是哪个更好呢？\n在设置分数阈值为70的熵比设置出勤率阈值为75%的熵要小，即前者分裂后数据比较纯一些，整个数据的确定性大了，因此设置分数阈值为70分裂方式效果更好，所以我们以该方式为基准进行分裂。\n\n\n现在来计算信息增益，**信息增益 = 分裂前的信息熵-分裂后的信息熵**，计算如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727141600553.png)\n*A1是以分数这个特征进行分裂，A2是以出勤率这个特征进行分裂*\n\n可以看到，以设置分数阈值为70进行分裂，信息增益更大。\n\n> 信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好，该特征具有更强的分类能力，如果一个特征的信息增益为0，则表示该特征没有什么分类能力。\n\n\n\n**ID3的原理：**\n\n**利用数据标注和信息增益以及遍历，可以完成一个决策树中的特征和阈值的选择(得到最大信息增益的特征和阈值)，利用这三个(标注、信息增益、遍历)就可以完成一颗树，决策的树，分类的树，这个算法就是ID3()算法的思想**\n\n**ID3(Iterative Dichotomisor 3) 迭代二分类三代，用信息增益准则选择特征，判定分类器的性能，从而构建决策树**\n\n经过上面的例子，可以得到如下结论：\n- 只要分类后的总熵为0，那么这课树就训练完了（也就是分类完了）\n- 熵在决策树中的作用：判断分类后，有没有达到我们的需求和目的，也就是数据是不是更加纯了，更加确定了。如果经过分类后，信息总熵的值更加小了，那么这次分类就是有效的。\n- 熵越大，不确定就越大，分类未完成时，熵不为0\n- 熵越小，分得越好，分类完成时，熵为0\n\n\n根据上述思想，可以完成如下一颗树：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725212940738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n\n## (纯度+基尼系数)&CART\n在上面的例子中，我们是用决策树做分类的，那么做回归的时候该怎么做呢？ 现在把上述的分类结果由好学生换成是否为好学生的概率，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725214016787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n分类的结果是概率，是个连续的变量，无法计算信息熵和信息增益了，这个时候怎么做呢？\n\n为了解决这个问题，引入了纯度的概念。\n\n**纯度的定义如下：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727145525183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*𝑃_𝑙 和𝑃_r  是按照某一特征分裂后，树的两个分裂结点各自占的比例，Var是方差，就是计算左侧和右侧部分的方差，y_i是具体的值(这里是好学生的概率)*\n\n**纯度增益 = 分裂前的纯度 - 分裂后的纯度**\n\n可以看到，纯度和方差的定义大致一样(这里的纯度没有除以n，方差的定义需要除以n)，在有的文献中也称为偏差，其实含义都一样，都是**表示数据的离散程度**\n\n细想一下，如果上述10个学生的是好学生的概率都一样(比如都是0.9)，那么纯度(方差)是0，说明数据完全没有离散性，非常的确定。\n> 可以看到，纯度和信息熵所代表的含义是一样的，只不过熵表示分类信息的不确定性，纯度表示数据的离散程度，下面即将要介绍的基尼系数，表示的也是分类(CART的分类)信息的确定性\n\n（1）在初始状态下，计算树的纯度(此时只有一个根节点) `纯度=0.4076`，如下所示：\n\n```python\nimport numpy as np\na = [0.9,0.9,0.8,0.5,0.3,0.8,0.85,0.74,0.92,0.99]\na = np.array(a)\na_mean = np.mean(a)\n# 不用除以n\na_va = np.sum(np.square(a-a_mean))\na_va\n# 输出 0.4076000000000001\n```\n\n(2) 设置分数阈值为70进行分裂，计算`纯度=0.2703`，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725215757527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n(3) 设置出勤率阈值为75%进行分类，计算纯度：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072522060291.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n从上面的计算中，我们得到了3个纯度：\n- 在初始状态下，纯度为0.4076，\n- 设置分数阈值为70进行分裂，纯度为0.2703\n- 设置出勤率阈值为75%进行分裂，纯度0.1048\n\n从这三个纯度可以得知，这两种方式的分裂均有效，按照出勤率阈值=75%计算的纯度更小，说明分裂的效果更好，信息更确定了。\n\n也可以计算出纯度的增益：\n- 设置分数阈值为70进行分裂，纯度的增益为`0.4079 - 0.2703 = 0.1376`\n- 设置出勤率阈值为75%进行分裂，纯度的增益为`0.4079 - 0.1048 =0.3031`\n\n设置出勤率阈值为75%进行分裂得到纯度增益更大，说分裂效果更好(这里和信息熵及信息增益的原理是一样的)\n\n\n**思考：为什么决策树分类和回归的结果不同？**\n因为我们的标注不同，之前标注的是好学生与坏学生，现在重新标注了是好学生的概率，标注不同对我们模型训练的引导就不同，造成的结果就不同。\n\n\n**思考：将模型训练完之后怎么去得到具体回归的那个值呢？**\n如果最后只有一个结点，就是指这个结点的值，如果最后有多个结点，那应该是平均值\n\n**利用数据标注和纯度以及遍历，可以完成一个决策树中的特征和阈值的选择(得到最大信息增益的特征和阈值)，利用这三个(标注、纯度、遍历)就可以完成一颗树，决策的树，回归的树，这个算法就是CART的回归算法**\n\nCART做回归时用的是纯度，做分类时用的是基尼系数。\n\n\n**基尼系数**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727152553331.png)\n\n*n代表n分类，$p_i$表示不同类别的概率*\n\n按照某一特征分裂后的基尼系数为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727152816890.png)\n\n**基尼增益 = 分裂前的基尼 - 分裂后的基尼**\n\n我们以硬币来举例子解释基尼系数，假设10个硬币做二分类，10个是正面，没有反面，那么基尼系数为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725231308198.png)\n如果10个硬币做二分类，5个是正面，5个是反面，那么基尼系数为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725231336871.png)\n\n可以看到，当分类完成时，基尼系数为0，因此基尼系数与信息的含义类似，基尼系数越大，信息的不确定性就越大。\n\n**注意：基尼系数和信息熵都是用于分类的**\n\n然后，按照某一特征进行分裂，比如按照硬币的大小(或面值)进行分裂，计算分裂后的基尼系数，最后计算基尼增益，得到的基尼增益最大的特征和阈值就是我们要找的分裂方式。\n> 纯度增益、基尼增益、信息增益的原理完全一样\n\n\n**CART（classification and regression tree）分类和回归树，分类是使用基尼系数判定分类器的性能，回归时使用纯度判定分类器的性能**\n\n##  信息增益率&C4.5\n**信息增益准则对可取值数目较多的特征有所偏好**，为了减少这种偏好可能带来的不利影响，C4.5决策树算法使用了“增益率”：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072715533054.png)\n其中IV(a)称为特征a的“固有值”，称为特征a的分裂信息度量，其实就是特征a的信息熵(注意：这个和信息增益减去的按特征a分裂后的信息熵不一样，在下面具体例子中可看到)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727155553702.png)\n**需要注意的是，信息增益率对可取值数目较少的特征所有偏好，因此，C4.5算法并不是直接选择信息增益率最大的特征进行分裂，而是使用了一个启发式：先找出信息增益高于平均水平的特征，再从中选择增益率最高的。** 可以看出，使用信息增益率可以解决分裂后叶子结点过多的问题，从而解决过拟合\n\n我们用下面的例子计算信息增益率(抱歉，没找到更清晰的图片)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727160759867.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（1）第一步：计算样本集D的信息熵\n\nEnt(D) = `-9/14*log2(9/14) – 5/14*log2(5/14) = 0.940`\n*Ent(D)表示熵，也可以H(D)表示*\n\n（2）第二步：依据每个特征划分样本集D，并计算每个特征（划分样本集D后）的信息熵\n\n- Ent(天气) = `5/14*[-2/5*log2(2/5)-3/5*log2(3/5)] + 4/14*[-4/4*log2(4/4)] + 5/14*[-3/5log2(3/5) – 2/5*log2(2/5)] = 0.694`\n- Ent(温度) = 0.911\n- Ent(湿度) = 0.789\n- Ent(风速) = 0.892\n\n（3）第三步：计算信息增益\n\n- Gain(天气) = Ent(D) - Ent(天气) = 0.246\n- Gain(温度) = Ent(D) - Ent(温度) = 0.029\n- Gain(湿度) = Ent(D) - Ent(湿度) = 0.150\n- Gain(风速) = Ent(D) - Ent(风速) = 0.048\n\n（4）第四步：计算特征(属性)分裂信息度量\n\n- IV(天气) = `-5/14*log2(5/14) – 4/14*log2(4/14) – 5/14*log2(5/14) = 1.577`\n- IV(温度) = 1.556\n- IV(湿度) = 1.000\n- IV(风速) = 0.985\n\n（5）第五步：计算信息增益率\n\n- Gain_ratio(天气) = 0.246 / 1.577 = 0.155\n- Gain_ratio(温度) = 0.0187\n- Gain_ratio(湿度) = 0.151\n- Gain_ratio(风速) = 0.048\n\n可以看到，天气的信息增益率最高，选择天气为分裂属性。发现分裂了之后，天气是“阴”的条件下，类别是”纯“的，所以把它定义为叶子节点，选择不“纯”的结点继续分裂。\n\n我们在ID3和CART中的例子特征值是离散的，即特征值是一个个数值，不是本例中的类别，如果特征是类别的样本，就没有阈值的选择，直接按该特征的类别进行分裂，比如本例中按天气把决策树分裂成晴、阴、雨三个结点。\n\n思考：为什么说**信息增益准则对可取值数目较多的特征有所偏好**？\n\n就如本例中的天气特征，它的可取值数据为3个：晴、阴、雨，如果天气特征的可取值数目增加到5个，那么按照天气分裂后的信息熵就会降低的更多，它的信息增益就越大，因为天气特征的可取值数目越多，分裂的就清晰。如果把天气特征的可取值数目增加到与样本数一样，那么分裂后的信息熵就是0了，所以说信息增益准则对可取值数目较多的特征有所偏好。\n\n思考：为什么说**信息增益率对可取值数目较少的特征所有偏好**？\n\n还拿本例中的天气特征来说，它的可取值数据为3个：晴、阴、雨，如果天气特征的可取值数目减少到一个，那么天气特征(属性)分裂信息度量就为0了，此时，信息增益率就是无穷大，因此信息增益率对可取值数目较少的特征所有偏好。\n\n## 各种决策树比较与总结\n\n**ID3、CART、C4.5比较**\n信息增益和信息增益率通常用于离散型的特征划分，ID3和C4.5通常情况下都是多叉树，也就是根据离散特征的取值会将数据分到多个子树中，当然采用信息增益和信息增益率的时候也可以对连续特征进行划分并计算最优点，如上面判断好坏学生的例子。CART树为二叉树，使用基尼指数作为划分准则，对于离散型特征和连续行特征都能很好的处理。\n**离散型的特征是指：特征是分类，如天气：晴、阴、雨，不是连续的值\n连续型的特征是指：特征是一个个值，如分数、身高等，不是分类**\n\n\n**决策树的参数和训练方法：**\n\n- 决策树的参数：选择的特征及其对应的阈值，还有它的拓扑结构\n- 训练方法：就是遍历的方法，用纯度、基尼系数、信息熵、信息增益或信息增益率来表示\n- 决策树既可以做分类，也可以做回归，既可以做二分类，也可以做多分类\n\n**决策树中的分类器**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725221843381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n**如何求解决策树？**\n\n求解决策树，实际上就是求解分类器(求解每个特征和阈值)，步骤如下：\n1. 设定一个评价分类结果的好坏的准则(信息增益、基尼系数、纯度、信息增益率)\n2. 用遍历的方法求解\n\n\n## 决策树编程实战\n\n### 调用sklearn库实现回归决策树\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import linear_model\n \n# Data set\nx = np.array(list(range(1, 11))).reshape(-1, 1)\ny = np.array([5.56, 5.70, 5.91, 6.40, 6.80, 7.05, 8.90, 8.70, 9.00, 9.05]).ravel()\n \n# Fit regression model\nmodel1 = DecisionTreeRegressor(max_depth=1)\nmodel2 = DecisionTreeRegressor(max_depth=3)\nmodel3 = linear_model.LinearRegression()\nmodel1.fit(x, y)\nmodel2.fit(x, y)\nmodel3.fit(x, y)\n \n# Predict\nX_test = np.arange(0.0, 10.0, 0.01)[:, np.newaxis]\ny_1 = model1.predict(X_test)\ny_2 = model2.predict(X_test)\ny_3 = model3.predict(X_test)\n \n# Plot the results\nplt.figure()\nplt.scatter(x, y, s=20, edgecolor=\"black\",\n            c=\"darkorange\", label=\"data\")\nplt.plot(X_test, y_1, color=\"cornflowerblue\",\n         label=\"max_depth=1\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=3\", linewidth=2)\nplt.plot(X_test, y_3, color='red', label='liner regression', linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n```\n输出\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727174125995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n### 手动实现ID3决策树\n\n```python\n#coding:utf-8\nimport torch\nimport pdb\n\n# 样本的特征，每个样本包含七个特征\nfeature_space=[[1., 3., 2., 2., 3., 0.,3.], \n[2., 0., 2., 5., 1., 2.,3.], \n[3., 2., 3., 3., 2., 3.,2.], \n[4., 0., 3., 3., 2., 0.,1.], \n[3., 1., 2., 2., 5., 1.,3.], \n[1., 4., 3., 3., 1., 5.,2.], \n[3., 3., 3., 3., 1., 0.,1.], \n[5., 1., 1., 4., 2., 2.,2.], \n[6., 2., 3., 3., 2., 3.,0.], \n[2., 2., 2., 2., 5., 1.,4.]]\n\ndef get_label(idx):\n    label= idx\n    return label\n\n#  计算以某个特征某个阈值进行分裂时的信息熵，即条件熵\ndef cut_by_node(d,value,feature_space,list_need_cut):\n    # 分别存放以某个特征某个阈值分裂后的左右侧样本点\n    right_list=[]\n    left_list=[]\n    for i in list_need_cut:\n\n        if feature_space[i][d]<=value:\n             right_list.append(i)\n        else:\n             left_list.append(i)\n\n    left_list_t = list2label(left_list,[0,0,0,0,0,0,0,0,0,0])\n    right_list_t = list2label(right_list,[0,0,0,0,0,0,0,0,0,0])\n    e1=get_emtropy(left_list_t) \n    e2=get_emtropy(right_list_t) \n    n1 = float(len(left_list))\n    n2 = float(len(right_list))\n    e = e1*n1/(n2+n1) + e2*n2/(n1+n2)\n\n    return e,right_list,left_list\n\n# 将分到样本点转为one-hot各式，方便计算信息熵\ndef list2label(list_need_label,list_label):\n     for i in list_need_label:\n         label=get_label(i)\n         list_label[label]+=1\n     return list_label\n\ndef get_emtropy(class_list):\n   E = 0\n   sumv = float(sum(class_list))\n   if sumv == 0:\n       sumv =0.000000000001\n   for cl in class_list:\n       if cl==0:\n           cl=0.00000000001\n       p = torch.tensor(float(cl/sumv))\n       # log以2为底\n       E += -1.0 * p*torch.log(p)/torch.log(torch.tensor(2.))\n   return E.item()\n```\n\n```python\ndef get_node(complate,d,list_need_cut):\n    # 初始时的信息熵，设为最大(根节点计算之前的信息熵)\n    e = 10000000\n    # node 代表以那个维度的那个特征值进行分裂\n    node=[]\n    # list_select:树分裂好的一个个结点序列，不包含已经分类好可以识别的结点\n    list_select=[]\n    # 存放可以识别的结点\n    complate_select=[]\n    # 0~8就是选择的阈值\n    for value in range(0,8):\n        complate_tmp=[]\n        etmp=0\n        list_select_tmp=[]\n        # 子序列总的长度\n        sumv=0.000000001\n        # 要进行分裂的所有结点序列\n        for lnc in list_need_cut:\n\n            # 计算条件熵，返回熵、左侧结点的样本点，右侧结点的样本点\n            etmptmp,r_list,l_list=cut_by_node(d,value,feature_space,lnc)\n            # 这行代码和etmp/sumv 就是求分裂后的总熵，就是那个pl*H1+pr*H2的公式\n            etmp+=etmptmp*len(lnc)\n            sumv+=float(len(lnc))\n            # 存放可以识别的结点\n            if len(r_list)>1:\n                list_select_tmp.append(r_list)\n            if len(l_list)>1:\n                list_select_tmp.append(l_list)\n            if len(r_list)==1:\n                complate_tmp.append(r_list)\n            if len(l_list)==1:\n                complate_tmp.append(l_list)\n            # print('子序列child:以每个样本的第{}个特征，特征值大小为{}划分{}子序列，得到的熵为{}'.format(d,value,lnc,etmptmp))    \n            \n        etmp = etmp/sumv\n        sumv=0\n        \n        # print('总序列All：以每个样本的第{}个特征，特征值大小为{}划分{}序列，得到的总熵为{}'.format(d,value,list_need_cut,etmp))    \n        \n        # 得到第n个特征以某一阈值分裂后的最小信息熵，及此时分裂后序列\n        if etmp<e:\n            e=etmp\n            node=[d,value]\n            list_select=list_select_tmp\n            complate_select=complate_tmp\n    for ll in complate_select:\n         complate.append(ll)\n    return node,list_select,complate\n```\n\n```python\nimport pdb\ndef get_tree():\n#     pdb.set_trace()\n    # 10个样本：0, 1, 2, 3, 4, 5, 6, 7, 8, 9，注意是这里是二维数组\n    ## 二维数组里刚开始只有一个序列，即初始时只有一个根节点\n    all_list=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] \n    # complate的含义是：比如树的某个结点只有一个样本，那么存放到complate中\n    ## 即complate存放的是已经分类好的可以识别的结点\n    complate=[]\n    # 遍历样本的7个特征，这个序列就是遍历的先后顺序\n    ## d 首先遍历每个样本的第3个特征，把分类的结果返回\n    for d in [3,4,2,5,0,1,6]:\n        # node 代表以那个维度的那个特征值进行分裂\n        # all_list:树分裂好的一个个结点序列，不包含已经分类好可以识别的结点\n        node,all_list,complate=get_node(complate,d,all_list)\n        print(\"node=%s,complate=%s,all_list=%s\"%(node,complate,all_list))\n\nif __name__==\"__main__\":\n    get_tree()\n```\n输出：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727185457146.png)\n\n### 使用sklearn库回归决策树预测boston房价\n[decision_sklearn_tree_regressor_boston](https://gitee.com/wxler/AIProjectTraining/blob/master/practice/%E5%86%B3%E7%AD%96%E6%A0%91/decision_sklearn_tree_regressor_boston.ipynb)\n### 使用sklearn库分类决策树对iris分类\n[decision_sklearn_tree_classify_iris](https://gitee.com/wxler/AIProjectTraining/blob/master/practice/%E5%86%B3%E7%AD%96%E6%A0%91/decision_sklearn_tree_classify_iris.ipynb)\n\n参考文档\n[信息增益、信息增益比、基尼指数的比较](https://www.cnblogs.com/rezero/p/13057584.html)\n[华校专的决策树](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/4_decision_tree.html)\n[机器学习（二）-信息熵，条件熵，信息增益，信息增益比，基尼系数](https://www.cnblogs.com/xiaofeiIDO/p/11947380.html)\n","tags":["决策树"],"categories":["机器学习"]},{"title":"k-means及k-means++原理【python代码实现】","url":"/2020/11/24/215155/","content":"\n\n\n## 前言\n\nk-means算法是无监督的聚类算法，实现起来较为简单，k-means++可以理解为k-means的增强版，在初始化中心点的方式上比k-means更友好。\n\n<!-- more -->\n\n\n\n## k-means原理\nk-means的实现步骤如下：\n1. 从样本中随机选取k个点作为聚类中心点\n2. 对于任意一个样本点，求其到k个聚类中心的距离，然后，将样本点归类到距离最小的聚类中心，直到归类完所有的样本点（聚成k类）\n3. 对每个聚类求平均值，然后将k个均值分别作为各自聚类新的中心点\n4. 重复2、3步，直到中心点位置不在变化或者中心点的位置变化小于阈值\n\n优点：\n- 原理简单，实现起来比较容易\n- 收敛速度较快，聚类效果较优\n\n缺点：\n- 初始中心点的选取具有随机性，可能会选取到不好的初始值。\n\n## k-means++原理\n\n**k-means++是k-means的增强版，它初始选取的聚类中心点尽可能的分散开来，这样可以有效减少迭代次数，加快运算速度**，实现步骤如下：\n1. 从样本中随机选取一个点作为聚类中心\n2. 计算每一个样本点到已选择的聚类中心的距离，用D(X)表示：D(X)越大，其被选取下一个聚类中心的概率就越大\n3. 利用**轮盘法**的方式选出下一个聚类中心(D(X)越大，被选取聚类中心的概率就越大)\n4. 重复步骤2，直到选出k个聚类中心\n5. 选出k个聚类中心后，使用标准的k-means算法聚类\n\n>这里不得不说明一点，有的文献中把**与已选择的聚类中心最大距离的点选作下一个中心点**，这个说法是不太准确的，准的说是**与已选择的聚类中心最大距离的点被选作下一个中心点的概率最大**，但不一定就是改点，因为总是取最大也不太好（遇到特殊数据，比如有一个点离某个聚类所有点都很远）。\n\n一般初始化部分，始终要给些随机。因为数据是随机的。\n\n尽管计算初始点时花费了额外的时间，但是在迭代过程中，k-mean 本身能快速收敛，因此算法实际上降低了计算时间。\n\n现在重点是利用**轮盘法**的方式选出下一个聚类中心，我们以一个例子说明K-means++是如何选取初始聚类中心的。\n\n假如数据集中有8个样本，分布分布以及对应序号如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200726212919102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n我们先用 k-means++的步骤1选择6号点作为第一个聚类中心，然后进行第二步，计算每个样本点到已选择的聚类中心的距离D(X)，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200726213127851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n- D(X)是每个样本点与所选取的聚类中心的距离(即第一个聚类中心)\n- P(X)每个样本被选为下一个聚类中心的概率\n- Sum是概率P(x)的累加和，用于轮盘法选择出第二个聚类中心。\n\n然后执行 k-means++的第三步：利用**轮盘法**的方式选出下一个聚类中心，**方法是随机产生出一个0~1之间的随机数，判断它属于哪个区间，那么该区间对应的序号就是被选择出来的第二个聚类中心了**。\n\n在上图1号点区间为[0,0.2)，2号点的区间为[0.2, 0.525)，4号点的区间为[0.65,0.9)\n\n从上表可以直观的看到，1号，2号，3号，4号总的概率之和为0.9，这4个点正好是离第一个初始聚类中心(即6号点)较远的四个点，因此选取的第二个聚类中心大概率会落在这4个点中的一个，其中2号点被选作为下一个聚类中心的概率最大。\n\n## k-means及k-means++代码实现\n\n这里选择的中心点是样本的特征(不是索引)，这样做是为了方便计算，选择的聚类点(中心点周围的点)是样本的索引。\n\n**k-means实现**\n\n```python\n# 定义欧式距离\nimport numpy as np\ndef get_distance(x1, x2):\n    return np.sqrt(np.sum(np.square(x1-x2)))\n```\n\n```python\nimport random\n# 定义中心初始化函数，中心点选择的是样本特征\ndef center_init(k, X):\n    n_samples, n_features = X.shape\n    centers = np.zeros((k, n_features))\n    selected_centers_index = []\n    for i in range(k):\n        # 每一次循环随机选择一个类别中心,判断不让centers重复\n        sel_index = random.choice(list(set(range(n_samples))-set(selected_centers_index)))\n        centers[i] = X[sel_index]\n        selected_centers_index.append(sel_index)\n    return centers\n```\n\n```python\n# 判断一个样本点离哪个中心点近， 返回的是该中心点的索引\n## 比如有三个中心点，返回的是0，1，2\ndef closest_center(sample, centers):\n    closest_i = 0\n    closest_dist = float('inf')\n    for i, c in enumerate(centers):\n        # 根据欧式距离判断，选择最小距离的中心点所属类别\n        distance = get_distance(sample, c)\n        if distance < closest_dist:\n            closest_i = i\n            closest_dist = distance\n    return closest_i\n```\n\n```python\n# 定义构建聚类的过程\n# 每一个聚类存的内容是样本的索引，即对样本索引进行聚类，方便操作\ndef create_clusters(centers, k, X):\n    clusters = [[] for _ in range(k)]\n    for sample_i, sample in enumerate(X):\n        # 将样本划分到最近的类别区域\n        center_i = closest_center(sample, centers)\n        # 存放样本的索引\n        clusters[center_i].append(sample_i)\n    return clusters\n```\n\n```python\n# 根据上一步聚类结果计算新的中心点\ndef calculate_new_centers(clusters, k, X):\n    n_samples, n_features = X.shape\n    centers = np.zeros((k, n_features))\n    # 以当前每个类样本的均值为新的中心点\n    for i, cluster in enumerate(clusters):  # cluster为分类后每一类的索引\n        new_center = np.mean(X[cluster], axis=0) # 按列求平均值\n        centers[i] = new_center\n    return centers\n```\n\n```python\n# 获取每个样本所属的聚类类别\ndef get_cluster_labels(clusters, X):\n    y_pred = np.zeros(np.shape(X)[0])\n    for cluster_i, cluster in enumerate(clusters):\n        for sample_i in cluster:\n            y_pred[sample_i] = cluster_i\n            #print('把样本{}归到{}类'.format(sample_i,cluster_i))\n    return y_pred\n```\n\n```python\n# 根据上述各流程定义kmeans算法流程\ndef Mykmeans(X, k, max_iterations,init):\n    # 1.初始化中心点\n    if init == 'kmeans':\n        centers = center_init(k, X)\n    else: centers = get_kmeansplus_centers(k, X)\n    # 遍历迭代求解\n    for _ in range(max_iterations):\n        # 2.根据当前中心点进行聚类\n        clusters = create_clusters(centers, k, X)\n        # 保存当前中心点\n        pre_centers = centers\n        # 3.根据聚类结果计算新的中心点\n        new_centers = calculate_new_centers(clusters, k, X)\n        # 4.设定收敛条件为中心点是否发生变化\n        diff = new_centers - pre_centers\n        # 说明中心点没有变化，停止更新\n        if diff.sum() == 0:\n            break\n    # 返回最终的聚类标签\n    return get_cluster_labels(clusters, X)\n```\n\n```python\n# 测试执行\nX = np.array([[0,2],[0,0],[1,0],[5,0],[5,2]])\n# 设定聚类类别为2个，最大迭代次数为10次\nlabels = Mykmeans(X, k = 2, max_iterations = 10,init = 'kmeans')\n# 打印每个样本所属的类别标签\nprint(\"最后分类结果\",labels)\n## 输出为  [1. 1. 1. 0. 0.]\n```\n\n```python\n# 使用sklearn验证\nfrom sklearn.cluster import KMeans\nX = np.array([[0,2],[0,0],[1,0],[5,0],[5,2]])\nkmeans = KMeans(n_clusters=2,init = 'random').fit(X)\n# 由于center的随机性，结果可能不一样\nprint(kmeans.labels_)\n```\n**k-means++实现**\n\n```python\n## 得到kmean++中心点\ndef get_kmeansplus_centers(k, X):\n    n_samples, n_features = X.shape\n    init_one_center_i = np.random.choice(range(n_samples))\n    centers = []\n    centers.append(X[init_one_center_i])\n    dists = [ 0 for _ in range(n_samples)]\n\n    # 执行\n    for _ in range(k-1):\n        total = 0\n        for sample_i,sample in enumerate(X):\n            # 得到最短距离\n            closet_i = closest_center(sample,centers)\n            d = get_distance(X[closet_i],sample)\n            dists[sample_i] = d\n            total += d\n        total = total * np.random.random()\n\n        for sample_i,d in enumerate(dists): # 轮盘法选出下一个聚类中心\n            total -= d\n            if total > 0:\n                continue\n            # 选取新的中心点\n            centers.append(X[sample_i])\n            break\n    return centers\n```\n\n```python\nX = np.array([[0,2],[0,0],[1,0],[5,0],[5,2]])\n# 设定聚类类别为2个，最大迭代次数为10次\nlabels = Mykmeans(X, k = 2, max_iterations = 10,init = 'kmeans++')\nprint(\"最后分类结果\",labels)\n## 输出为  [1. 1. 1. 0. 0.]\n```\n\n```python\n# 使用sklearn验证\nX = np.array([[0,2],[0,0],[1,0],[5,0],[5,2]])\nkmeans = KMeans(n_clusters=2,init='k-means++').fit(X)\nprint(kmeans.labels_)\n```\n\n参考文档\n[K-means与K-means++](https://www.cnblogs.com/wang2825/articles/8696830.html)\n[K-means原理、优化及应用](https://blog.csdn.net/weixin_42029738/article/details/81978038)","tags":["k-means"],"categories":["机器学习"]},{"title":"在jupyter中使用python pdb调试代码","url":"/2020/11/24/215133/","content":"\n> 目前在jupyter中还没有可视化调试界面，而python pdb是代码调试的一个不错的选择，它支持设置断点和单步调试，使用起来非常方便\n\n<!-- more -->\n\n\n\n\n## pdb常用命令\n\n| 参数 | 说明      |  实例\n|:--------:| :-----------|:-------------|\n|`h`  | help 帮助文档 | `h b`： 查看 b 命令的文档|\n|`b` | break 打断点|`b`：查看所有断点 <br> `b 5`： 给第5行打断点 <br>    `b function_name`：当前文件名为 function_name 的函数打断点<br> `b test1.A.add`：在 import test1 文件的 A 类的 add 方法打断点   <br> `b A.add`：在 A 类的 add 方法打断点 |\n|`tbreak` | 设置临时断点，运行完毕后会删除这个断点| 设置方法和 b 一样|\n|`w `| where 查看当前执行的位置| `w`|\n| `cl`| clear 清除断点| `cl`：清除所有断点  <br> `cl 2`：清除断点列表中编号为2断点 <br> `cl test.py:18`：清除 test.py 文件编号为18断点 <br>`cl test1:18`：清除 import test1 文件编号为18的断点 |\n|`condition ` | 给断点设置条件| `condition 1 i==4`：当断点列表中编号为1的断点中变量 i 等于 4 的时候执行断点|\n|`s` | step 执行下一条命令，遇到函数则进入\t| 参考下面执行效果|\n|`n` |next 执行下一条语句，遇到函数不进入 |参考下面执行效果 |\n| `c`|continue  继续执行，直到遇到下一条断点\t |参考下面执行效果 |\n|`r` | return  执行当前运行函数到结束| 参考下面执行效果  |\n|`args` | args  打印当前函数的所有参数及参数值 | 参考下面执行效果 |\n| `p`| print 打印出当前所在函数中的变量或表达式结果 | `p a`：打印变量a <br> `p dir(a)`：打印变量a所有属性|\n|`pp`| 格式化打印出来的结果| `pp a`：格式化打印变量a|\n`run`| 重新执行| |\n|`q` | quit 退出pdb调试| |\n\n## pdb进阶阶命令\n| 参数 | 说明      |  实例\n|:--------:| :-----------|:-------------|\n| `l`| list 列出当前或范围周围代码  |`l 5, 20`： 列出5到20行代码 <br>`l`： 查看当前位置的代码 |\n|`disable ` |  停用断点|  `disable`：清除所有断点  <br> `disable 2`：清除断点列表中编号为2断点 <br> `disable test.py:18`：清除 test.py 文件编号为18断点 <br>`disable test1:18`：清除 import test1 文件编号为18的断点|\n|`enable`  |启动断点\t | 用法和`disable`一样|\n| `ignore bpnumber`| 忽略某个断点几次| `ignore 1 3`：忽略断点列表中第1个断点3次，一般循环中用， |\n| `commands`| 给断点写一个脚本执行 | `commands 1`：给断点编号为1的的断点写脚本 |\n| `unt`| until 执行到下一行 | 参考下面 unt 执行效果 |\n| `j`| jump 跳转至指定程序行，如果是前行，则忽略中间行代码。<br>如果是后退，状态重设为回退行状态 | 注意：是跳转到不是执行 |\n|`alias` | 自定义一个函数，参数可由％1，％2来表示。<br>类似 Python 的 lambda| |\n|`unalias` | 删掉别名函数| `unalias name`|\n\n\n\n\n## 实例1\n代码如下：\n\n```python\nimport pdb\npdb.set_trace()\ndef mul(a, b,e = 88):\n    c = a * b\n    return c\n\nfor i in range(10):\n    a = i\n    b = i + 1\n    r = mul(a, b)\n    print(r)\n```\n\n\n使用`condition`给编号为6的断点设置条件为`i==3`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725175733203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n给第8行设置断点，然后输入n单步执行(遇到函数不进入)：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072518002253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n给第8行设置断点，但函数位置输入s遇到函数进入，然后输入r，直接执行到函数尾部：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725180402190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n输入`args`打印当前函数的所有参数及参数值，注意：只有在函数内部该命令才有效\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725181732246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n使用`commands 22`为编号为22的断点编写脚本\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725183606110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n使用`unt`命令执行到下一行\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725183902126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n\n## 示例2\n代码如下：\n\n```python\nimport pdb\npdb.set_trace()\nclass A():\n    def __init__(self,value):\n        self.value = value\n    def printParam(self):\n        print(self.value)\nv = 3\na = A(v)\na.printParam()\n```\n\n\n输入`p dir(a)` 打印a的所有属性：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725181606662.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n输入`l`列出当前位置的代码\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725182355224.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n参考文档\n[python pdb 代码调试 - 最全最详细的使用说明](https://www.jianshu.com/p/8e5fb5fe0931)\n\n","tags":["jupyter"],"categories":["工具"]},{"title":"手推公式带你轻松理解L1/L2正则化","url":"/2020/11/24/214850/","content":"\n## 前言\n\n\n>L1/L2正则化的目的是为了解决过拟合，因此我们先要明白什么是过拟合、欠拟合。\n\n- 过拟合：训练出的模型在测试集上Loss很小，在训练集上Loss较大\n- 欠拟合：训练出的模型在测试集上Loss很大，在训练集上Loss也很大\n- 拟合：训练的刚刚好，在测试集上Loss很小，在训练集上Loss也很小\n\n现在，让我们开启L1/L2正则化正则化之旅吧！\n\n<!-- more -->\n\n\n\n## L1/L2正则化原理\nL1与L2正则是通过在损失函数中增加一项对网络参数的约束，使得参数渐渐变小，模型趋于简单，以防止过拟合。\n\n**损失函数Loss**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724182727639.png)\n*上述Loss，MSE均方误差的Loss*\n\n**L1正则化的损失函数**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724182742754.png)\n*W代表网络中的参数，超参数λ需要人为指定。需要注意的是，**L1使用绝对值来约束参数***\n\n**L2正则化的损失函数**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724182808385.png)\n*相比于L1正则化，L2正则化则使用了平方函数来约束网络参数*\n\n> 需要注意的是，在有的文献中，把L2正则项定义为权值向量w中各个元素的平方和然后再求平方根，其实，L2正则加不加平方根影响不大，原理都是一样的，但不加平方根更容易数学公式推导\n\n\n我们知道，当W的值比较大时(即W的值距离0很远，取几百甚至几千的值)，则拟合的曲线比较陡，x稍微一变化，y的影响就比较大，如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724184041339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n可以看到，你的模型复杂度越大，拟合的曲线就越陡，惩罚项W就越大，在这种情况容易出现过拟合，所以要避免W出现比较大的值，一个有效的方法是给loss加上一个与W本身有关的值，即L1正则项或L2正则项，这样，我们在使用梯度下降法让Loss趋近于0的时候，也必须让W越来越小，W值越小，模型拟合的曲线会越平缓，从而防止过拟合。也可以从奥卡姆剃刀原理的角度去解释，即在所有可以选择的模型中，能够很好拟合当前数据，同时又十分简单的模型才是最好的。\n\nL1与L2正则化让W变小的原理是不同的：\n\n- **L1能产生等于0的权值，即能够剔除某些特征在模型中的作用（特征选择），即产生稀疏的效果**。\n- **L2可以得迅速得到比较小的权值，但是难以收敛到0，所以产生的不是稀疏而是平滑的效果**。\n\n下面，从两个角度理解L1/L2正则化这两个结论\n\n##  从数学的角度理解L1/L2正则化\n我们来看看**L1正则化的损失函数的求导及梯度更新公式**：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724201746252.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*lr是学习率(更新速率)，上述求导是Loss或$Loss_l1$对$w_i$的偏导，为了方便书写，将$w_i$写成W*\n\n- 上面是加上L1正则化的损失函数后，W更新公式及loss对W求梯度的公式(准确的说是对wi求偏导)，|W|对Wi的导数是1或-1，这样W更新公式就变为加上或减去一个常量lr，\n- 就是说权值每次更新都固定减少一个特定的值(比如0.01)，那么经过若干次迭代之后，权值就有可能减少到0。\n\n**L1正则化的损失函数的求导及梯度更新公式(假设$\\lambda=1/4$)**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724203126899.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n- 从上面的公式中可以看到，加上L2正则项后，W实际上每次变为原来的C倍(另一项忽略不计），假如C=0.5，那么，W每次缩小为原来的一半，虽然权值W不断变小，但是因为每次都等于上一次的一半，所以很快会收敛到较小的值但不为0。\n\n## 从几何的角度理解L1/L2正则化\n由于L1正则化项是$|w_1+w_2+...+w_n|$，为了便于理解，我们假设，L1正则项是$w_1+w_2$，将其画在二维坐标轴上如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724204849791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n- 横轴是w1,纵轴是w2，优化空间是一个等高线图，可以看优化曲线在圆圈上移动时，**只有直达W2轴的交点处，才能满足两个条件：让loss最小，让w1+w2最小，此时w2=0**，所以说，L1中两个权值倾向于一个较大另一个为0即产生稀疏的效果\n\n由于L2正则化使用了平方函数，而如果两个参数的平方和相同，呈现出的形状会是一个圆。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724205519269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n- 可以看优化曲线在圆圈上移动时，**只有在两个圆圈的交点处，才能满足两个条件：让loss最小，让$w^1+w^2$最小，此时L2中两个权值都倾向于为非零的较小数**，所以说，L2产生平滑的效果。\n\n\n\n## L1/L2正则化使用情形\n\n- L1能产生等于0的权值，即能够剔除某些特征在模型中的作用（特征选择），即产生稀疏的效果，如果需要做模型的压缩，L1正则是一个不错的选择。\n\n- 如果不做模型的压缩，在实际中更倾向于L2正则化，因为在实际操作的过程中，模型该用多少层，模型的参数量，这个不好确定，我们也不知道解决这个实际问题要用多少层网络，用多少个参数，只能去试。在试的过程中，最快最经济的做法是：**在模型最开始的时候就加上正则化，不管是在欠拟合或过拟合都加上正则化，然后就不断的去训练，后面根据模型的拟合情况只要增加模型参数和模型结构就行了，不用考虑其他的，即把正则化作为默认的选项去试就可以了。**\n\n\n\n","tags":["normalize"],"categories":["normalize"]},{"title":"numpy和torch数据类型转化问题","url":"/2020/11/24/214655/","content":"\n>在实际计算过程中，float类型使用最多，因此这里重点介绍numpy和torch数据float类型转化遇到的问题，其他类型同理。\n\n<!-- more -->\n\n\n\n## numpy数据类型转化\n\n- numpy使用astype转化数据类型，float默认转化为64位，可以使用`np.float32`指定为32位\n\n\n```python\n#numpy转化float类型\na= np.array([1,2,3])\na = a.astype(np.float)\nprint(a)\nprint(a.dtype)\n```\n\n`[1. 2. 3.]`  \n`float64`\n    \n\n- 不要使用a.dtype指定数据类型，会使数据丢失\n\n\n```python\n#numpy转化float类型\nb= np.array([1,2,3])\nb.dtype= np.float32\nprint(b)\nprint(b.dtype)\n```\n\n`[1.e-45 3.e-45 4.e-45]`  \n`float32`\n    \n\n- 不要用float代替np.float，否则可能出现意想不到的错误\n- 不能从np.float64位转化np.float32，会报错\n- np.float64与np.float32相乘，结果为np.float64\n\n> 在实际使用过程中，可以指定为np.float，也可以指定具体的位数，如np.float，不过直接指定np.float更方便。\n\n## torch数据类型转化\n\n- torch使用`torch.float()`转化数据类型，float默认转化为32位，torch中没有`torch.float64()`这个方法\n\n\n```python\n# torch转化float类型\nb = torch.tensor([4,5,6])\nb = b.float()\nb.dtype\n```\n\n\n\n\n    torch.float32\n\n\n\n- `np.float64`使用`torch.from_numpy`转化为torch后也是64位的\n\n\n```python\nprint(a.dtype)\nc = torch.from_numpy(a)\nc.dtype\n```\n\n`float64`  \n`torch.float64`\n\n\n\n- 不要用float代替torch.float，否则可能出现意想不到的错误\n- torch.float32与torch.float64数据类型相乘会出错，因此相乘的时候注意指定或转化数据float具体类型\n\n> np和torch数据类型转化大体原理一样，只有相乘的时候，torch.float不一致不可相乘，np.float不一致可以相乘，并且转化为np.float64\n\n\n## numpy和tensor互转\n- tensor转化为numpy\n\n```python\nimport torch\nb = torch.tensor([4.0,6])\n# b = b.float()\nprint(b.dtype)\nc = b.numpy()\nprint(c.dtype)\n```\n\n`torch.int64`  \n`int64`\n\n- numpy转化为tensor\n\n```python\nimport torch\nimport numpy as np\nb= np.array([1,2,3])\n# b = b.astype(np.float)\nprint(b.dtype)\nc = torch.from_numpy(b)\nprint(c.dtype)\n```\n`int32`  \n`torch.int32`  \n\n**可以看到，torch默认int型是64位的，numpy默认int型是32位的**","tags":["pytorch","numpy"],"categories":["pytorch"]},{"title":"Batch Normalization：批量归一化详解","url":"/2020/11/24/214400/","content":"\n## 为什么要使用BN\n在深度学习中，层数很多，可能有几十层甚至上百层，每次训练激活的过程中，参数不断改变，导致后续每一层输入的分布也发生变化，而学习的过程就是要使每一层适应输入的分布，如果不做BN就会使模型学的很小心，也会使学习速率变慢，因此我们不得不降低学习率、小心地初始化。\n\n**那怎么才能让我们的模型学习的更高效呢？**\n原有的方式可能是歪着学的，学的效果不是很好，如下图B所示，现在我们需要让它激活到一个更适合学习的位置上，那就需要把它放到原点，这个位置更适合，这时候就需要BN了\n>原点周围更敏感，假定一开始你的数据不在原点周围，后面如果越来越偏，说不定会偏去哪，也就是指W一会大一会小，一会是正值，一会是负值，如下图B所示，也不利于更新。\n\n<!-- more -->\n\n\n\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723220500179.png)\n\n\n## BN的工作原理\n>批量归一化 （Batch Normalization， BN）方法是一种有效的逐层归一化方法，可以对神经网络中任意的中间层进行归一化操作。\n\n\nBatch Normalization，顾名思义，以进行学习时的batch为单位，按batch进行规范化。具体而言，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布，用数学式表示的话，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072322071327.png)\n\n*m代表batch的大小，$μ_B$为批处理数据的均值，$σ^2_B$为批处理数据的方差。*\n\n减去平均值是将数据放在原点周围，除以方差是因为让原来挤在一起的数据变得更均匀一些，如下图C->图D，原来分散的，也会让它们更加紧促一些，在这样一个数据分布里面，使得非线性变换函数的输入值落入对输入比较敏感的区域，从而避免梯度消失问题。这样输入的小变化就会导致损失函数较大的变化（使得梯度变大，避免梯度消失)。可以看出，提高参数更新的效率条件有：让数据的分布在原点附近，让数据分布不离散也不紧凑。\n\n\n将数据转化为均值为0、方差为1的数据分布后，接着，BN层会对正规化后的数据进行缩放和平移的变换，用数学式可以如下表示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723222606550.png)\n\n*这里，γ和β是参数。一开始γ=1，β=0，然后再通过学习调整到合适的值。*\n\n**思考：为什么BN要引入线性变化操作？**\n\nBN层相当于固定了每一层的输入分布，从而加速网络模型的收敛速到，但是这也限制了网络模型中数据的表达能力，浅层学到的参数信息会被BN的操作屏蔽掉，因此，BN层又增加了一个线性变换操作，让数据尽可能地恢复本身的表达能力\n\n\n## BN的优点\n**缓解梯度消失，加速网络收敛速度**。BN层可以让激活函数(非线性变化函数)的输入数据\n落入比较敏感的区域，缓解了梯度消失问题。\n\n**简化调参的负担，网络更稳定**。在调参时，学习率调得过大容易出现震荡与不收敛，BN层则抑制了参数微小变化随网络加深而被放大的问题，因此对于参数变化的适应能力更强，更容易调参。\n\n**防止过拟合**。BN层将每一个batch的均值与方差引入到网络中，由于每个batch的这两个值都不相同，可看做为训练过程增加了随机噪音，可以起到一定的正则效果，防止过拟合\n\n\n在测试时应该注意的问题：\n**在测试时，由于是对单个样本进行测试，没有batch的均值与方差，通常做法是在训练时将每一个batch的均值与方差都保留下来，在测试时使用所有训练样本均值与方差的平均值。**\n\n\n## PyTorch中使用BN层\n\n```python\n>>> import torch.nn as nn\n>>> import torch\n# 使用BN层需要传入一个参数为num_features，即特征的通道数\n>>> bn = nn.BatchNorm2d(64)\n# eps为公式中的є，momentum为均值方差的动量，affine为添加可学习参数\n>>> bn\nBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>>> input = torch.randn(4, 64, 224, 224)\n>>> output = bn(input)\n>>> output.shape\n# BN层不改变输入、输出的特征大小\ntorch.Size([4, 64, 224, 224])\n>>>     \n```\n## BN的弊端\n管BN层取得了巨大的成功，但仍有一定的弊端，主要体现在以下两点：\n- 由于是在batch的维度进行归一化，BN层要求较大的batch才能有效地工作，而物体检测等任务由于占用内存较高，限制了batch的大小，这会限制BN层有效地发挥归一化功能。\n- 数据的batch大小在训练与测试时往往不一样。**在训练时一般采用滑动来计算平均值与方差(一个batch一个batch计算)，在测试时直接拿训练集的平均值与方差来使用**。这种方式会导致测试集依赖于训练集，然而有时训练集与测试集的数据分布并不一致。\n\n\n因此，我们能不能避开batch来进行归一化呢？答案是可以的，最新的GN（Group  Normalization）从通道方向计算均值与方差，使用更为灵活有效，避开了batch大小对归一化的影响。具体来讲，GN先将特征图的通道分为很多个组，对每一个组内的参数做归一化，而不是batch。GN之所以能够工作的原因，可以认为是在特征图中，不同的通道代表了不同的意义，例如形状、边缘和纹理等，这些不同的通道并不是完全独立地分布，而是可以放到一起进行归一化分析。\n\n\n参考文档\n\n深度学习之PyTorch物体检测实战[董洪义著]\n深度学习入门基于Python的理论与实现[斋藤康毅著，陆宇杰译]\n神经网络与深度学习[邱锡鹏著]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["normalize"],"categories":["normalize"]},{"title":"神经网络之多维卷积的那些事(一维、二维、三维)","url":"/2020/11/24/214142/","content":"\n## 前言\n一般来说，一维卷积用于文本数据，二维卷积用于图像数据，对宽度和高度都进行卷积，三维卷积用于视频及3D图像处理领域（检测动作及人物行为），对立方体的三个面进行卷积 。二维卷积的用处范围最广，在计算机视觉中广泛应用。\n<!-- more -->\n\n\n## 一维卷积Conv1d\n一维卷积最简单，实质是对一个词向量做卷积，如下所示：\n![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vd3hsZXIvYmxvZ2ltZy9yYXcvbWFzdGVyL2ltZ3MvMjAyMDA3MTAxNzM0NTEucG5n?x-oss-process=image/format,png)\n\n- 图中的输入的数据维度为8，过滤器的维度为5。卷积后输出的数据维度为8−5+1=4\n- 如果过滤器数量仍为1，输入数据的channel数量变为16，则输入数据维度为8×16\n- 一维卷积常用于序列模型，自然语言处理领域。\n\n\nPytorch中nn.Conv1d卷积运算要求输入源是3维，输入源的三个维度分别是：第一个维度代表每个序列的个数即样本数，第二个维度代表每一个序列的通道数，第三个维度代表这个词向量序列，如下所示：\n```python\nimport torch\nimport torch.nn as nn\n# 输入源：1个样本，16个通道，8个数据\na = torch.randn(1,16,8)\n# 卷积：输入通道为16，输出通道为1，卷积核大小 5*5\nconv = nn.Conv1d(16, 1, 5)\nc = conv(a)\nprint('a:', a.size())\nprint('c:', c.size())\n```\n\n**output**\n\n```\na: torch.Size([1, 16, 8])\nc: torch.Size([1, 1, 4])\n```\n\n## 二维卷积Conv2d\n二维卷积是最常见、用途最广泛的卷积。先假定卷积核(过滤器)数量为1，图片通道数为1，卷积操作如下：\n![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vd3hsZXIvYmxvZ2ltZy9yYXcvbWFzdGVyL2ltZ3MvMjAyMDA3MTAxNzM4NDUucG5n?x-oss-process=image/format,png)\n\n- 图中的输入的数据维度为`14×14`，卷积核数量为1，图片通道数为1\n- 二维卷积输出的数据尺寸为`8−5+1=4`，即`4×4`\n\n这是最简单的二维卷积的场景，现在重点来了，假定图片通道数为3，卷积核的数量为1，则卷积操作如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723171151348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n- 如上图所示，输入源是`6*6*3`（图片大小`6*6`,三个通道），卷积核大小`3*3`，filters(卷积核数量)=1,即权重矩阵是`3*3*3*1`，得到的结果`4*4*1`（图片大小`4*4`,一个通道）\n- 其实就是三个`3*3`的卷积核分别对图片的三个通道做卷积，然后把结果相加得到一个`4*4`的图片。所以，这里卷积核w的参数个数是`(3*3*3+1)*1`，(输入通道`3`，卷积核大小`3*3`，一个偏置，输出通道1)\n- 上图卷积 Pytorch中表示为：`nn.Conv2d(3,1,kernel_size=(3,3),stride=1)`\n\n**到这里可能有人会问，卷积核大小为3\\*3，为什么变成3\\*3\\*3了？**\n可以细想一下，图片的大小是`6*6*3`（图片大小6\\*6,三个通道），也就是三维的图片，二维的卷积是肯定不能对其操作的，所以卷积核的维度会随着图片的输入通道改变，如果图片的输入通道是3，那么卷积核的维度也是3，如果图片的输入通道是1，那么卷积核的维度也是1，这也就是为什么Pytorch中nn.Conv2d的输入通道要与图片的输入通道保持一致的原因，否则无法进行卷积操作。\n\n**现在，假定图片通道数为3，卷积核的数量为2，则卷积操作如下**：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723172020944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n- 如上图所示，输入源是`6*6*3`（图片大小`6*6`,三个通道），卷积核大小`3*3`，filters=2，即权重矩阵是`3*3*3*2`，得到的结果`4*4*2`\n- 上图每一个卷积核卷积参数的个数是`(3*3*3+1)*2`\n- 上图卷积 Pytorch中表示为：`nn.Conv2d(3,2,kernel_size=(3,3),stride=2)`\n\n计算图如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731150746946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n二维卷积常用于计算机视觉、图像处理领域。\n\n\nPytorch中nn.Conv2d卷积运算要求输入源是4维，输入源的四个维度分别是：第一个维度代表图片的个数即样本数，第二个维度代表每一张图片的通道数，后面二个维度代表图片的像素矩阵，如下所示：\n\n```python\nimport torch\nimport torch.nn as nn\na = torch.Tensor([[[[1,2,3,4],\n                [5,6,7,8],\n                [9,10,11,12],\n                [13,14,15,16]]],\n               [[[1,2,3,4],\n                [5,6,7,8],\n                [9,10,11,12],\n                [13,14,15,16]]]])\nprint('a:',a.size())\n# 卷积核：输入通道为1，输出通道6，卷积核大小2*2\nconv = nn.Conv2d(1,6,2)\nc = conv(a)\nprint('c:',c.size())\n\nconv1 = nn.Conv2d(6,16,2)\nc1 = conv1(c)\nprint('c1:',c1.size())\n```\n\n**output**\n```\na: torch.Size([2, 1, 4, 4])\nc: torch.Size([2, 6, 3, 3])\nc1: torch.Size([2, 16, 2, 2])\n```\n\n## 三维卷积Conv3d\n\n![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vd3hsZXIvYmxvZ2ltZy9yYXcvbWFzdGVyL2ltZ3MvMjAyMDA3MTAxNzU3NTMucG5n?x-oss-process=image/format,png)\n\n三维卷积具体思想与一维卷积、二维卷积相同，假设卷积核大小为`f1*f2*f3`(类似于二维卷积，三维卷积实际计算的时候卷积核是四维的，另一个维度由输入源的通道数决定)\n\n- 假设输入数据的大小为`a1×a2×a3`\n- 基于上述情况，三维卷积最终的输出为`(a1−f1+1)×(a2−f2+1)×(a3−f3+1)`\n- 三维卷积常用于医学领域（CT影响），视频处理领域（检测动作及人物行为）。\n\nPytorch中nn.Conv3d要求输入源是5维的，输入源的5个维度分别表示为：第一个维度代表样本的个数，第二个维度代表每个样本的通道数，后面三个维度代表三维立体图形的像素矩阵，如下所示：\n\n```python\nimport torch\nimport torch.nn as nn\nx = torch.randn(1,2,6,1,1)\nconv = nn.Conv3d(in_channels=2,\n                 out_channels=6,\n                 kernel_size=(2,1,1))\nc = conv(x)\nprint('x:', x.size())\nprint('c:', c.size())\n```\n\n**output**\n```\nx: torch.Size([1, 2, 6, 1, 1])\nc: torch.Size([1, 6, 5, 1, 1])\n```\n*说明：通道数从输入的2转化为6，一个立体像素矩阵，输入前大小为6\\*1\\*1, 卷积核2\\*1\\*1，得到结果为(6-2+1)\\*(1-1+1)\\*(1-1+1)=5\\*1\\*1*\n\n\n## 卷积中的特征图大小计算方式\n在神经网络卷积操作主要是提取特征的，因此卷积的输出称为**特征图(FeatureMap)**，神经网络中有多层卷积，所以除了最开始输入的原始图像外，我们认为卷积输入的也是特征图\n\n当卷积操作步长为1的时候，进行卷积操作后特征图的尺寸比较容易计算，如果步长为2或者更大就不容易计算了。其实这里有一个通用的公式，如下所示：\n$$W_{out} = \\frac{ W_{in}+2*padding-K}{stride} + 1$$\n其中，$W_{out}$为输出特征图的大小，$W_{in}$为输入特征图的大小，K为卷积核大小，stride为卷积步长，padding为特征图填充的圈数。\n\n这里不得不提一下，**池化操作对上述公式也适用，我看有的文章里说，卷积除不尽的结果都向下取整，池化除不尽的结果都向上取整，这个说法是错误，我经过测试得出Pytorch和TensorFlow默认都是向下取整**。如下所示：\n\n**卷积操作向下取整**\n```python\n# 卷积操作向下取整\nimport torch\nimport torch.nn as nn\na = torch.rand(1,1,10,10)\n# (10+2*2-5)/2.0+1 = 5.5\nconv2d = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(5,5),padding=2,stride=2)\noutput = conv2d(a)\noutput.size()\n# 输出为：torch.Size([1, 1, 5, 5])\n```\n**池化默认也是向下取整**\n```python\n# 池化默认也是向下取整\nimport torch\nimport torch.nn as nn\na = torch.rand(1,1,10,10)\n# (10+2*2-5)/2.0+1 = 5.5\nmaxpool = nn.MaxPool2d(kernel_size=(5,5),stride=2,padding=2)\noutput = maxpool(a)\noutput.size()\n# 输出为：torch.Size([1, 1, 5, 5])\n```\n**但是，Pytorch中池化层有个参数ceil_node=True是向上取整的，它默认是False，TensorFlow里面没找到类似的参数**\n\n```python\n# 池化层ceil_mode=True向上取整\nimport torch\nimport torch.nn as nn\na = torch.rand(1,1,10,10)\n# (10+2*2-5)/2.0+1 = 5.5\nmaxpool = nn.MaxPool2d(kernel_size=(5,5),stride=2,padding=2,ceil_mode=True)\noutput = maxpool(a)\noutput.size()\n# 输出为：torch.Size([1, 1, 6, 6])\n```\n\n\n\n\n## 总结\n到这里我们学到了\n- 不同卷积的应用\n- 多维卷积的原理\n- 多维卷积核参数的个数\n- 卷积中的特征图计算方式\n\n是不是感觉收获满满！！！\n\n\n\n","tags":["卷积"],"categories":["神经网络"]},{"title":"PyTorch之torchvision.transforms详解[原理+代码实现]","url":"/2020/11/24/213959/","content":"## 前言\n\n我们知道，在计算机视觉中处理的数据集有很大一部分是图片类型的，如果获取的数据是格式或者大小不一的图片，则需要进行归一化和大小缩放等操作，这些是常用的数据预处理方法。如果参与模型训练中的图片数据非常有限，则需要通过对有限的图片数据进行各种变换，如缩小或者放大图片的大小、对图片进行水平或者垂直翻转等，这些都是数据增强的方法。庆幸的是，这些方法在torch.transforms中都能找到，在torch.transforms中有大量的数据变换类，有很大一部分可以用于实现**数据预处理（Data Preprocessing）和数据增广（Data Argumentation）**。\n<!-- more -->\n\n\n## torchvision.transforms常用变换类\n\n\n### transforms.Compose\ntransforms.Compose类看作一种容器，它能够同时对多种数据变换进行组合。传入的参数是一个列表，列表中的元素就是对载入的数据进行的各种变换操作。\n\n**首先使用PIL加载原始图片**\n\n```python\n#Pyton Image Library  PIL 一个python图片库\nfrom PIL import Image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimg = Image.open(\"./imgs/dianwei.jpg\")\nprint(img.size)\nplt.imshow(img)\n```\n输出：\n`(1102, 735)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103029941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n```python\ntransformer = transforms.Compose([                                \n    transforms.Resize(256),\n    transforms.transforms.RandomResizedCrop((224), scale = (0.5,1.0)),\n    transforms.RandomHorizontalFlip(),\n])\ntest_a = transformer(img)\nplt.imshow(test_a)\n```\n输出：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103150561.png)\n\n\n### transforms.Normalize(mean, std)\n\n这里使用的是标准正态分布变换，这种方法需要使用原始数据的均值（Mean）和标准差（Standard Deviation）来进行数据的标准化，在经过标准化变换之后，数据全部符合均值为0、标准差为1的标准正态分布。计算公式如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723113549916.png)\n一般来说，mean和std是实现从原始数据计算出来的，对于计算机视觉，更常用的方法是从样本中抽样算出来的或者是事先从相似的样本预估一个标准差和均值。如下代码，对三通道的图片进行标准化：\n\n```python\n# 标准化是把图片3个通道中的数据整理到规范区间 x = (x - mean(x))/stddev(x)\n# [0.485, 0.456, 0.406]这一组平均值是从imagenet训练集中抽样算出来的\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n```\n### transforms.Resize(size)\n对载入的图片数据按照我们的需要进行缩放，传递给这个类的size可以是一个整型数据，也可以是一个类似于 (h  ,w) 的序列。如果输入是个(h,w)的序列，h代表高度，w代表宽度，h和w都是int，则直接将输入图像resize到这个(h,w)尺寸，相当于force。如果使用的是一个整型数据，则将图像的短边resize到这个int数，长边则根据对应比例调整，图像的长宽比不变。\n```python\n# 等比缩放\ntest1 = transforms.Resize(224)(img)\nprint(test1.size)\nplt.imshow(test1)\n```\n输出：\n`(335, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103305748.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n### transforms.Scale(size)\n对载入的图片数据我们的需要进行缩放，用法和torchvision.transforms.Resize类似。。**传入的size只能是一个整型数据**，`size`是指缩放后图片最小边的边长。举个例子，如果原图的`height>width`,那么改变大小后的图片大小是`(size*height/width, size)`。\n```python\n# 等比缩放\ntest2 = transforms.Scale(224)(img)\nprint(test2.size)\nplt.imshow(test2)\n```\n输出：\n`(335, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103356414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n### transforms.CenterCrop(size)\n以输入图的中心点为中心点为参考点，按我们需要的大小进行裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于(h,w)的序列。**如果输入的是一个整型数据，那么裁剪的长和宽都是这个数值**\n```python\ntest3 = transforms.CenterCrop((500,500))(img)\nprint(test3.size)\nplt.imshow(test3)\n```\n输出：\n`(500, 500)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103440830.png)\n```python\ntest4 = transforms.CenterCrop(224)(img)\nprint(test4.size)\nplt.imshow(test4)\n```\n输出：\n`(224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103532667.png)\n\n\n\n### transforms.RandomCrop(size)\n用于对载入的图片按我们需要的大小进行随机裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于(h,w)的序列。**如果输入的是一个整型数据，那么裁剪的长和宽都是这个数值**\n```python\ntest5 = transforms.RandomCrop(224)(img)\nprint(test5.size)\nplt.imshow(test5)\n```\n输出：\n`(224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103718102.png)\n```python\ntest6 = transforms.RandomCrop((300,300))(img)\nprint(test6.size)\nplt.imshow(test6)\n```\n输出：\n`(300, 300)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072710391757.png)\n\n\n\n### transforms.RandomResizedCrop(size,scale)\n\n先将给定图像随机裁剪为不同的大小和宽高比，然后缩放所裁剪得到的图像为size的大小。即先随机采集，然后对裁剪得到的图像安装要求缩放，默认scale=(0.08, 1.0)。scale是一个面积采样的范围，假如是一个100\\*100的图片，scale = (0.5,1.0)，采样面积最小是0.5\\*100\\*100=5000，最大面积就是原图大小100\\*100=10000。先按照scale将给定图像裁剪，然后再按照给定的输出大小进行缩放。\n```python\ntest9 = transforms.RandomResizedCrop(224)(img)\nprint(test9.size)\nplt.imshow(test9)\n```\n输出：\n`(224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104231793.png)\n```python\ntest9 = transforms.RandomResizedCrop(224,scale=(0.5,0.8))(img)\nprint(test9.size)\nplt.imshow(test9)\n```\n输出：\n`(224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104330982.png)\n\n\n\n\n### transforms.RandomHorizontalFlip\n用于对载入的图片按随机概率进行水平翻转。我们可以通过传递给这个类的参数自定义随机概率，如果没有定义，则使用默认的概率值0.5。\n```python\ntest7 = transforms.RandomHorizontalFlip()(img)\nprint(test7.size)\nplt.imshow(test7)\n```\n输出：\n`(1102, 735)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104041714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n### transforms.RandomVerticalFlip\n用于对载入的图片按随机概率进行垂直翻转。我们可以通过传递给这个类的参数自定义随机概率，如果没有定义，则使用默认的概率值0.5。\n```python\ntest8 = transforms.RandomVerticalFlip()(img)\nprint(test8.size)\nplt.imshow(test8)\n```\n输出：\n`(1102, 735)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104127721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n### transforms.RandomRotation\n```python\ntransforms.RandomRotation(\n    degrees,\n    resample=False,\n    expand=False,\n    center=None,\n    fill=None,\n)\n```\n- 功能：按照degree随机旋转一定角度\n- degree：加入degree是10，就是表示在（-10，10）之间随机旋转，如果是（30，60），就是30度到60度随机旋转\n- resample是重采样的方法\n- center表示中心旋转还是左上角旋转\n\n```python\ntest10 = transforms.RandomRotation((30,60))(img)\nprint(test10.size)\nplt.imshow(test10)\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/202009022028180.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n### transforms.ToTensor\n用于对载入的图片数据进行类型转换，将之前构成PIL图片的数据转换成Tensor数据类型的变量，让PyTorch能够对其进行计算和处理。\n\n### transforms.ToPILImage\n用于将Tensor变量的数据转换成PIL图片数据，主要是为了方便图片内容的显示。\n\n## torchvision.transforms编程实战\n```python\n# RandomResizedCrop 将给定图像随机裁剪为不同的大小和宽高比，然后缩放所裁剪得到的图像为制定的大小\nprint(\"原图大小：\",img.size)\n# Crop代表剪裁到某个尺寸\ndata1 = transforms.RandomResizedCrop(224)(img)\n# data1、data2、data3尺寸一样，长宽都是224*224  size也可以是一个Integer，在这种情况下，切出来的图片的形状是正方形\nprint(\"随机裁剪后的大小:\",data1.size)\ndata2 = transforms.RandomResizedCrop(224)(img)\ndata3 = transforms.RandomResizedCrop(224)(img)\n\n# 放四个格，布局为2*2\nplt.subplot(2,2,1),plt.imshow(img),plt.title(\"Original\")\nplt.subplot(2,2,2),plt.imshow(data1),plt.title(\"Transform 1\")\nplt.subplot(2,2,3),plt.imshow(data2),plt.title(\"Transform 2\")\nplt.subplot(2,2,4),plt.imshow(data3),plt.title(\"Transform 3\")\nplt.show()\n```\n输出：\n`原图大小： (1102, 735)`\n`随机裁剪后的大小: (224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072710460549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n```python\n# 以输入图的中心点为中心点做指定size的crop操作\nimg1 = transforms.CenterCrop(224)(img)\nimg2 = transforms.CenterCrop(224)(img)\nimg3 = transforms.CenterCrop(224)(img)\n# img1、img2、img3三个图是一样的\nplt.subplot(2,2,1),plt.imshow(img),plt.title(\"Original\")\nplt.subplot(2,2,2), plt.imshow(img1), plt.title(\"Transform 1\")\nplt.subplot(2,2,3), plt.imshow(img2), plt.title(\"Transform 2\")\nplt.subplot(2,2,4), plt.imshow(img3), plt.title(\"Transform 3\")\nplt.show()\n```\n输出：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104637657.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n```python\n# 以给定的概率随机水平旋转给定的PIL的图像，默认为0.5\nimg1 = transforms.RandomHorizontalFlip()(img)\nimg2 = transforms.RandomHorizontalFlip()(img)\nimg3 = transforms.RandomHorizontalFlip()(img)\n\nplt.subplot(2,2,1),plt.imshow(img),plt.title(\"Original\")\nplt.subplot(2,2,2), plt.imshow(img1), plt.title(\"Transform 1\")\nplt.subplot(2,2,3), plt.imshow(img2), plt.title(\"Transform 2\")\nplt.subplot(2,2,4), plt.imshow(img3), plt.title(\"Transform 3\")\nplt.show()\n```\n输出：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104701915.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n源码在[PyTorch之torchvision.transforms实战](https://gitee.com/wxler/AIProjectTraining/blob/master/practice/PyTorch%E4%B9%8Btorchvision.transforms%E5%AE%9E%E6%88%98.ipynb)，请自提！\n\n\n参考文档\n深度学习pytoch实战计算机视觉(唐进民著)\n\n\n","categories":["数据预处理"]},{"title":"深入理解GAN对抗生成网络","url":"/2020/11/24/213812/","content":"\n##  什么是GAN\n>Generative Adversarial Networks，生成式对抗网络，Ian Goodfellow 在2014 年提出的一种生成式模型\n基本思想来自博弈论的二人零和博弈（纳什均衡）, 由一个生成器和一个判别器构成，通过对抗学习来训练\n\n- 生成器的目的是尽量去学习真实的数据分布\n- 判别器的目的是尽量正确判别输入数据是来自真实数据还是来自生成器\n- 生成器和判别器就是一个矛和盾互相PK的过程\n- 为了取得游戏胜利，这两个游戏参与者需要不断优化， 各自提高自己的生成能力和判别能力，这个学习优化过程就是寻找二者之间的一个纳什均衡\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722120217603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*G代表生成器，D代表判别器，Z是输入源，称为Noise source，就是一个随机编码。给出一个random code(即Z)由生成器G生成假数据X'，假数据X'和真实数据X喂给判别器D，由D判别出哪个是real，哪个是fake，这个就是gan的基本原理*\n\n<!-- more -->\n\n@[TOC]\n\n\n## 纳什均衡\n我们以囚徒困境的例子来解释纳什均衡的含义\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722120703173.png)\nA和B属于零和游戏，需要在A和B的决策中进行Trade Off(权衡)，由此看出，抵赖对两个人来说都是最优的结果，这个就是纳什均衡。\n\n>亚当·斯密的“看不见的手”，在市场经济中，每一个人都从利己的目的出发，不断调和与迭代，最终全社会达到利他的效果\n\n## GAN的学习过程\nGAN的学习过程其实就是把D和G达成一个均衡，这是我们的目标，因此不仅要训练G，也要训练D\n\n**为什么D和G是对抗的？**\nG是生成器，它生成的数据是虚假的，它目标是让生成的数据骗过D。D是判别器，它的目标是要把虚假的数据给找出来，因此D和G是对抗的\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072212094128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n对于 GAN 的学习过程 ，需要训练模型D来最大化判别数据来源于真实数据或者伪数据分布 ，同时，我们需要训练模型 G来最小化 loss。\n\n**我们该采用怎样的优化方法，对生成器G和判别器D进行优化呢？**\n\n- Step1，固定生成器 G=>优化判别器 D，让D的判别准确率最大化\n- Step2，固定判别器 D => 优化生成器 G，让D的判别准确率最小化\n\n训练 GAN 时，在同一轮参数更新中，通常对 D 的参数更新 k 次，再对 G的参数更新 1 次，这样做的目的是让D学的更快点，因为我们最终要的是G，为此需要把D这个教练先变得越来越好，由此才能训练出更好的G\n\n**Generator与Discriminator的工作原理**\n\n- Generator，在输入一个随机编码（random code）z之后，它将输出一幅由神经网络自动生成的、假的图片G(z)\n- Discriminator，接受G输出的图像作为输入，然后判断这幅图像的真假，真的输出1，假的输出0\n- G生成的图像会越来越逼真，D也越来越会判断图片的真假，最后我们就不要D了，直接用G来生成图像\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722121543593.png)\n我们就是要在最大化D的能力的前提下，最小化D对G的判断能力 ，所以称之为 最小最大值问题\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722121618497.png)\n*损失函实际上是一个交叉熵，判别器的目的是尽可能的令D(x)接近1(对于真图像x的处理评分要高)，令D(G(z))接近0(对于假图像G(z)的处理评分要尽量降低)，所以D主要是最大化上面的损失函数(让D的辨别能力更强)，G恰恰相反，他主要是最小化上述损失函数(让生成的假图像G(z)变得更真实)。*\n\n为了增强D的能力，我们分别考虑输入真的图像和假的图像的情况\n\n**D的目标是什么？G的目标是什么？**\n- D的目标是：D(G(z))处理的是假图像G(z) => 评分D(G(z))要尽量降低，对于真图像x的处理 => 评分要高，这样D的辨别能力才会更强\n- G的目标是：让生成的假图像G(z)变得更真实、更逼真，让D难以辨别\n\n\n\n## GAN的局限性\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722121848640.png)\n\n用户输入random code由生成器G参数 fake image即G(Z)，传统的GAN中会出现如下局限性：\n\n 1. 在传统的GAN里，由于没有用户控制能力，输入一个随机噪声，就会输出一幅随机图像（可能输出猫在左边或是猫在右边的图像）\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722121929235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n>这里user input 是指输入的random code，output是G生成的G(Z)，还要从现实世界中取一张或者画一张真实图像，与output一起输入判别器\n\n2. 低分辨率（Low resolution）和低质量（Low quality）问题\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722122102813.png)\n生成的图片看起来不错，但放大看，会发现细节相当模糊\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722122125898.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n## 如何改善GAN的局限性\n如改善GAN的局限性可以从以下两个方面入手：\n- 提高GAN的用户控制能力\n- 提高GAN生成图片的分辨率和质量\n\n从以上两个方面提出新的算法模型：\n\n1. pix2pix，有条件的使用用户输入，使用成对的数据（paired data）进行训练。比如，输入的是猫在左边，你就不能生成猫在右边的图。Pix2pix的缺点：在训练过程中，需要人为给它标出数据的对应关系，比如 现在的输入条件是猫在左边，人要给它画一张或找一张猫在左边的图像，才会让G更好的学习，以产生猫在右边的图像，这对数据源的要求会很高\n2. CycleGAN，使用不成对的数据（unpaired data）就能训练。  \n以马为例，马训练马是成对的数据，用马生成斑马，是不成对数据。由于现实生活中成对的样本比较少，对于没有成对样本的情况，使用CycleGAN，CycleGAN有两个生成器，马->斑马，斑马->马，我们最终想要的是马->斑马，利用理论上开始的马和马->斑马->马是一样的，同时优化马->斑马，斑马->马两个生成器，最终使用马->斑马。\nCycleGAN本质是优化生成器的一个思想，拿文本翻译来说，你把一段英文翻译成中文，再把中文翻译回英文，假如翻译回来的英文和一开始的英文天差地别，那么这个两次翻译的结果肯定是很差的；反之，如果能够让翻译回来的英文和原本的一样，就相当于是改进了两次翻译的效果，CycleGAN利用这种方式来优化生成器。\n\n\n3. pix2pixHD，生成高分辨率、高质量的图像\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722122528208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n未来还会持续更新：\n\n- Conditional GAN\n- pix2pix\n- CycleGAN\n- GauGAN\n\n谢谢支持！","tags":["GAN"],"categories":["神经网络"]},{"title":"Pycharm 2020 中导入Anaconda3创建的环境","url":"/2020/11/24/213628/","content":"\n## 在pycharm配置环境Anaconda环境\n之前用的Anaconda3中的jupyter Lab写python程序，后来根据需要用到pycharm，又不想重新安装python库，直接用到Anaconda3中下载好的库该有多好，现在尝试用pycharm2020配置Anaconda3创建的环境。\n<!-- more -->\n\n\n\n\n假设pycharm2020和Anaconda3安装好了，现在就开始配置流程吧，选择File->Setting->Project Interpreter，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707153748538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n点击右上角齿轮->add![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707153849133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n选择Conda Environment->Existing enviroment\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707153952159.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n到这里只要选择你的python虚拟环境所在的目录就行了，不知道conda安装的python虚拟环境在哪里？这好办，在开始菜单打开Anconda Powershell Prompt\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707154203781.png)\n然后执行`jupyter kernelspec list`,就可以显示Anaconda所有的python内核环境,如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/202007071544150.png)\n不过这里显示的可不是python环境真正的目录，以上图第一个虚拟环境所示，在文件管理器中打开`D:\\install\\anaconda3\\share\\jupyter\\kernels\\python3`，可以找到文件kernel.json，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707154711688.png)\n上面划线部分才是真正python虚拟环境所在的目录，将`D:/install/anaconda3\\\\python.exe`应用到Interpreter所在位置即可\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707154847886.png)\n*注意：Make available to all projects:应用到所有项目，推荐勾选*\n\n目录加载之后，会自动加载该python环境下的包，如下所示，点击apply即可\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707155226636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n## 用配置好的环境新建项目\n在pycharm新建有两种方式，分别是New enviroment using 和 Existing interpreter，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707155611812.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n- New enviroment using：用新环境创建项目\n- Existing interpreter：用已存在的环境创建项目\n\n###  用Existing interpreter方式新建项目\n\n我们先用Existing interpreter创建项目，项目命名为test4，创建完成后，导入torch（在Anaconda安装了pytorch，并没有在pycahrm中安装），新建一个tset.py，可以看到，程序完美运行。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707160012207.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\ntest.py运行结果如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707160621212.png)\n### 用New enviroment using方式新建项目\n\n- New environment using 设置新的依赖环境。它是pycharm自带的virtualenv创建项目，可以在项目目录中新建一个venv（virtualenv）目录，用于存放虚拟的python环境，这里所有的类库依赖都可以直接脱离系统安装的python独立运行。\n- Location：填写新环境的文件目录\n- Base interpreter下拉框：选择基础解释器，默认是环境中配置的，可以修改。\n- Inherit global site-packages：可以使用base interpreter（基础解释器）中的第三方库，可能会花费时间进行复制；如果不勾选将和外界完全隔离，会在base interpreter的基础上创建一个新的虚拟解释器。\n- Make available to all projects：是否将此虚拟环境提供给其他项目使用。勾选之后，可以提供给其他项目，等再新建下一个项目的时候，可以修改Base interpreter，位置指向现在建立的虚拟环境。\n\n看到上面的解释，我们大致对New enviroment using方式新建项目有所了解了。现在，新建一个项目，命名为test5。该方式创建项目的过程会比较慢，因为它要项目加载所需要类库\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707161010286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n可以看到，以这种方式新建的项目目录下，会多出一个venv目录（用于存放该项目用到的类库），如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707162417938.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n下图可以看出，以这种方式新建的项目，不能直接使用Anaconda的python环境\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707162630586.png)\n如果想要使用Anaconda的python环境，只需要将其Project Interpreter改为我们刚刚配置的Conda环境即可。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707162849405.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n从上面可以看出，用New enviroment using方式新建项目，不仅无法使用conda中的环境，还会自己生成一个用于存放python解释器的目录，所以创建的项目会非常大，不推荐使用。\n\n\n参考文档\n\n[pycharm创建工程的两种方式](https://www.cnblogs.com/xiaohailuo/p/11083211.html)\n[PyCharm新建项目教程](https://blog.csdn.net/Aidiying/article/details/104889259)\n[Pycharm 2020 中导入Anaconda3创建的环境](https://blog.csdn.net/qq_37555071/article/details/107182623)\n","categories":["工具"]},{"title":"hexo设置permalink以比避免url中出现中文","url":"/2020/11/24/204707/","content":"\n当我把hexo的博客标题的时候，url中就会出现中文，很不雅观，这里我通过permalink设置博客链接！\n\n\n\n<!-- more -->\n\n\n\n## 第一步\n\n在_config.yml文件中修改permalink\n\n```bash\n# permalink: :year/:month/:day/:title/ 这是之前的设置\npermalink: :year/:month/:day/:id/\npermalink_defaults:\n```\n\n## 第二步\n\n第一步的`:id`是自己添加的，因此需要在`scaffolds/post.md`中添加id，如下:\n\n```bash\ntitle: {{ title }}\nid: \ndate: {{ date }}\ncategories: Life  #文章分类\ntags: [tag1,tag2]  #文章标签，多标签时使用英文逗号隔开\n```\n\n我一般是把id设置为时分秒，如现在是`20:47:07`，我将id设置为`204707`\n\n\n\n\n\n参考文档\n\n[hexo设置permalink-避免url中出现中文](https://blog.csdn.net/weixin_30394669/article/details/97839708)  ","tags":["hexo"],"categories":["工具"]},{"title":"Anaconda中离线升级jupyterlab并为jupyterlab安装插件","url":"/2020/11/24/194523/","content":"\n## Anaconda中升级jupyterlab\n\n我之前尝试了如下两种方法，升级失败：\n\n- `conda update -c conda-forge jupyterlab`\n- 在Anaconda Navigator 界面升级\n\n后来直接在[anaconda官网](https://anaconda.org/)下载jupyterlab的安装文件，然后执行`conda install 文件名`就安装成功了。\n<!-- more -->\n\n\n\n\n首先，在[anaconda官网](https://anaconda.org/)下载文件时，在搜索栏输入jupyterlab，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200706193847540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n然后点击文件名，进如下页面，再点击Files就可以下载文件\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200706194046685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n最后，打开Anaconda Powershell Prompt（如果配置了环境变量，直接打开cmd也可以），`cd 文件所在目录`，执行`conda install 文件名`就成功啦。比如我下载的是jupyterlab-2.1.5-py_0.tar.bz2，执行`conda install jupyterlab-2.1.5-py_0.tar.bz2`即可，**安装成功后，会默认覆盖Anaconda自带的jupyterlab，所以就意味着升级了jupyterlab**，安装jupyterlab之后，并不能在shell命令窗口直接输入jupyterlab直接启动，但是可以在Anaconda Navigator界面启动。如果想要在shell窗口启动，则需要配置conda环境变量。\n\n**这个安装方法其实算是一类离线安装方法，无法通过命令安装的anaconda插件，都可以在[anaconda官网](https://anaconda.org/)下载之后离线安装**\n\n## 升级jupyterlab插件出现问题\n升级之和，之前在低版本jupyterlab下安装的一部分插件就可能过时，在Anaconda Powershell Prompt执行`jupyter labextension list`就可以查看插件的情况，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200706195338601.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n遇见过时的插件，可以执行`jupyter labextensio update 插件名`或`jupyter labextension update --all`来更新插件，但是我尝试了之后，没有一个插件能够更新成功，索性就执行`jupyter labextensio uninstall 插件名`把过时的插件都卸载了，然后执行`jupyter labextensio install 插件名`安装需要的插件即可，比如我要安装jupyter的目录插件，可以执行：\n\n```powershell\njupyter labextension install @jupyterlab/toc\n或者\njupyter labextension install https://github.com/jupyterlab/jupyterlab-toc.git\n```\n\n## conda&jupyterlab插件相关命令\n### conda命令\n删除一个名为 mytest 的环境或库。-n为该环境或库的名字，--all 说明删除 mytest 环境下的所有内容，也就是这个环境被删除了：`conda remove -n mytest --all`\n\n删除一个库`conda remove 库名`,卸载一个库`conda uninstall 库名 --force`，根据帮助中的描述,这两个命令是一样的。假如如果你安装了tensorflow和numpy,想把numpy降级到另外一个版本。使用conda uninstall numpy会把tensorflow、pytorch等其他依赖numpy的库一起删除.此时加上conda uninstall numpy --force就仅卸载numpy了.一定要看看conda 的帮助.然后在安装需要的numpy版本。\n\n在不指定的情况下，conda install命令默认从 conda 官网 https://conda.anaconda.org/ 上下载。比如下面的，conda-forge 是一个用户，他上传了一个 opencv 的 python 库。opencv=3.2.0 指定了版本，不指定的情况下，下载最新版本：\n```powershell\nconda install -c conda-forge opencv=3.2.0\n```\n当然，你也可以使用 -c 参数，指定一个远程仓库，从这个仓库中下载：\n```powershell\nconda install -c https://conda.anaconda.org/menpo opencv3\n```\n\n### jupyterlab插件命令\n- 更新插件：`jupyter labextensio update 插件名`\n- 更新所有插件：`jupyter labextension update --all`\n- 卸载插件：`jupyter labextensio uninstall 插件名`\n- 安装插件：`jupyter labextensio install 插件名`\n- 远程仓库安装插件：`jupyter labextension install 参考地址`\n- 安装制定版本插件：`jupyter labextensio install 插件名=版本号`\n- 查看已安装插件：`jupyter labextension list`\n\n\n\n\n\n\n参考文档\n\n[附录C：conda相关命令](https://www.jianshu.com/p/b2b46dd6332b)","categories":["工具"]},{"title":"数据预处理：归一化/标准化详解","url":"/2020/11/24/192901/","content":"\n## 前言\n\n一般而言，样本的原始特征中的每一维特征由于来源以及度量单位不同，其特征取值的分布范围往往差异很大，比如身高、体重、血压等它们的度量和分布范围往往是不一样的。当我们计算不同样本之间的欧氏距离时，取值范围大的特征会起到主导作用。这样，**对于基于相似度比较的机器学习方法（比如最近邻分类器），必须先对样本进行预处理，将各个维度的特征归一化到同一个取值区间，并且消除不同特征之间的相关性，才能获得比较理想的结果**。虽然神经网络可以通过参数的调整来适应不同特征的取值范围，但是会导致训练效率比较低。\n\n<!-- more -->\n\n\n\n\n\n## 归一化的必要性及价值\n现在假设一个只有一层的网络：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723211349432.png)\n我们知道，tanh 函数的导数在区间 [−2, 2] 上是敏感的，其余的导数接近于 0，tanh图像如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723211508510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n因此，如果 $w1x1 + w2x2 + b$ 过大或过小，都会导致梯度过小，难以训练。**为了提高训练效率，我们需要使$w1x1 + w2x2 + b$在 [−2, 2] 区间，我们需要将w1 设得小一点，比如在 [−0.1, 0.1] 之间。可以想象，如果数据维数很多时，我们很难这样精心去选择每一个参数**。因此，如果每一个特征的取值范围都在相似的区间，比如 [0, 1] 或者 [−1, 1]，那该多好啊，我们就不太需要区别对待每一个参数，减少人工干预。\n\n>我们经常见到，归一化、标准化、规范化，其实他们的含义是一样的，都是消除数据量纲带来的差异，加快模型的训练效率\n\n除了参数初始化之外，不同输入特征的取值范围差异比较大时，梯度下降法的效率也会受到影响。下图给出了数据归一化对梯度的影响。其中，图a为未归一化数据的等高线图。**取值范围不同会造成在大多数位置上的梯度方向并不是最优的搜索方向。当使用梯度下降法寻求最优解时，会导致需要很多次迭代才能收敛**。如果我们把数据归一化为取值范围相同，如图b所示，大部分位置的梯度方向近似于最优搜索方向。这样，**在梯度下降求解时，每一步梯度的方向都基本指向最小值，训练效率会大大提高**。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723212620723.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n## 归一化的方式\n归一化的方法有很多种，最常用的是最小最大归一化和标准归一化\n\n**最小最大归一化**使结果落到[0,1]区间，转换函数如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723213531858.png)\n\n*其中 min(x) 和 max(x) 分别是特征 x 在所有样本上的最小值和最大值。*\n\n**标准归一化**也叫 z-score 归一化，将每一个维特征都处理为符合标准正态分布（均值为 0，标准差为 1）。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723214316862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*这里 σ 不能为 0。如果标准差为 0，说明这一维特征没有任务区分性，可以直接删掉。在标准归一化之后，每一维特征都服从标准正态分布。*\n\n## 总结\n\n总的来说，归一化的好处是：帮助你去除数据的量纲和数据大小的差异，让数据每一个特征的取值范围都在相似的区间，可以让数据在同一个数量级下来做一个比较。这样做可以让模型更快的收敛，因为它不需要去考虑那些夸大的特征，把所有特征的尺度看的同等重要\n\n参考文档\n神经网络与深度学习[邱锡鹏著]","categories":["数据预处理"]},{"title":"深入理解model.eval()与torch.no_grad()","url":"/2020/11/24/192512/","content":"\n我们用pytorch搭建神经网络经常见到model.eval()与torch.no_grad()，它们有什么区别？是怎么工作的呢？现在就让我们来探究其中的奥秘\n<!-- more -->\n\n## model.eval()\n\n- 使用model.eval()切换到测试模式，不会更新模型的k，b参数\n- 通知dropout层和batchnorm层在train和val中间进行切换\n在train模式，**dropout层会按照设定的参数p设置保留激活单元的概率（保留概率=p，比如keep_prob=0.8），batchnorm层会继续计算数据的mean和var并进行更新**\n在val模式下，**dropout层会让所有的激活单元都通过，而batchnorm层会停止计算和更新mean和var，直接使用在训练阶段已经学出的mean和var值**\n- model.eval()不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反向传播(backprobagation)\n\n## torch.no_grad()\n使用方法：\n```python\nwith torch.no_grad()：\n\t# 代码块\n```\n\n\n- 用于停止autograd模块的工作，起到加速和节省显存的作用（具体行为就是停止gradient计算，从而节省了GPU算力和显存）\n- 不会影响dropout和batchnorm层的行为\n\n\n`model.eval()`与`torch.no_grad()`可以同时用，更加节省cpu的算力\n\n## 思考\n\n在val模式下，为什么让dropout层所有的激活单元都通过，因为train阶段的dropout层已经屏蔽掉了一些激活单元，在val模式下，让所有的激活单元都通过还能预测数据吗?\n**在val模式下，让所有的激活单元都通过当然能预测数据了，相当于学习时限定你每次只能选择一份资料学，考试时开卷所有资料你都带着。val模式下，虽然让所有的激活单元都通过，但是对于各个神经元的输出， 要乘上训练时的删除比例后再输出。**","tags":["pytorch"],"categories":["pytorch"]},{"title":"使用VGG迁移学习开启《猫狗大战挑战赛》","url":"/2020/11/24/181519/","content":" 使用VGG迁移学习开启《猫狗大战挑战赛》，内容如下：\n一、前言\n二、加载数据集\n三、数据预处理\n四、构建VGG模型\n五、训练VGG模型\n六、保存与测试模型\n七、总结\n\n \n\n<!-- more -->\n\n\n\n\n\n# 一、前言\n\n猫狗大战挑战由Kaggle于2013年举办的，目前比赛已经结束，不过仍然可以把[AI研习社猫狗大战赛平台](https://god.yanxishe.com/41)作为练习赛每天提交测试结果，该平台数据集包含猫狗图片共24000张，没有任何标注数据，选手需要训练模型正确识别猫狗图片，**1= dog，0 = cat**。这里使用在 ImageNet 上预训练的 VGG 网络模型进行测试，因为原网络的分类结果是1000类，所以要进行迁移学习，对原网络进行 fine-tune （即固定前面若干层，作为特征提取器，只重新训练最后两层），并把测试结果提交到该平台。那么，现在就让我们开始吧。\n\n\n\n\n\n# 二、加载数据集\n\n前期如何把**解压后的竞赛数据集**放到colab上着实耗费了我大量的时间，我认为非常有必要把这个单独作为一章讲一下。如果你本地有很强的GPU，不需要在colab上跑代码，这章节可以忽略，由于我的电脑跑不动这么多数据，GPU也不行，所以只能在colab上运行。在这个过程中许多问题本是可以避免的，由于对一些操作和指令不熟练，导致许多时间白白流失，即打消了初学者的自信心，也拖慢了实验的进度，究其原因，主要有以下几点：\n\n1. 在google drive上传和解压数据集时间特别慢，需要数十个小时\n\n2. colab运行时间有时限，长时间不操作（大概20分钟左右）会导致当前训练的数据被回收\n\n3. 猫狗大战数据集是没有标签的，需要自己定义Dataset类加载数据\n\n现在就来一个个解决上面的几个痛点吧！\n\n**（1）colab上传和解压大数据集**\n\n我们的目的是要在colab上读取竞赛数据集的图片，达到目的的方式有三个：\n\n- 方式一：把数据集压缩包上传到google drive，在drive上解压\n- 方式二：数据集解压后再上传到google drive\n- 方式三：把数据集压缩包上传到google drive，在colab连接的虚拟机上解压\n\n上面几种方式哪个好呢？我先不直接说结果，来实验下吧！\n\n首先，采用方式一，把数据集压缩包上传到google drive，在drive上解压，操作很简单，在google drive上右键上传[竞赛数据集cat_dog.rar](https://static.leiphone.com/cat_dog.rar)，文件大小521MB，上传时间二十多分钟，上传完毕后，再drive上解压，现在痛点来了，**时间竟然要十几个小时**，具体操作如下：\n\n- 打开colab，挂载google drive，方法可以参考我的博客[Google Colab挂载drive上的数据文件](https://blog.csdn.net/qq_37555071/article/details/107544680)。\n\n- 解压drive上的cat_dog.rar文件，命令为\n\n  ```bash\n  ! apt-get install rar\n  !unrar x \"/content/drive/Colab/人工智能课/cat_dog.rar\" \"/content/drive/Colab/人工智能课/\"\n  ```\n\n  解压过程如下：\n\n  ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120155811.png)\n\n\n\n\n我大致算了一下，每张图片解压时间5秒钟左右，24000张图片要大约33小时啊！！！所以，这种方式直接pass掉。\n\n再来看，方式二，把数据集解压后再上传到google drive，解压后的数据集文件夹大小虽然只有五百多兆，但上传速度特别慢，大概要5至7个小时，**并且一旦中间断网或是网络不稳定，极有可能导致数据损坏**。我就是花费了大半天时间把所有解压后的文件上传完了，由于中间网络不稳定，导致数据读取不正确，最终这种方式也放弃了，哎，说多了都是泪！\n\n最后，就只有方式三了，把数据集压缩包上传到google drive，在colab连接的虚拟机上解压文件，方法是：\n\n- 将google drive上数据集文件cat_dog.rar拷贝到colab连接的虚拟机上\n  ```bash\n  !cp -i /content/drive/Colab/人工智能课/cat_dog.rar /content/\n  ```\n- 在虚拟机上解压压缩文件：\n  ```bash\n  ! apt-get install rar\n  ! unrar x cat_dog.rar\n  ```\n  运行过程如下：![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120162337.png)\n\n\n\n这种方式速度非常快，如果操作正确，**解压时间仅有一分钟左右**，非常值得推荐！\n\n**（2）阻止Colab自动掉线**\n\n在colab上训练代码，页面隔一段时间无操作之后就会自动掉线，之前训练的数据都会丢失。现在你体会到我之前连续几个小时在google drive解压数据集文件的艰辛路程了吧。不过好在最后终于找到了一种可以让其自动保持不离线的方法，用一个js程序自动点击连接按钮。代码如下：\n\n```js\nfunction ClickConnect(){\n  console.log(\"Working\"); \n  document\n    .querySelector(\"#top-toolbar > colab-connect-button\")\n    .shadowRoot\n    .querySelector(\"#connect\")\n    .click()\n}\n \nsetInterval(ClickConnect,60000)\n```\n\n使用方式是：按快捷键`ctrl+shift+i`，并选择`Console`，然后复制粘贴上面的代码，并点击回车，该程序便可以运行了，如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120163050.png)\n\n**（3）猫狗大战数据集是没有标签的，需要自己定义Dataset类才能加载数据**\n\n猫狗大战数据集是没有标签的，但是从其训练集和验证集的图片名字可以获取标签，这就需要我们自己定义Dataset类了，由于这个部分篇幅较多，我们放在下一章讲吧。\n\n\n\n\n\n\n\n# 三、数据预处理\n\n传统的mnist数据集是集成到`torchvision.datasets`，我们使用`datasets.MNIST`就可以方便加载数据，不用做过多的其它处理，而猫狗大战竞赛数据集是如下图方式，并没有用标签对文件夹分类存放，所以我们需要通过图片名称获取标签，并自定义Dataset类加载图片。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120201612.png)\n\n我定义的Dataset类如下所示：\n\n```python\nfrom torch.utils.data import Dataset,DataLoader\n# 创建自己的类：MyDataset,继承 Dataset 类\nclass MyDataset(Dataset):\n    def __init__(self, txt, data_path=None, transform=None, target_transform=None, loader=default_loader):\n        super(MyDataset, self).__init__()\n        file_path = data_path + txt\n        file = open(file_path, 'r', encoding='utf8')\n        imgs = []\n        for line in file:\n            line = line.split()\n            imgs.append((line[0],line[1].rstrip('\\n')))\n\n        self.imgs = imgs\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n        self.data_path = data_path\n\n    # 可以通过索引进行条用，如data[1]\n    def __getitem__(self, index):\n        # 按照索引读取每个元素的具体内容\n        imgName, label = self.imgs[index]\n        # imgPath = self.data_path + imgName\n        imgPath = imgName\n        # 调用那张图片读哪张，最大限度发挥GPU显存\n        img = self.loader(imgPath)\n        if self.transform is not None:\n            img = self.transform(img)\n            label = torch.from_numpy(np.array(int(label)))\n        return img, label\n\n    def __len__(self):\n        # 数据集的图片数量\n        return len(self.imgs)\n    \n# 定义读取文件的各式\ndef default_loader(path):\n    return Image.open(path).convert('RGB')\n```\n\n具体要加载图片数据还要进行几个处理，即事先准备好train、val数据集的路径和标签，以及test数据集的路径，然后使用`MyDataset`加载图片路径文件，最后就可以通过`torch.utils.data.DataLoader`加载图片数据了。具体步骤如下：\n\n（1）首先，读取cat_dog文件夹下的图片路径\n\n```python\n#读一个文件夹下的所有文件名称\ndef read_file_name(file_dir):\n    filename = []\n    for root, dirs, files in os.walk(file_dir):\n        filename = files #当前路径下所有非目录子文件\n        break #这里只要图片文件，执行一次即可退出\n    return filename\n```\n\n（2）然后将文件名格式化为竞赛要求的类型，这里cat标签为0，dog为1\n\n```python\n# 将文件名格式化为要求的类型，这里cat标签为0，dog为1\ndef format_inputAndlabel(file_dir):\n    format_result = []\n    filename = read_file_name(file_dir)\n    for n in filename:#cat为0，dog为1\n        if \"cat\" in n:\n            format_result.append(n+\" 0\")\n        else:\n            format_result.append(n+\" 1\")\n    return format_result\n```\n\n（3）分别传入train、test、val路径读取数据\n\n```python\n# 格式化读取train、test、val\nformat_train_result = format_inputAndlabel(\"cat_dog/train\")\nformat_test_result = format_inputAndlabel(\"cat_dog/test\")\nformat_val_result = format_inputAndlabel(\"cat_dog/val\")\n```\n\n（4）由于自定义的DataSet必须知道文件路径，所以先将格式化的文件名写入文件里，再用自定义的MyDataset读取\n\n```python\ndef convert_format(content):\n  result = []\n  for t in content:\n    v = t.split('.')\n    result.append(int(v[0]))\n  return result\n# 写入train、val文件\ndef write_file(path,file_prefix,content):\n  with open(path, 'w', encoding='utf8') as f:\n      for line in content:\n          f.write(file_prefix+line+'\\n')\n# 写入test文件，由于读取时候文件名是乱序的，因此要先排序\ndef write_test_file(path,test_file_prefix,content):\n  content=convert_format(content)\n  content.sort() #排序\n  with open(path, 'w', encoding='utf8') as f:\n      for line in content: \n          f.write(test_file_prefix+str(line)+'.jpg 0'+'\\n')# test文件没有标签，默认用0填充就行\n\n# 因为自定义的DataSet必须知道文件路径，所以先将格式化的文件名写入文件里，再用自定义MyDataset读取\nwrite_file(path=\"cat_dog/train.txt\",file_prefix=\"cat_dog/train/\",content=format_train_result)\nwrite_file(path=\"cat_dog/val.txt\",file_prefix=\"cat_dog/val/\",content=format_val_result)\nwrite_test_file(path=\"cat_dog/test.txt\",test_file_prefix=\"cat_dog/test/\",content=format_test_result)\n```\n\n（5）对数据进行预处理变换\n\n```python\nfrom torch.utils.data import Dataset,DataLoader\nimport torchvision.transforms as transforms\n# 预处理设置\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\ntrain_transformer = transforms.Compose([\n    transforms.Resize(256),\n    transforms.transforms.RandomResizedCrop((224), scale = (0.5,1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    normalize])\n\n# val和test是类似的，训练的时候可以多一些增强，这里只做验证就可以\nval_transformer = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    normalize\n])\n```\n\n（6）使用`MyDataset`加载图片路径文件\n\n```python\n# 数据集加载方式设置\ncmd_path='cat_dog/'\ntrainset = MyDataset(txt='train.txt',data_path=cmd_path,transform=train_transformer)\nvalset = MyDataset(txt='val.txt',data_path=cmd_path,transform=val_transformer)\ntestset = MyDataset(txt='test.txt',data_path=cmd_path,transform=val_transformer)\nprint('训练集：',trainset.__len__())\nprint('验证集：',valset.__len__())\nprint('测试集：',testset.__len__())\n\"\"\"\n输出：\n训练集： 20000\n验证集： 2000\n测试集： 2000\n\"\"\"\n```\n\n（7）使用`torch.utils.data.DataLoader`加载图片数据，并将其放入`dataloaders_dict`\n\n```python\nbatchsize=128\n# 构建DataLoader\ntrain_loader = DataLoader(trainset, batch_size = batchsize, drop_last = False, shuffle = True)\n## val_loader和train_loader不做shuffle\nval_loader = DataLoader(valset, batch_size = batchsize, drop_last = False, shuffle = False)\ntest_loader = DataLoader(testset, batch_size = batchsize, drop_last = False, shuffle = False)\ndataloaders_dict = {'train':train_loader,'val':val_loader,'test':test_loader}\n```\n\n最终，数据集文件被放入`dataloaders_dict`，后面就可以通过该字典方便的传入相应的数据集了。\n\n\n\n# 四、构建VGG模型\n\nVGG 模型如下图所示，主体由三种元素组成：\n\n- 卷积层（CONV）是发现图像中局部的 pattern\n- 全连接层（FC）是在全局上建立特征的关联\n- 池化（Pool）是给图像降维以提高特征的 invariance(不变性)\n\n关于VGG模型的更详细介绍，可以参考我的博客[深入解读VGG网络结构](https://blog.csdn.net/qq_37555071/article/details/108199352)\n\n![VGG](http://fenggao-image.stor.sinaapp.com/20191006215625.jpg)\n\n\n\n默认情况下，当我们加载预训练的模型时，所有参数都具有`requires_grad = True`，如果我们从头开始或进行微调训练就不用更改。但是，如果我们要进行特征提取，并且只想为新初始化的图层计算梯度，那么我们希望所有其他参数都不需要梯度更新，需要用`set_parameter_requires_grad`函数将模型中参数的requires_grad属性设置为False，具体如下：\n\n```python\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n```\n\n这里我使用预训练好的VGG模型进行迁移学习，只想更新最后一层的参数，并且希望所有其他参数都不需要梯度更新，所以要用`set_parameter_requires_grad`函数将模型最后一层参数的requires_grad属性设置为False，由于猫狗大战数据集是二分类，需要把最后的`nn.Linear` 层由1000类，替换为2类。如下：\n\n```python\ndef initialize_model(num_classes, feature_extract, use_pretrained=True):\n    # 初始化模型变量\n    model_vgg = None\n    # 加载预训练模型\n    model_vgg = models.vgg16(pretrained=use_pretrained)\n    # 更改输出层\n    set_parameter_requires_grad(model_vgg, feature_extract)\n    model_vgg.classifier[6] = nn.Linear(4096, num_classes)\n    model_vgg.classifier.add_module('7',torch.nn.LogSoftmax(dim = 1))\n    return model_vgg\n\nmodel_vgg_new = initialize_model(num_classes=2,feature_extract = True,use_pretrained=True)\nprint(model_vgg_new.classifier)\n```\n\n输出`model_vgg_new`的`classifier`层，如下所示，可以看到最后一层全连接输出为2，并且使用`LogSoftmax`为output层。\n\n```tex\nSequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=2, bias=True)\n  (7): LogSoftmax(dim=1)\n)\n```\n\n\n\n# 五、训练VGG模型\n\n训练定义好的VGG模型，即训练最后一层全连接层，具体操作步骤如下：\n\n（1）创建损失函数和优化器\n\n损失函数 `NLLLoss()` 的输入是一个对数概率向量和一个目标标签，它不会为我们计算对数概率，适合最后一层是`log_softmax()`的网络。Adam优化器是目前性能比较好的优化器之一，因此这里采用Adam。\n\n```python\n'''\n第一步：创建损失函数和优化器\n'''\n# 损失函数\ncriterion = nn.NLLLoss()\n# 学习率\nlr = 0.001\n# 优化器\noptimizer_vgg = torch.optim.Adam(model_vgg_new.classifier[6].parameters(),lr = lr)\n```\n\n（2）判断是否存在GPU设备，并将model切换到相应的device\n\n```py\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('Using gpu: %s ' % torch.cuda.is_available())\nmodel_vgg_new.to(device)\n```\n\n（3）训练模型\n\n这里我定义了一个`train_model`训练的方法，并将验证集上结果最好的一次训练存储下来，为了减少训练时间，我把`epoch`设置为4\n\n```python\n'''\n第三步：训练模型\n'''\ndef train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n    val_acc_history = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # 每个epoch都进行训练和验证\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # 将模型设置为训练模式\n            else:\n                model.eval()   # 将模型设置为验证模式\n\n            running_loss = 0.0 # 记录训练时的loss下降过程\n            running_corrects = 0\n\n            # 遍历数据\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                # 梯度初始化\n                optimizer.zero_grad()\n                # 前向传播\n                outputs = model(inputs)\n                loss = criterion(outputs, labels.long())\n                # 得到预测结果\n                _, preds = torch.max(outputs, 1)\n                # 仅在训练时更新梯度，反向传播，backward + optimize\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # 将验证集上结果最好的一次训练存储下来\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history\n\n# 训练\nmodel_new_vgg, hist = train_model(model_vgg_new, dataloaders_dict, criterion, optimizer_vgg, num_epochs=4)\n\n```\n\n经过4次epoch，输出的记录如下，可以看到虽然训练次数不多，但是在验证集上效果还是很不错的\n\n```python\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nplt.title(u\"val acc plot\")\nplt.xlabel(u\"epoch\")\nplt.ylabel(u\"val acc\")\nacc= hist\nplt.xticks(range(len(acc)))\nplt.plot(acc)\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120204636.png)\n\n\n\n\n\n\n\n# 六、保存与测试模型\n\n（1）保存训练好的模型\n\npytorch保存和加载模型有两种方式，**不同的保存方式对应不同的读取方式**，两者各有利弊。\n\n方式一：直接保存整个模型\n\n```python\ntorch.save(model_new_vgg, 'model_new_vgg.pt')\nmodel_new_vgg = torch.load('model_new_vgg.pt')\n```\n\n方式二：只保存模型中的参数\n\n```python\nmodel = initialize_model(num_classes=2,feature_extract = True,use_pretrained=True)\nmodel.to(device)\nmodel.load_state_dict(torch.load(\"model_new_vgg.pt\"))\n```\n\n可以看到，用第一种方法能够直接保存模型，加载模型的时候直接把读取的模型给一个参数就行。而第二种方法则只是保存参数，在读取模型参数前**要先定义一个模型**（模型必须与原模型相同的构造），然后对这个模型导入参数。虽然麻烦，但是可以**同时保存多个模型**的参数，而第一种方法则不能，而且第一种方法**有时不能保证模型的相同性**（你读取的模型并不是你想要的）。所以，这里我采用第二种方式来保存并加载模型。\n\n（2）对模型进行测试\n\n接下来就要用test数据集对模型进行测试了，把测试结果保存到`pred_outputs`，具体如下：\n\n```python\ndef test_model(model, test_loader):\n    model.eval() #把训练好的参数冻结\n    total,correct = 0,0\n    pos = 0\n    pred_outputs= np.empty(len(test_loader.dataset),dtype=np.int)\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            pred_outputs[pos:pos+len(preds)]=preds.cpu().numpy()\n            pos += len(preds)\n    return pred_outputs\n\npred_outputs = test_model(model,dataloaders_dict['test'])\n```\n\n（3）将测试结果写入`cat_dog_result.csv`\n\n```python\nwith open(\"cat_dog_result.csv\", 'w') as f:\n    for i in range(len(test_loader.dataset)):\n        f.write(\"{},{}\\n\".format(i, pred_outputs[i]))\n```\n\n因为我是在colab环境上训练的，还要把`cat_dog_result.csv`拷贝到google drive才能下载，命令如下：\n\n```bash\n!cp -i /content/cat_dog_result.csv /content/drive/\n```\n\n（4）提交测试结果\n\n把`cat_dog_result.csv`提交到[AI研习社猫狗大战--经典图像分类题](https://god.yanxishe.com/41)，现在就让我们见证奇迹的时刻吧！\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120181445.png)\n\n可以看到，只训练了4次epoch，测试就达到了98.9的准确率，把epoch设置得更大，结果应该会更好，由于时间原因，就不训练了。\n\n\n\n\n\n# 七、总结\n\n从加载猫狗大战竞赛数据集到colab上，到测试完模型并提交，我大概花费了几天的时间，并且主要时间不是用在定义模型和调参上，而是如何处理数据上。我认为这次的收获还是很大的，因为我知道了**如何以最快最有效的方式在colab上加载要训练的数据**，并定义了自己Dataset类，**以后对于任何类型、任何格式的训练数据，我应该都能定义相应Dataset类并且去处理它**。这次我用了近三天，下次可能一个小时不到就搞定了，这难道不是一个巨大的进步吗？此外，我通过预训练好的VGG模型进行迁移学习，训练了猫狗大战数据集，仅训练了4次epoch，测试数据就达到了98.9的准确率，说明预训练好的VGG模型是非常容易学习的，以后再遇到类似的识别分类任务，就不需要从头开始训练了，真的是非常快速又方便。\n\n最后，附上我的colab共享地址：https://drive.google.com/file/d/1t-DVQwo92dBuy3JgNhdYFD_CndwyBE3U/view?usp=sharing\n\n里面格式有点乱，但是内容一点都不少哦！","tags":["VGG"],"categories":["神经网络"]},{"title":"Hexo博客发布和配置的一些常用命令","url":"/2020/11/24/165533/","content":"\nHexo博客发布和配置的一些常用命令，在此记录！如果遇到新的且使用的命令，会不端完善！\n<!-- more -->\n\n\n\n## 基本命令\n\n`hexo init`  \n初始化站点，生成一个简单网站所需的各种文件。\n\n`hexo clean == hexo c`  \n清除缓存 网页正常情况下可以忽略此条命令\n\n`hexo generate == hexo g`  \n生效新增、修改、更新的文件\n\nHexo 能够监视文件变动并立即重新生成静态文件，在生成时会比对文件的 SHA1 checksum，只有变动的文件才会写入。`hexo generate --watch`\n\n\n\n`hexo server == hexo s`  \n启动本地网站，可在本地观察网站效果，同时也可以输入`http://localhost:4000/admin`管理文章\n\n`hexo s --draft`\n这个发布时可以预览草稿\n\n`hexo s --debug`  \n以调试模式启动本地网站，在此模式下，对文件的更改无需停止网站只需刷新即可看到效果，调试非常方便\n\n\n`hexo clean && hexo s`  \n一次执行两个命令\n\n`hexo deploy == hexo d`  \nhexo的一键部署功能，执行此命令即可将网站发布到配置中的仓库地址，执行此命令前需要配置站点配置文件_config.yml\n\n**一键本地启动**：`hexo clean && hexo g && hexo s`\n\n**一键部署**：`hexo clean && hexo g && hexo d`\n\n您可执行下列的其中一个命令，让 Hexo 在生成完毕后自动部署网站，两个命令的作用是相同的。\n\n```\n$ hexo generate --deploy\n$ hexo deploy --generate  或 hexo g -d or hexo d -g\n```\n\n## 创建和发布文章\n\n`hexo new [layout] <title>`\n新建一篇新文章，会自动按照模板里面的格式创建文章\n\n里面的布局（layout），默认为 post，布局共有三种：\n\n```tex\npost\tsource/_posts\npage\tsource\ndraft\tsource/_drafts\n```\n\n**发布草稿命令：**\n\n1. `hexo publish 文章文件名`\n2. 或者是手动将`_drafts`目录下的草稿移动到`_posts`目录下即可发布草稿为正式文章。\n\n\n\n## PicGO图床快捷键\n快捷键为：`ctrl+shift+p`\n\n\n\n## Hexo博客头部配置\n\n（1）文章置顶\n\n在文章的 Front-Matter 中，使用 `top: true` 来实现置顶。在文章的 Front-Matter 中，使用 `top: true` 来实现置顶。\n\n（2）自定义样式\n\n如果你想修改主题的样式，推荐将样式代码添加到 `source/css/_custom` 目录下的 `index.styl` 文件中。这样，当主题更新时，不会覆盖你已经修改了的样式代码。\n\n> 当然，你也可以进行模块化分类：在该目录下新建样式文件，然后通过 `@import xxx` 语句在同目录下的 `index.styl` 文件中引入你新建的样式文件。\n\n（3）文章左侧目录\n\n启用文章目录后，默认对所有文章页面生效。你可以在 Front-Matter 中，设置 `toc: false` 来指定某篇文章不启用该功能。\n\n（4）文章业内目录\n\n`@[TOC]( )`这个写到文章页面内任何一个地方即可\n\n更多详细设置，请参考[ hexo-theme-stun](https://theme-stun.github.io/docs/zh-CN/)\n\n","categories":["工具"]},{"title":"Typora需要注意的换行符","url":"/2020/11/24/104944/","content":"\n在Typora中一定要换行符，包括普通换行，以及整个段落的换行，这些和普通的markdown编辑器是不太一样的\n\n\n\n\n\n<!-- more -->\n\n（1）首先，Typora中如果只按`Enter`键，**则把其看做另起一个段落**，段落之间的间距是比较大的，从Typora的界面上就可以看出来，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124110809.png)\n\n（2）其次，如果要只换行，而不另起一个段落，则需要使用`<br/>`或`<br/>`或按下`Shift`+`Enter`键，这时候不会另起一段，两行仍然在一个段落里面，此时间距是比较小的，如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124111640.png)\n\n\n\n（3）另外，需要注意的是，对于**有序或无序排列**，如果要在一个排列里面写多行东西，不要只按`Enter`键，这样会另起一段，Typora会使所有的排序当做单独的一段，在源码模式下可以看到如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124111559.png)\n\n（4）可以采用`<br/>`或`<br/>`或按下`Shift`+`Enter`键的方式另起一行，这就所有的排序就在一段内了，如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124111952.png)\n\n\n\n（5）先按下两个空格，再按下`Shift`+`Enter`键，会出现如下符号，我认为这和只用`Shift`+`Enter`是一样的效果，目前还没发现什么问题，以后若发现区别会再补充。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124132953.png)\n\n\n\n（6）需要注意的是，一定要勾选，菜单栏中编辑->空格与换行->保留单独的换行符，如下图所示，这样才能使用`Shift`+`Enter`键的方式另起一行，如果不勾选，只能用`<br/>`或`<br/>`的方式另起一行。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124112415.png)\n\n","categories":["工具"]},{"title":"使用pytorch的auto_grad实现线性模型对mnist数据集多分类","url":"/2020/07/18/181520/","content":"\n使用pytorch的auto_grad实现线性模型对mnist数据集多分类，选取mnist100张图片，前80张为测试集，后20张为训练集，eporch 500次\n\n\n## 知识储备\n\n使用多个线性模型进行多分类 原理：每一个线性模型做二分类  \n多个线性模型 = 感知机，实质就是每一个线性模型做二分类\n\n<!-- more -->\n\n## 数据加载&归一化\n\n\n```python\nimport torch\nfrom mnist import MNIST\nimport numpy as np\nimport pdb\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nmndata = MNIST('dataset/python-mnist')\nimage_data_all, image_label_all = mndata.load_training()\nimage_data = image_data_all[0:100]\nimage_data = np.array(image_data,dtype = np.float)/255\nimage_label = image_label_all[0:100]\nimage_label = np.array(image_label,dtype = np.int)\nprint(image_data.shape,image_label.shape)\n```\n\n    (100, 784) (100,)\n\n\n## 定义模型\n\n\n```python\ndef model(image_data_one,weights,bias):\n    \"\"\"\n    这里直接使用图片本身作特征，也可以提取features后传入模型中\n    \"\"\"\n    # image_data_one转化为二维\n    xt = torch.from_numpy(image_data_one.reshape(1,28*28))\n    y = xt.mm(weights)+bias\n    return y\n\ndef get_acc(image_data,image_label,weights,bias,start_i,end_i):\n    correct = 0\n    # 这里可以不加，因为loss计算于此无关\n    with torch.no_grad():\n        for i in range(start_i,end_i):\n            y = model(image_data[i],weights,bias)\n            # 获取第i张图片的label\n            gt = image_label[i]\n            # 获取与y最近接的label值\n            pred = torch.argmin(torch.from_numpy(np.array([torch.min((torch.abs(y-j))).item() for j in range(0,10)]))).item()\n            if gt == pred:\n                correct += 1\n    # 确保万一，除法分子或分母一个指定为float        \n    return float(correct/float(end_i-start_i))\n```\n\n\n```python\n#显示训练集和测试集精度变换\ndef show_acc(train_accs,test_accs):\n    plt.figure(figsize = (10,4))\n    plt.title('train_accs and test_accs')\n    plt.plot(np.arange(len(train_accs)), train_accs, color='green', label='train_accs')\n    plt.plot(np.arange(len(test_accs)), test_accs, color='red', label='test_accs')\n    plt.legend() # 显示图例\n    plt.xlabel('index')\n    plt.ylabel('accs')\n    plt.show()\n```\n\n\n```python\ndef train_model(image_data,image_label,weights,bias,lr):\n    loss_value_before=1000000000000000.\n    loss_value=10000000000000.\n    train_accs = []\n    test_accs = []\n    for epoch in range(0,500): \n        loss_value_before=loss_value\n        loss_value=0\n        for i in range(0,80):\n            y = model(image_data[i],weights,bias)\n            # 获取第i张图片的label\n            gt = image_label[i]\n            # 只关心一个值，更新的时候也只更新对应线性模型的weights和bias\n            loss = torch.sum((y[0,gt:gt+1]-gt).mul(y[0,gt:gt+1]-gt))\n            loss_value += loss.data.item()\n            loss.backward()\n            weights.data.sub_(weights.grad.data*lr)\n            weights.grad.data.zero_()\n            bias.data.sub_(bias.grad.data*lr)\n            bias.grad.data.zero_()            \n\n        train_acc = get_acc(image_data,image_label,weights,bias,0,80)\n        test_acc = get_acc(image_data,image_label,weights,bias,80,100)\n        train_accs.append(train_acc)\n        test_accs.append(test_acc)\n        #print(\"epoch=%s,loss=%s/%s,train/test_acc=%s/%s,\"%(epoch,loss_value,loss_value_before,train_acc,test_acc))\n    show_acc(train_accs,test_accs)\n```\n\n## 训练\n\n\n```python\nweights = torch.randn(28*28,10,dtype = torch.float64,requires_grad = True)\nbias = torch.zeros(10,dtype = torch.float64,requires_grad = True)\nlr = 1e-3\n# 对模型进行训练：\ntrain_model(image_data,image_label,weights,bias,lr)    \n```\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200718181909.png)\n\n> 由于样本少，导致程序过拟合，结果是训练集精度高，测试集精度低。\n","tags":["pytorch","MLP"],"categories":["project实战"]},{"title":"numpy和torch数据类型转化","url":"/2020/07/18/164540/","content":"\n在实际计算过程中，float类型使用最多，因此这里只介绍numpy和torch数据float类型转化，其他类型同理。\n<!-- more -->\n\n## numpy数据类型转化\n\n- numpy使用astype转化数据类型，float默认转化为64位，可以使用`np.float32`指定为32位\n\n\n```python\n#numpy转化float类型\na= np.array([1,2,3])\na = a.astype(np.float)\nprint(a)\nprint(a.dtype)\n```\n\n`[1. 2. 3.]`  \n`float64`\n    \n\n- 不要使用a.dtype指定数据类型，会使数据丢失\n\n\n```python\n#numpy转化float类型\nb= np.array([1,2,3])\nb.dtype= np.float32\nprint(b)\nprint(b.dtype)\n```\n\n`[1.e-45 3.e-45 4.e-45]`  \n`float32`\n    \n\n- 不要用float代替np.float，否则可能出现意想不到的错误\n- 不能从np.float64位转化np.float32，会报错\n- np.float64与np.float32相乘，结果为np.float64\n\n> 在实际使用过程中，可以指定为np.float，也可以指定具体的位数，如np.float，不过直接指定np.float更方便。\n\n## torch数据类型转化\n\n- torch使用`torch.float()`转化数据类型，float默认转化为32位，torch中没有`torch.float64()`这个方法\n\n\n```python\n# torch转化float类型\nb = torch.tensor([4,5,6])\nb = b.float()\nb.dtype\n```\n\n\n\n\n    torch.float32\n\n\n\n- `np.float64`使用`torch.from_numpy`转化为torch后也是64位的\n\n\n```python\nprint(a.dtype)\nc = torch.from_numpy(a)\nc.dtype\n```\n\n`float64`  \n`torch.float64`\n\n\n\n- 不要用float代替torch.float，否则可能出现意想不到的错误\n- torch.float32与torch.float64数据类型相乘会出错，因此相乘的时候注意指定或转化数据float具体类型\n\n> np和torch数据类型转化大体原理一样，只有相乘的时候，torch.float不一致不可相乘，np.float不一致可以相乘，并且转化为np.float64\n","tags":["pytorch","numpy"]},{"title":"Google Colab挂载drive上的数据文件","url":"/2020/07/18/103558/","content":"\nGoogle Colab是完全云端的，所以，每次如果想让他访问谷歌云盘的内容，必须要先进性授权操作，直接在colab的jupyter中进行绑定授权操作\n\n**每次在Google Colab中打开notebook文件时，都必须重新执行命令获得授权。**\n<!-- more -->\n\n## 获取授权脚本代码\n```bash\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n!apt-get update -qq 2>&1 > /dev/null\n!apt-get -y install -qq google-drive-ocamlfuse fuse\nfrom google.colab import auth\nauth.authenticate_user()\nfrom oauth2client.client import GoogleCredentials\ncreds = GoogleCredentials.get_application_default()\nimport getpass\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\nvcode = getpass.getpass()\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n```\n**期间会输入两次授权码，点击相应链接复制即可**\n\n## 挂载到drive上\n```bash\n!mkdir -p drive\n!google-drive-ocamlfuse drive\n```\n\n## 切换到工作文件夹\n\n```python\n# 指定当前的工作文件夹\nimport os\n# google drive中的文件路径为/content/drive\nos.chdir(\"/content/drive/Colab\") \n```\n也可以使用`%cd`切换工作路径，推荐使用\n\n\n## 几个常用命令\n\n- `%cd`切换工作目录\n- `!ls`查看当前目录下的文件\n- `!pwd`查看当前的工作路径\n\n\n\n参考文档\n\n[谷歌云盘Colaboratory如何载入文件](https://blog.csdn.net/Einstellung/article/details/81006408)  \n","tags":["colab"],"categories":["Google Colab使用"]},{"title":"使用numpy实现逻辑回归对IRIS数据集二分类","url":"/2020/07/16/175900/","content":"使用numpy实现逻辑回归对IRIS数据集二分类，使用对数似然损失(Log-likelihood Loss)，并显示训练后loss变化曲线。\n\n知识储备如下：\n- 逻辑回归Logistic Regression\n- 对数似然损失\n- IRIS数据集介绍\n- np.concatenate使用\n<!-- more -->\n\n\n## 知识储备\n\n### 逻辑回归Logistic Regression\n\n$$y = \\frac{1}{1+e^{-(wx+b)}}$$\n名字虽然叫回归，但是一般处理的是分类问题，尤其是二分类，比如垃圾邮件的识别，推荐系统，医疗判断等，因为其逻辑与实现简单，在工业界有着广泛的应用。\n\n__优点__：\n\n* 实现简单，计算代价不高，易于理解和实现, 广泛的应用于工业问题上；\n* 分类时计算量非常小，速度很快，存储资源低；\n\n__缺点__：\n\n* 容易欠拟合，当特征空间很大时，逻辑回归的性能不是很好；\n* 不能很好地处理大量多类特征或变量；\n\n### 对数似然损失\n对数损失, 即对数似然损失(Log-likelihood Loss), 也称逻辑斯特回归损失(Logistic Loss)或交叉熵损失(cross-entropy Loss), 是在概率估计上定义的。它常用于(multi-nominal, 多项)逻辑斯特回归和神经网络,以及一些期望极大算法的变体,可用于评估分类器的概率输出。可参考[对数损失函数(Logarithmic Loss Function)的原理和 Python 实现](https://www.cnblogs.com/klchang/p/9217551.html)了解详情\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716170728.png)\n\n损失函数:\n\n\n$$L=\\frac{1}{m}*\\sum_i^m -y_ilog(f(x_i))-(1-y_i)log(1-f(x_i))$$\n梯度计算：\n$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m}X^T*(f(x)-y)$$\n\n权重更新：\n$$w = w -\\alpha\\frac{\\partial L}{\\partial w}$$\n\n### IRIS数据集介绍\n\n该数据集包含4个特征变量，1个类别变量。iris每个样本都包含了4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，以及1个类别变量（label）。详情见[加载数据](#加载数据)\n\n### np.concatenate使用\n\n\n```python\na = np.array([[1, 2],[3, 4]])\nb = np.array([[5, 6]])\nnp.concatenate((a, b), axis = 0)\n```\n\n\n\n\n    array([[1, 2],\n           [3, 4],\n           [5, 6]])\n\n\n\n\n```python\nnp.concatenate((a, b.T), axis = 1)\n```\n\n\n\n\n    array([[1, 2, 5],\n           [3, 4, 6]])\n\n\n\n## 加载数据\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_iris\n%matplotlib inline\n```\n\n\n```python\ndataset = load_iris()\ninputs = dataset['data']\ntarget = dataset['target']\nprint('inputs.shape:', inputs.shape)\nprint('target.shape:', target.shape)\n# 三个类别\nprint('labels:', set(target))\n```\n\n    inputs.shape: (150, 4)\n    target.shape: (150,)\n    labels: {0, 1, 2}\n\n\n\n```python\ntarget\n```\n\n\n\n\n    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\n\n```python\nvalues = [np.sum(target == 0), np.sum(target == 1), np.sum(target == 2)]\nplt.pie(values,labels=[0, 1, 2], autopct = '%.1f%%')\nplt.show()\n```\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716171020.png)\n\n\n关于参数train_test_split的`random_state`的解释：  \n>Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.   \nrandom_state即随机数种子，**目的是为了保证程序每次运行都分割一样的训练集和测试集。否则，同样的算法模型在不同的训练集和测试集上的效果不一样。**\n\n\n```python\nfrom sklearn.model_selection import train_test_split\n# 只取前两类， 做二分类\ntwo_class_input = inputs[:100]\ntwo_class_target = target[:100]\nx_train, x_test, y_train, y_test = train_test_split(\n                    two_class_input,two_class_target,\n                    test_size = 0.3,\n                    random_state = 0)\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n```\n\n    (70, 4) (30, 4) (70, 1) (30, 1)\n\n\n\n```python\n# add one feature to x\nx_train = np.concatenate([x_train, np.ones((x_train.shape[0], 1))], axis = 1)\nx_test = np.concatenate([x_test, np.ones((x_test.shape[0], 1))], axis = 1)\nprint(x_train.shape, x_test.shape)\n```\n\n    (70, 5) (30, 5)\n\n\n## 定义模型\n\n\n```python\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\nx = np.arange(-10, 10, step = 0.1)\nfig, ax = plt.subplots(figsize = (8, 4))\nax.plot(x, sigmoid(x), c = 'green')\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x28f77053348>]\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716171106.png)\n\n\n\n```python\ncompute_loss = lambda pred_y, y: np.mean(-y * np.log(pred_y)-(1-y) * np.log(1-pred_y))\n# weight and bias init\nw = np.random.randn(5, 1)\n# 上一个loss\nlosses = []\nlast_loss = 10000\npred_y =sigmoid(np.dot(x_train, w))\n# 当前loss\nnow_loss = compute_loss(pred_y, y_train)\ni = 0\nwhile abs(now_loss - last_loss)>1e-4:\n    last_loss = now_loss\n    i = i + 1\n    # 计算梯度\n    grad = x_train.T.dot((pred_y - y_train)) / len(y_train)\n    # 更新梯度\n    w = w - 0.001 * grad\n    \n    # 前导计算\n    pred_y = sigmoid(np.dot(x_train, w))\n    now_loss = compute_loss(pred_y, y_train)\n    losses.append(now_loss)\nfig, ax = plt.subplots(figsize = (10, 4))\nax.plot(np.arange(len(losses)), losses, c = 'r')\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x28f77053508>]\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716171124.png)\n\n\n## 测试样例\n\n\n```python\n# 测试\ntest_pred = sigmoid(np.dot(x_test, w))\npre_test_y = np.array(test_pred > 0.5, dtype = np.float32)\nacc = np.sum(pre_test_y == y_test) / len(y_test)\nprint(\"the accary of model is {}\".format(acc*100))\n```\n\n    the accary of model is 100.0\n\n\n\n```python\nprint(pre_test_y.reshape(1,-1))\nprint(y_test.reshape(1,-1))\n```\n\n`[[0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n      0. 0. 0. 1. 1. 1.]]`   \n`[[0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1]]`\n    \n","tags":["numpy","逻辑回归"],"categories":["project实战"]},{"title":"使用numpy实现线性模型预测boston房价","url":"/2020/07/16/115900/","content":"使用numpy实现线性模型预测boston房价，激活函数为Relu，使用MSE_loss，手动求导，并显示训练后loss变化曲线。\n\n知识储备如下：\n- Scikit-learn\n- boston房价数据解读\n- 标准差公式\n- Linear及MSE_loss求导公式\n<!-- more -->\n\n## 知识储备\n\n### Scikit-learn\n\nScikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。其优点为：\n- 简单高效的数据挖掘和数据分析工具\n- 让每个人能够在复杂环境中重复使用\n- 建立NumPy、Scipy、MatPlotLib之上\n\n安装方法 pip install scikit-learn\n\n### boston房价数据解读\n\n使用sklearn.datasets.load_boston即可加载相关数据。该数据集是一个回归问题。每个类的观察值数量是均等的，共有506个观察，13个输入变量和1个输出变量。每条数据包含房屋以及房屋周围的详细信息。其中包含城镇犯罪率，一氧化氮浓度，住宅平均房间数，到中心区域的加权距离以及自住房平均房价等等，具体如下：\n- CRIM：城镇人均犯罪率。\n- ZN：住宅用地超过 25000 sq.ft. 的比例。\n- INDUS：城镇非零售商用土地的比例。\n- CHAS：查理斯河空变量（如果边界是河流，则为1；否则为0）。\n- NOX：一氧化氮浓度。\n- RM：住宅平均房间数。\n- AGE：1940 年之前建成的自用房屋比例。\n- DIS：到波士顿五个中心区域的加权距离。\n- RAD：辐射性公路的接近指数。\n- TAX：每 10000 美元的全值财产税率。\n- PTRATIO：城镇师生比例。\n- B：1000（Bk-0.63）^ 2，其中 Bk 指代城镇中黑人的比例。\n- LSTAT：人口中地位低下者的比例。\n- MEDV：自住房的平均房价，以千美元计。\n- 预测平均值的基准性能的均方根误差（RMSE）是约 9.21 千美元。\n\n### 标准差公式\n\n如x1,x2,x3...xn的平均数为M，则方差可表示为：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125539.png)\n\n样本标准差=方差的算术平方根=s=sqrt(((x1-x)^2 +(x2-x)^2 +......(xn-x)^2)/(n-1) )  \n总体标准差=σ=sqrt(((x1-x)^2 +(x2-x)^2 +......(xn-x)^2)/n )  \n如是总体，标准差公式根号内除以n  \n如是样本，标准差公式根号内除以（n-1)。  \n因为我们大量接触的是样本，所以普遍使用根号内除以（n-1)。  \n\n\n```python\na=np.array([[1,2,3],[4,5,6]])\nnp.mean(a,axis=1)\n```\n\n\n\n\n    array([2., 5.])\n\n\n\n\n```python\n# axis = 1表示行，ddof = 1是除以n-1\nnp.std(a, axis = 1,ddof = 1) \n```\n\n\n\n\n    array([1., 1.])\n\n\n\n\n```python\n# ddof默认为0,是除以n\nnp.std(a, axis = 1) \n```\n\n\n\n\n    array([0.81649658, 0.81649658])\n\n\n\n\n```python\n# 求所有数平均值\nnp.mean(a)\n```\n\n\n\n\n    3.5\n\n\n\n### Linear及MSE_loss求导公式\n\n损失函数\n$L = \\frac{1}{2N}\\sum_{i=1}^{N}(z^{i} - y^{i})^{2}$\n\n线性函数\n$z^i = \\sum_{j=0}^{N}x_j^{(i)}w^{(j)} + b^{(j)}$\n\n对 $w$ 偏导，得到$w$ 更新梯度\n$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{N}\\sum_{i}^{N}(z^{(i)} - y^{(i)})x_j^{(i)}$\n\n对 $b$ 偏导，得到$b$ 更新梯度\n$\\frac{\\partial L}{\\partial b} = \\frac{1}{N}\\sum_{i}^{N}(z^{(i)} - y^{(i)})$\n\n## 数据加载\n\n\n```python\nfrom sklearn.datasets import load_boston\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n\n```python\ndata = load_boston()\nX_ = data['data']\ny = data['target']\nprint(type(data), type(X_), type(y))\nprint('data keys:', data.keys())\nprint('X_.shape:', X_.shape)\nprint('y.shape:', y.shape)\n```\n\n\n`<class 'sklearn.utils.Bunch'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> ` \n`data keys: dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])  `\n`X_.shape: (506, 13)  `\n `y.shape: (506,)  `\n\n\n\n\n\n## 数据规范化\n\n\n```python\n# 转化为标准正态分布\nX_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis = 0)\ny = y.reshape(-1,1) # reshape转化为vector\nprint(X_.shape)\nprint(y.shape)\n```\n\n`(506, 13)  `\n`(506, 1)`\n    \n\n## 建立激活函数\n\n\n```python\ndef sigmoid(x):\n    r = 1 / (1 + np.exp(-x))\n    return r\nnums = np.arange(-10, 10, step = 1)\nfig, ax = plt.subplots(figsize = (10, 4))\nax.plot(nums, sigmoid(nums), c='red')\n```\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125621.png)\n\n\n```python\ndef relu(x):\n    return (x > 0) * x\nfig, ax = plt.subplots(figsize = (10, 4))\nnums = np.arange(-10, 10, step = 1)\nax.plot(nums, relu(nums), c = 'blue')\n```\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125708.png)\n\n\n## 定义模型\n\n线性模型：$y = wx + b$\n\n\n```python\ndef Linear(x, w, b):\n    y_pre = x.dot(w) + b\n    return y_pre\n```\n\n**在计算损失时，需要把每个样本的损失都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数N。**\n\n\n```python\ndef MSE_loss(y_pre, y):\n    loss = np.mean(np.square(y_pre - y))\n    return loss\n```\n\n\n```python\ndef gradient(x, y_pre, y):\n    n = x.shape[0]\n    grad_w = x.T.dot(y_pre - y)/n\n    grad_b = np.mean(y_pre - y)\n    return grad_w, grad_b\n    \n```\n\n\n```python\n# 初始化网络\nn = X_.shape[0] # 样本数量506\nn_features = X_.shape[1] #特征数量13\n\n# 初始化网络参数\n# randn从标准正态分布中返回一个或多个样本值\nW = np.random.randn(n_features, 1)\nb = np.zeros(1)\n\n#设定学习率\nlearning_rate = 1e-2\n\n#训练次数\nepoch = 10000\n```\n\n## 训练(不加激活函数)\n\n\n```python\nlosses = []\n# 训练 \nfor t in range(epoch):\n    # 向前传播\n    y_pred = Linear(X_, W, b)\n    # 计算损失函数\n    loss = MSE_loss(y_pred,y)\n    losses.append(loss)\n    grad_w, grad_b = gradient(X_, y_pred, y)\n    \n    #权重更新\n    W = W - grad_w * learning_rate\n    b = b - grad_b * learning_rate\n\nfig, ax = plt.subplots(figsize = (10, 4))\nax.plot(np.arange(len(losses)), losses, c = 'r')\n```\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125735.png)\n\n\n```python\nn_hidden = 10 #设计隐藏神经元个数（可修改）\nW1 = np.random.randn(n_features, n_hidden)  # 维度 n_features * n_hidden\nb1 = np.zeros(n_hidden)                     # 维度 1 * n_hidden\nW2 = np.random.randn(n_hidden, 1)           # 维度 n_hidden * 1\nb2 = np.zeros(1)                            # 维度1\n```\n\n## 训练(加激活函数)\n\n\n```python\n# 训练\nlosses = []\nfor t in range(epoch):\n    #向前传播\n    y_pred1 = Linear(X_, W1, b1)     # 维度 n * n_hidden\n    y_relu = relu(y_pred1)           # 维度 n * n_hidden\n    y_pred = Linear(y_relu, W2, b2) # 维度 n * 1\n    \n    #计算损失函数\n    loss = MSE_loss(y_pred, y)\n    losses.append(loss)\n    \n    #反向传播，求梯度\n    grad_y_pred = y_pred - y                 # 维度n*1\n    grad_w2 = y_relu.T.dot(grad_y_pred) / n  # 维度n_hidden*1\n    grad_b2 = np.mean(grad_y_pred, axis = 0) # 维度1*1\n    grad_relu = grad_y_pred.dot(W2.T)        # 维度n*n_hidden\n    #注意：y_pred1与relu直接相关\n    grad_relu[y_pred1 < 0] = 0\n    grad_w1 = X_.T.dot(grad_relu) / n        # 维度n_features* n_hidden\n    grad_b1 = np.mean(grad_relu, axis = 0)   # 维度n_hidden*1\n    \n    #更新梯度\n    W1 = W1 - grad_w1 * learning_rate\n    b1 = b1 - grad_b1 * learning_rate\n    W2 = W2 - grad_w2 * learning_rate\n    b2 = b2 - grad_b2 * learning_rate\n\nfig, ax = plt.subplots(figsize = (10, 4))\nax.plot(np.arange(len(losses)), losses, c = 'r')\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125832.png)\n\n参考文档\n\n[波士顿房价数据集解读](https://blog.csdn.net/appleyuchi/article/details/84998894)\n","tags":["numpy","线性回归"],"categories":["project实战"]},{"title":"奥卡姆剃刀","url":"/2020/07/15/0/","content":"任何一件事情，都要从简单的开始做起，若无必要，勿增实体！！！\n\n![img](https://gitee.com/wxler/blogimg/raw/master/imgs/20200715105539.jpg)\n","categories":["Life"]},{"title":"git基本使用方法","url":"/2020/06/17/115900/","content":"由于平时写代码和博客常常用到git和github，每次用到都去百度，感觉太麻烦了，也大大降低了效率，索性自己整理一下常用到的git指令和使用方法，对git的使用能有一个系统的认识。这里只介绍一下基本用法，对更高级的用法如果以后用到再进行补充。\n<!-- more -->\n\n## git安装和配置\ngit的安装和配置在我的这篇[搭建个人博客](https://wxler.github.io/2020/06/01/hexoCreateAndConfig/#%E5%AE%89%E8%A3%85git)里，请自行参考配置，主要有一下几点：\n- 下载安装git程序\n- 配置github账户\n- 配置SSH KEY\n## git工作原理\nGit是分布式版本控制系统，那么它就没有中央服务器的，每个人的电脑就是一个完整的版本库，这样，工作的时候就不需要联网了，因为版本都是在自己的电脑上。既然每个人的电脑都有一个完整的版本库，那多个人如何协作呢？比如说自己在电脑上改了文件A，其他人也在电脑上改了文件A，这时，你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。Git在执行更新操作时，更像是对数据的一组快照，每次你提交更新，它主要对当时的全部文件制作一个快照并保存这个快照的索引。为了高效，如果文件没有修改，Git不再重新存储该文件，而是只保留一个链接指向之前存储的文件。  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125501.png)\n如上图所示，在version2中的B即是因为File B没有改变，所以直接存储了一个指向FileB的链接。只有修改了的文件才会产生一个新的文件，覆盖原来的文件。\ngit的工作原理/流程如下:  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125536.png)\n\n- Workspace：工作区(本地目录文件)\n- Index / Stage：暂存区/缓存区\n- Repository：仓库区（或本地仓库）\n- Remote：远程仓库\n\n## 基本操作\n\n### 初始化仓库\n仓库的初始化有两种方式：一种是直接从远程仓库克隆，另一种则是直接从当前目录初始化。远程初始化命令在[从远程仓库获取](#从远程仓库获取)，本地初始化命令的方法是首先创建一个文件夹，我命名为mygit，然后执行如下命令：  \n```bash\n$ git init\n```\n执行完毕后，当前目录下会出现一个隐藏的.git文件夹，git所需的数据和资源都放在改目录中。\n\n### 查看仓库状态\n\n通过`git status`来查看仓库状态，执行效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125612.png)\n可以看到nothing to commit，表示本地工作区没有要提交的文件，我们再创建一个one.txt的文件，然后在执行`git status`，效果如下： \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130149.png)\n从结果Untracked files可以看到，one.txt还没有被add到暂存区。\n\n### 添加文件到暂存区\n\n`git add`命令可以将一个文件添加到暂存区,执行如下命令将one.txt添加到暂存区：\n```bash\ngit add one.txt\n```\n将one.txt添加到暂存区后，再次执行`git status`,可看到如下效果：  ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125701.png)\n从Changes to be committed可以看出，one.txt已被添加到暂存区，但还未被添加到本地仓库。\n\n### 提交到本地仓库\n\n当文件提交到暂存区之后，执行`git commit`命令将当前暂存区的文件提交到本地仓库，执行如下命令：\n```bash\ngit commit -m '新增一个one.txt'\n```\n-m是指将当前暂存区的文件提交到本地仓库的时候，加上提交备注/说明，再次执行`git status`,可以看到已经没有要add或commit的文件了。这里要强调一下，如果直接执行`git commit`命令，会自动打开一个vi编辑器，在里面输入备注/说明即可。此外，当我们提交成功后，还可以通过`git commit --amend  `修改备注信息。\n### 查看更改前后的差异\n使用`git diff`命令可以查看**工作区和暂存区的区别**，在one.txt里面写入一行hello world，然后执行`git diff`命令，结果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130230.png)\n根据结果可以看到新增了一行hello world，如果我们要比较**工作区与最新本地版本库的区别**，可以执行`git diff HEAD`，结果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130230.png)\n\n### 查看提交历史\n\n使用`git log`查看提交历史，我们首先将工作区的内容提交到本地仓库，执行`git add one.txt`将更改后的one.txt添加到暂存区，执行` git commit -m '添加了一行hello world'`将暂存区的内容提交到本地仓库，然后执行`git log`，结果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125804.png)\n\n## git撤销修改\n\n### 工作区的代码撤销\n\n使用`git checkout`撤销工作区的代码。我们先向one.txt添加一行hello everyone，执行`cat one.txt`查看内容，再执行`git checkout -- one.txt`撤销之前的操作，让one,txt恢复之前的状态，然后执行`cat one.txt`再次查看内容，效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125909.png)  \n可以看到，工作区的内容已经被修改,这时候本地文件刚刚添加的内容就被撤销了。\n\n### 暂存区的代码撤销\n\n使用`git reset HEAD`撤销暂存区的代码。首先在one.txt添加一行hello people，执行`git add one.txt`将更改的内容提交到暂存区，`git reset HEAD`来撤销暂存区的代码，如下图：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130403.png)\n撤销暂存区的代码之后，如需要将代码添加到暂存区，则需要再次执行`git add`命令\n\n### 本地仓库的代码撤销\n\n可以使用`git reset --hard <版本号>`来撤销本地仓库的代码，版本号有几种不同的写法：\n1. 可以使用HEAD^来描述版本，一个^表示前一个版本，两个^^表示前两个版本，以此类推。\n2. 也可以使用数字来代替^，比如说前100个版本可以写作HEAD~100。\n3. 也可以直接写版本号，表示跳转到某一个版本处。我们每次提交成功后，都会生成一个哈希码作为版本号，所以这里我们也可以直接填版本号，哈希码很长，但是我们不用全部输入，只需要输入前面几个字符即可，就能识别出来。执行`git log`后那一串长符号就是哈希码版本号。\n\n依次执行如下命令：\n```bash\n$ git add 'one.txt'\n$ git commit -m '添加一行hello people'\n$ git reset --hard head^\n```\n执行`git reset --hard head^`后的效果如下：\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130427.png)  \n可以从结果看出，前半部分816b208是执行撤销操作以后，当前版本的版本号前七位，后半部分是该版本的备注，可以用`git log`来查看不同版本的版本号和备注。  \n\n再次查看本地one.txt文件，发现本地目录的刚刚添加的内容已经没有了，如需要再次提交到本地仓库，则可执行`git add`和`git commit`命令。**需要注意的是，当撤销到最开始版本的时候，`git reset --hard head^`就不能再用了，否则会报如下的错误：**  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130454.png)\n\n## git分支管理\n\n### 查看分支\n通过`git branch`来查看当前仓库有哪些分支和我们处于哪一分支中，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130516.png) \n可以看到，当前本地仓库只有一个master分支，这是git默认创建出来的，master前面的\\*表示我们当前处于这一个分支中。\n\n### 分支创建和切换\n可以利用`git branch <分支名>`来创建一个分支，利用`git checkout <分支名>`来切换分支，如下所示： \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130558.png)\n\n### 分支合并\n\n由于math分支是从master分支中创建出来的，所以此时math分支的内容和master分支的内容是一致的，现在，我们在math分支向one.txt添加一行hello branch math(由于刚刚执行了[本地仓库的代码撤销](#本地仓库的代码撤销)，所以one.txt现在的内容是空白的)，此时math分支的one.txt和math分支的one.txt就不同了，具体效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130558.png)  \n执行完毕后，我们也可以在本地查看，先在math分支下，打开one.txt可以看到我们刚刚添加的内容，然后再切换到master分支，再从本地打开one.txt文件，就看不到内容了。\n**可以通过`git merge <分支名>`合并分支**，先切换到master分支，然后执行` git merge math`合并math分支到master分支上，效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130633.png)  \n可以看到再次在master分支下查看one.txt，就可以显示math分支的内容了。\n\n通常合并分支时，git一般使用”Fast forward”模式，fast-forward方式表示当条件允许时，git直接把HEAD指针指向合并分支的头，完成合并，这种方式合并速度快，但是在整个过程中没有创建commit。在这种模式下，删除分支后，会丢掉分支信息，可使用带参数 `–no-ff`来禁用”Fast forward”模式，即删除时可以实用`git merge --no-ff <分支名>`\n\n### 以图表方式查看分支\n可以用`git log --graph`命令来直观的查看分支的创建和合并等操作，合并math和master分支前的效果如下： \n    ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130807.png)\n合并math和master分支后的效果如下：   \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130807.png)\n\n### 解决冲突\n\n我们创建一个新的分支dev,并在dev分支下给one.txt添加一行12345，然后提交，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130943.png)  \n同样，我们现在切换到master分支上来，也在one.txt添加一行内容，内容为56789，并提交，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130943.png)  \n现在，我们将dev分支合并到master上来，如下所示：\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131003.png)  \n从结果中可以看出，=======之前是主分支的内容，=======之后是dev分支的内容，此时我们用文本编辑器修改one.txt的冲突然后提交即可，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131140.png)\n\n**分支策略：首先master主分支应该是非常稳定的，也就是用来发布新版本，一般情况下不允许在上面干活，干活一般情况下在新建的dev分支上干活，干完后，dev分支代码可以合并到主分支master上来。**\n\n## github远程仓库\n\n### 关联远程仓库\n在此之前我相信你已经配置SSH KEY，如果没有，可以参考我的这篇[搭建个人博客](https://wxler.gitee.io/2020/06/01/hexoCreateAndConfig/#git%E9%85%8D%E7%BD%AE)里进行配置，配置完成以后在github上创建一个仓库，这里命名为test，我们可以看到仓库的地址，例如：`https://github.com/wxler/test.git`。然后将我们之前的本地仓库和这个远程仓库进行关联，使用`git remote add`命令，如下：\n```bash\n$ git remote add origin https://github.com/wxler/test.git\n```\n### 推送到远程仓库\n把本地库的内容推送到远程，使用`git push -u origin master`命令，实际上是把当前分支master推送到远程。由于远程库是空的，我们第一次推送master分支时，加上了–u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令,不用加-u了,效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131213.png) \n推送成功后，可以立刻在github页面中看到远程库的内容已经和本地一模一样了。从现在起，只要本地作了提交，就可以通过命令：`git push origin master`把本地master分支的最新修改推送到github上了，现在你就拥有了真正的分布式版本库了。  \n**我们一般不把其它分支推送到远程仓库，master主分支是最稳定的版本，一般情况下不允许在上面干活，干活一般情况下在新建的分支上干活，干完后，把分支代码可以合并到主分支master上来。**当然，你也可以将其它分支推送到远程仓库，可以执行如下命令：\n\n```bash\n$ git checkout fa\n$ git push -u origin fa\n```\n### 从远程仓库获取\n我们可以通过git clone命令克隆一个远程仓库到本地,方式也简单，在本地创建一个空文件夹，执行如下命令：\n```bash\n$ git clone https://github.com/wxler/test.git\n```\n此时克隆的是master分支到本地仓库，我们可以通过`git branch -a`来查看本地仓库和远程仓库的信息，-a参数可以同时显示本地仓库和远程仓库的信息，如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131310.png)\n我们也可以把远程仓库其它分支的内容clone下来，可以执行如下命令：\n\n```bash\n$ git branch fa origin/dev\n$ git checkout dev\n```\n上面的指令表示根据远程仓库的dev分支创建一个本地仓库的dev分支，然后再切换到dev分支，**注意由于dev分支就是从远程仓库克隆下来的，所以这里可以不添加-u参数。**\n\n### 从远程仓库更新\n使用`git pull`获取远程仓库最新的代码和数据，例如，我们可以通过以下代码将远程主机的master分支最新内容拉下来后与当前本地分支直接合并\n```bash\ngit pull origin master\n```\n\n## git命令大全\ngit常用命令速查表，方便查阅：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131337.png)\n\n## 遗留问题\n\n到现在，我们就可以使用git的大多数操作了，但是还有git的一些操作平常没有用到，我也就不主动去一个个试了，毕竟一口吃不成胖子，查了也记不住，就不自找苦吃了，遗留的问题主要有：\n- git分支衍合\n- git标签管理\n- bug分支&stash功能\n\n## 参考文档\n[松哥git教程](https://mp.weixin.qq.com/s?__biz=MzI1NDY0MTkzNQ==&mid=100004284&idx=1&sn=f9adbded2bac4d8efb9b07a0262a0e8a&chksm=69c343dc5eb4cacaa45d2a968ab935e85eccbcb11f9493d32ef147da5c1a1ff53cc45a1f32a2&mpshare=1&scene=1&srcid=0616SVENp6ameT2V21QsW2nZ&sharer_sharetime=1592289626242&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=1de400e48ea73360020a1786d1997d16a5b67307619595df7c6b06fc16be187dba8368d87386b6a57a40d3c3f3671fae6927462285e9b59479eb08a036e5aa45b02add3de42bb86044ef6649218a6531&ascene=1&uin=MjA2Nzc1NzU0Mg%3D%3D&devicetype=Windows+10+x64&version=6209007b&lang=zh_CN&exportkey=ASZhqnZ2JNrU1FCHtEQsA0s%3D&pass_ticket=DNBBiZIg7%2Fjg4GBZz9sSD49h8dYitJv5rAPR21lJbAICH1bIYdByWmp3fQOoDyHW)\n[Git从入门到熟练使用](https://www.jianshu.com/p/34cfe097e06a)\n[史上最简单Git入门教程](https://www.cnblogs.com/jjlee/p/10305194.html)\n[git命令大全](https://www.jianshu.com/p/46ffff059092)\n\n>特别声明：本篇博客只做个人学习交流和参考手册使用，不作任何商业目的，内容上较多参考了[松哥git教程](https://mp.weixin.qq.com/s?__biz=MzI1NDY0MTkzNQ==&mid=100004284&idx=1&sn=f9adbded2bac4d8efb9b07a0262a0e8a&chksm=69c343dc5eb4cacaa45d2a968ab935e85eccbcb11f9493d32ef147da5c1a1ff53cc45a1f32a2&mpshare=1&scene=1&srcid=0616SVENp6ameT2V21QsW2nZ&sharer_sharetime=1592289626242&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=1de400e48ea73360020a1786d1997d16a5b67307619595df7c6b06fc16be187dba8368d87386b6a57a40d3c3f3671fae6927462285e9b59479eb08a036e5aa45b02add3de42bb86044ef6649218a6531&ascene=1&uin=MjA2Nzc1NzU0Mg%3D%3D&devicetype=Windows+10+x64&version=6209007b&lang=zh_CN&exportkey=ASZhqnZ2JNrU1FCHtEQsA0s%3D&pass_ticket=DNBBiZIg7%2Fjg4GBZz9sSD49h8dYitJv5rAPR21lJbAICH1bIYdByWmp3fQOoDyHW)，根据自己实际应用进行删减，并加上了自己的理解和补充，如有侵权，请联系博主本人删除。","tags":["git"],"categories":["git使用"]},{"title":"使用hexo平台从0搭建个人博客","url":"/2020/06/01/122000/","content":"搭建这个博客，花费了我不少时间，这期间我遇到各种各样的问题，这些问题本可以避免，因操作不规范、对指令代码的不理解、网络不稳定、配置上的错误，使得最后暴露出来的各种bug很不容易解决。前前后后我也重新搭建了三次，经历了心态上的各种起伏。为此，我记录下我制作的过程，让想和我一样自建博客的人少走一些弯路。\n\n我使用的hexo博客框架，stun主题，搭建环境和过程可分为几个部分:\n\n1. 安装git\n2. 安装nodejs\n3. 安装hexo\n4. hexo搭桥github\n5. hexo-admin使用\n6. npm&hexo常用命令\n<!-- more -->\n\n## 安装git\n\n### git下载\n下载[git](https://git-scm.com/download)，双击安装，然后一直next，按键Ctrl+r，然后在弹出框中出入cmd，在弹出的界面输入git，回车,出来一大串命令符就代表安装成功了。可以使用git version查看自己的git版本。\n\n### git配置\n\n1. git安装好去GitHub上注册一个账号，注册好后，桌面空白地方右键选择Git Bash，要git账户进行环境配置  \n```bash\n//usename是用户名\ngit config --global user.name \"username\"\ngit config --global user.email \"username@email.com\"\n```\n2. 当以上命令执行结束后，可用 `git config --global --list` 命令查看配置是否OK  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133328.png)\n3. 在命令框中输入命令`ssh-keygen -t rsa`，连敲三次回车键，结束后去系统盘目录下（一般在 C:\\Users\\你的用户名.ssh）(mac: /Users/用户/.ssh）查看是否有。ssh文件夹生成，此文件夹中以下两个文件  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133402.png)\n4. 将ssh文件夹中的公钥（ id_rsa.pub）添加到GitHub管理平台中，在GitHub的个人账户的设置中找到如下界面，title随便起一个，将公钥（ id_rsa.pub）文件中内容复制粘贴到key中，然后点击Ass SSH key  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133430.png)\n5、测试一下配置是否成功，在Git Bush命令框（就是刚才配置账号和邮箱的命令框）中继续输入命令`ssh -T git@github.com`，回车,出现如下界面即说明成功  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133448.png)\n\n## 安装nodejs\n\nHexo是基于nodeJS环境的静态博客，里面的npm工具很有用。下载[nodejs](https://nodejs.org/en/)，(说明：LTS为长期支持版，Current为当前最新版)，下载后一路next进行安装，在git bash下使用node -v查看版本。\n\n## 安装hexo\n\n我建议先看一下npm&hexo常用命令部分，了解命令的结构和大体含义之后，在配置的过程中可以避免很多错误，少走很多弯路。  \n执行 `npm config list`查看当前的配置，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133516.png)\n可以看到，最初的镜像地址是官方的npm镜像，我最初使用的就是这个配置，执行起来很不稳定，导致大多数错误都是网络问题导致的,执行如下命令,切换淘宝镜像\n\n```bash\nnpm install -g cnpm --registry=https://registry.npm.taobao.org\nnpm config set registry https://registry.npm.taobao.org\n```\n接下来，就可以安装hexo了，执行`npm install -g hexo-cli`或`npm install -g hexo`，如果之前安装失败，可以先执行`npm uninstall hexo-cli -g`或`npm uninstall hexo -g` 卸载hexo，再进行安装，结果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133610.png)\n查看版本信息` hexo v`  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133628.png)\n初始化hexo,执行 `hexo init myblog`，然后`cd myblog`，再次执行`hexo v`，就可以看到hexo的版本：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133657.png)  \n打开myblog文件夹，我们可以看到hexo的结构\n\n> * node_modules：是依赖包\n> * public：存放的是生成的页面\n> * scaffolds：命令生成文章等的模板\n> * source：用命令创建的各种文章\n> * themes：主题\n> * _config.yml：整个博客的配置\n> * db.json：source解析所得到的\n> * package.json：项目所需模块项目的配置信息  \n\n\n到这里我们的hexo博客就安装完成啦，只有搭桥到github,才能进行部署。\n\n## hexo搭桥github\n\n创建一个repo，名称为yourname.github.io,其中yourname是你的github名称，按照这个规则创建才有用，这个仓库就是存放你博客的地方。\n1. 用编辑器打开你的blog项目，修改_config.yml  \n```text\ndeploy:  \n\ttype: git\n\trepo:https://github.com/YourgithubName/YourgithubName.github.io.git\n\tbranch: master\n```\n2. 回到gitbash中，进入你的blog目录，分别执行以下命令：\n```bash\nhexo clean\nhexo generate\nhexo server\n```\n需要注意的是，hexo 3.0把服务器独立成个别模块，需要单独安装：`npm i hexo-server`\n3. 打开浏览器输入：`http://localhost:4000`\n4. 先安装一波：`npm install hexo-deployer-git --save`（这样才能将你写好的文章部署到github服务器上并让别人浏览到） \n5. 执行命令\n```bash\nhexo clean\nhexo generate\nhexo deploy\n```\n6. 在浏览器中输入`http://yourgithubname.github.io`就可以看到你的个人博客啦。\n\n我使用的主题是stun，如果大家也想使用这个主题，可以到[ hexo-theme-stun](https://liuyib.github.io/hexo-theme-stun/zh-CN/)查阅配置。\n\n\n## hexo-admin使用\n\n\n用原生的方法来管理博文十分的不便，因此便有了Hexo Admin这一插件来方便我们的操作。执行`npm install --save hexo-admin`安装hexo-admin，安装成功后，在`http://localhost:4000/admin`就可以访问hexo-admin页面。\n详细情形我就不多说了，推荐大家到[hexo博客使用hexo-admin插件管理文章](https://blog.csdn.net/nineya_com/article/details/103380243?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase)，这位作者的[hexo-admin插件windows系统插入图片失败问题](https://blog.csdn.net/nineya_com/article/details/103384546)修复了windows下粘贴图片的裂图和显示功能，我使用起来非常好，推荐大家看看。\n\n除此之外，我还要强调一点，hexo-admin创建文章的时候，首先创建英文名，再在里面编辑成中文，这样你的文章显示的链接就不会带有中文了。hexo-admin的文章只有未发布状态才能删除，并且删除后在source/_discarded文件夹，未发布变成draft,发布直接到post。\n\n## npm&hexo常用命令\n### npm&cnpm介绍\nnpm（node package manager）：nodejs的包管理器，用于node插件管理（包括安装、卸载、管理依赖等），使用`npm -v`查看版本信息。\n\ncnpm:因为npm安装插件是从国外服务器下载，受网络的影响比较大，可能会出现异常，如果npm的服务器在中国就好了，所以我们乐于分享的淘宝团队干了这事。来自官网：“这是一个完整 npmjs.org 镜像，你可以用此代替官方版本(只读)，同步频率目前为 10分钟 一次以保证尽量与官方服务同步”，更多详情可以查看[淘宝 NPM 镜像](https://developer.aliyun.com/mirror/NPM?from=tnpm)，使用`cnpm -v`查看版本信息。\n\n\nnpm和cnpm安装命令一样，只不过是多了一个c。\n\n### npm命令\n使用npm命令首先要设置下载的镜像，模式是npm官网的镜像(服务器在国外)，建议设置国内的淘宝镜像,设置以后我们就可以用npm从淘宝镜像下载数据了  \n永久使用：  \n`npm config set registry https://registry.npm.taobao.org`  \n临时使用：  \n`npm install node-sass --registry=http://registry.npm.taobao.org`  \n还有个清除缓存命令，可以解决些奇怪的问题:  \n`npm cache clean --force`  \n查看已安装的npm插件，这个命令很实用，可以查看缺少哪些插件  \n`npm ls --depth 0`   \n可以通过定制的 cnpm 命令行工具代替默认的 npm  \n`npm install -g cnpm --registry=http://registry.npm.taobao.org`  \n**在使用过程中要么用npm，要么用cnpm，不能混用**  \n查看当前的配置命令`npm config list `,操作之前一定要先查看配置再进行操作。  \n**下面需要强调后缀参数的作用和区别**  \n`npm install packagename --save 或 -S`    \n--save、-S参数意思是把模块的版本信息保存到dependencies（生产环境依赖）中，即你的package.json文件的dependencies字段中。  \n`npm install packagename --save-dev 或 -D`  \n--save-dev 、 -D参数意思是吧模块版本信息保存到devDependencies（开发环境依赖）中，即你的package.json文件的devDependencies字段中。  \n`npm install packagename -g 或 --global`  \n安装全局的模块（不加参数的时候默认安装本地模块），\n**使用npm安装插件的时候一定要加上--save添加依赖，否则容易出错** ，更多关于npm详情，请点击[npm常用命令及参数详解](https://segmentfault.com/a/1190000012099112?utm_source=tag-newest)\n\n### hexo命令\n\n`hexo init`  \n初始化站点，生成一个简单网站所需的各种文件。\n\n`hexo clean == hexo c`  \n清除缓存 网页正常情况下可以忽略此条命令\n\n`hexo generate == hexo g`  \n生效新增、修改、更新的文件\n\n`hexo server == hexo s`  \n启动本地网站，可在本地观察网站效果，同时也可以输入`http://localhost:4000/admin`管理文章\n\n`hexo s --debug`  \n以调试模式启动本地网站，在此模式下，对文件的更改无需停止网站只需刷新即可看到效果，调试非常方便\n\n\n`hexo clean && hexo s`  \n一次执行两个命令\n\n`hexo deploy == hexo d`  \nhexo的一键部署功能，执行此命令即可将网站发布到配置中的仓库地址，执行此命令前需要配置站点配置文件_config.yml","tags":["hexo"],"categories":["工具"]}]