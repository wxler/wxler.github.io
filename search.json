[{"title":"十、Yarn案例实操","url":"/2021/04/02/214801/","content":"\n该篇基于上一篇的Yarn理论进行实际操作。调整下列参数之前尽量拍摄Linux快照，否则后续的案例，还需要重写准备集群。\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. Yarn 生产环境核心参数配置案例\n（1）需求：从 1G 数据中，统计每个单词出现次数。服务器 3 台，每台配置 4G 内存，4 核CPU，4 线程。\n\n（2）需求分析：\n1G数据，每个分片默认大小为128M，1G数据分8个MapTask，1 个 ReduceTask；1 个 mrAppMaster\n平均每个节点运行  `10 个  / 3 台  ≈  3 个任务（4  3  3）`\n\n（3）修改 yarn-site.xml 配置参数如下：\n\n```xml\n<!--  选择调度器，默认容量  -->\n<property>\n\t<description>The class to use as the resource scheduler.</description>\n\t<name>yarn.resourcemanager.scheduler.class</name>\n\t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>\n</property>\n\n<!-- ResourceManager 处理调度器请求的线程数量,默认 50；如果提交的任务数大于 50，\n可以增加该值，但是不能超过 3 台  * 4 线程  = 12 线程（去除其他应用程序实际不能超过 8）  -->\n<property>\n\t<description>Number  of  threads  to  handle  scheduler interface.</description>\n\t<name>yarn.resourcemanager.scheduler.client.thread-count</name>\n\t<value>8</value>\n</property>\n\n<!--  是否让 yarn 自动检测硬件进行配置，默认是 false，如果该节点有很多其他应用程序，\n建议手动配置。如果该节点没有其他应用程序，可以采用自动  -->\n<property>\n<description>Enable auto-detection of node capabilities such as memory and CPU.</description>\n<name>yarn.nodemanager.resource.detect-hardware-capabilities</name>\n<value>false</value>\n</property>\n\n<!--  是否将虚拟核数当作 CPU 核数，默认是 false，采用物理 CPU 核数  -->\n<property>\n\t<description>Flag to determine if logical processors(such ashyperthreads) should be \n\tcounted as cores. Only applicable on Linux\n\twhen yarn.nodemanager.resource.cpu-vcores is set to -1 and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is true.\n\t</description>\n\t<name>yarn.nodemanager.resource.count-logical-processors-ascores</name>\n\t<value>false</value>\n</property>\n\n<!--  虚拟核数和物理核数乘数，默认是 1.0 -->\n<property>\n\t<description>Multiplier to determine how to convert phyiscal cores to\n\tvcores. This value is used if yarn.nodemanager.resource.cpu-vcores\n\tis set to -1(which implies auto-calculate vcores) and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is set to true. \n\tThe number of vcores will be calculated as number of CPUs * multiplier.\n\t</description>\n\t<name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>\n\t<value>1.0</value>\n</property>\n\n<!-- NodeManager 使用内存数，默认 8G，修改为 4G 内存  -->\n<property>\n\t<description>Amount of physical memory, in MB, that can be allocated \n\tfor containers. If set to -1 and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is true, it is\n\tautomatically calculated(in case of Windows and Linux).\n\tIn other cases, the default is 8192MB.\n\t</description>\n\t<name>yarn.nodemanager.resource.memory-mb</name>\n\t<value>4096</value>\n</property>\n\n<!-- nodemanager 的 CPU 核数，不按照硬件环境自动设定时默认是 8 个，修改为 4 个  -->\n<property>\n\t<description>Number of vcores that can be allocated\n\tfor containers. This is used by the RM scheduler when allocating\n\tresources for containers. This is not used to limit the number of\n\tCPUs used by YARN containers. If it is set to -1 and\n\tyarn.nodemanager.resource.detect-hardware-capabilities is true, it is\n\tautomatically determined from the hardware in case of Windows and Linux.\n\tIn other cases, number of vcores is 8 by default.</description>\n\t<name>yarn.nodemanager.resource.cpu-vcores</name>\n\t<value>4</value>\n</property>\n\n<!--  容器最小内存，默认 1G -->\n<property>\n<description>The minimum allocation for every container request at the\nRM  in MBs. Memory requests lower than this will be set to the value of \nthis  property. Additionally, a node manager that is configured to have \nless memory  than this value will be shut down by the resource manager.\n</description>\n<name>yarn.scheduler.minimum-allocation-mb</name>\n<value>1024</value>\n</property>\n\n<!--  容器最大内存，默认 8G，修改为 2G -->\n<property>\n\t<description>The maximum allocation for every container request at the \n\tRM  in  MBs.  Memory  requests  higher  than  this  will  throw  an\n\tInvalidResourceRequestException.\n\t</description>\n\t<name>yarn.scheduler.maximum-allocation-mb</name>\n\t<value>2048</value>\n</property>\n\n<!--  容器最小 CPU 核数，默认 1 个  -->\n<property>\n\t<description>The minimum allocation for every container request at the \n\tRM  in terms of virtual CPU cores. Requests lower than this will be set to \n\tthe value of this property. Additionally, a node manager that is configured \n\tto  have  fewer  virtual  cores  than  this  value  will  be  shut  down  by  the \n\tresource  manager.\n\t</description>\n\t<name>yarn.scheduler.minimum-allocation-vcores</name>\n\t<value>1</value>\n</property>\n\n<!--  容器最大 CPU 核数，默认 4 个，修改为 2 个  -->\n<property>\n\t<description>The maximum allocation for every container request at the \n\tRM  in terms of virtual CPU cores. Requests higher than this will throw an\n\tInvalidResourceRequestException.</description>\n\t<name>yarn.scheduler.maximum-allocation-vcores</name>\n\t<value>2</value>\n</property>\n\n<!--  虚拟内存检查，默认打开，修改为关闭  -->\n<property>\n\t<description>Whether virtual memory limits will be enforced for\n\tcontainers.</description>\n\t<name>yarn.nodemanager.vmem-check-enabled</name>\n\t<value>false</value>\n</property>\n\n<!--  虚拟内存和物理内存设置比例,默认 2.1 -->\n<property>\n\t<description>Ratio  between  virtual  memory  to  physical  memory  when\n\tsetting  memory  limits  for  containers.  Container  allocations  are\n\texpressed in terms of physical memory, and virtual memory usage  is \n\tallowed to exceed this allocation by this ratio.\n\t</description>\n\t<name>yarn.nodemanager.vmem-pmem-ratio</name>\n\t<value>2.1</value>\n</property> \n```\n\n**关闭虚拟内存检查原因**\n\n在CentOS7.5中\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407152549786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（4）分发配置。\n\n注意：如果集群的硬件资源不一致，要每个 NodeManager 单独配置\n\n（5）重启集群\n\n（6）执行 WordCount 程序\n\n```bash\nhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar  wordcount /input /output\n```\n（7）观察任务页面：`http://wxler2:8088`\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407153033530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n## 2. 容量调度器多队列提交案例\n\n在生产环境怎么创建队列？\n1. 调度器默认就 1 个 default 队列，不能满足生产要求。\n2. 按照框架：hive /spark/ flink  每个框架的任务放入指定的队列（企业用的不是特别多）\n3. 按照业务模块：登录注册、购物车、下单、业务部门 1、业务部门 2 （企业用的多）\n\n创建多队列的好处？\n- 因为担心员工不小心，写递归死循环代码，把所有资源全部耗尽。\n- 实现任务的**降级**使用，特殊时期保证重要的任务队列资源充足。11.11、  6.18 等购物节\n业务部门 1 （重要） > 业务部门 2 （比较重要） >下单（一般） > 购物车（一般）> 登录注册（次要） \n\n\n### 2.1 需求\n需求 1： default 队列占总内存的 40%，最大资源容量占总资源 60%， hive 队列占总内存的 60%，最大资源容量占总资源 80%。\n\n需求 2：配置队列优先级\n\n### 2.2 配置多队列的容量调度器\n\n\n（1）在 `capacity-scheduler.xml` 中配置如下：\n① 修改如下配置\n\n```xml\n<!--  指定多队列，增加 hive 队列  -->\n<property>\n\t<name>yarn.scheduler.capacity.root.queues</name>\n\t<value>default,hive</value>\n\t<description>\n\tThe queues at the this level (root is the root queue).\n\t</description>\n</property>\n\n<!--  降低 default 队列资源额定容量为 40%，默认 100% -->\n<property>\n\t<name>yarn.scheduler.capacity.root.default.capacity</name>\n\t<value>40</value>\n</property>\n\n<!--  降低 default 队列资源最大容量为 60%，默认 100% -->\n<property>\n\t<name>yarn.scheduler.capacity.root.default.maximum-capacity</name>\n\t<value>60</value>\n</property>\n```\n② 为新加队列添加必要属性：\n\n```xml\n<!--  指定 hive 队列的资源额定容量  -->\n<property>\n\t<name>yarn.scheduler.capacity.root.hive.capacity</name>\n\t<value>60</value>\n</property>\n\n<!--  用户最多可以使用队列多少资源比例，1 表示用户可以占用队列所有资源  -->\n<property>\n\t<name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>\n\t<value>1</value>\n</property>\n\n<!--  指定 hive 队列的资源最大容量  -->\n<property>\n\t<name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>\n\t<value>80</value>\n</property>\n\n<!--  启动 hive 队列  -->\n<property>\n\t<name>yarn.scheduler.capacity.root.hive.state</name>\n\t<value>RUNNING</value>\n</property>\n\n<!--  哪些用户有权向队列提交作业  -->\n<property>\n\t<name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>\n\t<value>*</value>\n</property>\n\n<!--  哪些用户有权操作队列，管理员权限（查看/杀死）  -->\n<property>\n\t<name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>\n\t<value>*</value>\n</property>\n\n<!--  哪些用户有权配置提交任务优先级  -->\n<property>\n\t<name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>\n\t<value>*</value>\n</property>\n\n<!--  任务的超时时间设置： yarn application  -appId  appId  -updateLifetime  Timeout\n参考资料： https://blog.cloudera.com/enforcing-application-lifetime-slasyarn/ -->\n<!--  如果 application 指定了超时时间，则提交到该队列的 application 能够指定的最大超时时间不能超过该值。 -->\n<property>\n\t<name>yarn.scheduler.capacity.root.hive.maximum-applicationlifetime</name>\n\t<value>-1</value>\n</property>\n\n<!--  如果 application 没指定超时时间，则用 default-application-lifetime 作为默认值，-1代表无限长  -->\n<property>\n\t<name>yarn.scheduler.capacity.root.hive.default-applicationlifetime</name>\n\t<value>-1</value>\n</property>\n```\n\n（2）分发配置文件\n\n```bash\nxsync capacity-scheduler.xml\n```\n\n（3）重启 Yarn 或者执行 `yarn rmadmin -refreshQueues` 刷新队列，就可以看到两条队列：\n\n```bash\n[wxler@wxler1 hadoop]$ yarn rmadmin -refreshQueues\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407161752914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n### 2.3 向 Hive 队列提交任务\n（1）hadoop jar 的方式\n\n```bash\nhadoop  jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar  wordcount  -D mapreduce.job.queuename=hive /input /output\n```\n注: `-D` 表示运行时改变参数值\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407162351689.png)\n\n（2）打 jar 包的方式\n\n默认的任务提交都是提交到 default 队列的。如果希望向其他队列提交任务，需要在Driver 中声明：\n\n```java\npublic class WcDrvier {\n\tpublic  static  void  main(String[]  args)  throws  IOException, \n\t\tClassNotFoundException, InterruptedException {\n\t\tConfiguration conf = new Configuration();\n\t\tconf.set(\"mapreduce.job.queuename\",\"hive\");\n\t\t//1.  获取一个 Job 实例\n\t\tJob job = Job.getInstance(conf);\n\t\t。。。  。。。\n\t\t//6.  提交 Job\n\t\tboolean b = job.waitForCompletion(true);\n\t\tSystem.exit(b ? 0 : 1);\n\t}\n}\n```\n\n### 2.4 任务优先级\n容量调度器，支持任务优先级的配置，在资源紧张时，优先级高的任务将优先获取资源。默认情况， Yarn 将所有任务的优先级限制为 0，若想使用任务的优先级功能，须开放该限制。 \n（1）修改 yarn-site.xml 文件，增加以下参数\n\n```xml\n<property>\n\t<name>yarn.cluster.max-application-priority</name>\n\t<value>5</value>\n</property>\n```\n\n（2）分发配置\n\n```bash\nxsync yarn-site.xml\n```\n\n（3）重启 Yarn\n\n```bash\n[wxler@wxler2 ~]$ cd ${HADOOP_HOME}\n[wxler@wxler2 hadoop-3.1.3]$ ls\nbin   etc      lib      LICENSE.txt  NOTICE.txt  sbin   wcinput\ndata  include  libexec  logs         README.txt  share  wcoutput\n[wxler@wxler2 hadoop-3.1.3]$ sbin/stop-yarn.sh\nStopping nodemanagers\nStopping resourcemanager\n[wxler@wxler2 hadoop-3.1.3]$ sbin/start-yarn.sh\nStarting resourcemanager\nStarting nodemanagers\n```\n\n（4）模拟资源紧张环境，可连续提交以下任务，直到新提交的任务申请不到资源为止。\n\n```bash\nhadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407164734287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n（5）再次重新提交优先级高的任务\n\n```bash\nhadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -D mapreduce.job.priority=5 5 2000000\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407164826624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（6）也可以通过以下命令修改正在执行的任务的优先级\n\n```bash\nyarn application -appID <ApplicationID> -updatePriority  优先级\n```\n\n例如\n\n```bash\nyarn application -appID application_1611133087930_0009 -updatePriority 5\n```\n\n## 3. 公平调度器案例\n\n### 3.1 需求\n创建两个队列，分别是 test 和 atguigu （以用户所属组命名）。期望实现以下效果： 若用户提交任务时指定队列，则任务提交到指定队列运行；若未指定队列，test 用户提交的任务到 root.group.test 队列运行， atguigu 提交的任务到 root.group.atguigu 队列运行（注： group 为用户所属组）。\n\n公平调度器的配置涉及到两个文件，一个是 yarn-site.xml，另一个是公平调度器队列分配文件 fair-scheduler .xml（文件名可自定义）。\n\n（1）配置文件参考资料：\nhttps://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html\n\n（2）任务队列放置规则参考资料：\nhttps://blog.cloudera.com/untangling-apache-hadoop-yarn-part-4-fair-scheduler-queuebasics/\n\n\n### 3.2 配置多队列的公平调度器\n（1）修改 yarn-site.xml 文件，加入以下参数\n\n```scala\n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.class</name>\n\t\t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n\t\t<description>配置使用公平调度器</description>\n\t</property>\n\n\t<property>\n\t\t<name>yarn.scheduler.fair.allocation.file</name>\n\t\t<value>/opt/module/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml</value>\n\t\t<description>指明公平调度器队列分配配置文件</description>\n\t</property>\n\n\t<property>\n\t\t<name>yarn.scheduler.fair.preemption</name>\n\t\t<value>false</value>\n\t\t<description>禁止队列间资源抢占</description>\n\t</property>\n```\n\n（2）配置 fair-scheduler.xml\n\n```xml\n<?xml version=\"1.0\"?>\n<allocations>\n    <!--  单个队列中 Application Master 占用资源的最大比例,取值 0-1  ，企业一般配置 0.1 -->\n    <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>\n    <!--  单个队列最大资源的默认值  test atguigu default -->\n    <queueMaxResourcesDefault>4096mb,4vcores</queueMaxResourcesDefault>\n    <!--  增加一个队列 test -->\n    <queue name=\"test\">\n        <!--  队列最小资源  -->\n        <minResources>2048mb,2vcores</minResources>\n        <!--  队列最大资源  -->\n        <maxResources>4096mb,4vcores</maxResources>\n        <!--  队列中最多同时运行的应用数，默认 50，根据线程数配置  -->\n        <maxRunningApps>4</maxRunningApps>\n        <!--  队列中 Application Master 占用资源的最大比例  -->\n        <maxAMShare>0.5</maxAMShare>\n        <!--  该队列资源权重,默认值为 1.0 -->\n        <weight>1.0</weight>\n        <!--  队列内部的资源分配策略  -->\n        <schedulingPolicy>fair</schedulingPolicy>\n    </queue>\n    <!--  增加一个队列 wxler -->\n    <queue name=\"wxler\" type=\"parent\">\n        <!--  队列最小资源  -->\n        <minResources>2048mb,2vcores</minResources>\n        <!--  队列最大资源  -->\n        <maxResources>4096mb,4vcores</maxResources>\n        <!--  队列中最多同时运行的应用数，默认 50，根据线程数配置  -->\n        <maxRunningApps>4</maxRunningApps>\n        <!--  队列中 Application Master 占用资源的最大比例  -->\n        <maxAMShare>0.5</maxAMShare>\n        <!--  该队列资源权重,默认值为 1.0 -->\n        <weight>1.0</weight>\n        <!--  队列内部的资源分配策略  -->\n        <schedulingPolicy>fair</schedulingPolicy>\n    </queue>\n    <!--  任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功  -->\n    <queuePlacementPolicy>\n        <!--  提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false 表示：如果指\n\t\t定队列不存在,不允许自动创建-->\n        <rule name=\"specified\" create=\"false\"/>\n        <!--  提交到 root.group.username 队列,若 root.group 不存在,不允许自动创建；若\n        root.group.user 不存在,允许自动创建  -->\n        <rule name=\"nestedUserQueue\" create=\"true\">\n            <rule name=\"primaryGroup\" create=\"false\"/>\n        </rule>\n        <!--  最后一个规则必须为 reject 或者 default。Reject 表示拒绝创建提交失败，\n        default 表示把任务提交到 default 队列  -->\n        <rule name=\"reject\"/>\n    </queuePlacementPolicy>\n</allocations>\n```\n\n\n（3）分发配置并重启 Yarn\n\n```bash\n[wxler@wxler1 hadoop]$ xsync yarn-site.xml \n[wxler@wxler1 hadoop]$ xsync fair-scheduler.xml \n[wxler@wxler2 hadoop-3.1.3]$ sbin/stop-yarn.sh\n[wxler@wxler2 hadoop-3.1.3]$ sbin/start-yarn.sh\n```\n\n### 3.3 测试提交到任务\n（1）提交任务时指定队列，按照配置规则，任务会到指定的 root.test 队列\n\n```bash\nhadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -Dmapreduce.job.queuename=root.test 1 1\n```\n（2）提交任务时不指定队列，按照配置规则，任务会到 root.wxler.wxler 队列\n\n```bash\nhadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 1 1\n```\n\n## 4. Yarn 的 Tool 接口案例\n回顾\n\n执行下面代码，可以正常执行\n```bash\nhadoop jar wc.jar com.layne.mapreduce.wordcount2.WordCountDriver /input /output1\n```\n期望可以动态传参，结果报错，误认为是第一个输入参数。\n\n```bash\nhadoop jar wc.jar com.layne.mapreduce.wordcount2.WordCountDriver -Dmapreduce.job.queuename=root.test /input /output1\n```\n\n\n（1）需求：自己写的程序也可以动态修改参数。编写 Yarn 的 Tool 接口。\n\n（2）添加依赖\n\n```xml\n<dependencies>\n    <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-client</artifactId>\n        <version>3.1.3</version>\n    </dependency>\n</dependencies>\n```\n\n（3）新建 com.layne.yarn 包名\n（4）创建类 WordCount 并实现 Tool 接口\n\n```java\npackage com.layne.yarn;\n\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\n\nimport java.io.IOException;\n\npublic class WordCount implements Tool {\n\n    private Configuration conf;\n\n    // 核心驱动（conf 需要传入）\n    @Override\n    public int run(String[] args) throws Exception {\n\n        Job job = Job.getInstance(conf); //不要new\n\n        job.setJarByClass(WordCountDriver.class);\n\n        job.setMapperClass(WordCountMapper.class);\n        job.setReducerClass(WordCountReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        return job.waitForCompletion(true) ? 0 : 1;\n    }\n\n    @Override\n    public void setConf(Configuration conf) {\n        this.conf = conf;\n    }\n\n    @Override\n    public Configuration getConf() {\n\n        return conf;\n    }\n\n    // mapper\n    public static class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n\n        private Text outK = new Text();\n        private IntWritable outV = new IntWritable(1);\n\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            // ss  cls\n            // 1 获取一行\n            String line = value.toString();\n\n            // 2 切割\n            String[] words = line.split(\" \");\n\n            // 3 循环遍历写出\n            for (String word : words) {\n                outK.set(word);\n\n                context.write(outK, outV);\n            }\n\n        }\n    }\n\n    // reducer\n    public static class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n\n        private IntWritable outV = new IntWritable();\n\n        @Override\n        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n\n            int sum = 0;\n            for (IntWritable value : values) {\n                sum += value.get();\n            }\n\n            outV.set(sum);\n\n            context.write(key, outV);\n        }\n    }\n}\n\n```\n\n（5）新建 WordCountDriver\n\n```java\npackage com.layne.yarn;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.util.Arrays;\n\npublic class WordCountDriver {\n\n    private static Tool tool;\n\n    public static void main(String[] args) throws Exception {\n\n        // 创建配置\n        Configuration conf = new Configuration();\n\n        switch (args[0]){\n            case \"wordcount\":\n                tool = new WordCount();\n                break;\n            default:\n                throw new RuntimeException(\"no such tool \"+ args[0]);\n        }\n\n        // 执行程序,Arrays.copyOfRange(args, 1, args.length)只剩下输入和输出路径\n        int run = ToolRunner.run(conf, tool, Arrays.copyOfRange(args, 1, args.length));\n\n        System.exit(run);\n    }\n}\n\n```\n\n（6）打包jar传到集群中\n\n（7）测试1\n执行下面代码没有报错\n\n```bash\nyarn jar YarnDemo.jar com.layne.yarn.WordCountDriver wordcount /input /output333\n```\n（8）测试2\n执行下面代码也OK，并且在test队列上运行\n\n```bash\nyarn jar YarnDemo.jar com.layne.yarn.WordCountDriver wordcount -Dmapreduce.job.queuename=root.test /input /output444\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210410213401964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n\n\n\n\n注：**以上操作全部做完过后，快照回去或者手动将配置文件修改成之前的状态，因为本身资源就不够，分成了这么多，不方便以后测试**。","tags":["Hadoop","Yarn"],"categories":["Hadoop"]},{"title":"九、Yarn资源调度器","url":"/2021/04/01/214801/","content":"\n\n\nYarn资源调度器（重点）\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n思考：\n（1）如何管理集群资源？\n（2）如何给任务合理分配资源？\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407080959727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n\nYarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序。\n\n\n## 1. Yarn 基础架构\nYARN 主要由 ResourceManager、NodeManager、ApplicationMaster 和 Container 等组件构成。\n\nResourceManager（RM）主要作用如下\n- 处理客户端请求\n- 监控NodeManager运行情况\n- 启动或监控ApplicationMaster\n- 整个集群资源的分配与调度\n\nNodeManager（NM）主要作用如下\n- 管理单个节点上的资源\n- 处理来自ResourceManager的命令\n- 处理来自ApplicationMaster的命令\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407082113650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\nApplicationMaster（AM）作用如下\n- 为应用程序申请资源并分配给内部的任务\n- 任务的监控与容错\n\n\nContainer的作用\n- Container 是YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。\n\n\n## 2. Yarn工作机制\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407083524618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（1）MR 程序提交到客户端所在的节点。\n（2）YarnRunner 向 ResourceManager 申请一个 Application。\n（3）RM 将该应用程序的资源路径返回给 YarnRunner。\n（4）该程序将运行所需资源提交到 HDFS 上。\n（5）程序资源提交完毕后，申请运行 mrAppMaster。\n（6）RM 将用户的请求初始化成一个 Task。\n（7）其中一个 NodeManager 领取到 Task 任务。\n（8）该 NodeManager 创建容器 Container，并产生 MRAppmaster。\n（9）Container 从 HDFS 上拷贝资源到本地。\n（10）MRAppmaster 向 RM  申请运行 MapTask 资源。\n（11）RM 将运行 MapTask 任务分配给另外两个 NodeManager， 另两个 NodeManager 分别领取任务并创建容器。\n（12）MR 向两个接收到任务的 NodeManager 发送程序启动脚本， 这两个 NodeManager分别启动 MapTask，MapTask 对数据分区排序。\n（13）MrAppMaster 等待所有 MapTask 运行完毕后，向 RM 申请容器， 运行 ReduceTask。\n（14）ReduceTask 向 MapTask 获取相应分区的数据。\n（15）程序运行完毕后，MR 会向 RM 申请注销自己。\n\n## 3. 作业提交全过程\n（一）HDFS、YARN、MapReduce三者关系\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407084242294.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（二）作业提交过程之YARN\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407084559468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（三）作业提交过程之HDFS & MapReduce\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407084629308.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n作业提交全过程详解\n（1）作业提交\n第 1 步：Client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业。\n第 2 步：Client 向 RM 申请一个作业 id。\n第 3 步：RM 给 Client 返回该 job 资源的提交路径和作业 id。\n第 4 步：Client 提交 jar 包、切片信息和配置文件到指定的资源提交路径。\n第 5 步：Client 提交完资源后，向 RM 申请运行 MrAppMaster。\n（2）作业初始化\n第 6 步：当 RM 收到 Client 的请求后，将该 job 添加到容量调度器中。\n第 7 步：某一个空闲的 NM 领取到该 Job。\n第 8 步：该 NM 创建 Container，并产生 MRAppmaster。\n第 9 步：下载 Client 提交的资源到本地。\n（3）任务分配\n第 10 步：MrAppMaster 向 RM 申请运行多个 MapTask 任务资源。\n第 11 步： RM 将运行 MapTask 任务分配给另外两个 NodeManager， 另两个 NodeManager\n分别领取任务并创建容器。\n（4）任务运行\n第 12 步： MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个NodeManager 分别启动 MapTask，MapTask 对数据分区排序。\n第 13 步： MrAppMaster 等待所有 MapTask 运行完毕后，向 RM 申请容器， 运行 ReduceTask。\n第 14 步：ReduceTask 向 MapTask 获取相应分区的数据。\n第 15 步：程序运行完毕后，MR 会向 RM 申请注销自己。\n（5）进度和状态更新\nYARN 中的任务将其进度和状态(包括 counter)返回给应用管理器,  客户端每秒(通过`mapreduce.client.progressmonitor.pollinterval `设置)向应用管理器请求进度更新,  展示给用户。\n（6）作业完成\n除了向应用管理器请求作业进度外,  客户端每 5 秒都会通过调用 waitForCompletion()来检查作业是否完成。 时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。 作业完成之后,  应用管理器和 Container 会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。\n\n\n\n\n\n## 4. Yarn 调度器和调度算法\n\n\n目前，Hadoop 作业调度器主要有三种：FIFO、容量（Capacity Scheduler）和公平（Fair Scheduler）。Apache Hadoop3.1.3 默认的资源调度器是 Capacity Scheduler。\n\nCDH 框架默认调度器是 Fair Scheduler。\n\n具体设置详见：yarn-default.xml 文件\n```xml\n  <property>\n    <description>The class to use as the resource scheduler.</description>\n    <name>yarn.resourcemanager.scheduler.class</name>\n    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>\n  </property>\n```\n\n### 4.1 先进先出调度器（FIFO）\nFIFO 调度器（First In First Out）：单队列，根据提交作业的先后顺序，先来先服务。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407093105352.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n优点：简单易懂；\n缺点：不支持多队列，生产环境很少使用；\n\n\n### 4.2 容量调度器（Capacity Scheduler）\nCapacity Scheduler 是 Yahoo 开发的多用户调度器。\n\n（一）容量调度器特点\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407093834192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n1、多队列：每个队列可配置一定的资源量，每个队列采用FIFO调度策略。\n2、容量保证：管理员可为每个队列设置资源最低保证和资源使用上限\n3、灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。\n4、多租户：\n支持多用户共享集群和多应用程序同时运行。为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。\n\n（二）容量调度器资源分配算法\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021040709434636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n### 4.3 公平调度器（Fair Scheduler）\nFair Schedulere 是 Facebook 开发的多用户调度器。\n\n（一）公平调度器特点\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407100316875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（1）与容量调度器相同点\n① 多队列：支持多队列多作业\n② 容量保证：管理员可为每个队列设置资源最低保证和资源使用上线（雨露均沾，至少能有让每个队列运行起来的资源）\n③ 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。\n④ 多租户：支持多用户共享集群和多应用程序同时运行；为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。\n\n（2）与容量调度器不同点\n① 核心调度策略不同\n- 容量调度器：优先选择**资源利用率低**的队列\n- 公平调度器：优先选择对资源的**缺额**比例大的\n\n② 每个队列可以单独设置资源分配方式\n- 容量调度器：FIFO、 DRF\n- 公平调度器：FIFO、FAIR、DRF\n\n\n（二）公平调度器——缺额\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407100913599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n- 公平调度器设计目标是：在时间尺度上，所有作业获得公平的资源。某一时刻一个作业应获资源和实际获取资源的差距叫“缺额”\n- 调度器会优先为缺额大的作业分配资源\n\n（三）公平调度器队列资源分配方式\n（1）FIFO策略\n公平调度器每个队列资源分配策略如果选择FIFO的话，此时公平调度器相当于上面讲过的容量调度器。\n（2）Fair策略\nFair 策略（默认）是一种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。\n\n具体资源分配流程和容量调度器一致：\n- 选择队列\n- 选择作业\n- 选择容器\n\n以上三步，每一步都是按照公平策略分配资源\n\n四个指标：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407102653991.png)\n优先执行的顺序：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407102829733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n（3）DRF策略\nDRF（Dominant Resource Fairness），我们之前说的资源，都是单一标准，例如只考虑内存（也是Yarn默认的情况）。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。\n\n那么在YARN中，我们用DRF来决定如何调度：\n假设集群一共有100 CPU和10T 内存，而应用A需要（2 CPU, 300GB），应用B需要（6 CPU，100GB）。则两个应用分别需要A（2%CPU, 3%内存）和B（6%CPU, 1%内存）的资源，这就意味着A是内存主导的, B是CPU主导的，针对这种情况，我们可以选择DRF策略对不同应用进行不同资源（CPU和内存）的一个不同比例的限制。\n\n\n（四）公平调度器资源分配案例\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407103032825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（1）队列资源分配\n\n需求：集群总资源100，有三个队列，对资源的需求分别是：\n\n```tex\nqueueA -> 20， queueB ->50， queueC -> 30\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407103126564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n（2）作业资源分配\n\n① 不加权（关注点是Job的个数）：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407103237962.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n\n② 加权（关注点是Job的权重）：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021040710331762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n## 5. Yarn 常用命令\n\nYarn 状态的查询，除了可以在`http://wxler2:8088/ ` 页面查看外，还可以通过命令操作。常见的命令操作如下所示：\n\n需求：执行 WordCount 案例，并用 Yarn 命令查看任务运行情况。\n\n```bash\n### 在这之前先启动Hadoop集群\n[wxler@wxler1 hadoop-3.1.3]$ hadoop  jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar  wordcount /input /output\n```\n\n### 5.1 yarn application 查看任务\n（1）列出所有 Application：\n\n```bash\n[wxler@wxler1 ~]$ yarn application -list\n2021-04-07 10:44:01,948 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8032\nTotal number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1\n                Application-Id\t    Application-Name\t    Application-Type         User\t     Queue\t             State\t       Final-State            Progress\t                       Tracking-URL\napplication_1617763295598_0001\t          word count\t           MAPREDUCE        wxler\t   default\t          ACCEPTED\t         UNDEFINED                  0%\t                                N/A\n```\n\n\n（2）根据 Application 状态过滤： `yarn application  -list  -appStates`  （所有状态： ALL、 NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）\n\n```bash\n[wxler@wxler1 ~]$ yarn  application  -list  -appStates FINISHED\n2021-04-07 10:50:14,960 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8032\nTotal number of applications (application-types: [], states: [FINISHED] and tags: []):1\n                Application-Id\t    Application-Name\t    Application-Type\t      Use     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\napplication_1617763295598_0001\t          word count\t           MAPREDUCE\t     wxle   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://wxler1:19888/jobhistory/job/job_1617763295598_0001\n```\n（3）Kill 掉 Application：\n\n```bash\n[wxler@wxler1 ~]$ yarn  application  -kill application_1617763295598_0001\n2021-04-07 10:53:42,842 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8032\nApplication application_1617763295598_0001 has already finished \n```\n### 5.2 yarn logs查看日志\n（1）查询 Application 日志：`yarn logs -applicationId <ApplicationId>`\n\n```bash\n[wxler@wxler1 ~]$ yarn logs -applicationId application_1617763295598_0001\n```\n\n（2）查询 Container 日志：`yarn logs -applicationId <ApplicationId> -containerId <ContainerId> `\n\n```bash\n[wxler@wxler1 ~]$ yarn  logs  -applicationId application_1617763295598_0001  -containerId container_1617763295598_0001_01_000001\n\n```\n\n### 5.3 yarn applicationattempt 查看尝试运行的任务\n\n（1）列出所有 Application 尝试的列表：`yarn applicationattempt -list <ApplicationId>`\n\n```bash\n[wxler@wxler1 ~]$ yarn  applicationattempt  -list application_1617763295598_0001\n2021-04-07 11:09:58,086 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8032\nTotal number of application attempts :1\n         ApplicationAttempt-Id\t               State\t                    AM-Container-Id\t                       Tracking-URL\nappattempt_1617763295598_0001_000001\t            FINISHED\tcontainer_1617763295598_0001_01_000001\thttp://wxler2:8088/proxy/application_1617763295598_0001/\n```\n\n（2）打印 ApplicationAttemp 状态： `yarn  applicationattempt  -status  <ApplicationAttemptId>`\n\n```bash\n[wxler@wxler1 ~]$ yarn  applicationattempt  -status appattempt_1617763295598_0001_000001\n2021-04-07 11:14:05,268 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8032\nApplication Attempt Report : \n\tApplicationAttempt-Id : appattempt_1617763295598_0001_000001\n\tState : FINISHED\n\tAMContainer : container_1617763295598_0001_01_000001\n\tTracking-URL : http://wxler2:8088/proxy/application_1617763295598_0001/\n\tRPC Port : 34808\n\tAM Host : wxler2\n\tDiagnostics : \n[wxler@wxler1 ~]$ \n\n```\n\n\n### 5.4 yarn container 查看容器\n**必须在任务运行的时候查看**\n\n（1）列出所有 Container：`yarn container -list <ApplicationAttemptId>`\n\n```bash\n[wxler@wxler1 ~]$ yarn  container  -list appattempt_1617763295598_0001_000001\n2021-04-07 11:18:10,670 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8032\nTotal number of containers :0\n                  Container-Id\t          Start Time\t         Finish Time\t               State\t                Host\t   Node Http Address\t                            LOG-URL\n\n```\n\n（2）打印 Container 状态：  `yarn container -status <ContainerId>`\n\n```bash\n[wxler@wxler1 ~]$ yarn  container  -status container_1617763295598_0001_01_000001\n2021-04-07 11:19:26,066 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8032\nContainer with id 'container_1617763295598_0001_01_000001' doesn't exist in RM or Timeline Server.\n\n```\n注：只有在任务跑的途中才能看到 container 的状态\n\n### 5.5 yarn node 查看节点状态\n列出所有节点：`yarn node -list -all`\n\n```bash\n[wxler@wxler1 ~]$ yarn node -list -all\n2021-04-07 11:20:16,066 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8032\nTotal Nodes:3\n         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n    wxler1:39487\t        RUNNING\t      wxler1:8042\t                           0\n    wxler2:34599\t        RUNNING\t      wxler2:8042\t                           0\n    wxler3:45588\t        RUNNING\t      wxler3:8042\t                           0\n\n```\n\n### 5.6 yarn rmadmin 更新配置\n刷新加载队列配置：`yarn rmadmin -refreshQueues`\n\n```bash\n[wxler@wxler1 ~]$ yarn rmadmin -refreshQueues\n2021-04-07 11:21:33,101 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8033\n```\n\n### 5.7 yarn queue 查看队列\n打印队列信息：`yarn queue -status <QueueName>`\n\n```bash\n[wxler@wxler1 ~]$ yarn queue -status default\n2021-04-07 11:22:33,835 INFO client.RMProxy: Connecting to ResourceManager at wxler2/192.168.218.72:8032\nQueue Information : \nQueue Name : default\n\tState : RUNNING\n\tCapacity : 100.0%\n\tCurrent Capacity : .0%\n\tMaximum Capacity : 100.0%\n\tDefault Node Label expression : <DEFAULT_PARTITION>\n\tAccessible Node Labels : *\n\tPreemption : disabled\n\tIntra-queue Preemption : disabled\n```\n当然，也可以在`http://wxler2:8088`里面查看\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021040711241772.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n## 6. Yarn 生产环境核心参数\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210407114359411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n\n相关核心参数如下\n（1）ResourceManager相关\n- `yarn.resourcemanager .scheduler .class` 配置调度器，默认容量\n- `yarn.resourcemanager .scheduler .client.thread-count` ResourceManager处理调度器请求的线程数量，默认50\n\n（2）NodeManager相关\n- `yarn.nodemanager.resource.detect-hardware-capabilities` 是否让yarn自己检测硬件进行配置，默认false\n- `yarn.nodemanager.resource.count-logical-processors-as-cores` 是否将虚拟核数当作CPU核数，默认false\n- `yarn.nodemanager.resource.pcores-vcores-multiplier` 虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2，默认1.0\n\nNodeManager内存参数\n- `yarn.nodemanager.resource.memory-mb` NodeManager使用内存，默认8G\n- `yarn.nodemanager.resource.system-reserved-memory-mb` NodeManager为系统保留多少内存\n以上二个参数配置一个即可\n\n\n- `yarn.nodemanager.resource.cpu-vcores` NodeManager使用CPU核数，默认8个\n- `yarn.nodemanager.pmem-check-enabled` 是否开启物理内存检查限制container，默认打开\n- `yarn.nodemanager.vmem-check-enabled` 是否开启虚拟内存检查限制container，默认打开\n- `yarn.nodemanager.vmem-pmem-ratio` 虚拟内存物理内存比例，默认2.1，即虚拟内存是物理内存的2.1倍\n\n（3）Container相关\n- `yarn.scheduler .minimum-allocation-mb` 容器最最小内存，默认1G\n- `yarn.scheduler .maximum-allocation-mb` 容器最最大内存，默认8G\n- `yarn.scheduler .minimum-allocation-vcores` 容器最小CPU核数，默认1个\n- `yarn.scheduler .maximum-allocation-vcores` 容器最大CPU核数，默认4个\n\n\n","tags":["Hadoop","Yarn"],"categories":["Hadoop"]},{"title":"八、Hadoop数据压缩","url":"/2021/03/31/224801/","content":"\n\n\nHadoop数据压缩\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n## 1. 压缩概述\n\n（1）压缩的好处和坏处\n压缩的优点：以减少磁盘 IO、减少磁盘存储空间。\n压缩的缺点：增加 CPU 开销。\n\n（2）压缩原则\n\n- 运算密集型的 Job，少用压缩\n- IO 密集型的 Job，多用压缩\n\n\n\n## 2. MR支持的压缩编码\n\n（1）压缩算法对比介绍\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406223805.png)\n\n\n\n（2）压缩性能的比较\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406223944.png)\n\nhttp://google.github.io/snappy/\nSnappy  is  a  compression/decompression  library.  It  does  not  aim  for  maximum  compression,  or compatibility  with  any  other  compression  library;  instead,  it  aims  for  very  high  speeds  and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger.On a single core of a Core i7 processor in 64-bit mode, **Snappy  compresses  at about  250 MB/sec or more and decompresses at about 500 MB/sec or more**.\n\n\n\n## 3. 压缩方式选择\n\n压缩方式选择时重点考虑： 压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片。\n\n\n\n### 3.1 Gzip压缩\n\n优点：压缩率比较高； \n缺点：不支持 Split；压缩/解压速度一般；\n\n\n\n### 3.2 Bzip2 压缩\n\n优点：压缩率高；支持 Split； \n缺点：压缩/解压速度慢。\n\n\n\n### 3.3 Lzo 压缩\n\n优点：压缩/解压速度比较快；支持 Split；\n缺点：压缩率一般；想支持切片需要额外创建索引。\n\n\n\n### 3.4 Snappy 压缩\n\n优点：压缩和解压缩速度快； \n缺点：不支持 Split；压缩率一般；\n\n\n\n### 3.5 压缩位置选择\n\n压缩可以在 MapReduce 作用的任意阶段启用。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406224752.png)\n\n\n\n## 4. 压缩参数配置\n\n（1）为了支持多种压缩/解压缩算法，Hadoop 引入了编码/解码器\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406224913.png)\n\n（2）要在 Hadoop 中启用压缩，可以配置如下参数\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406225112.png)\n\n\n\n## 5. 压缩实操案例\n\n\n\n**Map输出端和Reduce输出端采用压缩**\n\n即使你的 MapReduce 的输入输出文件都是未压缩的文件，你仍然可以对 Map 任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到 Reduce 节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。\n\n该案例在WordCount的基础上实现\n\n（1）给大家提供的 Hadoop 源码支持的压缩格式有：BZip2Codec、DefaultCodec\n\n```java\npackage com.layne.mapreduce.yasuo;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.*;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class WordCountDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        // 1 获取job\n        Configuration conf = new Configuration();\n\n        // 开启map端输出压缩\n        conf.setBoolean(\"mapreduce.map.output.compress\", true);\n\n        // 设置map端输出压缩方式，传输至Reduce段后自动解压\n        conf.setClass(\"mapreduce.map.output.compress.codec\", BZip2Codec.class, CompressionCodec.class);\n        //conf.setClass(\"mapreduce.map.output.compress.codec\", SnappyCodec.class, CompressionCodec.class); //报错，提示不可用\n\n        Job job = Job.getInstance(conf);\n\n        // 2 设置jar包路径\n        job.setJarByClass(WordCountDriver.class);\n\n        // 3 关联mapper和reducer\n        job.setMapperClass(WordCountMapper.class);\n        job.setReducerClass(WordCountReducer.class);\n\n        // 4 设置map输出的kv类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        // 5 设置最终输出的kV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 6 设置输入路径和输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\test\\\\inputword\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\test\\\\output992\"));\n\n\n        // 设置reduce端输出压缩开启，输出后为压缩包格式\n        FileOutputFormat.setCompressOutput(job, true);\n\n        // 设置压缩的方式\n//      FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);\n\t    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n        //   FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);\n\n\n        // 7 提交job\n        boolean result = job.waitForCompletion(true);\n\n        System.exit(result ? 0 : 1);\n    }\n}\n\n```\n\n（2）Mapper和Reduce 保持不变\n\n","tags":["Hadoop"],"categories":["Hadoop"]},{"title":"七、MapReduce框架核心原理","url":"/2021/03/31/214801/","content":"\n\n\nMapReduce框架核心原理\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n**MapReduce 框架原理**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402105741767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n\n## 1. InputFormat 数据输入\n### 1.1 切片与 MapTask 并行度决定机制\n（1）问题引入\nMapT ask 的并行度决定 Map 阶段的任务处理并发度，进而影响到整个 Job 的处理速度。\n\n思考：1G 的数据，启动 8 个 MapTask，可以提高集群的并发处理能力。那么 1K 的数据，也启动 8 个 MapTask，会提高集群性能吗？MapTask 并行任务是否越多越好呢？哪些因素影响了 MapTask 并行度？\n\nMapTask并不是也多越好，如果1K的数据也分为8个MapTask，则MapTask启动的时间就比任务运行的时间长，这就得不偿失了。\n\n\n（2）MapTask 并行度决定机制\n\n- 数据块：Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。\n- 数据切片： 数据切片只是在逻辑上对输入进行分片， 并不会在磁盘上将其切分成片进行存储。 数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。  \n\n**数据切片与MapTask并行度决定机制**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402110146142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n### 1.2 Job 提交流程源码和切片源码详解\n\n一、**Job提交流程源码详解**\n\n```java\nwaitForCompletion();\n\nsubmit();\n\n// 一、建立连接\n\tconnect();\n\t\t// 创建提交 Job 的代理\n\t\tnew Cluster(getConfiguration());\n\t\t\t// 判断是本地运行环境还是 yarn 集群运行环境\n\t\t\tinitialize(jobTrackAddr, conf);\n\n// 二、提交 job\nsubmitter.submitJobInternal(Job.this, cluster)\n\t// （1）创建给集群提交数据的 Stag 路径\n\tPath jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);\n\t// （2）获取 jobid  ，并创建 Job 路径\n\tJobID jobId = submitClient.getNewJobID();\n\t// （3）拷贝 jar 包到集群\n\tcopyAndConfigureFiles(job, submitJobDir);\n\trUploader.uploadFiles(job, jobSubmitDir);\n\t// （4）计算切片，生成切片规划文件\n\twriteSplits(job, submitJobDir);\n\tmaps = writeNewSplits(job, jobSubmitDir);\n\tinput.getSplits(job);\n\t// （5）向 Stag 路径写 XML 配置文件\n\twriteConf(conf, submitJobFile);\n\tconf.writeXml(out);\n\t// （6）提交 Job,返回提交状态\n\tstatus  =  submitClient.submitJob(jobId,  submitJobDir.toString(),job.getCredentials());\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402113858694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n二、**FileInputFormat切片源码解析**（input.getSplits(job)）\n\n（1）程序先找到你数据存储的目录。\n（2）开始遍历处理（规划切片）目录下的每一个文件\n（3）遍历第一个文件ss.txt\n\n```tex\n\t（a）获取文件大小fs.sizeOf(ss.txt)\n\t（b）计算切片大小`computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M`\n\t（c）默认情况下，切片大小=blocksize，如果增大切片大小，则将minSize设置大于128M，如果要减小切片大小，则将maxSize设置小于128M\n    （d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，大于1.1倍就划分一块切片，小于1.1倍就不再切了）\n\t（e）将切片信息写到一个切片规划文件中\n    （f）整个切片的核心过程在getSplit()方法中完成\n    （g）InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等。\n```\n\n（4）提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数。\n\n### 1.3 FileInputFormat 切片机制\n\n一、**FileInputFormat切片机制**\n\n（1）切片机制\n1. 简单地按照文件的内容长度进行切片\n2. 切片大小，默认等于Block大小\n3. 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片\n\n（2）案例分析\n① 输入数据有两个文件：\n\n```tex\nfile1.txt 320M\nfile2.txt 10M\n```\n② 经过FileInputFormat的切片机制运算后，形成的切片信息如下：\n\n```tex\nfile1.txt.split1-- 0~128\nfile1.txt.split2-- 128~256\nfile1.txt.split3-- 256~320\nfile2.txt.split1-- 0~10M\n```\n\n二、**FileInputFormat切片大小的参数配置**\n\n（1）源码中计算切片大小的公式\n\n```java\nMath.max(minSize, Math.min(maxSize, blockSize));\n```\n`mapreduce.input.fileinputformat.split.minsize=1` 默认值为1\n`mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue` 默认值Long.MAXValue\n因此，默认情况下，切片大小=blocksize。\n\n（2）切片大小设置\n- maxsize（切片最大值）：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。\n- minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blockSize还大。\n\n（3）获取切片信息API\n\n```java\n// 获取切片的文件名称\nString name = inputSplit.getPath().getName();\n// 根据文件类型获取切片信息\nFileSplit inputSplit = (FileSplit) context.getInputSplit();\n```\n\n### 1.4 FileInputFormat实现类\n（1）FileInputFormat 实现类\n思考：在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。 那么，针对不同的数据类型， MapReduce 是如何读取这些数据的呢？\n\nFileInputFormat 常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat 和自定义 InputFormat 等。\n\n（2）TextInputFormat\nTextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。 键是存储该行在整个文件中的起始字节偏移量，  LongWritable 类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text 类型。\n\n以下是一个示例，比如，一个分片包含了如下 4 条文本记录。\n\n```tex\nRich learning form\nIntelligent learning engine\nLearning more convenient\nFrom the real demand for more close to the enterprise\n```\n每条记录表示为以下键/值对：\n\n```tex\n(0,Rich learning form)\n(20,Intelligent learning engine)\n(49,Learning more convenient)\n(74,From the real demand for more close to the enterprise)\n```\n（3）KeyValueTextInputFormat\n每行均为一条记录，被分隔符分割为key，value。可以通过在驱动类中设置`conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \"\\t\");`来设定分隔符。默认分隔符是tab（`\\t`）。\n以下是一个示例，输入时一个包含4条记录的分片，其中`——>`表示一个（水平方向的）制表符\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021040214484549.png)\n\n每条记录表示以下键值对：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402144927250.png)\n此时的健是每行排在制表符之前的Text序列。\n\n（4）NLineInputFormat\n如果使用NLineInputFormat指定的行数N来划分。即输入文件的总行数 除以 N =切片数，如果不整除，切片数=商+1。\n以下是一个示例，仍然以上面的4行输入为例。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402145436545.png)\n\n例如，如果N是2，则每个输入分片包含两行，开启2个MapTask。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021040214553316.png)\n另一个mapper则收到后两行。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402145602114.png)\n\n\n\n### 1.5 CombineTextInputFormat 切片机制\n框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，**不管文件多小，都会是一个单独的切片，都会交给一个 MapTask**，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。\n\n（1）应用场景：\nCombineTextInputFormat 用于小文件过多的场景， 它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。\n\n（2）虚拟存储切片最大值设置\n\n```java\nCombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m\n```\n注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。\n\n（3）切片机制\n生成切片过程包括：虚拟存储过程和切片过程二部分。 举个例子来说明：\n假如有四个文件，每个文件大小如下：\n```tex\na.txt 1.7M\nb.txt 5.1M\nc.txt 3.4M\nd.txt 6.8M\n```\n处理过程如下图所示\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402141939635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n\n\n**对于虚拟存储过程**\n\n将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块； 当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片）。\n\n例如 setMaxInputSplitSize 值为 4M，输入文件大小为 8.02M，则先逻辑上分成一个4M。剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储文件，所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件。\n\n**对于切片过程**\n\n（a）判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独形成一个切片。\n（b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片，依次类推，直到产生大于4M的切片为止。\n（c） 测试举例：有 4 个小文件大小分别为 1.7M、 5.1M、 3.4M 以及 6.8M 这四个小文件，则虚拟存储之后形成 6 个文件块，大小分别为：\n\n```tex\n1.7M，（2.55M、2.55M），3.4M 以及（3.4M、3.4M）\n```\n最终会形成 3 个切片，大小分别为：\n\n```tex\n（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M\n```\n\n### 1.6 CombineTextInputFormat 案例实操\n（1）需求\n将输入的大量小文件合并成一个切片统一处理。\n① 输入数据，准备 4 个小文件\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402142824388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n\n② 期望一个切片处理 4 个文件\n\n（2）实现过程\n① 不做任何处理，运行之前的 WordCount 案例程序，即采用默认的TextInputFormat处理，观察切片个数为 4。\n\n```tex\nnumber of splits:4\n```\n\n② 在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 3。\n\n- 驱动类中添加代码如下：\n\n```java\n//  如果不设置 InputFormat，它默认用的是 TextInputFormat.class\njob.setInputFormatClass(CombineTextInputFormat.class);\n//虚拟存储切片最大值设置 4m\nCombineTextInputFormat.setMaxInputSplitSize(job, 4194304);\n```\n\n- 运行如果为 3 个切片\n\n```tex\nnumber of splits:3\n```\n\n③ 在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 1。\n\n- 驱动中添加代码如下：\n\n```java\n//  如果不设置 InputFormat，它默认用的是 TextInputFormat.class\njob.setInputFormatClass(CombineTextInputFormat.class);\n//虚拟存储切片最大值设置 20m\nCombineTextInputFormat.setMaxInputSplitSize(job, 20971520);\n```\n- 运行如果为 1 个切片\n\n```tex\nnumber of splits:1 \n```\n\n### 1.7 KeyValueTextInputFormat使用案例\n（1）需求\n统计输入文件中每一行的第一个单词相同的行数。\n\n① 输入数据\n\n```tex\nbanzhang ni hao\nxihuan hadoop banzhang\nbanzhang ni hao\nxihuan hadoop banzhang\n```\n② 期望结果数据\n\n```tex\nbanzhang\t2\nxihuan\t2\n```\n\n（2）需求分析\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402150723649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（3）代码实现\n① 编写Mapper类\n\n```java\npackage com.layne.KeyValueTextInputFormat;\n\nimport java.io.IOException;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\npublic class KVTextMapper extends Mapper<Text, Text, Text, LongWritable>{\n\n    // 1 设置value\n    LongWritable v = new LongWritable(1);\n\n    @Override\n    protected void map(Text key, Text value, Context context)\n            throws IOException, InterruptedException {\n\n        // banzhang ni hao\n\n        // 2 写出\n        context.write(key, v);\n    }\n}\n\n```\n\n② 编写reduce类\n\n```java\npackage com.layne.KeyValueTextInputFormat;\n\nimport java.io.IOException;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\npublic class KVTextReducer extends Reducer<Text, LongWritable, Text, LongWritable>{\n\n    LongWritable v = new LongWritable();\n\n    @Override\n    protected void reduce(Text key, Iterable<LongWritable> values,\tContext context) throws IOException, InterruptedException {\n\n        long sum = 0L;\n\n        // 1 汇总统计\n        for (LongWritable value : values) {\n            sum += value.get();\n        }\n\n        v.set(sum);\n\n        // 2 输出\n        context.write(key, v);\n    }\n}\n\n```\n\n③ 编写driver类\n\n```java\npackage com.layne.KeyValueTextInputFormat;\n\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;\nimport org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class KVTextDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        Configuration conf = new Configuration();\n        // 设置切割符\n        conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \" \");\n        // 1 获取job对象\n        Job job = Job.getInstance(conf);\n\n        // 2 设置jar包位置，关联mapper和reducer\n        job.setJarByClass(KVTextDriver.class);\n        job.setMapperClass(KVTextMapper.class);\n        job.setReducerClass(KVTextReducer.class);\n\n        // 3 设置map输出kv类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(LongWritable.class);\n\n        // 4 设置最终输出kv类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(LongWritable.class);\n\n        // 5 设置输入输出数据路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\test\\\\kvinputformat\"));\n\n        // 设置输入格式\n        job.setInputFormatClass(KeyValueTextInputFormat.class);\n\n        // 6 设置输出数据路径\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\test\\\\kvoutputformat\"));\n\n        // 7 提交job\n        job.waitForCompletion(true);\n    }\n}\n\n```\n\n\n### 1.8 NLineInputFormat使用案例\n（1）需求\n对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中。\n\n① 输入数据\n\n```tex\nbanzhang ni hao\nxihuan hadoop banzhang\nbanzhang ni hao\nxihuan hadoop banzhang\nbanzhang ni hao\nxihuan hadoop banzhang\nbanzhang ni hao\nxihuan hadoop banzhang\nbanzhang ni hao\nxihuan hadoop banzhang banzhang ni hao\nxihuan hadoop banzhang\n```\n② 期望输出数据\n\n```tex\nNumber of splits:4\n```\n\n（2）需求分析\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402152431322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n（3）代码实现\n\n① Mapper类\n\n```java\npackage com.layne.nline;\n\nimport java.io.IOException;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\npublic class NLineMapper extends Mapper<LongWritable, Text, Text, LongWritable>{\n\n    private Text k = new Text();\n    private LongWritable v = new LongWritable(1);\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context)\tthrows IOException, InterruptedException {\n\n        // 1 获取一行\n        String line = value.toString();\n\n        // 2 切割\n        String[] splited = line.split(\" \");\n\n        // 3 循环写出\n        for (int i = 0; i < splited.length; i++) {\n\n            k.set(splited[i]);\n\n            context.write(k, v);\n        }\n    }\n}\n\n```\n\n②Reduce类\n\n```java\npackage com.layne.nline;\n\nimport java.io.IOException;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\npublic class NLineReducer extends Reducer<Text, LongWritable, Text, LongWritable>{\n\n    LongWritable v = new LongWritable();\n\n    @Override\n    protected void reduce(Text key, Iterable<LongWritable> values,\tContext context) throws IOException, InterruptedException {\n\n        long sum = 0l;\n\n        // 1 汇总\n        for (LongWritable value : values) {\n            sum += value.get();\n        }\n\n        v.set(sum);\n\n        // 2 输出\n        context.write(key, v);\n    }\n}\n\n```\n③ Driver类\n\n```java\npackage com.layne.nline;\n\nimport java.io.IOException;\nimport java.net.URISyntaxException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class NLineDriver {\n\n    public static void main(String[] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException {\n\n        // 输入输出路径需要根据自己电脑上实际的输入输出路径设置\n        args = new String[] { \"D:\\\\test\\\\nlineinput\", \"D:\\\\test\\\\nlineoutput\" };\n\n        // 1 获取job对象\n        Configuration configuration = new Configuration();\n        Job job = Job.getInstance(configuration);\n\n        // 7设置每个切片InputSplit中划分三条记录\n        NLineInputFormat.setNumLinesPerSplit(job, 3);\n\n        // 8使用NLineInputFormat处理记录数\n        job.setInputFormatClass(NLineInputFormat.class);\n\n        // 2设置jar包位置，关联mapper和reducer\n        job.setJarByClass(NLineDriver.class);\n        job.setMapperClass(NLineMapper.class);\n        job.setReducerClass(NLineReducer.class);\n\n        // 3设置map输出kv类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(LongWritable.class);\n\n        // 4设置最终输出kv类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(LongWritable.class);\n\n        // 5设置输入输出数据路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        // 6提交job\n        job.waitForCompletion(true);\n    }\n}\n\n```\n\n（4）测试\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402153324149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n## 2. MapReduce 工作流程\nMapReduce详细工作流程\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406111821682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406111901460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n上面的流程是整个 MapReduce 最全工作流程，但是 Shuffle 过程只是从第 7 步开始到第16 步结束，具体 Shuffle 过程详解，如下：\n（1）MapT ask 收集我们的 map()方法输出的 kv 对，放到内存缓冲区中\n（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件\n（3）多个溢出文件会被合并成大的溢出文件\n（4）在溢出过程及合并的过程中，都要调用 Partitioner 进行分区和针对 key 进行排序\n（5）ReduceTask 根据自己的分区号，去各个 MapTask 机器上取相应的结果分区数据\n（6）ReduceTask 会抓取到同一个分区的来自不同 MapTask 的结果文件， ReduceTask 会将这些文件再进行合并（归并排序）\n（7）合并成大文件后， Shuffle的过程也就结束了，后面进入ReduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 Group，调用用户自定义的 reduce()方法）\n注意：\n（1）Shuffle 中的缓冲区大小会影响到 MapReduce 程序的执行效率，原则上说，缓冲区越大，磁盘 io 的次数越少，执行速度就越快。\n（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认 100M。\n\n## 3. Shuffle 机制\n### 3.1 Shuffle 机制\nMap 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406115725933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n\n### 3.2 Partition 分区\n（1）问题引出\n\n要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）\n\n（2）默认Partitioner分区\n\n```java\npublic class HashPartitioner<K, V> extends Partitioner<K, V> {\n\tpublic int getPartition(K key, V value, int numReduceTasks) {\n\t\treturn (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n\t}\n}\n```\n\n默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。\n\n\n\n（3）自定义Partitioner步骤\n\n① 自定义类继承Partitioner，重写getPartition()方法\n\n```java\npublic class CustomPartitioner extends Partitioner<Text, FlowBean> {\n\t@Override\n\tpublic int getPartition(Text key, FlowBean value, int numPartitions) {\n\t\t// 控制分区代码逻辑\n\t\t… …\n\t\treturn partition;\n\t}\n}\n```\n\n② 在Job驱动中，设置自定义Partitioner\n\n```java\njob.setPartitionerClass(CustomPartitioner.class);\n```\n\n③ 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask\n\n```java\njob.setNumReduceTasks(5);\n```\n\n（4）分区总结\n- 如果ReduceTask的数量 `>` getPartition的结果数，则会多产生几个空的输出文件part-r-000xx\n- 如果1 `<` ReduceTask的数量 `<` getPartition的结果数，则有一部分分区数据无处安放，会抛出Exception\n- 如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000；\n- 分区号必须从零开始，逐一累加。\n\n\n\n（5）案例分析\n例如：假设自定义分区数为5，则\n\n```java\njob.setNumReduceTasks(1); //会正常运行，只不过会产生一个输出文件\njob.setNumReduceTasks(2); //会报错\njob.setNumReduceTasks(6); //大于5，程序会正常运行，会产生空文件\n// 不写job.setNumReduceTasks，默认同job.setNumReduceTasks(1)\njob.setNumReduceTasks(0); //相当于没有reduce阶段\n```\n\n\n\n### 3.3 Partition 分区案例实操\n\n（1）需求分析\n\n① 需求：将统计结果按照手机归属地不同省份输出到不同文件中（分区）\n\n② 输入数据phone_data.txt\n\n```tex\n1\t13736230513\t192.196.100.1\twww.atguigu.com\t2481\t24681\t200\n2\t13846544121\t192.196.100.2\t\t\t264\t0\t200\n3 \t13956435636\t192.196.100.3\t\t\t132\t1512\t200\n4 \t13966251146\t192.168.100.1\t\t\t240\t0\t404\n5 \t18271575951\t192.168.100.2\twww.atguigu.com\t1527\t2106\t200\n6 \t84188413\t192.168.100.3\twww.atguigu.com\t4116\t1432\t200\n7 \t13590439668\t192.168.100.4\t\t\t1116\t954\t200\n8 \t15910133277\t192.168.100.5\twww.hao123.com\t3156\t2936\t200\n9 \t13729199489\t192.168.100.6\t\t\t240\t0\t200\n10 \t13630577991\t192.168.100.7\twww.shouhu.com\t6960\t690\t200\n11 \t15043685818\t192.168.100.8\twww.baidu.com\t3659\t3538\t200\n12 \t15959002129\t192.168.100.9\twww.atguigu.com\t1938\t180\t500\n13 \t13560439638\t192.168.100.10\t\t\t918\t4938\t200\n14 \t13470253144\t192.168.100.11\t\t\t180\t180\t200\n15 \t13682846555\t192.168.100.12\twww.qq.com\t1938\t2910\t200\n16 \t13992314666\t192.168.100.13\twww.gaga.com\t3008\t3720\t200\n17 \t13509468723\t192.168.100.14\twww.qinghua.com\t7335\t110349\t404\n18 \t18390173782\t192.168.100.15\twww.sogou.com\t9531\t2412\t200\n19 \t13975057813\t192.168.100.16\twww.baidu.com\t11058\t48243\t200\n20 \t13768778790\t192.168.100.17\t\t\t120\t120\t200\n21 \t13568436656\t192.168.100.18\twww.alibaba.com\t2481\t24681\t200\n22 \t13568436656\t192.168.100.19\t\t\t1116\t954\t200\n```\n③ 期望数据输出\n\n```tex\n文件1\n文件2\n文件3\n文件4\n文件5\n```\n④ 增加一个ProvincePartitioner分区\n\n```tex\n36 分区0\n137 分区1\n138 分区2\n139 分区3\n其他 分区4\n```\n⑤  Drive驱动类\n\n```java\n// 指定自定义数据分区\njob.setPartitionerClass(ProvincePartitioner.class);\n\n// 同时指定相应数量的reduceTask\njob.setNumReduceTasks(5);\n```\n\n（2）实现\n\n在序列化案例实操中增加一个分区类\n\n```java\npackage com.layne.partitioner2;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n\n//这个输入是Map输出的K，V\npublic class ProvincePartitioner extends Partitioner<Text, FlowBean> {\n    @Override\n    public int getPartition(Text text, FlowBean flowBean, int numPartitions) {\n        // text 是手机号\n\n        String phone = text.toString();\n\n        String prePhone = phone.substring(0, 3);\n\n        int partition ;\n\n        if (\"136\".equals(prePhone)){\n            partition = 0;\n        }else if (\"137\".equals(prePhone)){\n            partition = 1;\n        }else if (\"138\".equals(prePhone)){\n            partition = 2;\n        }else if (\"139\".equals(prePhone)){\n            partition = 3;\n        }else {\n            partition = 4;\n        }\n\n        return partition;\n    }\n}\n```\n\n在驱动函数中增加自定义数据分区设置和 ReduceTask 设置\n\n```java\npackage com.layne.partitioner2;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class FlowDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        // 1 获取job\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        // 2 设置jar\n        job.setJarByClass(FlowDriver.class);\n\n        // 3 关联mapper 和Reducer\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n\n        // 4 设置mapper 输出的key和value类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(FlowBean.class);\n\n        // 5 设置最终数据输出的key和value类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        job.setPartitionerClass(ProvincePartitioner.class);\n        job.setNumReduceTasks(5);\n\n\n        // 6 设置数据的输入路径和输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\test\\\\inputflow\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\test\\\\output666\"));\n\n        // 7 提交job\n        boolean result = job.waitForCompletion(true);\n        System.exit(result ? 0 : 1);\n    }\n}\n```\n\n### 3.4 WritableComparable 排序\n\n排序是MapReduce框架中最重要的操作之一。\n\nMapTask 和 ReduceTask 均会对数据 按照 key 进行排序 。 该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。\n\n默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。\n\n对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值（80%）后，再对缓冲区中的数据进行一次快速排序（排序的过程是在内存中完成的），并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。\n\n对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值（即超过ReduceTask进程内存缓冲区的大小），则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask **统一对内存和磁盘上的所有数据进行一次归并排序**。\n\n\n**排序分类**\n\n（1）部分排序\nMapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。\n（2）全排序\n最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。\n（3）辅助排序：（GroupingComparator分组）\n在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（**全部字段比较不相同**）的key进入到同一个reduce方法时，可以采用分组排序。\n（4）二次排序（或自定义排序）\n在自定义排序过程中，如果compareTo中的**判断条件为两个**即为二次排序。\n\n**自定义排序WritableComparable原理分析**\n\n**bean对象**做为 key 传输，需要实现 WritableComparable 接口重写 compareTo 方法， 就可以实现排序。\n\n```java\n@Override\npublic int compareTo(FlowBean bean) {\n\tint result;\n\t//  按照总流量大小，倒序排列\n\tif (this.sumFlow > bean.getSumFlow()) {\n\t\tresult = -1;\n\t}else if (this.sumFlow < bean.getSumFlow()) {\n\t\tresult = 1;\n\t}else {\n\t\tresult = 0;\n\t}\n\treturn result;\n}\n```\n\n### 3.5 WritableComparable 排序案例实操（全排序）\n该案例在在序列化案例实操中基础上实现\n\n（1）需求分析\n\n① 需求：根据手机的总流量进行倒序排序\n\n② 输入数据\n\n```tex\n1\t13736230513\t192.196.100.1\twww.atguigu.com\t2481\t24681\t200\n2\t13846544121\t192.196.100.2\t\t\t264\t0\t200\n3 \t13956435636\t192.196.100.3\t\t\t132\t1512\t200\n4 \t13966251146\t192.168.100.1\t\t\t240\t0\t404\n5 \t18271575951\t192.168.100.2\twww.atguigu.com\t1527\t2106\t200\n6 \t84188413\t192.168.100.3\twww.atguigu.com\t4116\t1432\t200\n7 \t13590439668\t192.168.100.4\t\t\t1116\t954\t200\n8 \t15910133277\t192.168.100.5\twww.hao123.com\t3156\t2936\t200\n9 \t13729199489\t192.168.100.6\t\t\t240\t0\t200\n10 \t13630577991\t192.168.100.7\twww.shouhu.com\t6960\t690\t200\n11 \t15043685818\t192.168.100.8\twww.baidu.com\t3659\t3538\t200\n12 \t15959002129\t192.168.100.9\twww.atguigu.com\t1938\t180\t500\n13 \t13560439638\t192.168.100.10\t\t\t918\t4938\t200\n14 \t13470253144\t192.168.100.11\t\t\t180\t180\t200\n15 \t13682846555\t192.168.100.12\twww.qq.com\t1938\t2910\t200\n16 \t13992314666\t192.168.100.13\twww.gaga.com\t3008\t3720\t200\n17 \t13509468723\t192.168.100.14\twww.qinghua.com\t7335\t110349\t404\n18 \t18390173782\t192.168.100.15\twww.sogou.com\t9531\t2412\t200\n19 \t13975057813\t192.168.100.16\twww.baidu.com\t11058\t48243\t200\n20 \t13768778790\t192.168.100.17\t\t\t120\t120\t200\n21 \t13568436656\t192.168.100.18\twww.alibaba.com\t2481\t24681\t200\n22 \t13568436656\t192.168.100.19\t\t\t1116\t954\t200\n```\n③ 输出数据\n\n```tex\n13509468723 7335 110349  117684\n13736230513 2481 24681 27162\n13956435636 132 1512 1644\n13846544121 264 0 264\n。。。 。。。\n```\n\n④ FlowBean实现WritableComparable接口重写compareTo方法\n\n```java\n@Override\npublic int compareTo(FlowBean o) {\n\t// 倒序排列，按照总流量从大到小\n\treturn this.sumFlow > o.getSumFlow() ? -1 : 1;\n}\n```\n\n⑤ Mapper类\n\n```java\ncontext.write(bean，手机号)\n```\n\n⑥ Reducer类\n\n```java\n// 循环输出，避免总流量相同情况\nfor (Text text : values) {\n\tcontext.write(text, key);\n}\n```\n\n（2）代码实现\n① FlowBean 对象在在需求 1 基础上增加了比较功能\n\n```java\npackage com.layne.writableComparable;\n\nimport org.apache.hadoop.io.Writable;\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n/**\n * 1、定义类实现WritableComparable接口\n * 2、重写序列化和反序列化方法\n * 3、重写空参构造\n * 4、toString方法\n */\npublic class FlowBean implements WritableComparable<FlowBean> {\n    private long upFlow; // 上行流量\n    private long downFlow; // 下行流量\n    private long sumFlow; // 总流量\n\n    // 空参构造\n    public FlowBean() {\n    }\n\n    public long getUpFlow() {\n        return upFlow;\n    }\n\n    public void setUpFlow(long upFlow) {\n        this.upFlow = upFlow;\n    }\n\n    public long getDownFlow() {\n        return downFlow;\n    }\n\n    public void setDownFlow(long downFlow) {\n        this.downFlow = downFlow;\n    }\n\n    public long getSumFlow() {\n        return sumFlow;\n    }\n\n    public void setSumFlow(long sumFlow) {\n        this.sumFlow = sumFlow;\n    }\n\n    public void setSumFlow() {\n        this.sumFlow = this.upFlow + this.downFlow;\n    }\n\n    @Override\n    public void write(DataOutput out) throws IOException {\n\n        out.writeLong(upFlow);\n        out.writeLong(downFlow);\n        out.writeLong(sumFlow);\n    }\n\n    @Override\n    public void readFields(DataInput in) throws IOException {\n        this.upFlow = in.readLong();\n        this.downFlow = in.readLong();\n        this.sumFlow = in.readLong();\n    }\n\n    @Override\n    public String toString() {\n        return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow;\n    }\n\n    @Override\n    public int compareTo(FlowBean o) {\n\n        // 总流量的倒序排序\n        if (this.sumFlow > o.sumFlow) {\n            return -1;\n        } else if (this.sumFlow < o.sumFlow) {\n            return 1;\n        } else {\n            // 按照上行流量的正序排\n            if (this.upFlow > o.upFlow) {\n                return 1;\n            } else if (this.upFlow < o.upFlow) {\n                return -1;\n            } else {\n\n                return 0;\n            }\n        }\n    }\n}\n```\n② 编写 Mapper 类\n\n```java\npackage com.layne.writableComparable;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n//注意输出的Key是FlowBean，Valuer是Text\npublic class FlowMapper extends Mapper<LongWritable, Text, FlowBean, Text> {\n\n    private FlowBean outK = new FlowBean();\n    private Text outV = new Text();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        // 1 获取一行\n        // 1\t13736230513\t192.196.100.1\twww.atguigu.com\t2481\t24681\t200\n        String line = value.toString();\n\n        // 2 切割\n        // 1,13736230513,192.196.100.1,www.atguigu.com,2481,24681,200   7 - 3= 4\n        // 2\t13846544121\t192.196.100.2\t\t\t264\t0\t200  6 - 3 = 3\n        String[] split = line.split(\"\\t\");\n\n        // 3 抓取想要的数据\n        // 手机号：13736230513\n        // 上行流量和下行流量：2481,24681\n        String phone = split[1];\n        String up = split[split.length - 3];\n        String down  = split[split.length - 2];\n\n        // 4封装\n        outV.set(phone);\n        outK.setUpFlow(Long.parseLong(up));\n        outK.setDownFlow(Long.parseLong(down));\n        outK.setSumFlow();\n\n        // 写出\n        context.write(outK, outV);\n    }\n}\n\n```\n\n③ 编写 Reducer 类\n\n```java\npackage com.layne.writableComparable;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class FlowReducer extends Reducer<FlowBean, Text, Text, FlowBean> {\n\n    @Override\n    protected void reduce(FlowBean key, Iterable<Text> values, Context context) throws IOException, InterruptedException {\n\n        ////遍历 values 集合,循环写出,避免总流量相同的情况\n        for (Text value : values) {\n\n            ////调换 KV 位置,反向写出\n            context.write(value,key);\n        }\n    }\n}\n\n```\n④ 编写Drive类\n\n```java\npackage com.layne.writableComparable;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class FlowDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        // 1 获取job\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        // 2 设置jar\n        job.setJarByClass(FlowDriver.class);\n\n        // 3 关联mapper 和Reducer\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n\n        // 4 设置mapper 输出的key和value类型，注意Key和Value的变化\n        job.setMapOutputKeyClass(FlowBean.class);\n        job.setMapOutputValueClass(Text.class);\n\n        // 5 设置最终数据输出的key和value类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        // 6 设置数据的输入路径和输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\test\\\\inputflow\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\test\\\\output777\"));\n\n        // 7 提交job\n        boolean result = job.waitForCompletion(true);\n        System.exit(result ? 0 : 1);\n    }\n}\n\n```\n\n⑤ 输出结果\n\n```bash\n13509468723\t7335\t110349\t117684\n13975057813\t11058\t48243\t59301\n13736230513\t2481\t24681\t27162\n13568436656\t2481\t24681\t27162\n18390173782\t9531\t2412\t11943\n13630577991\t6960\t690\t7650\n15043685818\t3659\t3538\t7197\n13992314666\t3008\t3720\t6728\n15910133277\t3156\t2936\t6092\n13560439638\t918\t4938\t5856\n84188413\t4116\t1432\t5548\n13682846555\t1938\t2910\t4848\n18271575951\t1527\t2106\t3633\n15959002129\t1938\t180\t2118\n13590439668\t1116\t954\t2070\n13568436656\t1116\t954\t2070\n13956435636\t132\t1512\t1644\n13470253144\t180\t180\t360\n13846544121\t264\t0\t264\n13768778790\t120\t120\t240\n13729199489\t240\t0\t240\n13966251146\t240\t0\t240\n```\n\n### 3.6 WritableComparable 排序案例实操（区内排序）\n该案例基于 WritableComparable 排序案例实操（全排序）实现\n\n（1）需求分析\n① 需求：要求每个省份手机号输出的文件中按照总流量内部排序。即基于前一个需求，增加自定义分区类，分区按照省份手机号设置。 \n② 输入\n\n```tex\n1\t13736230513\t192.196.100.1\twww.atguigu.com\t2481\t24681\t200\n2\t13846544121\t192.196.100.2\t\t\t264\t0\t200\n3 \t13956435636\t192.196.100.3\t\t\t132\t1512\t200\n4 \t13966251146\t192.168.100.1\t\t\t240\t0\t404\n5 \t18271575951\t192.168.100.2\twww.atguigu.com\t1527\t2106\t200\n6 \t84188413\t192.168.100.3\twww.atguigu.com\t4116\t1432\t200\n7 \t13590439668\t192.168.100.4\t\t\t1116\t954\t200\n8 \t15910133277\t192.168.100.5\twww.hao123.com\t3156\t2936\t200\n9 \t13729199489\t192.168.100.6\t\t\t240\t0\t200\n10 \t13630577991\t192.168.100.7\twww.shouhu.com\t6960\t690\t200\n11 \t15043685818\t192.168.100.8\twww.baidu.com\t3659\t3538\t200\n12 \t15959002129\t192.168.100.9\twww.atguigu.com\t1938\t180\t500\n13 \t13560439638\t192.168.100.10\t\t\t918\t4938\t200\n14 \t13470253144\t192.168.100.11\t\t\t180\t180\t200\n15 \t13682846555\t192.168.100.12\twww.qq.com\t1938\t2910\t200\n16 \t13992314666\t192.168.100.13\twww.gaga.com\t3008\t3720\t200\n17 \t13509468723\t192.168.100.14\twww.qinghua.com\t7335\t110349\t404\n18 \t18390173782\t192.168.100.15\twww.sogou.com\t9531\t2412\t200\n19 \t13975057813\t192.168.100.16\twww.baidu.com\t11058\t48243\t200\n20 \t13768778790\t192.168.100.17\t\t\t120\t120\t200\n21 \t13568436656\t192.168.100.18\twww.alibaba.com\t2481\t24681\t200\n22 \t13568436656\t192.168.100.19\t\t\t1116\t954\t200\n```\n\n③ 期望输出\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406153228865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（2）代码实现\n\n① 增加自定义区类\n\n```java\npackage com.layne.partitionercompable;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n//注意K,V的变化\npublic class ProvincePartitioner2 extends Partitioner<FlowBean, Text> {\n    @Override\n    public int getPartition(FlowBean flowBean, Text text, int numPartitions) {\n\n        String phone = text.toString();\n\n        String prePhone = phone.substring(0, 3);\n\n        int partition;\n        if (\"136\".equals(prePhone)){\n            partition =  0;\n        }else if (\"137\".equals(prePhone)){\n            partition =  1;\n        }else if (\"138\".equals(prePhone)){\n            partition =  2;\n        }else if (\"139\".equals(prePhone)){\n            partition =  3;\n        }else {\n            partition = 4;\n        }\n\n        return partition;\n    }\n}\n```\n\n② 在驱动类中添加分区类\n\n```java\n//  设置自定义分区器\njob.setPartitionerClass(ProvincePartitioner2.class);\n//  设置对应的 ReduceTask 的个数\njob.setNumReduceTasks(5);\n```\n\n### 3.7 Combiner 合并\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406154912654.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n由于溢出文件有很多，但是归并文件的个数有限，默认一次归并10个文件，所以有很多次归并排序操作。\n\n\n\n**Combiner合并**\n（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。\n（2）Combiner组件的父类就是Reducer。\n（3）Combiner和Reducer的区别在于运行的位置\n- Combiner是在每一个MapTask所在的节点运行；\n- Reducer是接收全局所有Mapper的输出结果；\n\n\n（4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。\n（5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟Reducer的输入kv类型要对应起来。\n\n例如，我们要求平均值，就不符合要求\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406155913331.png)\n但是，如果我们要求和，那么使用Combiner后，不影响最终结果。\n\n\n（6）自定义 Combiner 实现步骤\n① 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法\n\n```java\npublic  class  WordCountCombiner  extends  Reducer<Text,  IntWritable,  Text, IntWritable> {\n\tprivate IntWritable outV = new IntWritable();\n\t@Override\n\tprotected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n\t\tint sum = 0;\n\t\tfor (IntWritable value : values) {\n\t\t\tsum += value.get();\n\t\t}\n\t\toutV.set(sum);\n\t\tcontext.write(key,outV);\n\t}\n}\n```\n② 在 Job 驱动类中设置： \n\n```java\njob.setCombinerClass(WordCountCombiner.class);\n```\n\n\n### 3.8 Combiner 合并案例实操\n（1）需求分析\n\n① 需求：统计过程中对每一个 MapTask 的输出进行局部汇总，以减小网络传输量即采用Combiner 功能。   \n\n② 数据输入\n\n```tex\nbanzhang ni hao\nxihuan hadoop\nbanzhang\nbanzhang ni hao\nxihuan hadoop\nbanzhang\n```\n总12个单词\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406161106715.png)\n\n\n③ 期望：Combine 输入数据多，输出时经过合并，输出数据降低。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021040616104256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（2）方案一\n① 增加一个WordcountCombiner类继承Reducer\n② 在WordcountCombiner中\n- 统计单词汇总\n- 将结果输出\n\n（3）方案二\n\n将WordcountReducer作为Combiner 在WordcountDriver驱动类中指定\n\n```java\njob.setCombinerClass(WordcountReducer.class);\n```\n\n（4）案例实操-方案一\n本案例在wordCount基础上实现\n\n① 增加一个 WordCountCombiner 类继承 Reducer\n\n```java\npackage com.layne.combiner;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class WordCountCombiner extends Reducer<Text, IntWritable,Text, IntWritable> {\n\n    private IntWritable outV = new IntWritable();\n    @Override\n    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n\n        int sum =0;\n        for (IntWritable value : values) {\n            sum += value.get();\n        }\n\n        outV.set(sum);\n\n        context.write(key,outV);\n    }\n}\n```\n（2）在 WordcountDriver 驱动类中指定 Combiner\n\n```java\n//  指定需要使用 combiner，以及用哪个类作为 combiner 的逻辑\njob.setCombinerClass(WordCountCombiner.class);\n```\n（5）案例实操-方案二\n① 将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定\n\n```java\n//  指定需要使用 Combiner，以及用哪个类作为 Combiner 的逻辑\njob.setCombinerClass(WordCountReducer.class);\n```\n运行结果如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406163016528.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n## 4. OutputFormat 数据输出\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406171843851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n### 4.1 OutputFormat 接口实现类\n\nOutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。\n\n其子类继承关系如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406172403280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n1. 默认输出格式为TextOutputFormat\n2. 自定义OutputFormat\n-- 应用场景：\n例如：输出数据到MySQL/HBase/Elasticsearch等存储框架中。\n-- 自定义OutputFormat步骤\n➢ 自定义一个类继承FileOutputFormat。\n➢ 改写RecordWriter，具体改写输出数据的方法write()。\n\n\n### 4.2 自定义 OutputFormat 案例实操\n\n（1）需求分析\n① 需求：过滤输入的 log 日志，包含 atguigu 的网站输出到 e:/atguigu.log，不包含 atguigu 的网站输出到 e:/other.log。\n\n② 输入数据\n\n```tex\nhttp://www.baidu.com\nhttp://www.google.com\nhttp://cn.bing.com\nhttp://www.atguigu.com\nhttp://www.sohu.com\nhttp://www.sina.com\nhttp://www.sin2a.com\nhttp://www.sin2desa.com\nhttp://www.sindsafa.com\nhttp://www.atguigu.com\nhttp://cn.bing.com\nhttp://www.baidu.com\nhttp://www.google.com\nhttp://www.sin2a.com\nhttp://www.sin2desa.com\n```\n\n③ 输出数据\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210406172805607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n④ 自定义一个OutputFormat类\n创建一个类LogRecordWriter继承RecordWriter\n- 创建两个文件的输出流：atguiguOut、otherOut\n- 如果输入数据包含atguigu，输出到atguiguOut流如果不包含atguigu，输出到otherOut流\n\n⑤ 驱动类Driver\n\n```java\n// 要将自定义的输出格式组件设置到job中\njob.setOutputFormatClass(LogOutputFormat.class);\n```\n\n（2）代码实现\n\n① 编写 LogMapper 类\n\n```java\npackage com.layne.mapreduce.outputFormat;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class LogMapper extends Mapper<LongWritable, Text,Text, NullWritable> {\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        // http://www.baidu.com\n        //http://www.google.com\n        // (http://www.google.com, NullWritable)\n        // 不做任何处理\n        context.write(value, NullWritable.get());\n    }\n}\n```\n\n② 编写 LogReducer 类\n\n```java\npackage com.layne.mapreduce.outputFormat;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class LogReducer extends Reducer<Text, NullWritable, Text, NullWritable> {\n\n    @Override\n    protected void reduce(Text key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {\n\n        // http://www.baidu.com\n        // http://www.baidu.com\n        // 防止有相同数据，丢数据\n        for (NullWritable value : values) {\n            context.write(key, NullWritable.get());\n        }\n    }\n}\n```\n\n③ 自定义一个 LogOutputFormat 类\n\n```java\npackage com.layne.mapreduce.outputFormat;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n//这个reduce输出的K,V\npublic class LogOutputFormat extends FileOutputFormat<Text, NullWritable> {\n    @Override\n    public RecordWriter<Text, NullWritable> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {\n\n        LogRecordWriter lrw = new LogRecordWriter(job);\n\n        return lrw;\n    }\n}\n```\n\n④ 编写 LogRecordWriter 类\n\n```java\npackage com.layne.mapreduce.outputFormat;\n\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\n\nimport java.io.IOException;\n\npublic class LogRecordWriter extends RecordWriter<Text, NullWritable> {\n\n    private  FSDataOutputStream otherOut;\n    private  FSDataOutputStream atguiguOut;\n\n    public LogRecordWriter(TaskAttemptContext job) {\n        // 创建两条流\n        try {\n            FileSystem fs = FileSystem.get(job.getConfiguration());\n\n            atguiguOut = fs.create(new Path(\"D:\\\\test\\\\atguigu.log\"));\n\n            otherOut = fs.create(new Path(\"D:\\\\test\\\\other.log\"));\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    @Override\n    public void write(Text key, NullWritable value) throws IOException, InterruptedException {\n        String log = key.toString();\n\n        //根据一行的 log 数据是否包含 atguigu,判断两条输出流输出的内容\n        if (log.contains(\"atguigu\")){\n            atguiguOut.writeBytes(log+\"\\n\");\n        }else {\n            otherOut.writeBytes(log+\"\\n\");\n        }\n    }\n\n    @Override\n    public void close(TaskAttemptContext context) throws IOException, InterruptedException {\n        // 关流\n        IOUtils.closeStream(atguiguOut);\n        IOUtils.closeStream(otherOut);\n    }\n}\n\n```\n\n⑤ 编写 LogDriver 类\n\n```java\npackage com.layne.mapreduce.outputFormat;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class LogDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(LogDriver.class);\n        job.setMapperClass(LogMapper.class);\n        job.setReducerClass(LogReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        //设置自定义的outputformat\n        job.setOutputFormatClass(LogOutputFormat.class);\n\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\test\\\\inputoutputformat\"));\n        //虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat\n        //而fileoutputformat要输出一个_SUCCESS文件，所以在这还得指定一个输出目录\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\test\\\\output1111\"));\n\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n\n    }\n}\n```\n\n\n\n## 5. MapReduce 内核源码解析\n\n### 5.1 MapTask 工作机制\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406193836.png)\n\n（1）Read 阶段：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中解析出一个个 key/value。\n（2）Map 阶段：该节点主要是将解析出的 key/value 交给用户编写 map()函数处理，并产生一系列新的 key/value。\n（3）Collect 收集阶段：在用户编写 map()函数中，当数据处理完成后，一般会调用OutputCollector.collect() 输出结果。在该函数内部，它会将生成的 key/value 分区（调用Partitioner），并写入一个环形内存缓冲区中。\n（4）Spill 阶段：即“溢写”， 当环形缓冲区满后， MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。\n\n溢写阶段详情：\n\n- 步骤 1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。\n- 步骤 2： 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。\n- 步骤 3： 将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件 output/spillN.out.index 中。\n\n（5）Merge 阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。\n\n当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件output/file.out 中，同时生成相应的索引文件 output/file.out.index。\n\n在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区， 它将采用多轮递归合并的方式。 每轮合并 mapreduce.task.io.sort.factor （默认 10） 个文件，并将产生的文件重新加入待合并列表中，对文件排后，重复以上过程，直到最终得到一个大文件。让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。\n\n\n\n### 5.2 ReduceTask 并行度决定机制\n\n回顾：MapTask 并行度由切片个数决定，切片个数由输入文件和切片规则决定。\n\n思考：ReduceTask 并行度由谁决定？\n\n（1）设置 ReduceTask 并行度（个数）\n\nReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置：\n\n```java\n//  默认值是 1，手动设置为 4\njob.setNumReduceTasks(4);\n```\n\n（2）实验：测试 ReduceTask 多少合适\n\n① 实验环境：1 个 Master 节点，16 个 Slave 节点：CPU:8GHZ，内存: 2G\n\n② 实验结论：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406195046.png)\n\n\n\n**注意事项**\n\n- ReduceTask=0，表示没有Reduce阶段，输出文件个数和Map个数一致。\n- ReduceTask默认值就是1，所以输出文件个数为一个。\n- 如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜\n- ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask。\n- 具体多少个ReduceTask，需要根据集群性能而定。\n- 如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。\n\n\n\n### 5.3 MapTask & ReduceTask 源码解析\n\n\n\n这里以 Partition 分区案例实操 为例，源码解析MapTask & ReduceTask流程\n\n**MapTask**\n\n```java\n//=================== MapTask ===================\ncontext.write(k, NullWritable.get());    //自定义的 map 方法的写出，进入\n\toutput.write(key, value);  \n\t\t//MapTask727 行，收集方法，进入两次 \n\t\tcollector.collect(key, value,partitioner.getPartition(key, value, partitions));\n\t\t\tHashPartitioner(); //默认分区器\n\t\tcollect()  //MapTask1082 行  map 端所有的 kv 全部写出后会走下面的 close 方法\n\t\t\tclose() //MapTask732 行\n\t\t\t\tcollector.flush() //  溢出刷写方法，MapTask735 行，提前打个断点，进入\n\t\t\t\t\tsortAndSpill() //溢写排序，MapTask1505 行，进入\n\t\t\t\t\t\tsorter.sort()    QuickSort //溢写排序方法，MapTask1625 行，进入\n\t\t\t\t\tmergeParts(); //合并文件，MapTask1527 行，生成file.out和file.out.index\n\t\t\t\tcollector.close(); //MapTask739 行,收集器关闭,即将进入 ReduceTask\n```\n\n**ReduceTask**\n\n```java\n=================== ReduceT ask ===================\nif (isMapOrReduce())   //reduceTask324 行，提前打断点 \n\tinitialize()    // reduceTask333 行,进入\n\tinit(shuffleContext);   // reduceTask375 行,走到这需要先给下面的打断点\n\t\ttotalMaps = job.getNumMapTasks(); // ShuffleSchedulerImpl 第 120 行，提前打断点\n\t\tmerger = createMergeManager(context); //合并方法，Shuffle 第 80 行\n\t\t\t// MergeManagerImpl 第 232 235 行，提前打断点\n\t\t\tthis.inMemoryMerger = createInMemoryMerger(); //内存合并\n\t\t\tthis.onDiskMerger = new OnDiskMerger(this); //磁盘合并\n\trIter = shuffleConsumerPlugin.run();\n\t\teventFetcher.start();  //开始抓取数据，Shuffle 第 107 行，提前打断点\n\t\teventFetcher.shutDown();  //抓取结束，Shuffle 第 141 行，提前打断点\n\t\tcopyPhase.complete();   //copy 阶段完成，Shuffle 第 151 行\n\t\ttaskStatus.setPhase(TaskStatus.Phase.SORT);  //开始排序阶段，Shuffle 第 152 行\n\tsortPhase.complete();   //排序阶段完成，即将进入 reduce 阶段  reduceTask382 行\nreduce();   //reduce 阶段调用的就是我们自定义的 reduce 方法，会被调用多次\n\tcleanup(context); //reduce 完成之前，会最后调用一次 Reducer 里面的 cleanup 方法\n```\n\n\n\n## 6. Join应用\n\n### 6.1 Reduce Join\n\nMap 端的主要工作：为来自不同表或文件的 key/value 对，打标签以区别不同来源的记录。然后用连接字段作为 key，其余部分和新加的标志作为 value，最后进行输出。\n\nReduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进行合并就 ok 了。\n\n\n\n### 6.2  Reduce Join 案例实操\n\n（1）需求\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406201739.png)\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406201720.png)\n\n\n\n将商品信息表中数据根据商品 pid 合并到订单数据表中。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202011.png)\n\n（2）需求分析\n\n通过将关联条件作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。\n\n① 输入数据\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202105.png)\n\n② 预期输出数据\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202142.png)\n\n③ MapTask\n\nMap中处理的事情\n\n- 获取输入文件类型\n- 获取输入数据\n- 不同文件分别处理\n- 封装Bean对象输出\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202304.png)\n\n\n\n默认产品id排序\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202332.png)\n\n④ ReduceTask\n\nReduce方法缓存订单数据集合，和产品表，然后合并\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202554.png)\n\n\n\n（3）代码实现\n\n① 创建商品和订单合并后的 TableBean 类\n\n```java\npackage com.layne.mapreduce.reduceJoin;\n\nimport org.apache.hadoop.io.Writable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class TableBean implements Writable {\n\n    private String id; // 订单id\n    private String pid; // 商品id\n    private int amount; // 商品数量\n    private String pname;// 商品名称\n    private String flag; // 标记是什么表 order pd\n\n    // 空参构造\n    public TableBean() {\n    }\n\n    public String getId() {\n        return id;\n    }\n\n    public void setId(String id) {\n        this.id = id;\n    }\n\n    public String getPid() {\n        return pid;\n    }\n\n    public void setPid(String pid) {\n        this.pid = pid;\n    }\n\n    public int getAmount() {\n        return amount;\n    }\n\n    public void setAmount(int amount) {\n        this.amount = amount;\n    }\n\n    public String getPname() {\n        return pname;\n    }\n\n    public void setPname(String pname) {\n        this.pname = pname;\n    }\n\n    public String getFlag() {\n        return flag;\n    }\n\n    public void setFlag(String flag) {\n        this.flag = flag;\n    }\n\n    @Override\n    public void write(DataOutput out) throws IOException {\n        out.writeUTF(id);\n        out.writeUTF(pid);\n        out.writeInt(amount);\n        out.writeUTF(pname);\n        out.writeUTF(flag);\n    }\n\n    @Override\n    public void readFields(DataInput in) throws IOException {\n\n        this.id = in.readUTF();\n        this.pid = in.readUTF();\n        this.amount = in.readInt();\n        this.pname = in.readUTF();\n        this.flag = in.readUTF();\n    }\n\n    @Override\n    public String toString() {\n        // id\tpname\tamount\n        return  id + \"\\t\" +  pname + \"\\t\" + amount ;\n    }\n}\n\n```\n\n② 编写 TableMapper 类\n\n```java\npackage com.layne.mapreduce.reduceJoin;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.InputSplit;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileSplit;\n\nimport java.io.IOException;\n\npublic class TableMapper extends Mapper<LongWritable, Text, Text, TableBean> {\n\n    private String fileName;\n    private Text outK  = new Text();\n    private TableBean outV = new TableBean();\n\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n        // 初始化  order.txt  pd.txt\n        FileSplit split = (FileSplit) context.getInputSplit();\n\t\t//获取对应文件名称\n        fileName = split.getPath().getName();\n    }\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        // 1 获取一行\n        String line = value.toString();\n\n        // 2 判断是哪个文件的\n        if (fileName.contains(\"order\")){// 处理的是订单表\n\n            String[] split = line.split(\"\\t\");\n\n            // 封装k  v\n            outK.set(split[1]);\n            outV.setId(split[0]);\n            outV.setPid(split[1]);\n            outV.setAmount(Integer.parseInt(split[2]));\n            outV.setPname(\"\");\n            outV.setFlag(\"order\");\n\n        }else {// 处理的是商品表\n            String[] split = line.split(\"\\t\");\n\n            outK.set(split[0]);\n            outV.setId(\"\");\n            outV.setPid(split[0]);\n            outV.setAmount(0);\n            outV.setPname(split[1]);\n            outV.setFlag(\"pd\");\n        }\n\n        // 写出\n        context.write(outK, outV);\n    }\n}\n\n```\n\n\n\n③ 编写 TableReducer 类\n\n```java\npackage com.layne.mapreduce.reduceJoin;\n\nimport org.apache.commons.beanutils.BeanUtils;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\nimport java.lang.reflect.InvocationTargetException;\nimport java.util.ArrayList;\n\npublic class TableReducer extends Reducer<Text, TableBean,TableBean, NullWritable> {\n\n    @Override\n    protected void reduce(Text key, Iterable<TableBean> values, Context context) throws IOException, InterruptedException {\n//        01 \t1001\t1   order\n//        01 \t1004\t4   order\n//        01\t小米   \t     pd\n        // 准备初始化集合\n        ArrayList<TableBean> orderBeans = new ArrayList<>();\n        TableBean pdBean = new TableBean();\n\n        // 循环遍历\n        for (TableBean value : values) {\n\n            if (\"order\".equals(value.getFlag())){// 订单表\n\t\t\t\t//创建一个临时 TableBean 对象接收 value\n                TableBean tmptableBean = new TableBean();\n\n                //如果直接用orderBeans.add(value)，则只能添加一个对象\n                //因为Hadoop处理的时候下一个value会将上一个value的地址覆盖\n                try {\n                    //复制对象\n                    BeanUtils.copyProperties(tmptableBean,value);\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                } catch (InvocationTargetException e) {\n                    e.printStackTrace();\n                }\n\t\t\t\t//将临时 TableBean 对象添加到集合 orderBeans\n                orderBeans.add(tmptableBean);\n            }else {// 商品表\n\n                try {\n                    BeanUtils.copyProperties(pdBean,value);\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                } catch (InvocationTargetException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n\n        // 循环遍历orderBeans，赋值 pdname\n        for (TableBean orderBean : orderBeans) {\n\n            orderBean.setPname(pdBean.getPname());\n\n            context.write(orderBean,NullWritable.get());\n        }\n    }\n}\n```\n\n\n\n④ 编写 TableDriver 类\n\n```java\npackage com.layne.mapreduce.reduceJoin;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class TableDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        Job job = Job.getInstance(new Configuration());\n\n        job.setJarByClass(TableDriver.class);\n        job.setMapperClass(TableMapper.class);\n        job.setReducerClass(TableReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(TableBean.class);\n\n        job.setOutputKeyClass(TableBean.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\test\\\\inputtable\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\test\\\\output2345\"));\n\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n\n}\n```\n\n\n\n（4）测试\n\n运行程序查看结果\n\n```tex\n1004\t小米\t4\n1001\t小米\t1\n1005\t华为\t5\n1002\t华为\t2\n1006\t格力\t6\n1003\t格力\t3\n```\n\n（5）总结\n\n缺点：这种方式中，合并的操作是在 Reduce 阶段完成， Reduce 端的处理压力太大， Map节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜（毕竟在Reduce把所有数据都合并了）。\n\n\n\n### 6.3 Map Join\n\n（1）使用场景\nMap Join 适用于一张表十分小、一张表很大的场景。\n\n（2）优点\n思考：在 Reduce 端处理过多的表，非常容易产生数据倾斜。怎么办？\n在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数据的压力，尽可能的减少数据倾斜。\n\n（3）具体办法：采用 DistributedCache\n\n① 在 Mapper 的 setup 阶段，将文件读取到缓存集合中。\n\n② 在 Driver 驱动类中加载缓存。\n\n```java\n//缓存普通文件到 Task 运行节点。\njob.addCacheFile(new URI(\"file:///e:/cache/pd.txt\"));\n//如果是集群运行,需要设置 HDFS 路径\njob.addCacheFile(new URI(\"hdfs://hadoop102:8020/cache/pd.txt\"));\n```\n\n\n\n\n\n### 6.4 Map Join 案例实操\n\n这里使用6.2节的需求分析案例\n\n（1）DistributedCacheDriver 缓存文件\n\n```java\n// 1 加载缓存数据\njob.addCacheFile(new URI(\"file:///d:/test/cache/pd.txt\"));\n//2 Map端join的逻辑不需要Reduce 阶段，设置ReduceTask数量为0\njob.setNumReduceTasks(0); //不需要Reduce阶段\n```\n\n（2）读取缓存的文件数据\n\n```tex\nsetup()方法中\n// 1 获取缓存的文件\n// 2 循环读取缓存文件一行\n// 3 切割\n// 4 缓存数据到集合\n    <pid, pname>\n    01,小米\n    02,华为\n    03,格力\n// 5 关流\n\nmap方法中\n// 1 获取一行\n// 2 截取\n// 3 获取pid\n// 4 获取订单id和商品名称\n// 5 拼接\n// 6 写出\n```\n\n（3）代码实现\n\n① 先在 MapJoinDriver 驱动类中添加缓存文件\n\n```java\npackage com.layne.mapreduce.mapjoin;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\n\npublic class MapJoinDriver {\n    public static void main(String[] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException {\n\n        // 1 获取job信息\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n        // 2 设置加载jar包路径\n        job.setJarByClass(MapJoinDriver.class);\n        // 3 关联mapper\n        job.setMapperClass(MapJoinMapper.class);\n        // 4 设置Map输出KV类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n        // 5 设置最终输出KV类型，其实这里没啥用，和map输出保持一致就行\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        // 加载缓存数据\n        job.addCacheFile(new URI(\"file:///D:/test/tablecache/pd.txt\"));\n        // Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0\n        job.setNumReduceTasks(0);\n\n        // 6 设置输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\test\\\\inputtable2\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\test\\\\output8888\"));\n        // 7 提交\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n}\n```\n\n② 在 MapJoinMapper 类中的 setup 方法中读取缓存文件\n\n```java\npackage com.layne.mapreduce.mapjoin;\n\nimport org.apache.commons.lang.StringUtils;\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.URI;\nimport java.util.HashMap;\n\npublic class MapJoinMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n    private HashMap<String, String> pdMap = new HashMap<>();\n    private Text outK = new Text();\n\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n        // 获取缓存的文件，并把文件内容封装到集合 pd.txt\n        URI[] cacheFiles = context.getCacheFiles();\n\n        ////获取文件系统对象,并开流\n        FileSystem fs = FileSystem.get(context.getConfiguration());\n        FSDataInputStream fis = fs.open(new Path(cacheFiles[0]));\n\n        // 通过包装流转换为 reader,方便按行读取\n        BufferedReader reader = new BufferedReader(new InputStreamReader(fis, \"UTF-8\"));\n\t\t//逐行读取，按行处理\n        String line;\n        while (StringUtils.isNotEmpty(line = reader.readLine())) {\n            // 切割\n            //01  小米\n            String[] fields = line.split(\"\\t\");\n\n            // 赋值\n            pdMap.put(fields[0], fields[1]);\n        }\n\n        // 关流\n        IOUtils.closeStream(reader);\n    }\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        // 处理 order.txt\n        String line = value.toString();\n\n        String[] fields = line.split(\"\\t\");\n\n        // 获取pid\n        String pname = pdMap.get(fields[1]);\n\n        // 获取订单id 和订单数量\n        // 封装\n        outK.set(fields[0] + \"\\t\" + pname + \"\\t\" + fields[2]);\n\n        context.write(outK, NullWritable.get());\n    }\n}\n\n```\n\n\n\n\n\n## 7. 数据清洗（ETL）\n\n“ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据仓库。\n\n在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序。\n\n\n\n（1）需求\n\n去除日志中字段个数小于等于 11 的日志。\n\n① 输入数据`inputlog\\web.log`\n\n② 期望输出数据：每行字段长度都大于 11。\n\n\n\n（2）需求分析\n\n需要在 Map 阶段对输入的数据根据规则进行过滤清洗。\n\n\n\n（3）代码实现\n\n① 编写 WebLogMapper 类\n\n```java\npackage com.layne.mapreduce.etl;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class WebLogMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        // 1 获取一行\n        String line = value.toString();\n\n        // 2 ETL\n        boolean result = parseLog(line, context);\n\n        if (!result){\n            return; //过滤掉\n        }\n\n        // 3 写出\n        context.write(value, NullWritable.get());\n    }\n\n    private boolean parseLog(String line, Context context) {\n        // 切割\n        // 1.206.126.5 - - [19/Sep/2013:05:41:41 +0000] \"-\" 400 0 \"-\" \"-\"\n        String[] fields = line.split(\" \");\n\n        // 2 判断一下日志的长度是否大于11\n        if (fields.length > 11){\n            return true;\n        }else {\n            return false;\n        }\n    }\n}\n\n```\n\n\n\n② Drive类\n\n```java\npackage com.layne.mapreduce.etl;\n\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WebLogDriver {\n\n    public static void main(String[] args) throws Exception {\n\n        // 输入输出路径需要根据自己电脑上实际的输入输出路径设置\n        args = new String[]{\"D:/test/inputlog\", \"D:/test/output11111\"};\n\n        // 1 获取job信息\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        // 2 加载jar包\n        job.setJarByClass(WebLogDriver.class);\n\n        // 3 关联map\n        job.setMapperClass(WebLogMapper.class);\n\n        // 4 设置最终输出类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        // 设置reducetask个数为0\n        job.setNumReduceTasks(0);//取消reduce阶段\n\n        // 5 设置输入和输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        // 6 提交\n        boolean b = job.waitForCompletion(true);\n        System.exit(b ? 0 : 1);\n    }\n\n}\n\n```\n\n\n\n## 8. ETL清洗规则\n\n正则匹配全部汇总\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406213048.png)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406213124.png)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406213152.png)\n\n\n\n比如上面第15条，手机号的清洗规则\n\n```java\npackage com.layne.mapreduce.etl;\n\npublic class TestETL {\n\n    public static void main(String[] args) {\n\n        //手机号的校验规则\n        String check = \"^((13[0-9])|(14[0-9])|(15[0-9])|(17[0-9])|(18[0-9]))\\\\d{8}$\";\n\n        String phone = \"1352235001311\";\n\n        System.out.println(phone.matches(check));\n\n    }\n}\n\n```\n\n\n\n## 9. MapReduce 开发总结\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406214042.png)\n\n\n\n（1）输入数据接口：InputFormat\n\n- 默认使用的实现类是：TextInputFormat\n\n- TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为 value 返回。\n  \n- CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。\n  \n  \n  \n\n（2）逻辑处理接口：Mapper \n  用户根据业务需求实现其中三个方法：map()    setup()    cleanup () \n\n- `setup()`用来处理初始化\n- ` setup()`处理用户业务逻辑\n- ` cleanup () `用来关闭资源\n\n\n\n（3）Partitioner 分区\n\n- 有默认实现  HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；\n\n  ```java\n  key.hashCode()&Integer.MAXVALUE % numReduces\n  ```\n\n- 如果业务上有特别的需求，可以自定义分区。\n\n\n\n\n\n（4）Comparable 排序\n\n- 当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接口，重写其中的 compareTo()方法。\n- 部分排序：对最终输出的每一个文件进行内部排序。\n- 全排序：对所有数据进行排序，通常只有一个 Reduce。\n- 二次排序：排序的条件有两个。\n\n\n\n（5）Combiner 合并\nCombiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。\n\n（6）逻辑处理接口：Reducer\n用户根据业务需求实现其中三个方法：reduce()    setup()    cleanup () \n\n- `setup()`初始化\n- `reduce() `业务逻辑\n- `cleanup () `关闭资源\n\n\n\n（7）输出数据接口：OutputFormat\n\n- 默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。\n- 用户还可以自定义 OutputFormat。\n\n\n\n\n\n## 10. 常见错误及解决方案\n\n1. 导包容易出错。尤其 Text 和 CombineTextInputFormat。\n\n2.  Mapper 中第一个输入的参数必须是 LongWritable 或者 NullWritable， 不可以是 IntWritable.  报的错误是类型转换异常。\n\n3. java.lang.Exception: java.io.IOException: Illegal partition for 13926435656 (4)，说明 Partition和 ReduceT ask 个数没对上，调整 ReduceTask 个数。\n\n4. 如果分区数不是 1， 但是 reducetask 为 1， 是否执行分区过程。答案是：不执行分区过程。因为在 MapT ask 的源码中，执行分区的前提是先判断 ReduceNum 个数是否大于 1。不大于1 肯定不执行。\n\n5. 在 Windows 环境编译的 jar 包导入到 Linux 环境中运行，`hadoop  jar  wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver  /input /output`\n   报如下错误：\n\n   ```bash\n   Exception  in  thread  \"main\"  java.lang.UnsupportedClassVersionError: \n   com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version 52.0 \n   ```\n\n   原因是 Windows 环境用的 jdk1.7，Linux 环境用的 jdk1.8。\n\n6. 缓存 pd.txt 小文件案例中，报找不到 pd.txt 文件。错误原因：大部分为路径书写错误。还有就是要检查 pd.txt.txt 的问题。 还有个别电脑写相对路径找不到 pd.txt，可以修改为绝对路径。\n\n7. 报类型转换异常。通常都是在驱动函数中设置 Map 输出和最终输出时编写错误，Map 输出的 key 如果没有排序，也会报类型转换异常。\n\n8. 集群中运行 wc.jar 时出现了无法获得输入文件。原因：WordCount 案例的输入文件不能放用 HDFS 集群的根目录。\n\n9. 出现了如下相关异常\n\n   ```bash\n   Exception  in  thread  \"main\"  java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n   at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n   at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:609)\n   at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)\n   java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n   at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:356)\n   at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:371)\n   at org.apache.hadoop.util.Shell.<clinit>(Shell.java:364)\n   ```\n\n   解决方案：拷贝 hadoop.dll 文件到 Windows 目录 C:\\Windows\\System32。个别电脑还需要修改 Hadoop 源码。\n   方案二：创建如下包名，并将 NativeIO.java 拷贝到该包名下\n   ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210406232017.png)\n\n10. 自定义 Outputformat 时，注意在 RecordWirter 中的 close 方法必须关闭流资源。否则输出的文件内容中数据为空。\n\n    ```java\n    @Override\n    public  void  close(TaskAttemptContext  context)  throws  IOException, \n    \tInterruptedException {\n    \tif (atguigufos != null) {\n    \t\tatguigufos.close();\n    \t}\n    \tif (otherfos != null) {\n    \t\totherfos.close();\n    \t}\n    } \n    ```\n\n    \n\n","tags":["Hadoop","MapReduce"],"categories":["Hadoop"]},{"title":"六、MapReduce之Hadoop的序列化","url":"/2021/03/31/204801/","content":"\n\n\nMapReduce之Hadoop的序列化\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n## 1. 序列化概述\n（1）什么是序列化\n\n序列化就是把内存中的对象，转换成字节序列 （或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 \n\n反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。\n\n（2）为什么要序列化\n\n一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。\n\n（3）为什么不用 Java 的序列化\n\nJava 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息， Header，继承体系等），不便于在网络中高效传输。所以，Hadoop 自己开发了一套序列化机制（Writable）。\n\n（4）Hadoop 序列化特点：\n\n\n- 紧凑  ：高效使用存储空间。\n- 快速：读写数据的额外开销小。\n- 互操作：支持多语言的交互\n\n\n## 2. 自定义 bean 对象实现序列化接口（Writable）\n在企业开发中往往常用的基本序列化类型不能满足所有需求， 比如在 Hadoop 框架内部传递一个 bean 对象，那么该对象就需要实现序列化接口。\n\n具体实现 bean 对象序列化步骤如下 7 步。\n（1）必须实现 Writable 接口\n（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造\n\n```java\npublic FlowBean() {\n\tsuper();\n}\n```\n（3）重写序列化方法\n\n```java\n@Override\npublic void write(DataOutput out) throws IOException { \n\tout.writeLong(upFlow);\n\tout.writeLong(downFlow);\n\tout.writeLong(sumFlow);\n}\n```\n（4）重写反序列化方法\n\n```java\n@Override\npublic void readFields(DataInput in) throws IOException {\n\tupFlow = in.readLong();\n\tdownFlow = in.readLong();\n\tsumFlow = in.readLong();\n}\n```\n（5）注意反序列化的顺序和序列化的顺序完全一致\n\n（6）要想把结果显示在文件中，需要重写 toString()，可用\"\\t\"分开，方便后续用。\n\n（7）如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口，因为MapReduce 框中的 Shuffle 过程要求对 key 必须能排序。详见后面排序案例。\n\n```java\n@Override\npublic int compareTo(FlowBean o) {\n\t//  倒序排列，从大到小\n\treturn this.sumFlow > o.getSumFlow() ? -1 : 1;\n}\n```\n\n## 3. 序列化案例实操 \n（1）需求\n统计每一个手机号耗费的总上行流量、总下行流量、总流量\n① 输入数据phone_data.txt\n\n```tex\n1\t13736230513\t192.196.100.1\twww.atguigu.com\t2481\t24681\t200\n2\t13846544121\t192.196.100.2\t\t\t264\t0\t200\n3 \t13956435636\t192.196.100.3\t\t\t132\t1512\t200\n4 \t13966251146\t192.168.100.1\t\t\t240\t0\t404\n5 \t18271575951\t192.168.100.2\twww.atguigu.com\t1527\t2106\t200\n6 \t84188413\t192.168.100.3\twww.atguigu.com\t4116\t1432\t200\n7 \t13590439668\t192.168.100.4\t\t\t1116\t954\t200\n8 \t15910133277\t192.168.100.5\twww.hao123.com\t3156\t2936\t200\n9 \t13729199489\t192.168.100.6\t\t\t240\t0\t200\n10 \t13630577991\t192.168.100.7\twww.shouhu.com\t6960\t690\t200\n11 \t15043685818\t192.168.100.8\twww.baidu.com\t3659\t3538\t200\n12 \t15959002129\t192.168.100.9\twww.atguigu.com\t1938\t180\t500\n13 \t13560439638\t192.168.100.10\t\t\t918\t4938\t200\n14 \t13470253144\t192.168.100.11\t\t\t180\t180\t200\n15 \t13682846555\t192.168.100.12\twww.qq.com\t1938\t2910\t200\n16 \t13992314666\t192.168.100.13\twww.gaga.com\t3008\t3720\t200\n17 \t13509468723\t192.168.100.14\twww.qinghua.com\t7335\t110349\t404\n18 \t18390173782\t192.168.100.15\twww.sogou.com\t9531\t2412\t200\n19 \t13975057813\t192.168.100.16\twww.baidu.com\t11058\t48243\t200\n20 \t13768778790\t192.168.100.17\t\t\t120\t120\t200\n21 \t13568436656\t192.168.100.18\twww.alibaba.com\t2481\t24681\t200\n22 \t13568436656\t192.168.100.19\t\t\t1116\t954\t200\n```\n② 输入数据格式：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402100217213.png)\n③ 期望输出数据格式\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402100237938.png)\n（2）需求分析\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402100519669.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n（3）编写 MapReduce 程序\n\n① 编写流量统计的 Bean 对象\n\n```java\npackage com.layne.mapreduce.writable;\n\nimport org.apache.hadoop.io.Writable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n/**\n * 1、定义类实现writable接口\n * 2、重写序列化和反序列化方法\n * 3、重写空参构造\n * 4、toString方法\n */\npublic class FlowBean implements Writable {\n    private long upFlow; // 上行流量\n    private long downFlow; // 下行流量\n    private long sumFlow; // 总流量\n\n    // 空参构造\n    public FlowBean() {\n    }\n\n    public long getUpFlow() {\n        return upFlow;\n    }\n\n    public void setUpFlow(long upFlow) {\n        this.upFlow = upFlow;\n    }\n\n    public long getDownFlow() {\n        return downFlow;\n    }\n\n    public void setDownFlow(long downFlow) {\n        this.downFlow = downFlow;\n    }\n\n    public long getSumFlow() {\n        return sumFlow;\n    }\n\n    public void setSumFlow(long sumFlow) {\n        this.sumFlow = sumFlow;\n    }\n    public void setSumFlow() {\n        this.sumFlow = this.upFlow + this.downFlow;\n    }\n\n    @Override\n    public void write(DataOutput out) throws IOException {\n\n        out.writeLong(upFlow);\n        out.writeLong(downFlow);\n        out.writeLong(sumFlow);\n    }\n\n    @Override\n    public void readFields(DataInput in) throws IOException {\n        this.upFlow = in.readLong();\n        this.downFlow = in.readLong();\n        this.sumFlow = in.readLong();\n    }\n\n    @Override\n    public String toString() {\n        return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow;\n    }\n}\n\n```\n\n② 编写 Mapper 类\n\n```java\npackage com.layne.mapreduce.writable;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class FlowMapper extends Mapper<LongWritable, Text, Text, FlowBean> {\n\n    private Text outK = new Text();\n    private FlowBean outV = new FlowBean();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        // 1 获取一行\n        // 1\t13736230513\t192.196.100.1\twww.atguigu.com\t2481\t24681\t200\n        String line = value.toString();\n\n        // 2 切割\n        // 1,13736230513,192.196.100.1,www.atguigu.com,2481,24681,200   7 - 3= 4\n        // 2\t13846544121\t192.196.100.2\t\t\t264\t0\t200  6 - 3 = 3\n        String[] split = line.split(\"\\t\");\n\n        // 3 抓取想要的数据\n        // 手机号：13736230513\n        // 上行流量和下行流量：2481,24681\n        String phone = split[1];\n        String up = split[split.length - 3];\n        String down  = split[split.length - 2];\n\n        // 4封装\n        outK.set(phone);\n        outV.setUpFlow(Long.parseLong(up));\n        outV.setDownFlow(Long.parseLong(down));\n        outV.setSumFlow();\n\n        // 5 写出\n        context.write(outK, outV);\n    }\n}\n\n```\n\n③ 编写 Reducer 类\n\n```java\npackage com.layne.mapreduce.writable;\n\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class FlowReducer extends Reducer<Text, FlowBean,Text, FlowBean> {\n    private FlowBean outV = new FlowBean();\n\n    @Override\n    protected void reduce(Text key, Iterable<FlowBean> values, Context context) throws IOException, InterruptedException {\n\n        // 1 遍历集合累加值\n        long totalUp = 0;\n        long totaldown = 0;\n        \n\t\t// 遍历 values,将其中的上行流量,下行流量分别累加\n        for (FlowBean value : values) {\n            totalUp += value.getUpFlow();\n            totaldown += value.getDownFlow();\n        }\n\n        // 2 封装outk, outv\n        outV.setUpFlow(totalUp);\n        outV.setDownFlow(totaldown);\n        outV.setSumFlow();\n\n        // 3 写出\n        context.write(key, outV);\n    }\n}\n\n```\n\n④ 编写 Driver 驱动类\n\n```java\npackage com.layne.mapreduce.writable;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class FlowDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        // 1 获取job\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        // 2 设置jar\n        job.setJarByClass(FlowDriver.class);\n\n        // 3 关联mapper 和Reducer\n        job.setMapperClass(FlowMapper.class);\n        job.setReducerClass(FlowReducer.class);\n\n        // 4 设置mapper 输出的key和value类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(FlowBean.class);\n\n        // 5 设置最终数据输出的key和value类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(FlowBean.class);\n\n        // 6 设置数据的输入路径和输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\test\\\\inputflow\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\test\\\\outputflow\"));\n\n        // 7 提交job\n        boolean result = job.waitForCompletion(true);\n        System.exit(result ? 0 : 1);\n    }\n}\n```\n\n⑤ 输出结果\n\n```tex\n13470253144\t180\t180\t360\n13509468723\t7335\t110349\t117684\n13560439638\t918\t4938\t5856\n13568436656\t3597\t25635\t29232\n13590439668\t1116\t954\t2070\n13630577991\t6960\t690\t7650\n13682846555\t1938\t2910\t4848\n13729199489\t240\t0\t240\n13736230513\t2481\t24681\t27162\n13768778790\t120\t120\t240\n13846544121\t264\t0\t264\n13956435636\t132\t1512\t1644\n13966251146\t240\t0\t240\n13975057813\t11058\t48243\t59301\n13992314666\t3008\t3720\t6728\n15043685818\t3659\t3538\t7197\n15910133277\t3156\t2936\t6092\n15959002129\t1938\t180\t2118\n18271575951\t1527\t2106\t3633\n18390173782\t9531\t2412\t11943\n84188413\t4116\t1432\t5548\n```\n\n\n","tags":["Hadoop"],"categories":["Hadoop"]},{"title":"五、MapReduce概述及WordCount案例实操","url":"/2021/03/31/194801/","content":"\n\n\nMapReduce概述及WordCount案例实操\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. MapReduce 定义\n\nMapReduce 是一个**分布式运算程序**的编程框架，是用户开发“基于 Hadoop 的数据分析应用”的核心框架。\n\nMapReduce 核心功能是将**用户编写的业务逻辑代码**和**自带默认组件**整合成一个完整的**分布式运算程序**，并发运行在一个 Hadoop 集群上\n\n## 2. MapReduce 优缺点\n\n**优点**\n\n1）MapReduce 易于编程\n它简单的实现一些接口，就可以完成一个分布式程序， 这个分布式程序可以分布到大量廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得 MapReduce 编程变得非常流行。\n\n2）良好的扩展性\n当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。\n\n3）高容错性\nMapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop 内部完成的。\n\n4）适合 PB 级以上海量数据的离线处理\n可以实现上千台服务器集群并发工作，提供数据处理能力。\n\n\n\n**缺点**\n\n1）不擅长实时计算\n\nMapReduce 无法像 MySQL 一样，在毫秒或者秒级内返回结果。\n\n2）不擅长流式计算\n流式计算的输入数据是动态的，而 MapReduce 的输入数据集是静态的，不能动态变化。这是因为 MapReduce自身的设计特点决定了数据源必须是静态的。\n\n3）不擅长 DAG（有向无环图）计算\n多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce 并不是不能做，而是使用后， 每个 MapReduce 作业的输出结果都会写入到磁盘，会造成大量的磁盘 IO，导致性能非常的低下。\n\n\n\n## 3. MapReduce 核心思想\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210401190408.png)\n\n（1）分布式的运算程序往往需要分成至少 2 个阶段。\n（2）第一个阶段的 MapTask 并发实例，完全并行运行，互不相干。\n（3）第二个阶段的 ReduceTask 并发实例互不相干，但是他们的数据依赖于上一个阶段的所有 MapTask 并发实例的输出。\n（4）MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce阶段（不管是Map 阶段，还是Reduce阶段，都可以有多个子Task并行处理），如果用户的业务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行。\n\n总结：分析 WordCount 数据流走向深入理解 MapReduce 核心思想。\n\n\n\n## 4. MapReduce 进程\n\n一个完整的 MapReduce 程序在分布式运行时有三类实例进程：\n\n（1）MrAppMaster：负责整个程序的过程调度及状态协调。 \n\n（2）MapTask：负责 Map 阶段的整个数据处理流程。\n\n（3）ReduceTask：负责 Reduce 阶段的整个数据处理流程。\n\n\n\n## 5. 官方 WordCount 源码\n\n采用反编译工具反编译源码，发现 WordCount 案例有 Map 类、Reduce 类和驱动类。且数据的类型是 Hadoop自身封装的序列化类型。\n\n\n\n## 6. 常用数据序列化类型\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210401191328.png)\n\n\n\n## 7. MapReduce 编程规范\n\n用户编写的程序分成三个部分：Mapper、Reducer 和 Driver。\n\n1、Mapper阶段\n（1）用户自定义的Mapper要继承相应的父类\n（2）Mapper的输入数据是KV对的形式（KV的类型可自定义）（wordCount的输入是一行一个KV，K是该行的偏移量，V是该行的内容）\n（3）Mapper中的业务逻辑写在map()方法中\n（4）Mapper的输出数据是KV对的形式（KV的类型可自定义）\n（5）map()方法（MapTask进程）对**每一个**<K,V>调用一次\n\n\n\n2、Reducer阶段\n（1）用户自定义的Reducer要继承自己的父类\n（2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV\n（3）Reducer的业务逻辑写在reduce()方法中\n（4）ReduceTask进程**对每一组相同k**的<k,v>组调用一次reduce()方法\n\n\n\n3、Driver阶段\n相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象\n\n\n\n## 8. WordCount 案例实操\n\n\n\n### 8.1 本地测试\n\n（1）**需求**\n\n在给定的文本文件中统计输出每一个单词出现的总次数\n\n① 输入数据hello.txt\n\n```tex\natguigu atguigu\nss ss\ncls cls\njiao\nbanzhang\nxue\nhadoop\n```\n\n② 期望输出数据\n\n```tex\natguigu  2\nbanzhang  1\ncls  2\nhadoop  1\njiao  1\nss  2\nxue  1\n```\n\n\n\n（2）需求分析\n\n按照MapReduce编程规范，分别编写Mapper，Reducer，Driver。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210401205205.png)\n\n（3）**环境准备**\n\n① 创建 maven 工程，MapReduceDemo\n\n② 在 pom.xml 文件中添加如下依赖\n\n```xml\n<dependencies>\n    <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-client</artifactId>\n        <version>3.1.3</version>\n    </dependency>\n    <dependency>\n        <groupId>junit</groupId>\n        <artifactId>junit</artifactId>\n        <version>4.12</version>\n    </dependency>\n    <dependency>\n        <groupId>org.slf4j</groupId>\n        <artifactId>slf4j-log4j12</artifactId>\n        <version>1.7.30</version>\n    </dependency>\n</dependencies>\n```\n\n③ 在项目的 src/main/resources 目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。\n\n```prop\nlog4j.rootLogger=INFO, stdout\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n\nlog4j.appender.logfile=org.apache.log4j.FileAppender\nlog4j.appender.logfile.File=target/spring.log\nlog4j.appender.logfile.layout=org.apache.log4j.PatternLayout\nlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n\n```\n\n④ 创建包名：com.layne.mapreduce.wordcount\n\n\n\n（4）**编写程序**\n\n① 编写 Mapper 类\n\n```java\npackage com.layne.mapreduce.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * KEYIN, map阶段输入的key的类型：LongWritable\n * VALUEIN,map阶段输入value类型：Text\n * KEYOUT,map阶段输出的Key类型：Text\n * VALUEOUT,map阶段输出的value类型：IntWritable\n */\npublic class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n    private Text outK = new Text();\n    private IntWritable outV = new IntWritable(1);\n\n    //Called once for each key/value pair in the input split\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        // 1 获取一行\n        // 进来的内容：atguigu atguigu\n        String line = value.toString();\n\n        // 2 切割\n        // atguigu\n        // atguigu\n        String[] words = line.split(\" \");\n\n        // 3 循环写出\n        for (String word : words) {\n            // 封装outk\n            outK.set(word);\n\n            // 写出\n            context.write(outK, outV);\n        }\n    }\n}\n\n```\n\n② 编写 Reducer 类\n\n```java\npackage com.layne.mapreduce.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n/**\n * KEYIN, reduce阶段输入的key的类型：Text\n * VALUEIN,reduce阶段输入value类型：IntWritable\n * KEYOUT,reduce阶段输出的Key类型：Text\n * VALUEOUT,reduce阶段输出的value类型：IntWritable\n */\npublic class WordCountReducer extends Reducer<Text, IntWritable,Text,IntWritable> {\n    private IntWritable outV = new IntWritable();\n\n    //This method is called once for each key\n    @Override\n    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n\n        int sum = 0;\n        // atguigu, (1,1)\n        // 累加\n        for (IntWritable value : values) {\n            sum += value.get();\n        }\n\n        outV.set(sum);\n\n        // 写出\n        context.write(key,outV);\n    }\n}\n```\n\n③ 编写 Driver 驱动类\n\n```java\npackage com.layne.mapreduce.wordcount;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class WordCountDriver {\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        // 1 获取job\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        // 2 设置jar包路径\n        job.setJarByClass(WordCountDriver.class);\n\n        // 3 关联mapper和reducer\n        job.setMapperClass(WordCountMapper.class);\n        job.setReducerClass(WordCountReducer.class);\n\n        // 4 设置map输出的kv类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        // 5 设置最终输出的kV类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 6 设置输入路径和输出路径\n        FileInputFormat.setInputPaths(job, new Path(\"D:\\\\test\\\\inputword\"));\n        FileOutputFormat.setOutputPath(job, new Path(\"D:\\\\test\\\\outputword\"));\n\n        // 7 提交job，参数为true，则monitorAndPrintJob\n        boolean result = job.waitForCompletion(true);\n\n        System.exit(result ? 0 : 1);\n    }\n}\n```\n\n现在执行WordCountDriver，如果报如下错误：\n\n```tex\nException in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n```\n\n解决方法：将hadoop.dll文件放到C:\\Windows\\System32里面就好了\n\n执行完毕后，在`D:\\\\test\\\\outputword`目录下会得到输出结果\n\n```tex\natguigu\t2\nbanzhang\t1\ncls\t2\nhadoop\t1\njiao\t1\nss\t2\nxue\t1\n```\n\n\n\n注意，运行以上程序需要首先配置好 HADOOP_HOME 变量以及 Windows 运行依赖，然后重启IDE即可。\n\n\n\n\n\n### 8.2 提交到集群测试\n\n上面相当于以本地模式的方式运行，并没有在通过集群运行\n\n（1）用 maven 打 jar 包，需要添加的打包插件依赖\n\n```properties\n    <build>\n        <plugins>\n            <plugin>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.6.1</version>\n                <configuration>\n                    <!-- jdk1.8-->\n                    <source>1.8</source>\n                    <target>1.8</target>\n                </configuration>\n            </plugin>\n\n            <!--下面这个插件用于打印带有依赖的jar包-->\n            <plugin>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <configuration>\n                    <descriptorRefs>\n                        <descriptorRef>jar-with-dependencies</descriptorRef>\n                    </descriptorRefs>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n注意：如果工程上显示红叉。在项目上右键->maven->Reimport 刷新即可\n\n（2）修改程序\n\n新建一个包`com.layne.mapreduce.wordcount2`，复制Driver、Mapper、Reducer，并修改Diver的输入和输出路径，设置为可以传参的方式。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210401231241.png)\n\n（3）将程序打成 jar 包\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210401231424.png)\n\n（4）修改不带依赖的 jar 包名称 为 wc.jar ，并拷贝该 jar 包到 Hadoop 集群 的`/opt/module/hadoop-3.1.3` 路径。\n\n（5）启动 Hadoop 集群\n\n```bash\n#在wxler1上执行\nsbin/start-dfs.sh\n#在wxler2上执行\nsbin/start-yarn.sh\n```\n\n（6）执行 WordCount 程序\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ ls\nbin   etc      lib      LICENSE.txt  logs        README.txt  share    wc.jar    weibo.txt   wuguo.txt\ndata  include  libexec  liubei.txt   NOTICE.txt  sbin        wcinput  wcoutput  weiguo.txt\n[wxler@wxler1 hadoop-3.1.3]$ hadoop jar wc.jar com.layne.mapreduce.wordcount2.WordCountDriver /input /output\n```\n\n这里的`/input`和`output`是hdfs上的路径，并不是本地路径。\n\n\n\n","tags":["Hadoop"],"categories":["Hadoop"]},{"title":"四、大数据技术之Hadoop（HDFS）","url":"/2021/03/31/191801/","content":"\n\n\n大数据技术之Hadoop（HDFS）\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n## 一、HDFS 概述\n\n### 1.1 HDFS 产出背景及定义\n\n1）HDFS 产生背景\n\n随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。**HDFS 只是分布式文件管理系统中的一种**。\n\n2）HDFS 定义\n\nHDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件； 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。HDFS 的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。\n\nHDFS 的使用场景：**适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变**。\n\n\n\n### 1.2 HDFS 优缺点\n\n**HDFS优点**\n\n1）高容错性\n\n- 数据自动保存多个副本。它通过增加副本的形式，提高容错性。\n  ![](https://img-blog.csdnimg.cn/img_convert/3cffd4c503bfc5da975fc9770b56c5ee.png)\n- 某一个副本丢失以后，它可以自动恢复。\n  ![](https://img-blog.csdnimg.cn/img_convert/7a0601207392c83e8f142cbac2a8b8e6.png)\n\n2）适合处理大数据\n\n- 数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；\n- 文件规模：**能够处理百万规模以上的文件数量**，数量相当之大。\n\n3）可构建在廉价机器上，通过多副本机制，提高可靠性。\n\n**HDFS缺点**\n\n1）不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。\n\n2）无法高效的对大量小文件进行存储。\n\n- 存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；\n- 小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。\n- HDFS的block大小可以设置，默认为128M，一个文件块目录都是占用150字节，128M能存储9亿个小文件目录信息\n\n3）不支持并发写入、文件随机修改。\n\n- 一个文件只能有一个写，不允许多个线程同时写\n- 仅支持数据append（追加），不支持文件的随机修改。\n\n\n\n\n\n### 1.3 HDFS 组成架构\n\n![](https://img-blog.csdnimg.cn/img_convert/075dbeb1b68eb34684719f791ca59dbb.png)\n\n1）NameNode（nn）：就是Master，它是一个主管、管理者。\n（1）管理HDFS的名称空间；\n（2）配置副本策略；\n（3）管理数据块（Block）映射信息；\n（4）处理客户端读写请求。\n\n2）DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。\n（1）存储实际的数据块；\n（2）执行数据块的读/写操作。\n\n3）Client：就是客户端。\n（1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传；\n（2）与NameNode交互，获取文件的位置信息；\n（3）与DataNode交互，读取或者写入数据；\n（4）Client提供一些命令来管理HDFS，比如NameNode格式化；\n（5）Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作；\n\n4）SecondaryNameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。\n\n（1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode ；\n（2）在紧急情况下，可辅助恢复NameNode。\n\n\n\n### 1.4 HDFS 文件块大小（面试重点）\n\nHDFS 中的文件在物理上是分块存储 （Block ） ， 块的大小可以通过配置参数( dfs.blocksize）来规定，默认大小在Hadoop2.x/3.x版本中是128M，1.x版本中是64M。\n\n![](https://img-blog.csdnimg.cn/img_convert/f77c01c7cd791d34076c5da493807438.png)\n\n**思考**：为什么块的大小不能设置太小，也不能设置太大？\n\n（1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；\n\n（2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。\n\n总结：HDFS块的大小设置主要取决于磁盘传输速率。对于一般硬盘来说，传输速率为100M/s，一般设置块的大小128M，因为128是2的7次方，最接近于100M。固态硬盘一般传输速率为200M/s~300M/s，可以设置块大小为256M。在企业，128M和256M是常用的块大小。\n\n\n\n## 二、HDFS 的 Shell 操作\n\n\n\n### 2.1 基本语法\n\n```bash\nhadoop fs  具体命令\n```\n\n或者\n\n```bash\nhdfs dfs  具体命令\n```\n\n两个是完全相同的。\n\n### 2.2 命令大全\n\n```bash\n[wxler@wxler1 ~]$ cd /opt/module/hadoop-3.1.3/\n[wxler@wxler1 hadoop-3.1.3]$ bin/hadoop fs\nUsage: hadoop fs [generic options]\n\t[-appendToFile <localsrc> ... <dst>]\n\t[-cat [-ignoreCrc] <src> ...]\n\t[-checksum <src> ...]\n\t[-chgrp [-R] GROUP PATH...]\n\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\n\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]\n\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n\t[-df [-h] [<path> ...]]\n\t[-du [-s] [-h] [-v] [-x] <path> ...]\n\t[-expunge]\n\t[-find <path> ... <expression> ...]\n\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n\t[-getfacl [-R] <path>]\n\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n\t[-head <file>]\n\t[-help [cmd ...]]\n\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n\t[-mkdir [-p] <path> ...]\n\t[-moveFromLocal <localsrc> ... <dst>]\n\t[-moveToLocal <src> <localdst>]\n\t[-mv <src> ... <dst>]\n\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n\t[-setfattr {-n name [-v value] | -x name} <path>]\n\t[-setrep [-R] [-w] <rep> <path> ...]\n\t[-stat [format] <path> ...]\n\t[-tail [-f] [-s <sleep interval>] <file>]\n\t[-test -[defsz] <path>]\n\t[-text [-ignoreCrc] <src> ...]\n\t[-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]\n\t[-touchz <path> ...]\n\t[-truncate [-w] <length> <path> ...]\n\t[-usage [cmd ...]]\n\nGeneric options supported are:\n-conf <configuration file>        specify an application configuration file\n-D <property=value>               define a value for a given property\n-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n-jt <local|resourcemanager:port>  specify a ResourceManager\n-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n\nThe general command line syntax is:\ncommand [genericOptions] [commandOptions]\n\n[wxler@wxler1 hadoop-3.1.3]$ \n\n```\n\n### 2.3 常用命令实操\n\n#### 2.3.1 准备工作\n\n1）启动 Hadoop 集群（方便后续的测试）\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$  sbin/start-dfs.sh\n[wxler@wxler2 hadoop-3.1.3]$  sbin/start-yarn.sh\n```\n\n或者\n\n```bash\njpsall\n```\n\n2）-help：输出这个命令参数\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -help rm\n-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ... :\n  Delete all files that match the specified file pattern. Equivalent to the Unix\n  command \"rm <src>\"\n                                                                                 \n  -f          If the file does not exist, do not display a diagnostic message or \n              modify the exit status to reflect an error.                        \n  -[rR]       Recursively deletes directories.                                   \n  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>.  \n  -safely     option requires safety confirmation, if enabled, requires          \n              confirmation before deleting large directory with more than        \n              <hadoop.shell.delete.limit.num.files> files. Delay is expected when\n              walking over large directory recursively to count the number of    \n              files to be deleted before the confirmation.   \n```\n\n3）创建/sanguo 文件夹\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -mkdir /sanguo\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -mkdir /jinguo\n```\n\n\n\n#### 2.3.2 上传\n\n1）-moveFromLocal：从本地**移动**粘贴到 HDFS\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ vim shuguo.txt\n输入：\nshuguo\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs  -moveFromLocal ./shuguo.txt /sanguo\n```\n\n如果移动到的`/sanguo`目录存在，则移动到该目录下，否则，将`/sanguo`作为移动后的文件，以下所有命令均如此。\n\n2）-copyFromLocal：从本地文件系统中拷贝文件到 HDFS 路径去\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ vim weiguo.txt\n输入：\nweiguo\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -copyFromLocal weiguo.txt /sanguo\n```\n\n3）-put：等同于 copyFromLocal，生产环境更习惯用 put\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ vim weiguo.txt\n输入：\nweiguo\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -put weiguo.txt /sanguo\n```\n\n4）-appendToFile：将一个文件中的内容追加到已经存在的文件末尾\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ vim liubei.txt\n输入：\nliubei\n[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt\n```\n\n\n\n#### 2.3.3 下载\n\n1）-copyToLocal：从 HDFS 拷贝到本地\n\n```bash\nhadoop fs -copyToLocal /sanguo/shuguo.txt ./\n```\n\n2）-get：等同于 copyToLocal，生产环境更习惯用 get\n\n```bash\nhadoop fs -get /sanguo/shuguo.txt ./\n```\n\n\n\n#### 2.3.4 HDFS 直接操作\n\n1）-ls:  显示目录信息\n\n```bash\nhadoop fs -ls /sanguo\n```\n\n2）-cat：显示文件内容\n\n```bash\nhadoop fs -cat /sanguo/shuguo.txt\n```\n\n3）-chgrp、-chmod、-chown：Linux 文件系统中的用法一样，修改文件所属权限\n\n```bash\nhadoop fs  -chmod 666 /sanguo/shuguo.txt\nhadoop fs  -chown wxler:wxler /sanguo/shuguo.txt\n```\n\n4）-mkdir：创建路径\n\n```bash\nhadoop fs -mkdir /jinguo\n```\n\n5）-cp：从 HDFS 的一个路径拷贝到 HDFS 的另一个路径\n\n```bash\nhadoop fs -cp /sanguo/shuguo.txt /jinguo\n```\n\n6）-mv：在 HDFS 目录中移动文件\n\n```bash\nhadoop fs -mv /sanguo/wuguo.txt /jinguo\nhadoop fs -mv /sanguo/weiguo.txt /jinguo\n```\n\n7）-tail：显示一个文件的末尾 1kb 的数据\n\n```bash\nhadoop fs -tail /sanguo/shuguo.txt\n```\n\n8）-rm：删除文件或文件夹\n\n```bash\nhadoop fs -rm /sanguo/shuguo.txt\n```\n\n9）-rm -r：递归删除目录及目录里面内容\n\n```bash\nhadoop fs -rm -r /sanguo\n```\n\n10）-du 统计文件夹的大小信息\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -du -s -h /jinguo\n13  39  /jinguo\n\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -du  -h /jinguo\n7  21  /jinguo/shuguo.txt\n6  18  /jinguo/wuguo.txt\n```\n\n11）-setrep：设置 HDFS 中文件的副本数量\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -setrep 10 /jinguo/shuguo.txt\nReplication 10 set: /jinguo/shuguo.txt\n```\n\n![](https://img-blog.csdnimg.cn/img_convert/d530cf885b520fadcc2495922c0ec12d.png)\n\n\n\n这里设置的副本数只是记录在 NameNode 的元数据中，**是否真的会有这么多副本，还得看DataNode 的数量**。因为目前只有 3 台设备，最多也就 3 个副本，只有节点数的增加到 10台时，副本数才能达到 10。\n\n\n\n\n## 三、HDFS 的 API 操作\n\n### 3.1 客户端环境准备\n1） 找到资料包路径下的 Windows 依赖文件夹， 拷贝 hadoop-3.1.0 到非中文路径 （比如 d:\\） 。\n2）配置 HADOOP_HOME 环境变量\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021040108215712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n3）配置 Path 环境变量。\n注意：如果环境变量不起作用，可以重启电脑试试。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401082216124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n验证 Hadoop 环境变量是否正常。双击 winutils.exe，如果报如下错误。说明缺少微软运行库 （正版系统往往有这个问题）。再资料包里面有对应的微软运行库安装包双击安装即可。\n\n4）在 IDEA 中创建一个 Maven 工程 HdfsClientDemo，并导入相应的依赖坐标+日志添加\n\n```xml\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>3.1.3</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n        </dependency>\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n            <version>1.7.30</version>\n        </dependency>\n    </dependencies>\n```\n\n在项目的 src/main/resources 目录下，新建一个文件，命名为“log4j.properties”，在文件中填入\n\n```properties\nlog4j.rootLogger=INFO, stdout\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n\nlog4j.appender.logfile=org.apache.log4j.FileAppender\nlog4j.appender.logfile.File=target/spring.log\nlog4j.appender.logfile.layout=org.apache.log4j.PatternLayout\nlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n\n```\n\n5）创建包名：com.layne.hdfs\n\n6）创建 HdfsClient 类\n\n```java\n    private FileSystem fs;\n\n    @Before\n    public void init() throws URISyntaxException, IOException, InterruptedException {\n        // 连接的集群nn地址，一定要获取NameNode的地址\n        URI uri = new URI(\"hdfs://wxler1:8020\");\n        // 创建一个配置文件\n        Configuration configuration = new Configuration();\n        //configuration.set(\"dfs.replication\", \"2\");\n        // 用户\n        String user = \"wxler\";\n\n        // 1 获取到了客户端对象\n        fs = FileSystem.get(uri, configuration, user);\n    }\n\n    @After\n    public void close() throws IOException {\n        // 3 关闭资源\n        fs.close();\n    }\n\n    // 创建目录\n    @Test\n    public void testmkdir() throws URISyntaxException, IOException, InterruptedException {\n        // 2 创建一个文件夹\n        fs.mkdirs(new Path(\"/xiyou/huaguoshan1\"));\n    }\n```\n\n 7）执行程序\n\n客户端去操作 HDFS 时，是有一个用户身份的。默认情况下， HDFS 客户端 API 会从采用 Windows 默认用户访问 HDFS，会报权限异常错误。所以在访问 HDFS 时，一定要配置用户。\n\n```bash\norg.apache.hadoop.security.AccessControlException: Permission denied: user=56576, access=WRITE, inode=\"/xiyou/huaguoshan\":atguigu:supergroup:drwxr-xr-x\n```\n\n### 3.2 HDFS 的 API 案例实操\n#### 3.2.1 HDFS 文件上传（测试参数优先级）\n1）编写源代码\n\n```java\n    /**\n     * 参数优先级: 低=》高\n     * hdfs-default.xml => hdfs-site.xml=> 在项目资源目录下的配置文件 =》代码里面的配置\n     *\n     * @throws IOException\n     */\n    @Test\n    public void testPut() throws IOException {\n        // 参数解读：参数一：表示删除原数据； 参数二：是否允许覆盖；参数三：原数据路径； 参数四：目的地路径\n        fs.copyFromLocalFile(false, true, new Path(\"D:\\\\test\\\\sunwukong.txt\"), new Path(\"hdfs://wxler1/xiyou/huaguoshan\"));\n    }\n    \n    @Test\n    public void testPut2() throws IOException {\n    \t//如果目录不存在会自动创建目录，并把最后一级目录（这里为testhello）作为文件名\n        FSDataOutputStream fos = fs.create(new Path(\"/test/testHello\"));\n\n        fos.write(\"hello world\".getBytes());\n    }\n```\n\n2）resources 资源目录下新建hdfs-site.xml设置如下内容\n\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n</configuration>\n```\n\n\n3）参数优先级\n参数优先级排序： （1） 客户端代码中设置的值  > （2） ClassPath 下的用户自定义配置文件  > （3） 然后是服务器的自定义配置 （xxx-site.xml）   > （4）服务器的默认配置 （xxx-default.xml）\n\n\n#### 3.2.3 HDFS 文件下载\n\n```java\n    // 文件下载\n    @Test\n    public void testGet() throws IOException {\n        // 参数的解读：参数一：原文件是否删除；参数二：原文件路径HDFS； 参数三：目标地址路径Win ; 参数四：false开启校验，true不开启校验\n        fs.copyToLocalFile(false, new Path(\"/input/word.txt\"), new Path(\"D:\\\\test\\\\\"), false);\n                                            //这里path的hdfs://wxler1/前缀加不加都行\n        //fs.copyToLocalFile(false, new Path(\"hdfs://wxler1/input/word.txt\"), new Path(\"D:\\\\test\\\\\"), false);\n    }\n```\n注意：如果执行上面代码，下载不了文件，有可能是你电脑的微软支持的运行库少，需要安装一下微软运行库。\n\n#### 3.2.3 HDFS 文件更名和移动\n\n```java\n    // 文件的更名和移动\n    @Test\n    public void testmv() throws IOException {\n        // 参数解读：参数1 ：原文件路径； 参数2 ：目标文件路径\n        // 对文件名称的修改\n        //fs.rename(new Path(\"/input/word.txt\"), new Path(\"/input/ss.txt\"));\n\n        // 文件的移动和更名\n        //fs.rename(new Path(\"/input/ss.txt\"),new Path(\"/cls.txt\"));\n\n        // 目录更名\n        fs.rename(new Path(\"/input\"), new Path(\"/output\"));\n\n    }\n```\n\n#### 3.2.4 HDFS 删除文件和目录\n\n```java\n    // 删除\n    @Test\n    public void testRm() throws IOException {\n\n        // 参数解读：参数1：要删除的路径； 参数2 ： 是否递归删除\n        // 删除文件\n        //fs.delete(new Path(\"/jdk-8u212-linux-x64.tar.gz\"),false);\n\n        // 删除空目录\n        //fs.delete(new Path(\"/xiyou\"), false);\n\n        // 删除非空目录\n        fs.delete(new Path(\"/jinguo\"), true);\n    }\n```\n\n#### 3.2.5 HDFS 文件详情查看\n查看文件名称、权限、长度、块信息\n\n```java\n    // 获取文件详细信息\n    @Test\n    public void fileDetail() throws IOException {\n\n        // 获取所有文件信息\n        RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path(\"/\"), true);\n\n        // 遍历文件\n        while (listFiles.hasNext()) {\n            LocatedFileStatus fileStatus = listFiles.next();\n\n            System.out.println(\"==========\" + fileStatus.getPath() + \"=========\");\n            System.out.println(fileStatus.getPermission());\n            System.out.println(fileStatus.getOwner());\n            System.out.println(fileStatus.getGroup());\n            System.out.println(fileStatus.getLen());\n            System.out.println(fileStatus.getModificationTime());\n            System.out.println(fileStatus.getReplication());\n            System.out.println(fileStatus.getBlockSize());\n            System.out.println(fileStatus.getPath().getName());\n\n            // 获取块信息\n            BlockLocation[] blockLocations = fileStatus.getBlockLocations();\n\n            System.out.println(Arrays.toString(blockLocations));\n\n        }\n    }\n```\n\n#### 3.2.6 HDFS 文件和文件夹判断\n\n```java\n    // 判断是文件夹还是文件\n    @Test\n    public void testFile() throws IOException {\n\n        FileStatus[] listStatus = fs.listStatus(new Path(\"/\"));\n\n        for (FileStatus status : listStatus) {\n\n            if (status.isFile()) {\n                System.out.println(\"文件：\" + status.getPath().getName());\n            } else {\n                System.out.println(\"目录：\" + status.getPath().getName());\n            }\n        }\n    }\n```\n\n\n\n\n## 四.、HDFS 写数据流程\n\n### 4.1  剖析文件写入\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401113651975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（1）客户端通过 DistributedFileSystem 模块（或对象）向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。\n（2）NameNode 返回是否可以上传。\n（3）客户端请求第一个  Block 上传到哪几个 DataNode 服务器上。\n（4）NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。（这一步要考虑服务器是否可用、距离最近原则、DataNode负载均衡）\n（5） 客户端通过 FSDataOutputStream 模块**请求** dn1（即DataNode1） 上传数据， dn1 收到请求会继续调用dn2，然后 dn2 调用 dn3，将这个**通信管道建立完成**。\n（6）dn1、dn2、dn3 逐级应答客户端\n（7） 客户端开始往 dn1 上传第一个 Block （先从磁盘读取数据放到一个本地内存缓存），以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3。【这里传输的packet大小是64K，这个64K的packet包含多个（chunk和chunksum），一个chunk是512byte，其校验码chunksum是4byte】。dn1 每传一个 packet会放入一个应答队列等待应答，当所有的DataNode应答成功后，会将该packet从应答队列中移除。\n（8） 当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务\n器。（重复执行 3-7 步）。\n\n### 4.2 网络拓扑-节点距离计算\n在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接收数据。那么这个最近距离怎么计算呢？\n\n节点距离：**两个节点到达最近的共同祖先的距离总和**（其实就是数路径个数）。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401115450614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n例如，假设有数据中心 d1 机架 r1 中的节点 n1。该节点可以表示为/d1/r1/n1。利用这种标记，上图给出四种距离描述。\n\n大家算一算每两个节点之间的距离。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401115602733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n- 9和3的距离是3\n- 10和4的距离是3\n- 9和6的距离是2\n\n### 4.3 机架感知（副本存储节点选择）\n\n1）**机架感知说明**\n（1）官方说明\n[http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication](http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication)\n\nFor the common case, when the replication factor is three, HDFS’s placement policy is to **put one replica on the local machine if the writer is on a datanode**, otherwise on a random datanode, **another replica on a node in a different (remote) rack**, and the **last on a different node in the same remote rack**. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance.\n\n（2）源码说明\nCrtl + n  查找 BlockPlacementPolicyDefault，在该类中查找 chooseTargetInOrder 方法。\n\n2）Hadoop3.1.3 **副本节点选择**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401140059609.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n- 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。\n- 第二个副本在另一个机架的随机一个节点\n- 第三个副本在第二个副本所在机架的随机节点\n\n\n\n## 五、HDFS 读数据流程\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401140931942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（1）客户端通过 DistributedFileSystem （对象或模块）向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。\n（2）挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。（这一步要考虑就近原则，负载均衡）\n（3）DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验），串行读取，即先读取第一个块，再读取第二个块拼接到上一个块后面。\n（4）客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。\n\n\n## 六、NameNode 和 SecondaryNameNode\n\n### 6.1 NN 和 2NN 工作机制\n思考：NameNode 中的元数据是存储在哪里的？\n首先，我们做个假设，如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。 **因此产生在磁盘中备份元数据的FsImage**。\n\n这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数据丢失。因此，引入Edits 文件（只进行追加操作，效率很高）。**每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits中**。这样，一旦 NameNode 节点断电，可以通过 FsImage 和 Edits 的合并，合成元数据。\n\n但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这个操作由 NameNode 节点完成，又会效率过低。 因此，引入一个新的节点 SecondaryNamenode，专门用于 FsImage 和 Edits 的合并。\n\n**NameNode工作机制**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401145044387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n1）第一阶段：NameNode 启动\n（1）第一次启动 NameNode 格式化后，创建 fsimage 和 edits_inprogress_001 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。\n（2）客户端对元数据进行增删改的请求。\n（3）NameNode 记录操作日志，更新滚动日志（即先将客户端的操作追加到edits_inprogress_001 中）。\n（4）NameNode 在内存中对元数据进行增删改（然后再对元数据进行修改）。\n\n2）第二阶段：Secondary NameNode 工作\n（1）Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode是否检查结果。\n（2）Secondary NameNode 请求执行 CheckPoint（即请求服务）。\n（3）NameNode 滚动正在写的 edits_inprogress_001 日志，将其命名为edits_001，并生产新的日志文件edits_inprogress_002，以后再有客户端操作，日志将记录到edits_inprogress_002中。\n（4）将滚动前的编辑日志edits_001和镜像文件fsimage拷贝到 SecondaryNameNode。\n（5）Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。\n（6）生成新的镜像文件 fsimage.chkpoint。\n（7）拷贝 fsimage.chkpoint 到 NameNode。\n（8）NameNode 将 fsimage.chkpoint 重新命名成 fsimage。\n\n\n### 6.2 Fsimage 和 Edits 解析\nNameNode被格式化之后，将在`/opt/module/hadoop-3.1.3/data/dfs/name/current`目录中产生如下文件\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401151910852.png)\n\n（1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息（保存的是文件目录信息、文件目录和文件的关系，文件和目录的拥有者和权限等信息，并不存储每个文件的block在哪个节点的哪个位置）。\n（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。\n（3）seen_txid文件保存的是一个数字，就是最后一个`edits_`的数字\n\n\n1）oiv 查看 Fsimage 文件\n（1）查看 oiv 和 oev 命令\n\n```bash\n[wxler@wxler1 current]$ hdfs oiv\nUsage: bin/hdfs oiv [OPTIONS] -i INPUTFILE -o OUTPUTFILE\nOffline Image Viewer\nView a Hadoop fsimage INPUTFILE using the specified PROCESSOR,\nsaving the results in OUTPUTFILE.\n\n[wxler@wxler1 current]$ hdfs oev\nUsage: bin/hdfs oev [OPTIONS] -i INPUT_FILE -o OUTPUT_FILE\nOffline edits viewer\nParse a Hadoop edits log file INPUT_FILE and save results\nin OUTPUT_FILE.\n```\n（2）基本语法\n\n```tex\nhdfs oiv -p  输出后的件类型  -i 镜像文件  -o  转换后文件输出路径\n```\n（3）案例实操\n\n```bash\n[wxler@wxler1 current]$ pwd\n/opt/module/hadoop-3.1.3/data/dfs/name/current\n[wxler@wxler1 current]$ hdfs oiv -p XML -i fsimage_0000000000000000785 -o /opt/software/fsimage.xml\n2021-04-01 15:27:49,788 INFO offlineImageViewer.FSImageHandler: Loading 3 strings\n```\n将显示的 xml 文件内容拷贝到 Idea 中创建的 xml 文件中，并格式化。显示结果如下。\n\n```xml\n<?xml version=\"1.0\"?>\n<fsimage><version><layoutVersion>-64</layoutVersion><onDiskVersion>1</onDiskVersion><oivRevision>ba631c436b806728f8ec2f54ab1e289526c90579</oivRevision></version>\n<NameSection><namespaceId>1997025465</namespaceId><genstampV1>1000</genstampV1><genstampV2>1088</genstampV2><genstampV1Limit>0</genstampV1Limit><lastAllocatedBlockId>1073741910</lastAllocatedBlockId><txid>785</txid></NameSection>\n<ErasureCodingSection>\n<erasureCodingPolicy>\n<policyId>1</policyId><policyName>RS-6-3-1024k</policyName><cellSize>1048576</cellSize><policyState>DISABLED</policyState><ecSchema>\n<codecName>rs</codecName><dataUnits>6</dataUnits><parityUnits>3</parityUnits></ecSchema>\n</erasureCodingPolicy>\n\n<erasureCodingPolicy>\n<policyId>2</policyId><policyName>RS-3-2-1024k</policyName><cellSize>1048576</cellSize><policyState>DISABLED</policyState><ecSchema>\n<codecName>rs</codecName><dataUnits>3</dataUnits><parityUnits>2</parityUnits></ecSchema>\n</erasureCodingPolicy>\n\n<erasureCodingPolicy>\n<policyId>3</policyId><policyName>RS-LEGACY-6-3-1024k</policyName><cellSize>1048576</cellSize><policyState>DISABLED</policyState><ecSchema>\n<codecName>rs-legacy</codecName><dataUnits>6</dataUnits><parityUnits>3</parityUnits></ecSchema>\n</erasureCodingPolicy>\n\n<erasureCodingPolicy>\n<policyId>4</policyId><policyName>XOR-2-1-1024k</policyName><cellSize>1048576</cellSize><policyState>DISABLED</policyState><ecSchema>\n<codecName>xor</codecName><dataUnits>2</dataUnits><parityUnits>1</parityUnits></ecSchema>\n</erasureCodingPolicy>\n\n<erasureCodingPolicy>\n<policyId>5</policyId><policyName>RS-10-4-1024k</policyName><cellSize>1048576</cellSize><policyState>DISABLED</policyState><ecSchema>\n<codecName>rs</codecName><dataUnits>10</dataUnits><parityUnits>4</parityUnits></ecSchema>\n</erasureCodingPolicy>\n\n</ErasureCodingSection>\n\n<INodeSection><lastInodeId>16577</lastInodeId><numInodes>42</numInodes><inode><id>16385</id><type>DIRECTORY</type><name></name><mtime>1617241935401</mtime><permission>wxler:supergroup:0755</permission><nsquota>9223372036854775807</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16386</id><type>DIRECTORY</type><name>input</name><mtime>1617179030778</mtime><permission>wxler:supergroup:0755</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16387</id><type>FILE</type><name>word.txt</name><replication>3</replication><mtime>1617179030747</mtime><atime>1617241762636</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><blocks><block><id>1073741825</id><genstamp>1001</genstamp><numBytes>41</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16388</id><type>FILE</type><name>jdk-8u212-linux-x64.tar.gz</name><replication>3</replication><mtime>1617179195102</mtime><atime>1617200154241</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><blocks><block><id>1073741826</id><genstamp>1002</genstamp><numBytes>134217728</numBytes></block>\n<block><id>1073741827</id><genstamp>1003</genstamp><numBytes>60795424</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16481</id><type>DIRECTORY</type><name>tmp</name><mtime>1617193613381</mtime><permission>wxler:supergroup:0700</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16482</id><type>DIRECTORY</type><name>hadoop-yarn</name><mtime>1617183460322</mtime><permission>wxler:supergroup:0700</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16483</id><type>DIRECTORY</type><name>staging</name><mtime>1617183479242</mtime><permission>wxler:supergroup:0700</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16484</id><type>DIRECTORY</type><name>wxler</name><mtime>1617183460322</mtime><permission>wxler:supergroup:0700</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16485</id><type>DIRECTORY</type><name>.staging</name><mtime>1617193660199</mtime><permission>wxler:supergroup:0700</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16491</id><type>DIRECTORY</type><name>history</name><mtime>1617192501743</mtime><permission>wxler:supergroup:0755</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16492</id><type>DIRECTORY</type><name>done_intermediate</name><mtime>1617183479283</mtime><permission>wxler:supergroup:1777</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16493</id><type>DIRECTORY</type><name>wxler</name><mtime>1617193668311</mtime><permission>wxler:supergroup:0770</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16506</id><type>FILE</type><name>job_1617183364434_0001-1617183465062-wxler-word+count-1617183512363-1-1-SUCCEEDED-default-1617183485850.jhist</name><replication>3</replication><mtime>1617183512613</mtime><atime>1617192833535</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0770</permission><blocks><block><id>1073741873</id><genstamp>1049</genstamp><numBytes>22313</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16507</id><type>FILE</type><name>job_1617183364434_0001_conf.xml</name><replication>3</replication><mtime>1617183512745</mtime><atime>1617183512638</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0770</permission><blocks><block><id>1073741874</id><genstamp>1050</genstamp><numBytes>214438</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16508</id><type>DIRECTORY</type><name>done</name><mtime>1617192683004</mtime><permission>wxler:supergroup:0770</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16509</id><type>DIRECTORY</type><name>2021</name><mtime>1617192683004</mtime><permission>wxler:supergroup:0770</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16510</id><type>DIRECTORY</type><name>03</name><mtime>1617192683004</mtime><permission>wxler:supergroup:0770</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16511</id><type>DIRECTORY</type><name>31</name><mtime>1617192683004</mtime><permission>wxler:supergroup:0770</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16512</id><type>DIRECTORY</type><name>000000</name><mtime>1617193668311</mtime><permission>wxler:supergroup:0770</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16530</id><type>FILE</type><name>job_1617183364434_0002-1617192796187-wxler-word+count-1617192836116-1-1-SUCCEEDED-default-1617192808343.jhist</name><replication>3</replication><mtime>1617192837231</mtime><atime>1617192837151</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0770</permission><blocks><block><id>1073741883</id><genstamp>1059</genstamp><numBytes>22312</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16531</id><type>FILE</type><name>job_1617183364434_0002_conf.xml</name><replication>3</replication><mtime>1617192837335</mtime><atime>1617192837251</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0770</permission><blocks><block><id>1073741884</id><genstamp>1060</genstamp><numBytes>214430</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16537</id><type>DIRECTORY</type><name>logs</name><mtime>1617193613519</mtime><permission>wxler:wxler:1777</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16538</id><type>DIRECTORY</type><name>wxler</name><mtime>1617193613527</mtime><permission>wxler:wxler:0770</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16539</id><type>DIRECTORY</type><name>logs-tfile</name><mtime>1617193613532</mtime><permission>wxler:wxler:0770</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16540</id><type>DIRECTORY</type><name>application_1617193351532_0001</name><mtime>1617193667187</mtime><permission>wxler:wxler:0770</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16542</id><type>DIRECTORY</type><name>output</name><mtime>1617193657742</mtime><permission>wxler:supergroup:0755</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16548</id><type>FILE</type><name>part-r-00000</name><replication>3</replication><mtime>1617193657514</mtime><atime>1617193657020</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><blocks><block><id>1073741891</id><genstamp>1067</genstamp><numBytes>36</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16550</id><type>FILE</type><name>_SUCCESS</name><replication>3</replication><mtime>1617193657756</mtime><atime>1617193657742</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16553</id><type>FILE</type><name>job_1617193351532_0001-1617193609664-wxler-word+count-1617193657905-1-1-SUCCEEDED-default-1617193629222.jhist</name><replication>3</replication><mtime>1617193658493</mtime><atime>1617193658335</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0770</permission><blocks><block><id>1073741893</id><genstamp>1069</genstamp><numBytes>22312</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16554</id><type>FILE</type><name>job_1617193351532_0001_conf.xml</name><replication>3</replication><mtime>1617193658907</mtime><atime>1617193658550</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0770</permission><blocks><block><id>1073741894</id><genstamp>1070</genstamp><numBytes>214605</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16555</id><type>FILE</type><name>wxler2_38923</name><replication>3</replication><mtime>1617193667153</mtime><atime>1617193666741</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:wxler:0640</permission><blocks><block><id>1073741895</id><genstamp>1071</genstamp><numBytes>133415</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16562</id><type>DIRECTORY</type><name>sanguo</name><mtime>1617202922040</mtime><permission>wxler:supergroup:0755</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16564</id><type>DIRECTORY</type><name>jinguo</name><mtime>1617202922040</mtime><permission>wxler:supergroup:0755</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16565</id><type>FILE</type><name>shuguo.txt</name><replication>10</replication><mtime>1617202640991</mtime><atime>1617202640469</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><blocks><block><id>1073741902</id><genstamp>1080</genstamp><numBytes>7</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16566</id><type>FILE</type><name>wuguo.txt</name><replication>3</replication><mtime>1617202754263</mtime><atime>1617202753913</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><blocks><block><id>1073741903</id><genstamp>1081</genstamp><numBytes>6</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16567</id><type>FILE</type><name>weibo.txt</name><replication>3</replication><mtime>1617202773064</mtime><atime>1617202772773</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><blocks><block><id>1073741904</id><genstamp>1082</genstamp><numBytes>6</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16568</id><type>DIRECTORY</type><name>xiyou</name><mtime>1617241702332</mtime><permission>wxler:supergroup:0755</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16571</id><type>DIRECTORY</type><name>huaguoshan</name><mtime>1617243483208</mtime><permission>wxler:supergroup:0755</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16572</id><type>FILE</type><name>sanguo.txt</name><replication>3</replication><mtime>1617241739247</mtime><atime>1617246645971</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><blocks><block><id>1073741906</id><genstamp>1084</genstamp><numBytes>6</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16574</id><type>DIRECTORY</type><name>test</name><mtime>1617241935402</mtime><permission>wxler:supergroup:0755</permission><nsquota>-1</nsquota><dsquota>-1</dsquota></inode>\n<inode><id>16575</id><type>FILE</type><name>testHello</name><replication>3</replication><mtime>1617241935843</mtime><atime>1617241935402</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><blocks><block><id>1073741908</id><genstamp>1086</genstamp><numBytes>11</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n<inode><id>16577</id><type>FILE</type><name>sunwukong.txt</name><replication>2</replication><mtime>1617243483601</mtime><atime>1617243483208</atime><preferredBlockSize>134217728</preferredBlockSize><permission>wxler:supergroup:0644</permission><blocks><block><id>1073741910</id><genstamp>1088</genstamp><numBytes>9</numBytes></block>\n</blocks>\n<storagePolicyId>0</storagePolicyId></inode>\n</INodeSection>\n<INodeReferenceSection></INodeReferenceSection><SnapshotSection><snapshotCounter>0</snapshotCounter><numSnapshots>0</numSnapshots></SnapshotSection>\n<INodeDirectorySection><directory><parent>16385</parent><child>16386</child><child>16388</child><child>16564</child><child>16542</child><child>16562</child><child>16574</child><child>16481</child><child>16568</child></directory>\n<directory><parent>16386</parent><child>16387</child></directory>\n<directory><parent>16481</parent><child>16482</child><child>16537</child></directory>\n<directory><parent>16482</parent><child>16483</child></directory>\n<directory><parent>16483</parent><child>16491</child><child>16484</child></directory>\n<directory><parent>16484</parent><child>16485</child></directory>\n<directory><parent>16491</parent><child>16508</child><child>16492</child></directory>\n<directory><parent>16492</parent><child>16493</child></directory>\n<directory><parent>16508</parent><child>16509</child></directory>\n<directory><parent>16509</parent><child>16510</child></directory>\n<directory><parent>16510</parent><child>16511</child></directory>\n<directory><parent>16511</parent><child>16512</child></directory>\n<directory><parent>16512</parent><child>16506</child><child>16507</child><child>16530</child><child>16531</child><child>16553</child><child>16554</child></directory>\n<directory><parent>16537</parent><child>16538</child></directory>\n<directory><parent>16538</parent><child>16539</child></directory>\n<directory><parent>16539</parent><child>16540</child></directory>\n<directory><parent>16540</parent><child>16555</child></directory>\n<directory><parent>16542</parent><child>16550</child><child>16548</child></directory>\n<directory><parent>16562</parent><child>16567</child></directory>\n<directory><parent>16564</parent><child>16565</child><child>16566</child></directory>\n<directory><parent>16568</parent><child>16571</child></directory>\n<directory><parent>16571</parent><child>16572</child><child>16577</child></directory>\n<directory><parent>16574</parent><child>16575</child></directory>\n</INodeDirectorySection>\n<FileUnderConstructionSection></FileUnderConstructionSection>\n<SecretManagerSection><currentId>0</currentId><tokenSequenceNumber>0</tokenSequenceNumber><numDelegationKeys>0</numDelegationKeys><numTokens>0</numTokens></SecretManagerSection><CacheManagerSection><nextDirectiveId>1</nextDirectiveId><numDirectives>0</numDirectives><numPools>0</numPools></CacheManagerSection>\n</fsimage>\n\n```\n思考：可以看出，Fsimage 中没有记录块所对应 DataNode，为什么？\n\n在集群启动后，要求 DataNode 上报数据块信息，并间隔一段时间后再次上报。\n\n2）oev 查看 Edits 文件\n（1）基本语法\n\n```bash\nhdfs oev -p  转换后的文件类型  -i 编辑日志  -o  转换后文件输出路径\n```\n（2）案例实操\n\n```bash\n[wxler@wxler1 current]$ hdfs oev -p XML -i edits_inprogress_0000000000000000786 -o /opt/software/edits.xml\n[wxler@wxler1 current]$ pwd\n/opt/module/hadoop-3.1.3/data/dfs/name/current\n```\n将显示的 xml 文件内容拷贝到 Idea 中创建的 xml 文件中，并格式化。显示结果如下。\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<EDITS>\n  <EDITS_VERSION>-64</EDITS_VERSION>\n  <RECORD>\n    <OPCODE>OP_START_LOG_SEGMENT</OPCODE>\n    <DATA>\n      <TXID>786</TXID>\n    </DATA>\n  </RECORD>\n</EDITS>\n\n```\n可以看到，edits记录了操作，只不过这段时间操作比较少。\n\n思考：NameNode 如何确定下次开机启动的时候合并哪些 Edits？\n可以根据seen_txid文件保存的是一个数字，seen_txid保存的就是最后一个`edits_`的数字，是最新的edits。\n\n### 6.3 CheckPoint 时间设置\n1）通常情况下，SecondaryNameNode 每隔一小时执行一次，或每执行一百万次事务执行一次（每隔60s检查一次事务操作次数）。\n查看hdfs-default.xml\n\n```bash\n<property>\n  <name>dfs.namenode.checkpoint.period</name>\n  <value>3600s</value>\n  <description>\n    The number of seconds between two periodic checkpoints.\n    Support multiple time unit suffix(case insensitive), as described\n    in dfs.heartbeat.interval.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.checkpoint.txns</name>\n  <value>1000000</value>\n  <description>The Secondary NameNode or CheckpointNode will create a checkpoint\n  of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless\n  of whether 'dfs.namenode.checkpoint.period' has expired.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.checkpoint.check.period</name>\n  <value>60s</value>\n  <description>The SecondaryNameNode and CheckpointNode will poll the NameNode\n  every 'dfs.namenode.checkpoint.check.period' seconds to query the number\n  of uncheckpointed transactions. Support multiple time unit suffix(case insensitive),\n  as described in dfs.heartbeat.interval.\n  </description>\n</property>\n```\n可以在hdfs-site.xml里进行修改\n\n\n## 七、DataNode\n### 7.1 DataNode 工作机制\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401160952275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n（1）一个数据块在 DataNode 上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。\n（2） DataNode 启动后向 NameNode 注册，通过后，周期性（6 小时） 的向 NameNode 上报所有的块信息。\n\nDN 扫描自己节点块信息列表的时间，默认 6 小时，目的是查好DN中的块是否完好\n\n```xml\n<property>\n  <name>dfs.datanode.directoryscan.interval</name>\n  <value>21600s</value>\n  <description>Interval in seconds for Datanode to scan data directories and\n  reconcile the difference between blocks in memory and on the disk.\n  Support multiple time unit suffix(case insensitive), as described\n  in dfs.heartbeat.interval.\n  </description>\n</property>\n```\n\nDN 向 NN 汇报当前解读信息的时间间隔（将完好的块信息报告给NN），默认 6 小时；\n\n```bash\n<property>\n  <name>dfs.blockreport.intervalMsec</name>\n  <value>21600000</value>\n  <description>Determines block reporting interval in milliseconds.</description>\n</property>\n```\n\n\n（3）心跳是每 3 秒一次，心跳返回结果带有 NameNode 给该 DataNode 的命令如复制块数据到另一台机器，或删除某个数据块。 如果超过 10 分钟 + 30s 没有收到某个 DataNode 的心跳，则认为该节点不可用。\n\n（4）集群运行中可以安全加入和退出一些机器。\n\n\n### 7.2 数据完整性\n思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号 （1）和绿灯信号 （0） ，但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理 DataNode 节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？\n\n思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号 （1）和绿灯信号 （0） ，但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理 DataNode 节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？\n如下是 DataNode 节点保证数据完整性的方法。\n（1）当 DataNode 读取 Block 的时候，它会计算 CheckSum。\n（2）如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。\n（3）Client 读取其他 DataNode 上的 Block。\n（4）常见的校验算法 crc（32），md5（128），sha1（160），hadoop采用的是crc（32校验）\n（5）DataNode 在其文件创建后周期验证 CheckSum。\n\n**循环冗余校验补充**\n\n在发送端，**先把数据划分为组，假定每组k个比特**。现假定待传送的数据M=101001（k=6). CRC运算就是在数据M的后面添加供差错检测用的n位冗余码，然后构成一个帧发送出去，一共发送（k+n)位。在所要发送的数据后面增加n位的冗余码，虽然增大了数据传输的开销，但却可以进行差错检测。 当传输可能出现差错时，付出这种代价往往是很值得的。\n\n![](https://img-blog.csdnimg.cn/img_convert/f416af4babf1cd6db5a25712c1315519.png)\n\n在接收端把接收到的数据以帧为单位进行 CRC 检验： 把收到的每一个帧都除以同样的除数P(模2运算），然后检查得到的余数R。如果在传输过程中无差错，那么经过CRC 检验后得出的余数R肯定是0(读者可以自己验算一下。被除数现在是101001001,而除数是P=1101,看余数R是否为0)。\n但如果出现误码，那么余数 R 仍等于零的概率是非常非常小的。\n\n\n\n### 7.3 掉线时限参数设置\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210401164830163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n需要注意的是 hdfs-site.xml  配置文件中的 heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval 的单位为秒。\n\n```bash\n<property>\n  <name>dfs.namenode.heartbeat.recheck-interval</name>\n  <value>300000</value>\n  <description>\n    This time decides the interval to check for expired datanodes.\n    With this value and dfs.heartbeat.interval, the interval of\n    deciding the datanode is stale or not is also calculated.\n    The unit of this configuration is millisecond.\n  </description>\n</property>\n\n<property>\n  <name>dfs.heartbeat.interval</name>\n  <value>3s</value>\n  <description>\n    Determines datanode heartbeat interval in seconds.\n    Can use the following suffix (case insensitive):\n    ms(millis), s(sec), m(min), h(hour), d(day)\n    to specify the time (such as 2s, 2m, 1h, etc.).\n    Or provide complete number in seconds (such as 30 for 30 seconds).\n  </description>\n</property>\n\n```\n\n\n","tags":["Hadoop"],"categories":["Hadoop"]},{"title":"三、Hadoop 运行模式","url":"/2021/03/31/191601/","content":"\nHadoop 运行模式\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\nHadoop 官方网站：http://hadoop.apache.org/\nHadoop 运行模式包括：本地模式、伪分布式模式以及完全分布式模式。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210331111804331.png)\n\n- 本地模式：单机运行，只是用来演示一下官方案例。生产环境不用。\n- 伪分布式模式：也是单机运行，但是具备 Hadoop 集群的所有功能，一台服务器模拟一个分布式的环境。个别缺钱的公司用来测试，生产环境不用。\n- 完全分布式模式：多台服务器组成分布式环境。生产环境使用。\n\n## 1. 本地运行模式（官方 WordCount）\n1）创建在 hadoop-3.1.3 文件下面创建一个 wcinput 文件夹\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ mkdir wcinput\n```\n\n2）在 wcinput 文件下创建一个 word.txt 文件\n\n```bash\ncd wcinput\n```\n3）编辑 word.txt 文件\n\n```bash\nvim word.txt\n```\n\n在文件中输入如下内容\n\n```tex\nhadoop yarn\nhadoop mapreduce\nwxler\nwxler\n```\n4）回到 Hadoop 目录/opt/module/hadoop-3.1.3\n5）执行程序\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop  jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput\n```\n- wcinput是输入目录\n- wcoutput是输出目录\n\n6）查看结果\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ cat wcoutput/part-r-00000 \nhadoop\t2\nmapreduce\t1\nwxler\t2\nyarn\t1\n```\n\n## 2.  完全分布式运行模式（开发重点）\n分析：\n1）准备 3 台客户机（关闭防火墙、静态 IP、主机名称）\n2）安装 JDK\n3）配置环境变量\n4）安装 Hadoop\n5）配置环境变量\n6）配置集群 \n7）单点启动\n8）配置 ssh\n9）群起并测试集群\n\n### 2.1 编写集群分发脚本 xsync\n1）scp（secure copy）安全拷贝\n（1）scp 定义\nscp 可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）\n（2）基本语法\n\n```bash\nscp    -r        $pdir/$fname              $user@$host:$pdir/$fname\n命令    递归      要拷贝的文件路径/名称   目的地用户@主机:目的地路径/名称\n```\n\n（3）案例实操\n前提：在 wxler1、wxler2、wxler3都已经创建好的/opt/module、/opt/software 两个目录，并且已经把这两个目录修改为 wxler:wxler\n\n```bash\nsudo  chown  wxler:wxler  -R  /opt/module\n```\n（a ）在 wxler1上，将 wxler1 中/opt/module/jdk1.8.0_212 目录拷贝到wxler2上。\n\n```bash\n[wxler@wxler1 ~]$ scp  -r  /opt/module/jdk1.8.0_212 wxler@wxler2:/opt/module\n```\n（b ）在 wxler2 上，将 wxler1中/opt/module/jdk1.8.0_212 目录拷贝到wxler2上。\n\n```bash\n[wxler@wxler2 module]$ scp  -r wxler@wxler1:/opt/module/hadoop-3.1.3 /opt/module/\n```\n（c）在 wxler2 上操作，将 wxler1中/opt/module 目录下所有目录拷贝到wxler3上。\n\n```bash\nscp  -r wxler@wxler1:/opt/module/* wxler@wxler3:/opt/module\n```\n\n2）rsync 远程同步工具\n\nrsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点\n\nrsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更新。scp 是把所有文件都复制过去。\n\n（1）基本语法\n```bash\nrsync    -av       $pdir/$fname              $user@$host:$pdir/$fname\n命令    选项参数    要拷贝的文件路径/名称  目的地用户@主机:目的地路径/名称\n```\n选项参数说明\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210331115210217.png)\n（2）案例实操\n\n（a）删除 wxler2 中/opt/module/hadoop-3.1.3/wcinput\n\n```bash\n[wxler@wxler2 hadoop-3.1.3]$ rm -rf wcinput/\n```\n（b）同步 wxler1 中的/opt/module/hadoop-3.1.3 到 wxler2\n\n```bash\nrsync  -av  hadoop-3.1.3/ wxler@wxler2:/opt/module/hadoop-3.1.3/\n```\n注意：两个服务器都要安装` yum install rsync -y`\n\n3）xsync 集群分发脚本\n（1）需求：循环复制文件到所有节点的相同目录下\n（2）需求分析：\n（a）rsync 命令原始拷贝\n\n```bash\nrsync  -av  /opt/module        wxler@wxler2:/opt/\n```\n（b）期望脚本：\n\n```tex\nxsync 要同步的文件名称\n```\n\n（c）期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径）\n\n```bash\n[wxler@wxler2 hadoop-3.1.3]$ echo $PATH\n/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/wxler/.local/bin:/home/wxler/bin\n```\n（3）脚本实现\n（a）在/home/wxler/bin 目录下创建 xsync 文件\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ cd /home/wxler\n[wxler@wxler1 ~]$ mkdir bin\n[wxler@wxler1 ~]$ cd bin\n[wxler@wxler1 bin]$ vim xsync\n\n```\n在该文件中编写如下代码\n\n```bash\n#!/bin/bash\n#1.  判断参数个数\nif [ $# -lt 1 ]\nthen\n\techo Not Enough Arguement!\n\texit;\nfi\n\n#2.  遍历集群所有机器\nfor host in wxler1 wxler2 wxler3\ndo\n\techo ==================== $host ====================\n\t#3.  遍历所有目录，挨个发送\n\tfor file in $@\n\tdo\n\t\t#4.  判断文件是否存在\n\t\tif [ -e $file ]\n\t\t\tthen\n\t\t\t\t#5.  获取父目录\n\t\t\t\tpdir=$(cd -P $(dirname $file); pwd)\n\t\t\t\t\n\t\t\t\t#6.  获取当前文件的名称\n\t\t\t\tfname=$(basename $file)\n\t\t\t\tssh $host \"mkdir -p $pdir\"\n\t\t\t\trsync -av $pdir/$fname $host:$pdir\n\t\t\telse\n\t\t\t\techo $file does not exists!\n\t\tfi\n\tdone\ndone\n```\n（b）修改脚本  xsync  具有执行权限\n\n```bash\nchmod +x xsync\n```\n（c）测试脚本\n\n```bash\nxsync /home/wxler/bin\n```\n（d）将脚本复制到/bin 中，以便全局调用\n\n```bash\nsudo cp xsync /bin/\n```\n（e）同步环境变量配置（root 所有者）\n\n```bash\nsudo xsync /etc/profile.d/my_env.sh\n```\n让环境变量生效，在另外两台虚拟机上执行`source /etc/profile`\n\n### 2.2 SSH 无密登录配置\n1）配置 ssh\n（1）基本语法\n\t\n\n```tex\nssh 另一台电脑的 IP 地址\n```\n\n（2）ssh 连接时出现 Host key verification failed 的解决方法\n\n```bash\nssh wxler2\n```\n如果出现如下内容\n\n```tex\nAre you sure you want to continue connecting (yes/no)? \n```\n输入 yes，并回车\n\n（3）退回到 wxler1，输入`exit`\n\n\n2）无秘钥配置\n（1）免密登录原理\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210331142259962.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（2）生成公钥和私钥\nssh-keygen  产生公钥与私钥对.\n```bash\n[wxler@wxler1 ~]$ ls -a\n.  ..  a.txt  .bash_history  .bash_logout  .bash_profile  .bashrc  bin  .ssh  .viminfo\n[wxler@wxler1 ~]$ cd .ssh\n[wxler@wxler1 .ssh]$ pwd\n/home/wxler/.ssh\n[wxler@wxler1 .ssh]$ ssh-keygen -t rsa\n```\n然后敲（三个回车），就会生成两个文件 id_rsa（私钥）、id_rsa.pub（公钥）\n\n（3）将公钥拷贝到要免密登录的目标机器上\nssh-copy-id 将本机的公钥复制到远程机器的authorized_keys文件中，ssh-copy-id也能让你有到远程机器的home, ~./ssh , 和 ~/.ssh/authorized_keys的权利\n\n```bash\n[wxler@wxler1 .ssh]$ ssh-copy-id wxler1\n[wxler@wxler1 .ssh]$ ssh-copy-id wxler2\n[wxler@wxler1 .ssh]$ ssh-copy-id wxler3\n[wxler@wxler1 .ssh]$ ssh wxler1\n[wxler@wxler1 .ssh]$ exit\n[wxler@wxler1 .ssh]$ ssh wxler2\n[wxler@wxler1 .ssh]$ exit\n[wxler@wxler1 .ssh]$ ssh wxler3\n[wxler@wxler1 .ssh]$ exit\n```\n然后在wxler2、wxler3上分别执行第（2）、（3）步\n\n到这里，我们用wxler用户进行了免密配置，但是用别的用户登录时（比如root用户），还是要输入密码，为了方便起见，我们在wxler1主机上再采用root账号配置一下无密登录到wxler1、wxler2、wxler3上，别的机器上用到再说。\n\n```bash\n[wxler@wxler1 .ssh]$ su root\n[root@wxler1 ~]# cd ~/.ssh\n[root@wxler1 .ssh]# pwd\n/root/.ssh\n```\n再在wxler1主机上分别执行第（2）、（3）步即可\n\n3）.ssh 文件夹下（~/.ssh）的文件功能解释\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210331145411961.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n\n### 2.3 集群配置\n以下命令均在wxler1虚拟机上执行\n1）**集群部署规划**\n注意：\n- NameNode 和 SecondaryNameNode 不要安装在同一台服务器\n- ResourceManager 也很消耗内存，不要和 NameNode、SecondaryNameNode 配置在同一台机器上。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210331152137465.png)\n\n2）**配置文件说明**\nHadoop 配置文件分两类：**默认配置文件和自定义配置文件**，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。\n\n（1）默认配置文件：\n从官网上下载之后，会有这四个文件\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210331152303326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70)\n（2）自定义配置文件：\ncore-site.xml 、 hdfs-site.xml 、 yarn-site.xml 、 mapred-site.xml 四个配置文件存放在`$HADOOP_HOME/etc/hadoop` 这个路径上，用户可以根据项目需求重新进行修改配置。\n\n3）**配置集群**\n\n（1）核心配置文件\n配置 core-site.xml\n\n```bash\ncd $HADOOP_HOME/etc/hadoop\nvim core-site.xml\n```\n文件内容如下：\n\n```bash\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\t<!--  指定 NameNode 的地址  -->\n\t<property>\n\t\t<name>fs.defaultFS</name>\n\t\t<value>hdfs://wxler1:8020</value>\n\t</property>\n\t<!--  指定 hadoop 数据的存储目录  -->\n\t<property>\n\t\t<name>hadoop.tmp.dir</name>\n\t\t<value>/opt/module/hadoop-3.1.3/data</value>\n\t</property>\n\t<!--  配置 HDFS 网页登录使用的静态用户为 wxler-->\n\t<property>\n\t\t<name>hadoop.http.staticuser.user</name>\n\t\t<value>wxler</value>\n\t</property>\n</configuration>\n```\n注意，8020是hadoop内部通讯的端口\n\n（2）HDFS 配置文件\n配置 hdfs-site.xml\n\n```bash\nvim hdfs-site.xml\n```\n文件内容如下：\n\n```bash\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n\t<!-- nn web 端访问地址-->\n\t<property>\n\t<name>dfs.namenode.http-address</name>\n\t<value>wxler1:9870</value>\n\t</property>\n\t<!-- 2nn web 端访问地址-->\n\t<property>\n\t<name>dfs.namenode.secondary.http-address</name>\n\t<value>wxler3:9868</value>\n\t</property>\n</configuration>\n```\n（3）YARN 配置文件\n\n```bash\nvim yarn-site.xml\n```\n文件内容如下:\n\n```bash\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\t<!--  指定 MR 走 shuffle -->\n\t<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n\t\n\t<!--  指定 ResourceManager 的地址-->\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname</name>\n\t\t<value>wxler2</value>\n\t</property>\n\t\n\t<!--  环境变量的继承  -->\n\t<property>\n\t\t<name>yarn.nodemanager.env-whitelist</name>\n\t\t<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n\t</property>\n</configuration>\n```\n注意：`HADOOP_MAPRED_HOME`是比默认多的配置，这是hadoop-3.1.3的小Bug，高版本就没有这个问题了\n\n（4）MapReduce 配置文件\n配置 mapred-site.xml\n\n```bash\nvim mapred-site.xml\n```\n文件内容如下：\n\n```bash\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n\t<!--  指定 MapReduce 程序运行在 Yarn 上  -->\n\t<property>\n\t\t<name>mapreduce.framework.name</name>\n\t\t<value>yarn</value>\n\t</property>\n</configuration>\n```\n4）在集群上分发配置好的 Hadoop 配置文件\n\n```bash\nxsync  /opt/module/hadoop-3.1.3/etc/hadoop/\n```\n5）去 wxler2 和 wxler3上查看文件分发情况\n\n```bash\n[wxler@wxler2 .ssh]$ cat  /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml\n[wxler@wxler3 .ssh]$ cat  /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml\n```\n\n### 2.4 群起集群\n以下命令均在wxler1虚拟机上执行\n1）**配置 workers**\n\n```bash\nvim  /opt/module/hadoop-3.1.3/etc/hadoop/workers\n```\n在该文件中增加如下内容：\n\n```tex\nwxler1\nwxler2\nwxler3\n```\n注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。\n\n同步所有节点配置文件\n\n```bash\nxsync /opt/module/hadoop-3.1.3/etc\n```\n2）**启动集群**\n\n以下命令均在wxler1虚拟机上执行\n\n（1）**如果集群是第一次启动**，需要在 wxler1 节点格式化 NameNode（注意：格式化 NameNode，会产生新的集群 id，导致 NameNode 和 DataNode 的集群 id 不一致，集群找不到已往数据。 如果集群在运行过程中报错，需要重新格式化 NameNode 的话， 一定要先停止 namenode 和 datanode 进程，并且要删除所有机器的 data 和 logs 目录，然后再进行格式化。）\ndata 目录存放在：`/opt/module/hadoop-3.1.3/data`\nlogs 目录存放在：`/opt/module/hadoop-3.1.3/logs`\n\n格式化：\n\n```bash\nhdfs namenode -format\n```\n格式化以后会在`/opt/module/hadoop-3.1.3/`目录下产生data目录存放数据\n\n（2）启动 HDFS\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ sbin/start-dfs.sh\nStarting namenodes on [wxler1]\nStarting datanodes\nwxler2: WARNING: /opt/module/hadoop-3.1.3/logs does not exist. Creating.\nwxler3: WARNING: /opt/module/hadoop-3.1.3/logs does not exist. Creating.\nStarting secondary namenodes [wxler3]\n[wxler@wxler1 hadoop-3.1.3]$ ls\nbin  data  etc  include  lib  libexec  LICENSE.txt  logs  NOTICE.txt  README.txt  sbin  share  wcinput  wcoutput\n```\n启动之后会在`/opt/module/hadoop-3.1.3/`产生logs目录\n（3）在配置了 ResourceManager 的节点（wxler2）启动 YARN\n\n```bash\n[wxler@wxler2 hadoop-3.1.3]$ sbin/start-yarn.sh\n```\n\n（4）Web 端查看 HDFS 的 NameNode\n（a）浏览器中输入：`http://wxler1:9870`\n（b）查看 HDFS 上存储的数据信息\n\n\n（5）Web 端查看 Y ARN 的 ResourceManager\n（a）浏览器中输入：`http://wxler2:8088`\n（b）查看 YARN 上运行的 Job 信息\n\n3）**集群基本测试**\n\n（1）上传文件到集群\n上传小文件\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -mkdir /input\n[wxler@wxler1 hadoop-3.1.3]$ hadoop  fs  -put $HADOOP_HOME/wcinput/word.txt /input\n2021-03-31 16:23:49,631 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n```\n上传大文件\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -put    /opt/software/jdk-8u212-linux-x64.tar.gz  /\n```\n\n（2）上传文件后查看文件存放在什么位置\n\n查看 HDFS 文件存储路径\n\n```bash\n[wxler@wxler1 subdir0]$ pwd\n/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-767171352-192.168.218.71-1617178286979/current/finalized/subdir0/subdir0\n```\n\n查看 HDFS 在磁盘存储文件内容\n\n```bash\n[wxler@wxler1 subdir0]$ cat blk_1073741825\nhadoop yarn\nhadoop mapreduce\nwxler\nwxler\n```\n（3）**拼接**\n\n```bash\n[wxler@wxler1 subdir0]$ ll\n总用量 191944\n-rw-rw-r--. 1 wxler wxler        41 3月  31 16:23 blk_1073741825\n-rw-rw-r--. 1 wxler wxler        11 3月  31 16:23 blk_1073741825_1001.meta\n-rw-rw-r--. 1 wxler wxler 134217728 3月  31 16:26 blk_1073741826\n-rw-rw-r--. 1 wxler wxler   1048583 3月  31 16:26 blk_1073741826_1002.meta\n-rw-rw-r--. 1 wxler wxler  60795424 3月  31 16:26 blk_1073741827\n-rw-rw-r--. 1 wxler wxler    474975 3月  31 16:26 blk_1073741827_1003.meta\n\n[wxler@wxler1 subdir0]$ cat blk_1073741826>>tmp.tar.gz\n[wxler@wxler1 subdir0]$ ls\nblk_1073741825            blk_1073741826            blk_1073741827            jdk1.8.0_212\nblk_1073741825_1001.meta  blk_1073741826_1002.meta  blk_1073741827_1003.meta  tmp.tar.gz\n```\n可以看到，解压出来就是一个jdk\n\n\n（4）**下载**\n\n```bash\n[wxler@wxler1 subdir0]$ cd ~\n[wxler@wxler1 ~]$ ls\na.txt  bin\n[wxler@wxler1 ~]$ hadoop fs  -get /jdk-8u212-linux-x64.tar.gz ./\n2021-03-31 16:38:31,931 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n[wxler@wxler1 ~]$ ls\na.txt  bin  jdk-8u212-linux-x64.tar.gz\n```\n\n\n（5）**执行 wordcount 程序**\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ pwd\n/opt/module/hadoop-3.1.3\n[wxler@wxler1 hadoop-3.1.3]$ hadoop  jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output\n```\n\n如果执行过程中，报如下错误，任务执行失败\n\n```bash\n[2021-03-31 17:26:59.903]Container killed on request. Exit code is 143\n[2021-03-31 17:26:59.963]Container exited with a non-zero exit code 143. \n\n2021-03-31 17:27:09,379 INFO mapreduce.Job:  map 100% reduce 100%\n2021-03-31 17:27:10,421 INFO mapreduce.Job: Job job_1617178807949_0005 failed with state FAILED due to: Task failed task_1617178807949_0005_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0\n\n2021-03-31 17:27:10,580 INFO mapreduce.Job: Counters: 13\n\tJob Counters \n\t\tFailed map tasks=4\n\t\tKilled reduce tasks=1\n\t\tLaunched map tasks=4\n\t\tOther local map tasks=3\n\t\tData-local map tasks=1\n\t\tTotal time spent by all maps in occupied slots (ms)=26504\n\t\tTotal time spent by all reduces in occupied slots (ms)=0\n\t\tTotal time spent by all map tasks (ms)=26504\n\t\tTotal vcore-milliseconds taken by all map tasks=26504\n\t\tTotal megabyte-milliseconds taken by all map tasks=27140096\n\tMap-Reduce Framework\n\t\tCPU time spent (ms)=0\n\t\tPhysical memory (bytes) snapshot=0\n\t\tVirtual memory (bytes) snapshot=0\n```\n\n解决方法：\n\n①修改配置文件 `/opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml ` 添加如下内容\n\n```xml\n<!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->\n <property>\n     <name>yarn.nodemanager.pmem-check-enabled</name>\n     <value>false</value>\n</property>\n<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->\n<property>\n     <name>yarn.nodemanager.vmem-check-enabled</name>\n     <value>false</value>\n</property>\n```\n\n②分发配置（支持相对路径）\n\n```bash\nxsync  /opt/module/hadoop-3.1.3/etc/hadoop/\n```\n\n③重启hadoop集群\n\n\n\n### 2.5 配置历史服务器\n\n为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下：\n\n1）配置 mapred-site.xml\n\n```bash\n[wxler@wxler1 hadoop]$ pwd\n/opt/module/hadoop-3.1.3/etc/hadoop\n[wxler@wxler1 hadoop]$ vim mapred-site.xml\n```\n\n\n\n在该文件里面增加如下配置。\n\n```xml\n<!--  历史服务器端地址  -->\n<property>\n<name>mapreduce.jobhistory.address</name>\n<value>wxler1:10020</value>\n</property>\n<!--  历史服务器 web 端地址  -->\n<property>\n<name>mapreduce.jobhistory.webapp.address</name>\n<value>wxler1:19888</value>\n</property>\n```\n\n- 10020是历史服务器内部通讯的端口\n- 19888是外部暴露的端口\n\n2）分发配置（支持相对路径）\n\n```bash\nxsync $HADOOP_HOME/etc/hadoop/mapred-site.xml\n```\n\n3）在 wxler1 启动历史服务器\n\n```bash\n[wxler@wxler1 hadoop]$ mapred --daemon start historyserver\n[wxler@wxler1 hadoop]$ jps\n15585 NameNode\n17313 Jps\n16658 NodeManager\n15700 DataNode\n17256 JobHistoryServer\n```\n\n- `--daemon`表示在后台启动\n\n4）查看 JobHistory\n\n`http://wxler1:19888/jobhistory`\n\n\n\n### 2.6 配置日志的聚集\n\n日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331201500.png)\n\n日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。\n注意 ：开启日志聚集功能， 需要 重新启动 NodeManager  、 ResourceManager 和HistoryServer。\n\n开启日志聚集功能具体步骤如下：\n\n1）配置 yarn-site.xml\n\n```bash\n[wxler@wxler1 hadoop]$ pwd\n/opt/module/hadoop-3.1.3/etc/hadoop\n[wxler@wxler1 hadoop]$ vim yarn-site.xml\n```\n\n在该文件里面增加如下配置。\n\n```bash\n<!--  开启日志聚集功能  -->\n<property>\n<name>yarn.log-aggregation-enable</name>\n<value>true</value>\n</property>\n<!--  设置日志聚集服务器地址  -->\n<property> \n<name>yarn.log.server.url</name> \n<value>http://wxler1:19888/jobhistory/logs</value>\n</property>\n<!--  设置日志保留时间为 7 天  -->\n<property>\n<name>yarn.log-aggregation.retain-seconds</name>\n<value>604800</value>\n</property>\n```\n\n2）分发配置\n\n```bash\nxsync  $HADOOP_HOME/etc/hadoop/yarnsite.xml\n```\n\n3）关闭 NodeManager  、ResourceManager 和 HistoryServer\n\n```bash\n[wxler@wxler2 hadoop]$ cd /opt/module/hadoop-3.1.3/\n[wxler@wxler2 hadoop-3.1.3]$ sbin/stop-yarn.sh\n[wxler@wxler1 hadoop-3.1.3]$ mapred  --daemon  stop historyserver\n```\n\n注意，因为ResourceManager 是在wxler2上配置的，所以一定要在wxler2上关闭。而historyserver是在wxler1上配置的，所以historyserver要在weler1上关闭。\n\n4）启动 NodeManager  、ResourceManage 和 HistoryServer\n\n```bash\n[wxler@wxler2 hadoop-3.1.3]$ start-yarn.sh\n[wxler@wxler1 hadoop-3.1.3]$ mapred --daemon start historyserver\n```\n\n同理，启动的位置同上一步\n\n5）删除 HDFS 上已经存在的输出文件\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop fs -rm -r /output\nDeleted /output\n```\n\n6）执行 WordCount 程序\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop  jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output\n```\n\n7）查看日志\n\n（1）历史服务器地址\n\n`http://wxler1:19888/jobhistory`\n\n（2）历史任务列表\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331202946.png)\n\n（3）查看任务运行日志\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331203011.png)\n\n（4）运行日志详情\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331203052.png)\n\n\n\n### 2.7 集群启动/停止方式总结\n\n1）各个模块分开启动/停止（配置 ssh 是前提）常用\n\n（1）整体启动/停止 HDFS\n\n```bash\nstart-dfs.sh/stop-dfs.sh\n```\n\n（2）整体启动/停止 YARN\n\n```bash\nstart-yarn.sh/stop-yarn.sh\n```\n\n2）各个服务组件逐一启动/停止\n\n（1）分别启动/停止 HDFS 组件\n\n```bash\nhdfs --daemon start/stop namenode/datanode/secondarynamenode\n```\n\n（2）启动/停止 YARN\n\n```bash\nyarn --daemon start/stop resourcemanager/nodemanager\n```\n\n\n\n### 2.8 编写 Hadoop 集群常用脚本\n\n1）Hadoop 集群启停脚本（包含 HDFS，Y arn，Historyserver）：myhadoop.sh\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ cd /home/wxler/bin\n[wxler@wxler1 bin]$ vim myhadoop.sh\n```\n\n输入以下内容：\n\n```bash\n#!/bin/bash\nif [ $# -lt 1 ]\nthen\n\techo \"No Args Input...\"\n\texit ;\nfi\n\ncase $1 in\n\"start\")\n\techo \" ===================  启动  hadoop 集群  ===================\"\n\t\n\techo \" ---------------  启动  hdfs ---------------\"\n\tssh wxler1 \"/opt/module/hadoop-3.1.3/sbin/start-dfs.sh\"\n\techo \" ---------------  启动  yarn ---------------\" \n\tssh wxler2 \"/opt/module/hadoop-3.1.3/sbin/start-yarn.sh\"\n\techo \" ---------------  启动  historyserver ---------------\"\n\tssh wxler1 \"/opt/module/hadoop-3.1.3/bin/mapred  --daemon  start historyserver\"\n;;\n\"stop\")\n\techo \" ===================  关闭  hadoop 集群  ===================\"\n\techo \" ---------------  关闭  historyserver ---------------\"\n\tssh  wxler1  \"/opt/module/hadoop-3.1.3/bin/mapred  --daemon  stop historyserver\"\n\techo \" ---------------  关闭  yarn ---------------\"\n\tssh wxler2 \"/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh\"\n\techo \" ---------------  关闭  hdfs ---------------\"\n\tssh wxler1 \"/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh\"\n;;\n*)\n\techo \"Input Args Error...\"\n;;\nesac\n```\n\n保存后退出，然后赋予脚本执行权限\n\n```bash\nchmod +x myhadoop.sh\n```\n\n2）查看三台服务器 Java 进程脚本：jpsall\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ cd /home/wxler/bin\n[wxler@wxler1 bin]$ vim jpsall\n```\n\n输入以下内容：\n\n保存后退出，然后赋予脚本执行权限\n\n```bash\nchmod +x jpsall\n```\n\n3）分发/home/atguigu/bin 目录，保证自定义脚本在三台机器上都可以使用\n\n```bash\n[wxler@wxler1 bin]$ pwd\n/home/wxler/bin\n[wxler@wxler1 bin]$ xsync ./\n```\n\n\n\n### 2.9 常用端口号说明\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331205308.png)\n\n\n\n\n\n### 2.10 集群时间同步\n\n如果服务器在公网环境（能连接外网），可以不采用集群时间同步，因为服务器会定期和公网时间进行校准；\n\n如果服务器在内网环境，必须要配置集群时间同步，否则时间久了，会产生时间偏差，导致集群执行任务时间不同\n\n步。\n\n1）**需求**\n\n找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步， 生产环境根据任务对时间的准确程度要求周期同步。 测试环境为了尽快看到效果，采用 1 分钟同步一次。\n\n2）**时间服务器配置**\n\n以下代码均在wxler1上执行\n\n（1）查看所有节点 ntpd 服务状态和开机自启动状态\n\n查看ntpd状态\n\n```bash\n[wxler@wxler1 ~]$ sudo systemctl start ntpd\nFailed to start ntpd.service: Unit not found.\n```\n\n安装ntpd\n\n```bash\n[wxler@wxler1 ~]$ sudo yum -y install ntp\n```\n\n下面依次执行\n\n```bash\n[wxler@wxler1 ~]$ sudo systemctl start ntpd  #启动ntpd服务\n[wxler@wxler1 ~]$ sudo systemctl status ntpd #查看ntpd状态\n[wxler@wxler1 ~]$ sudo systemctl is-enabled ntpd #设置开启启动\n```\n\n（2）修改 hadoop102 的 ntp.conf 配置文件\n\n```bash\nsudo vim /etc/ntp.conf\n```\n\n修改内容如下，总共修改3出\n\n（a）修改 1（授权 192.168.10.0-192.168.10.255 网段上的所有机器可以从这台机器上查询和同步时间），即都可以查询wxler1虚拟机上的时间并同步\n\n```bash\n#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap\n```\n\n改为\n\n```\nrestrict 192.168.218.0 mask 255.255.255.0 nomodify notrap\n```\n\n\n\n（b）修改 2（集群在局域网中，不使用其他互联网上的时间）\n\n```bash\nserver 0.centos.pool.ntp.org iburst\nserver 1.centos.pool.ntp.org iburst\nserver 2.centos.pool.ntp.org iburst\nserver 3.centos.pool.ntp.org iburst\n```\n\n修改为\n\n```bash\n# server 0.centos.pool.ntp.org iburst\n# server 1.centos.pool.ntp.org iburst\n# server 2.centos.pool.ntp.org iburst\n# server 3.centos.pool.ntp.org iburst\n```\n\n（c） 添加 3 （当该节点丢失网络连接，wxler1依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）\n\n在`/etc/ntp.conf`最后增加\n\n```bash\nserver 127.127.1.0\nfudge 127.127.1.0 stratum 10\n```\n\n\n\n（3）修改 wxler1 的/etc/sysconfig/ntpd  文件\n\n```bash\n[wxler@wxler1 ~]$ sudo vim /etc/sysconfig/ntpd\n```\n\n增加内容如下（让硬件时间与系统时间一起同步）\n\n```bash\nSYNC_HWCLOCK=yes\n```\n\n（4）重新启动 ntpd 服务\n\n```bash\nsudo systemctl restart ntpd\n```\n\n3）**其他机器配置**\n\n（1）关闭所有节点上 ntp 服务和自启动\n\n```bash\nsudo systemctl stop ntpd\nsudo systemctl disable ntpd\n```\n\n（2）在其他机器配置 1 分钟与时间服务器同步一次\n\n```bash\nsudo crontab -e\n```\n\n编写定时任务如下：\n\n```bash\n*/1 * * * * /usr/sbin/ntpdate wxler1\n```\n\n注意：除了wxler1之外，wxler2、wxler3都要执行\n\n\n\n\n\n## 3. 常见错误及解决方案\n\n1）防火墙没关闭、或者没有启动 YARN\n\n```bash\nINFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032\n```\n\n2）主机名称配置错误\n3）IP 地址配置错误\n4）ssh 没有配置好\n5）root 用户和 atguigu 两个用户启动集群不统一\n6）配置文件修改不细心\n7）不识别主机名称 \n\n```bash\njava.net.UnknownHostException: hadoop102: hadoop102\nat \njava.net.InetAddress.getLocalHost(InetAddress.java:1475)\nat \norg.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(Job\nSubmitter.java:146)\nat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\nat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\nat  java.security.AccessController.doPrivileged(Native \nMethod)\nat javax.security.auth.Subject.doAs(Subject.java:415)\n```\n\n解决办法：\n（1）在/etc/hosts 文件中添加 192.168.10.102 wxler1\n（2）主机名称不要起 hadoop、hadoop000 等特殊名称\n\n8）DataNode 和 NameNode 进程同时只能工作一个。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331213651.png)\n\n三个步骤：\n\n- 关闭hdfs、yarn、历史服务器等\n- 删除所有节点的data和logs目录\n- 重新格式化\n\n9）执行命令不生效，粘贴 Word 中命令时，遇到-和长–没区分开。导致命令失效\n解决办法：尽量不要粘贴 Word 中代码。\n\n10）jps 发现进程已经没有，但是重新启动集群，提示进程已经开启。\n原因是在 Linux 的根目录下/tmp 目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。\n\n11）jps 不生效\n原因：全局变量 hadoop java 没有生效。解决办法：需要 source /etc/profile 文件。\n\n12）8088 端口连接不上\n\n```bash\nvim /etc/hosts\n```\n\n注释掉如下代码 \n\n```bash\n#127.0.0.1    localhost localhost.localdomain localhost4 localhost4.localdomain4\n#::1          hadoop102 \n```\n\n13）hdfs删除文件保存\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331214143.png)\n\n原因是没有在core-site.xml配置HDFS 网页登录使用的静态用户，解决方法参见2.3节的 3）\n\n\n\n","tags":["Hadoop"],"categories":["Hadoop"]},{"title":"二、Hadoop 运行环境搭建","url":"/2021/03/31/190343/","content":"\nHadoop 运行环境搭建\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. 模板虚拟机环境准备\n\n### 1.1 安装CentOS-7.5-x86-1804 \n\n安装模板虚拟机，IP 地址 192.168.218.70、主机名称 wxler0、内存 1G、硬盘 50G \n\n（1）选择CentOS7-64镜像位置\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190442.png)\n\n（2）开启虚拟机，选择如下，按Enter\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190504.png)\n（3）选择中文简体\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190518.png)\n\n（4）设置时间和日期，选择亚洲->上海\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190533.png)\n\n（5）软件选择，设置为最小安装\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190609.png)\n\n（6）设置安装位置\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190626.png)\n\n\n\n（7）选择我要配置分区，点击完成\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190641.png)\n\n（8）点击+号，为Boot分配1g空间\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190701.png)\n\n（9）文件系统设置为ext4\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190715.png)\n\n(10）再设置swap分区为4g\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190741.png)\n\n\n\n(11)最后设置根目录\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190801.png)\n\n（12）点击完成\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190815.png)\n\n（13）禁用KDUMP\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190831.png)\n\n（14）配置网络和主机名，后面可以修改\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190848.png)\n\n（15）设置root密码，完成之后重启\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190901.png)\n\n（16）点击vmvare->虚拟网络编辑器，查看子网IP\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190918.png)\n\n（17）设置VMnet8的ip如下，主机要和上面子网的网络号保持一致\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190931.png)\n\n（18）在虚拟机中登录，输入`vi /etc/sysconfig/network-scripts/ifcfg-ens33`，将BOOTPROTO改为static，再加IPADDR、GATEWAT、DNS1这三行内容。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331191347.png)\n\n（19）输入`vi /etc/hostname`修改主机名\n（20）输入`vi /etc/hosts`添加ip映射\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331191409.png)\n\n（21）输入`init 6`重启电脑，如果提示没有`ifconfig`命令，可以参考[centos7没有ifconfig命令解决办法](https://jingyan.baidu.com/article/eb9f7b6d42636d869364e8c9.html)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331191423.png)\n\n依次输入`ping wxler0`和`ping www.baidu.com`看看能否ping通\n\n到这里就大功告成了。\n\n### 1.2 虚拟机配置要求\n\n本文 Linux 系统全部以 CentOS-7.5-x86-1804 为例\n（1）使用 yum 安装需要虚拟机可以正常上网， yum 安装前可以先测试下虚拟机联网情况\n（2）安装 epel-release\nExtra Packages for Enterprise  Linux 是为“红帽系”的操作系统提供额外的软件包，适用于 RHEL、CentOS 和 Scientific Linux。相当于是一个软件仓库，大多数 rpm 包在官方repository  中是找不到的）\n\n```bash\n[root@wxler0 ~]# yum install -y epel-release\n```\n（3） 注意： 如果 Linux 安装的是最小系统版，还需要安装如下工具；如果安装的是 Linux桌面标准版，不需要执行如下操作\n\n- net-tool：工具包集合，包含 ifconfig 等命令：`yum install -y net-tools`\n- vim：编辑器：`yum install -y vim`\n\n\n（4）关闭防火墙，关闭防火墙开机自启\n\n```bash\nsystemctl stop firewalld\nsystemctl disable firewalld.service\n```\n注意：在企业开发时，通常单个服务器的防火墙时关闭的。公司整体对外会设置非常安全的防火墙\n\n（5）创建 wxler 用户，并修改 wxler 用户的密码\n```bash\nuseradd wxler\npasswd wxler\n```\n（6）配置 wxler 用户具有 root 权限，**方便后期加 sudo 执行 root 权限的命令**\n```bash\nvim /etc/sudoers\n```\n修改/etc/sudoers 文件，在`%wheel` 这行下面添加一行，如下所示：\n```tex\n## Allow root to run any commands anywhere\nroot ALL=(ALL) ALL\n## Allows people in group wheel to run all commands\n%wheel  ALL=(ALL) ALL \nwxler ALL=(ALL) NOPASSWD:ALL\n```\n注意：wxler 这一行不要直接放到 root 行下面，因为所有用户都属于 wheel 组，你先配置了 wxler 具有免密功能，但是程序执行到`%wheel` 行时，该功能又被覆盖回需要密码。所以 wxler 要放到`%wheel` 这行下面。\n\n（7）在`/opt` 目录下创建文件夹，并修改所属主和所属组\n\n①在`/opt` 目录下创建 module、software 文件夹\n\n```bash\nmkdir /opt/module\nmkdir /opt/software\n```\n\n②修改 module、software 文件夹的所有者和所属组均为 atguigu 用户\n```bash\nchown wxler:wxler /opt/module\nchown wxler:wxler /opt/software\n```\n③查看 module、software 文件夹的所有者和所属组\n\n```bash\n[root@wxler0 ~]# cd /opt\n[root@wxler0 opt]# ll\n总用量 8\ndrwxr-xr-x. 2 wxler wxler 4096 3月  30 17:19 module\ndrwxr-xr-x. 2 wxler wxler 4096 3月  30 17:19 software\n```\n\n（8）卸载虚拟机自带的 JDK\n注意：如果你的虚拟机是最小化安装不需要执行这一步。\n```bash\nrpm -qa | grep -i java | xargs -n1 rpm -e --nodeps\n```\n- rpm -qa：查询所安装的所有 rpm 软件包\n- grep -i：忽略大小写\n- xargs -n1：表示每次只传递一个参数\n- rpm -e –nodeps：强制卸载软件\n\n（9）重启虚拟机：`init 6`\n\n\n## 2. 克隆虚拟机\n1）利用模板机 wxler0，克隆三台虚拟机：wxler1 wxler2 wxler3\n注意：克隆时，要先关闭 wxler1\n\n2）修改克隆机 IP，以下以 wxler1 举例说明\n\n修改克隆虚拟机的静态 IP\n\n```bash\nvim /etc/sysconfig/network-scripts/ifcfg-ens33\n```\n改为（其实只要把改一下即可，别的不用改）\n```tex\nDEVICE=ens33\nTYPE=Ethernet \nONBOOT=yes\nBOOTPROTO=static\nNAME=\"ens33\"\nIPADDR=192.168.218.71\nPREFIX=24\nGATEWAY=192.168.218.2\nDNS1=192.168.218.2\n```\n\n3）修改克隆机主机名，以下以 wxler1 举例说明\n\n①修改主机名称\n```bash\nvim /etc/hostname\n```\n②配置 Linux 克隆机主机名称映射 hosts 文件，打开/etc/hosts\n```bash\nvim /etc/hosts\n```\n添加如下内容：\n```tex\n192.168.218.70 wxler0\n192.168.218.71 wxler1\n192.168.218.72 wxler2\n192.168.218.73 wxler3\n192.168.218.74 wxler4\n192.168.218.75 wxler5\n192.168.218.76 wxler6\n192.168.218.77 wxler7\n192.168.218.78 wxler8\n192.168.218.79 wxler9\n```\n4）重启克隆机 hadoop102\n```bash\ninit 6\n```\n5）修改 windows 的主机映射文件（hosts 文件）\n\n进入 `C:\\Windows\\System32\\drivers\\etc` 路径，打开 hosts 文件并添加如下内容，然后保存\n```tex\n192.168.218.70 wxler0\n192.168.218.71 wxler1\n192.168.218.72 wxler2\n192.168.218.73 wxler3\n192.168.218.74 wxler4\n192.168.218.75 wxler5\n192.168.218.76 wxler6\n192.168.218.77 wxler7\n192.168.218.78 wxler8\n192.168.218.79 wxler9\n```\n如果操作系统是 window7，可以直接修改。如果操作系统是 window10，先拷贝出来，修改保存以后，再覆盖即可。\n\n## 3. 在Linux上安装JDK\n用xshell连接3台linux，并用wxler用户登录\n\n1）卸载现有 JDK\n注意：安装 JDK 前，一定确保提前删除了虚拟机自带的 JDK。\n\n2）用 XShell 传输工具将 JDK 导入到 opt 目录下面的 software 文件夹下面\n\n3）在 Linux 系统下的 opt 目录中查看软件包是否导入成功\n```bash\nls /opt/software/\n```\n看到如下结果：\n```tex\njdk-8u212-linux-x64.tar.gz\n```\n\n4）解压 JDK 到/opt/module 目录下\n\n```bash\ncd /opt/software/\ntar  -zxvf  jdk-8u212-linux64.tar.gz -C /opt/module/\n```\n5）配置 JDK 环境变量\n在`/etc/profile`里面包含如下一部分代码\n```bash\nfor i in /etc/profile.d/*.sh /etc/profile.d/sh.local ; do\n    if [ -r \"$i\" ]; then\n        if [ \"${-#*i}\" != \"$-\" ]; then\n            . \"$i\"\n        else\n            . \"$i\" >/dev/null\n        fi\n    fi\ndone\n```\n该代码意思就是让`/etc/profile.d/`下面所有以`.sh`结尾的文件生效，所以这里我们新建一个my_env.sh文件，里面存放我们的环境变量，这也是企业中的用法。\n\n①新建/etc/profile.d/my_env.sh 文件\n```bash\nsudo vim /etc/profile.d/my_env.sh\n```\n添加如下内容\n```tex\n#JAVA_HOME\nexport JAVA_HOME=/opt/module/jdk1.8.0_212\nexport PATH=$PATH:$JAVA_HOME/bin\n```\n②source 一下/etc/profile 文件，让新的环境变量 PATH 生效`source /etc/profile`\n\n6）测试 JDK 是否安装成功`java -version`\n如果能看到以下结果，则代表 Java 安装成功。\n```tex\njava version \"1.8.0_212\"\n```\n## 4. 在Linux上安装 Hadoop\nHadoop 下载地址：https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/\n下面所有操作都在wxler1虚拟机上操作\n\n1）用 XShell 文件传输工具将 hadoop-3.1.3.tar.gz 导入到 opt 目录下面的 software 文件夹下面\n2）进入到 Hadoop 安装包路径下\n```bash\ncd /opt/software/\n```\n3）解压安装文件到/opt/module 下面\n\n```bash\ntar  -zxvf hadoop-3.1.3.tar.gz -C /opt/module/\n```\n4）查看是否解压成功\n\n```bash\n[wxler@wxler1 software]$ ls /opt/module\nhadoop-3.1.3  jdk1.8.0_212\n```\n\n5）将 Hadoop 添加到环境变量\n\n①获取 Hadoop 安装路径\n\n \n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ pwd\n/opt/module/hadoop-3.1.3\n```\n②打开/etc/profile.d/my_env.sh 文件\n\n```bash\nsudo  vim /etc/profile.d/my_env.sh \n```\n在 my_env.sh 文件末尾添加如下内容：（shift+g）\n\n```bash\n#HADOOP_HOME\nexport HADOOP_HOME=/opt/module/hadoop-3.1.3\nexport PATH=$PATH:$HADOOP_HOME/bin\nexport PATH=$PATH:$HADOOP_HOME/sbin\n```\n让修改后的文件生效\n\n```bash\nsource /etc/profile\n```\n6）测试是否安装成功\n\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ hadoop version\nHadoop 3.1.3\n```\n7）重启（如果 Hadoop 命令不能用再重启虚拟机）：`init 6`\n\n\n## 5. Hadoop 目录结构\n1）查看 Hadoop 目录结构\n```bash\n[wxler@wxler1 hadoop-3.1.3]$ ll\n总用量 200\ndrwxr-xr-x. 2 wxler wxler   4096 9月  12 2019 bin\ndrwxr-xr-x. 3 wxler wxler   4096 9月  12 2019 etc\ndrwxr-xr-x. 2 wxler wxler   4096 9月  12 2019 include\ndrwxr-xr-x. 3 wxler wxler   4096 9月  12 2019 lib\ndrwxr-xr-x. 4 wxler wxler   4096 9月  12 2019 libexec\n-rw-rw-r--. 1 wxler wxler 147145 9月   4 2019 LICENSE.txt\n-rw-rw-r--. 1 wxler wxler  21867 9月   4 2019 NOTICE.txt\n-rw-rw-r--. 1 wxler wxler   1366 9月   4 2019 README.txt\ndrwxr-xr-x. 3 wxler wxler   4096 9月  12 2019 sbin\ndrwxr-xr-x. 4 wxler wxler   4096 9月  12 2019 share\n```\n2）重要目录\n（1）bin 目录：存放对 Hadoop 相关服务（hdfs，yarn，mapred）进行操作的脚本\n（2）etc 目录：Hadoop 的配置文件目录，存放 Hadoop 的配置文件\n（3）lib 目录：存放 Hadoop 的本地库（对数据进行压缩解压缩功能）\n（4）sbin 目录：存放启动或停止 Hadoop 相关服务的脚本\n（5）share 目录：存放 Hadoop 的依赖 jar 包、文档、和官方案例","tags":["Hadoop"],"categories":["Hadoop"]},{"title":"一、Hadoop 组成","url":"/2021/03/31/185000/","content":"\nHadoop 的组成\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. Hadoop1.x、2.x、3.x区别\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331185550.png)\n\n在 Hadoop1.x 时代 ，Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。\n在Hadoop2.x时代，增加了Yarn 。 Yarn 只负责资 源 的 调 度 ，MapReduce 只负责运算 。\nHadoop3.x在组成上没有变化。\n\n## 2. HDFS 架构概述\nHadoop Distributed File System，简称 HDFS，是一个分布式文件系统。\n\n1）NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331185648.png)\n\n2）DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331185705.png)\n\n## 3. YARN 架构概述\nYet Another Resource Negotiator 简称 YARN ，另一种资源协调者， 是 Hadoop 的资源管理器。 \n1）ResourceManager（RM）：整个集群资源（内存、CPU等）的老大\n3）ApplicationMaster（AM）：单个任务运行的老大\n2）NodeManager（NM）：单个节点服务器资源老大\n4）Container：容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331185739.png)\n\n## 4. MapReduce 架构概述\nMapReduce 将计算过程分为两个阶段：Map 和 Reduce\n1）Map 阶段并行处理输入数据\n2）Reduce 阶段对 Map 结果进行汇总\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331185819.png)\n\n## 5. HDFS、YARN、MapReduce 三者关系\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331185847.png)\n\n- MapTask检索各自机器上的数据\n- 检索到数据后，ReduceTask将检索到的数据写回到磁盘上，并告知NameNode该数据存放到位置\n\n\n## 6. 大数据技术生态体系\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331185939.png)\n\n图中涉及的技术名词解释如下：\n1） Sqoop： Sqoop 是一款开源的工具，主要用于在 Hadoop、 Hive 与传统的数据库 （MySQL）间进行数据的传递，可以将一个关系型数据库（例如  ：MySQL，Oracle  等）中的数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。\n2） Flume： Flume 是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume 支持在日志系统中定制各类数据发送方，用于收集数据； \n3）Kafka：Kafka 是一种高吞吐量的分布式发布订阅消息系统；   \n4）Spark： Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数据进行计算。\n5）Flink：Flink 是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。\n6）Oozie：Oozie 是一个管理 Hadoop 作业（job）的工作流程调度管理系统。\n7） Hbase： HBase 是一个分布式的、面向列的开源数据库。 HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。\n8）Hive：Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开\n发专门的 MapReduce 应用，十分适合数据仓库的统计分析。\n9） ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。\n\n\n\n## 7. 推荐系统框架图\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210331190129.png)","tags":["Hadoop"],"categories":["Hadoop"]},{"title":"四、RDD弹性分布式数据集介绍","url":"/2021/03/28/171102/","content":"\nSpark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：\n\n- RDD :  弹性分布式数据集\n\n- 累加器：分布式共享只写变量\n\n- 广播变量：分布式共享只读变量\n\n\n接下来我们一起看看这三大数据结构是如何在数据处理中使用的。\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. IO与RDD\n\nJava IO分为字节流与字符流：\n\n- 字节流：传输过程中，传输数据的最基本单位是字节(8bit)的流。字节byte\n- 字符流：传输过程中，传输数据的最基本单位是字符(16bit)的流。字符 char\n\n字节流包含两个**抽象类** InputStream(输入流)和OutputStream(输出流)。\n\n字符流包含两个**抽象类** Reader(输入流)和Writer(输出流)。\n\n下面我们一步步来说明JavaIO的封装流程。\n\n（1）FileInputStream \n\n```java\nInputStream in = new FileInputStream(\"path\")\nint i = -1\nwhile ( (i = in.read()) != -1 ) {\n    println(i);\n}\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328211236.png)\n\nFileInputStream 每次读取一个字节都需要和磁盘进行交互。\n\n（2）BufferedInputStream\n\n```java\nInputStream in = new BufferedInputStream(new FileInputStream(\"path\"))\nint i = -1\nwhile ( (i = in.read()) != -1 ) {\n    println(i);\n}\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328211547.png)\n\nBufferedInputStream通过封装FileInputStream，把FileInputStream读取的一个个字节封装到Bufffer里，再从Buffer里获取数据\n\n（3）BufferedReader\n\n```java\nReader in = new BufferedReader(\n    new InputStreamReader(\n           new FileInputStream(\"path\"),\n           \"UTF-8\"\n    )\n)\nString s = null\nwhile ( (s = in.readLine()) != null ) {\n    println(i);\n}\n```\n\nBufferedReader封装InputStreamReader，InputStreamReader封装了FileInputStream。首先，InputStreamReader将底层的字节流转成字符流，再放入其缓冲区，BufferedReader通过`UTF-8`编码格式，每遇到三个字节编码成一个中文，放入BufferedReader缓冲区中。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328211917.png)\n\n上面提到的一级一级封装，其实就是装饰者设计模式，执行的具体操作还是在最底层类。\n\n\n\n\n\n（4）Scala的WordCount\n\n```scala\nsc.textFile(\"input/word.txt\").flatMap(_.split(\" \")).map((_,1)).reduceByKey(_+_).collect\n```\n\n执行过程如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328212430.png)\n\n\n\n上面每个操作都继承了RDD这个抽象类，完成一个基本的功能。\n\n- RDD的数据处理方式类似于IO流，也有装饰者设计模式。\n- RDD的数据只有在调用collect方法时，才会真正执行业务逻辑操作。之前的封装全部都是功能（逻辑）的扩展，并没有真正执行\n- RDD是不保存数据的，数据来了直接是往下走，但是IO可以临时保存一部分数据。\n- rdd是最小的逻辑计算单元，一个rdd基本上只有一个功能，可以通过多个rdd组合的方式实现复杂的逻辑。\n\n\n\n举一个小例子，比如我们想要（1,2,3,4）中的每个元素都乘以2，提交Spark实现，这时候Spark会封装成2个RDD（注意：RDD只封装了计算逻辑，不保存数据），以分片的方式将数据分成2个区，然后通过Drive将RDD封装的计算逻辑分发给不同的Executor，从而实现并行计算。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328215609.png)\n\n\n\n\n\n\n\n## 2. 什么是 RDD\n\nRDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中**最基本的数据处理模型**。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。\n\n- 弹性\n  - 存储的弹性：内存与磁盘的自动切换；\n  - 容错的弹性：数据丢失可以自动恢复；\n  - 计算的弹性：计算出错重试机制；\n  - 分片的弹性：可根据需要重新分片（即分区）。\n- 分布式：数据存储在大数据集群不同节点上\n- 数据集：RDD 封装了计算逻辑，并不保存数据\n- 数据抽象：RDD 是一个抽象类，需要子类具体实现\n- 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑\n- 可分区、并行计算\n\n\n\n## 3. RDD核心属性\n\n查看RDD的源码，在其注释说明里面可以看到：\n\n```tex\nInternally, each RDD is characterized by five main properties:\n    A list of partitions\n    A function for computing each split\n    A list of dependencies on other RDDs\n    Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)\n    Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)\n```\n\n（1）分区列表（A list of partitions）\n\nRDD 数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。\n\n```scala\n  /**\n   * Implemented by subclasses to return the set of partitions in this RDD. This method will only\n   * be called once, so it is safe to implement a time-consuming computation in it.\n   *\n   * The partitions in this array must satisfy the following property:\n   *   `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`\n   */\n  protected def getPartitions: Array[Partition] //获取分区列表\n```\n\n（2）分区计算函数（A function for computing each split）\n\nSpark 在计算时，是使用分区函数对每一个分区进行计算。由于计算逻辑是事先封装好传递过来的，所以每个分区的计算逻辑完全相同\n\n```scala\n  /**\n   * :: DeveloperApi ::\n   * Implemented by subclasses to compute a given partition.\n   */\n  @DeveloperApi\n  def compute(split: Partition, context: TaskContext): Iterator[T] //分区计算函数\n```\n\n（3）RDD 之间的依赖关系（A list of dependencies on other RDDs）\n\nRDD 是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个 RDD 建立依赖关系。\n\n```scala\n\n  /**\n   * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only\n   * be called once, so it is safe to implement a time-consuming computation in it.\n   */\n  protected def getDependencies: Seq[Dependency[_]] = deps  //获取RDD依赖\n```\n\n（4）分区器（可选）（a Partitioner for key-value RDDs）\n\n当数据为 KV 类型数据时，可以通过设定分区器自定义数据的分区（可以理解为用于定义分区的规则）\n\n```scala\n\n  /** Optionally overridden by subclasses to specify how they are partitioned. */\n  @transient val partitioner: Option[Partitioner] = None\n```\n\nScala中的Option类是None和Some的父类，表示如果有值，则取Some，如果为空，则为None\n\n（5）首选位置（可选）\n\n计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算\n\n```scala\n  /**\n   * Optionally overridden by subclasses to specify placement preferences.\n   */\n  protected def getPreferredLocations(split: Partition): Seq[String] = Nil\n```\n\n注意：由于数据是分布式存储的，如果需要的数据和某一个Executor在同一个节点上时，只需要将计算逻辑发给这个Executor就行，不需要移动数据。\n\n首选位置：判断计算逻辑发送到哪个节点，效率最优，移动数据不如移动计算逻辑\n\n如下图所示，如果需要的数据实现存在这个Executor所在的节点上，将计算逻辑分发给这个节点就行，不需要移动数据到其他地方。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328220624.png)\n\n\n\n\n\n\n\n## 4. 执行原理\n\n从计算的角度来讲，数据处理过程中需要计算资源（内存  & CPU）和计算模型（逻辑）。执行时，需要将计算资源和计算模型进行协调和整合。\n\nSpark 框架在执行时，先申请资源，然后将应用程序的**数据处理逻辑**分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上,  按照指定的计算模型进行数据计算。最后得到计算结果。\n\nRDD 是 Spark 框架中用于数据处理的核心模型，接下来我们看看，在 Yarn 环境中， RDD的工作原理:\n\n1)  启动 Yarn 集群环境\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328221037.png)\n\n2)  Spark 通过申请资源创建调度节点和计算节点\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328221106.png)\n\n3)  Spark 框架根据需求将计算逻辑根据分区划分成不同的任务，并放到任务池当中\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328221446.png)\n\n4)  调度节点从任务池中将任务取出，根据计算节点状态发送到对应的计算节点进行计算\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328221859.png)\n\n\n\n从以上流程可以看出 RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算，接下来我们就一起看看 Spark 框架中 RDD 是具体是如何进行数据处理的。\n\n\n\n\n\n【资料推荐】\n\n1、推荐一遍博客讲的特别棒\n\n[Spark中RDD的宽窄依赖 & 图解RDD执行中Application、Job、Stage、Task的关系](https://blog.csdn.net/wx1528159409/article/details/87636135)\n\n2、job和stage的概念区分\n\n[spark job stage task概念与区分](https://blog.csdn.net/u010711495/article/details/109812043?utm_term=task%E5%92%8Cjob%E5%8C%BA%E5%88%AB&utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduweb~default-2-109812043&spm=3001.4430)\n\n\n\n\n\n\n\n","tags":["Spark"],"categories":["Spark"]},{"title":"三、Spark运行架构","url":"/2021/03/28/161528/","content":"\n## 1. 运行架构\n\nSpark 框架的核心是一个计算引擎，整体来说，它采用了标准  master-slave  的结构。如下图所示，它展示了一个  Spark 执行时的基本结构。图形中的 Driver 表示 master，负责管理整个集群中的作业任务调度。图形中的 Executor  则是  slave，负责实际执行任务。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328162241.png)\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n\n\n## 2. 核心组件\n\n由上图可以看出，对于 Spark 框架有两个核心组件：\n\n### 2.1 Driver\n\nSpark **驱动器节点**，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在 Spark 作业执行时主要负责：\n\n- 将用户程序转化为作业（job）\n- 在 Executor 之间调度任务(task)\n- 跟踪 Executor 的执行情况\n- 通过 UI 展示查询运行情况\n\n\n\n实际上，我们无法准确地描述 Driver 的定义，因为在整个的编程过程中没有看到任何有关Driver 的字眼。所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类。\n\n\n\n### 2.2 Executor\n\nSpark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在  Spark  作业中运行具体任务（Task），任务彼此之间相互独立。Spark  应用启动时，Executor 节点被同时启动，并且始终伴随着整个  Spark  应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃， Spark应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。\n\nExecutor 有两个核心功能：\n\n- 负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程\n- 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的  RDD  提供内存式存储。RDD  是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。\n\n\n\n### 2.3 Master & Worker\n\nSpark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker，这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM（ResourceManage）,  而Worker 呢，也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算，类似于 Yarn 环境中 NM（NodeManage）。\n\n\n\n### 2.4 ApplicationMaster\n\nHadoop 用户向 YARN 集群提交应用程序时，提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。\n\n说的简单点就是，ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是ApplicationMaster。\n\n\n\n## 3. 核心概念\n\n### 3.1 Executor 与 Core\n\nSpark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门**用于计算的节点**。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。\n\n应用程序相关启动参数如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328163733.png)\n\n\n\n### 3.2 并行度（Parallelism）\n\n在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，**这里是并行，而不是并发**。这里我们将整个集群并行执行任务的数量称之为**并行度**。那么一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。\n\n\n\n### 3.3 有向无环图（DAG）\n\n> 有向无环图（DAG）的作用就是调度，通过点和线就知道要执行哪些任务，先执行哪些任何等等\n\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328164156.png)\n\n\n\n大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是Hadoop 所承载的 MapReduce，它将计算分为两个阶段，分别为  Map 阶段  和  Reduce 阶段。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个  Job 的串联，以完成一个完整的算法，例如迭代计算。  由于这样的弊端，催生了支持  DAG  框架的产生。因此，支持  DAG  的框架被划分为第二代计算引擎。如  Tez  以及更上层的Oozie。这里我们不去细究各种  DAG  实现之间的区别，不过对于当时的  Tez  和  Oozie  来说，大多还是批处理的任务。接下来就是以  Spark  为代表的第三代的计算引擎。第三代计算引擎的特点主要是  Job  内部的  DAG  支持（不跨越  Job），以及实时计算。\n\n这里所谓的有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来，这样更直观，更便于理解，可以用于表示程序的拓扑结构。 \n\nDAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。\n\n\n\n## 4. 提交流程\n\n所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将 Spark 引用部署到Yarn 环境中会更多一些，所以这里的提交流程是基于 Yarn 环境的。\n\n提交流程分两大块：一个是资源的申请，一个是计算的准备，当资源和计算都准备好了，把计算发给资源就可以执行了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328165845.png)\n\nSpark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client和 Cluster。两种模式主要区别在于：Driver 程序的运行节点位置。如果Driver在集群里面运行的，那么是集群模式，如果Driver在集群之外，那么是Client模式。\n\n\n\n\n\n### 4.1 Yarn Client 模式\n\nClient 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。\n\n- Driver 在任务提交的本地机器上运行\n- Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster\n- ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存\n- ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程\n- Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数\n- 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。\n\n\n\n### 4.2 Yarn Cluster 模式\n\nCluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。\n\n- 在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster\n- 随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。\n- Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程\n- Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数\n- 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。\n\n","tags":["Spark"],"categories":["Spark"]},{"title":"二、Spark运行环境","url":"/2021/03/26/214809/","content":"\n\n\nSpark 作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行,  在国内工作中主流的环境为Yarn，不过逐渐容器式环境也慢慢流行起来。接下来，我们就分别看看不同环境下 Spark 的运行。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326215135.png)\n\n<!-- more -->\n\n\n\n\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. Local模式\n\n所谓的 Local 模式，就是不需要其他任何节点资源就可以在本地执行 Spark 代码的环境，一般用于教学，调试，演示等，之前在 IDEA 中运行代码的环境我们称之为开发环境，不太一样。\n\n将 `spark-3.0.0-bin-hadoop3.2.tgz` 文件上传到 Linux 并解压缩，放置在指定位置，路径中不要包含中文或空格，后续如果涉及到解压缩操作，不再强调。\n\n```bash\ntar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module\ncd /opt/module \nmv spark-3.0.0-bin-hadoop3.2 spark-local\n```\n\n\n\n## 2.  启动 Local 环境\n\n1)  进入解压缩后的路径，执行如下指令\n\n```bash\ncd spark-local\nbin/spark-shell\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326222814.png)\n\n\n\n2)  启动成功后，可以输入网址进行 Web UI 监控页面访问\n\n```tex\nhttp://虚拟机地址:4040\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326222924.png)\n\n\n\n## 3. 命令行工具\n\n在解压缩文件夹下的 data 目录中，添加 word.txt 文件，文件内容如下：\n\n```tex\nHello Scala\nHello Spark\nHello Scala\nHello Spark\n```\n\n在命令行工具中执行如下代码指令（和 IDEA 中代码简化版一致）\n\n```scala\nsc.textFile(\"data/word.txt\").flatMap(_.split(\" \")).map((_,1)).reduceByKey(_+_).collect\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326223450.png)\n\n\n\n## 4. 退出本地模式\n\n按键 Ctrl+C 或输入 Scala 指令\n\n```bash\n:quit\n```\n\n\n\n## 5. 提交应用\n\n在`spark-local`目录下执行如下指令：\n\n```scala\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master local[2] \\\n./examples/jars/spark-examples_2.12-3.0.0.jar \\\n10\n```\n\n1)  `--class` 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序\n2)  `--master local[2]`  部署模式，默认为本地模式，数字表示分配的虚拟 CPU 核数量\n3)  `spark-examples_2.12-3.0.0.jar`  运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包\n4)  数字 10 表示程序的入口参数，用于设定当前应用的任务数量\n\n\n\n## 6. Standalone 模式\n\n> 使用spark必须安装hadoop吗？\n>\n> 一般都是要先装hadoop的，如果你只是玩Spark On Standalon 或 Local 的话，就不需要，如果你想用Spark On Yarn或者是需要去hdfs取数据的话，就应该先装hadoop。\n\n\n\nlocal 本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用 Spark 自身节点运行的**集群模式**，也就是我们所谓的独立部署（Standalone）模式。\n\nSpark 的Standalone 模式体现了经典的 master-slave  模式。\n\n现在我们来实现Standalone模式，集群规划：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327100604.png)\n\n下面所有操作都在spark1主机上进行\n\n### 6.1 解压缩文件\n\n将 `spark-3.0.0-bin-hadoop3.2.tgz` 文件上传到 spark1 并解压缩在指定位置\n\n```bash\ntar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module\ncd /opt/module \nmv spark-3.0.0-bin-hadoop3.2 spark-standalone\n```\n\n\n\n### 6.2 修改配置文件\n\n1)  进入解压缩后路径的 conf 目录，复制 slaves.template 并修改文件名为 slaves\n\n```bash\ncp slaves.template slaves\n```\n\n2)  修改 slaves 文件，添加 work 节点\n\n```tex\nspark1\nspark2\nspark3\n```\n\n3)  复制 spark-env.sh.template 并修改文件名为 spark-env.sh\n\n```bash\ncp spark-env.sh.template spark-env.sh\n```\n\n4)  修改 spark-env.sh 文件，添加 JAVA_HOME 环境变量和集群对应的 master 节点\n\n```bash\nexport JAVA_HOME=/usr/java/default\nSPARK_MASTER_HOST=spark1\nSPARK_MASTER_PORT=7077\n```\n\n注意： 7077 端口，相当于 hadoop3 内部通信的 8020 端口，此处的端口需要确认自己的 Hadoop配置\n\n5)  分发 spark-standalone 目录到其他虚拟机上\n\n```bash\ncd /opt/module/\nscp -r  spark-standalone spark2:/opt/module\nscp -r  spark-standalone spark3:/opt/module\n```\n\n\n\n### 6.3 启动集群\n\n1)  执行脚本命令：\n\n```bash\ncd spark-standalone/\nsbin/start-all.sh\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327124528.png)\n\n2)  查看三台服务器运行进程\n\n在三台虚拟机上分别输入`jps`出现如下信息\n\n```bash\n####### spark1\n[root@spark1 spark-standalone]# jps\n1241 Master\n1387 Jps\n1307 Worker\n####### spark2\n[root@spark2 module]# jps\n1284 Jps\n1223 Worker\n####### spark3\n[root@spark3 spark-standalone]# jps\n1289 Jps\n1228 Worker\n```\n\n3)  查看 Master 资源监控 Web UI 界面: `http://spark1:8080`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327125039.png)\n\n\n\n### 6.4 提交应用\n\n```bash\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://spark1:7077 \\\n./examples/jars/spark-examples_2.12-3.0.0.jar \\\n10\n```\n\n说明\n\n1)  `--class` 表示要执行程序的主类\n2) ` --master spark://spark1:7077`  独立部署模式，连接到 Spark 集群\n3)  `spark-examples_2.12-3.0.0.jar`  运行类所在的 jar 包\n4)  数字 10 表示程序的入口参数，用于设定当前应用的任务数量 \n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327125605.png)\n\n执行任务时，会产生多个 Java 进程\n\n```bash\n[root@spark2 module]# jps \n1362 CoarseGrainedExecutorBackend  //执行节点进程\n1378 Jps\n1223 Worker\n[root@spark3 spark-standalone]# jps\n1360 Jps\n1347 CoarseGrainedExecutorBackend\n1228 Worker\n```\n\n执行任务时，默认采用服务器集群节点的总核数，每个节点内存 1024M。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327125916.png)\n\n\n\n### 6.5 提交参数说明\n\n在提交应用中，一般会同时一些提交参数\n\n```bash\nbin/spark-submit \\\n--class <main-class>\n--master <master-url> \\\n... # other options\n<application-jar> \\\n[application-arguments]\n```\n\n| 参数                       | 解释                                                         | 可选值举例                                      |\n| -------------------------- | ------------------------------------------------------------ | ----------------------------------------------- |\n| `--class`                  | Spark 程序中包含主函数的类                                   |                                                 |\n| `--master`                 | Spark 程序运行的模式(环境)                                   | 模式：`local[*]`、`spark://spark1:7077`、`Yarn` |\n| `--executor-memory 1G`     | 指定每个 executor 可用内存为 1G                              | 符合集群内存配置即可，具体情况具体分析。        |\n| `--total-executor-cores 2` | 指定所有 executor 使用的 cpu 核数为 2 个                     | 符合集群内存配置即可，具体情况具体分析。        |\n| `--executor-cores`         | 指定每个 executor 使用的 cpu 核数                            | 符合集群内存配置即可，具体情况具体分析。        |\n| `application-jar`          | 打包好的应用 jar，包含依赖。这个 URL 在集群中全局可见。  比如 `hdfs://`  共享存储系统，如果是`file://  path` ，那么所有的节点的path 都包含同样的 jar | 符合集群内存配置即可，具体情况具体分析。        |\n| `application-arguments`    | 传给 main()方法的参数                                        |                                                 |\n\n\n\n### 6.6 配置历史服务\n\n由于 spark-shell 停止掉后，集群监控 `spark1:4040` 页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况。\n\n以下指令在spark1虚拟机上执行\n\n1)  复制 spark-defaults.conf.template 并修改文件名为 spark-defaults.conf\n\n```bash\ncp spark-defaults.conf.template spark-defaults.conf\n```\n\n2)  修改 spark-default.conf 文件，配置日志存储路径\n\n```sh\nspark.eventLog.enabled true\nspark.eventLog.dir               hdfs://spark1:8020/sparklog\n```\n\n注意：需要启动 hadoop 集群，HDFS 上的 sparklog 目录需要提前存在。\n\n如果目录不存在，可以在hadoop集群上执行如下命令创建：\n\n```bash\nsbin/start-dfs.sh\nhadoop fs -mkdir /sparklog\n```\n\n3)  修改 spark-env .sh 文件,  添加日志配置\n\n```sh\nexport SPARK_HISTORY_OPTS=\"\n-Dspark.history.ui.port=18080 \n-Dspark.history.fs.logDirectory=hdfs://spark1:8020/sparklog \n-Dspark.history.retainedApplications=30\"\n```\n\n- 参数 1 含义：WEB UI 访问的端口号为 18080\n- 参数 2 含义：指定历史服务器日志存储路径\n- 参数 3 含义：指定保存 Application 历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。\n\n4)  分发配置文件\n\n```bash\ncd /opt/module/spark-standalone\nscp -r  conf spark2:`pwd`\nscp -r  conf spark3:`pwd`\n```\n\n5)  重新启动集群和历史服务\n\n```bash\nsbin/stop-all.sh\nsbin/start-all.sh\nsbin/start-history-server.sh\n```\n\n6)  重新执行任务\n\n```bash\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://spark1:7077 \\\n./examples/jars/spark-examples_2.12-3.0.0.jar \\\n10\n```\n\n7)  查看历史服务：`http://spark1:18080`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327151322.png)\n\n\n\n## 7. 配置高可用（HA）\n\n所谓的高可用是因为当前集群中的 Master 节点只有一个，所以会存在单点故障问题。所以为了解决单点故障问题，需要在集群中配置多个 Master 节点，一旦处于活动状态的 Master发生故障时，由备用 Master 提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper 设置\n\n现在我们来实现高可用，集群规划为：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327152008.png)\n\n以下指令没有特别说明，均在spark1上执行\n\n1)  停止集群\n\n```bash\nsbin/stop-all.sh\n```\n\n2)  启动 Zookeeper\n\n在三台虚拟机上分别执行：`zkServer.sh start`\n\n3)  修改 spark-env.sh 文件添加如下配置\n\n```bash\n注释如下内容：\n# SPARK_MASTER_HOST=spark1\n# SPARK_MASTER_PORT=7077\n\n\n添加如下内容:\n#Master 监控页面默认访问端口为 8080，但是可能会和 Zookeeper 冲突，所以改成 8989，也可以自定义，访问 UI 监控页面时请注意\nSPARK_MASTER_WEBUI_PORT=8989\n\nexport SPARK_DAEMON_JAVA_OPTS=\"\n-Dspark.deploy.recoveryMode=ZOOKEEPER \n-Dspark.deploy.zookeeper.url=spark1,spark2,spark3\n-Dspark.deploy.zookeeper.dir=/spark\"\n```\n\n4)  分发配置文件\n\n```bash\ncd /opt/module/spark-standalone\nscp -r  conf spark2:`pwd`\nscp -r  conf spark3:`pwd`\n```\n\n5)  启动集群\n\n```bash\nsbin/start-all.sh\n```\n\n在浏览器打开`http://spark1:8989`，可以看到spark1节点处于活动状态\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327154926.png)\n\n6)  启动 spark2 的单独 Master 节点，此时 linux2 节点 Master 状态处于备用状态\n\n```bash\n[root@spark2 spark-standalone]# pwd\n/opt/module/spark-standalone\n[root@spark2 spark-standalone]# sbin/start-master.sh\n```\n\n在浏览器打开`http://spark2:8989`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327155326.png)\n\n7)  提交应用到高可用集群\n\n```bash\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://spark1:7077,spark2:7077 \\\n./examples/jars/spark-examples_2.12-3.0.0.jar \\\n10\n```\n\n8)  停止 linux1 的 Master 资源监控进程\n\n```scala\n[root@spark1 spark-standalone]# jps\n3600 Worker\n3537 Master\n2193 SecondaryNameNode\n3893 Jps\n2775 HistoryServer\n3352 QuorumPeerMain\n2028 DataNode\n1950 NameNode\n[root@spark1 spark-standalone]# kill -9 3537\n```\n\n9)  查看 linux2 的 Master  资源监控 Web UI，稍等一段时间后（大概15s），linux2 节点的 Master 状态提升为活动状态\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210327155728.png)\n\n\n\n## 8. Yarn 模式\n\n独立部署（Standalone）模式由 Spark 自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark 主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的 Yarn 环境下 Spark 是如何工作的（其实是因为在国内工作中，Yarn 使用的非常多）。\n\n\n\n### 8.1 解压缩文件\n\n将 `spark-3.0.0-bin-hadoop3.2.tgz` 文件上传到 linux 并解压缩，放置在指定位置。\n\n```scala\ntar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module\ncd /opt/module \nmv spark-3.0.0-bin-hadoop3.2 spark-yarn\n```\n\n下面的所有命令都只在spark1虚拟机上执行\n\n### 8.2 修改配置文件\n\n1)  修改 hadoop 配置文件`/opt/hadoop-2.6.5/etc/hadoop/yarn-site.xml`,  并分发\n\n```xml\n<!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是 true -->\n<property>\n\t<name>yarn.nodemanager.pmem-check-enabled</name>\n\t<value>false</value>\n</property>\n<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是 true -->\n<property>\n\t<name>yarn.nodemanager.vmem-check-enabled</name>\n\t<value>false</value>\n</property>\n```\n\n2)  修改 `/opt/module/spark-yarn/conf/spark-env.sh`，添加 JAVA_HOME 和 YARN_CONF_DIR 配置\n\n```bash\ncd /opt/module/spark-yarn/conf\ncp spark-env.sh.template spark-env.sh\nvim spark-env.sh\n\n## 在最后添加\nexport JAVA_HOME=/usr/java/default\nYARN_CONF_DIR=/opt/hadoop-2.6.5/etc/hadoop\n```\n\n\n\n### 8.3 启动 HDFS 以及 YARN 集群\n\n```bash\nstart-dfs.sh # 启动HDFS\nstart-yarn.sh # 启动yarn\nsbin/start-all.sh #启动Spark\n```\n\n\n\n### 8.4 提交应用\n\n先`cd /opt/module/spark-yarn/` 再提交应用：\n\n```bash\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode cluster \\\n./examples/jars/spark-examples_2.12-3.0.0.jar \\\n10\n```\n\n- `--deploy-mode cluster`：以集群模式执行，控制台不显示执行结果\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328152100.png)\n\n查看`http://spark1:8088`页面\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328152345.png)\n\n\n\n### 8.5 配置历史服务器\n\n1)  复制 `spark-defaults.conf.template` 并修改文件名为 `spark-defaults.conf`\n\n```bash\ncp spark-defaults.conf.template spark-defaults.conf\n```\n\n2)  修改 spark-default.conf 文件，配置日志存储路径\n\n```bash\nspark.eventLog.enabled           true\nspark.eventLog.dir               hdfs://spark1:8020/sparklog\n```\n\n注意：需要启动 hadoop 集群，HDFS 上的目录需要提前存在。\n\n如果目录不存在，可以在hadoop集群上执行如下命令创建：\n\n```bash\nsbin/start-dfs.sh\nhadoop fs -mkdir /sparklog\n```\n\n3)  修改 spark-env .sh 文件,  添加日志配置\n\n```sh\nexport SPARK_HISTORY_OPTS=\"\n-Dspark.history.ui.port=18080 \n-Dspark.history.fs.logDirectory=hdfs://spark1:8020/sparklog \n-Dspark.history.retainedApplications=30\"\n```\n\n- 参数 1 含义：WEB UI 访问的端口号为 18080\n- 参数 2 含义：指定历史服务器日志存储路径\n- 参数 3 含义：指定保存 Application 历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。\n\n4)  修改 spark-defaults.conf，再最后添加\n\n```conf\nspark.yarn.historyServer.address=spark1:18080\nspark.history.ui.port=18080\n```\n\n5)  启动历史服务\n\n```bash\nsbin/start-history-server.sh\n```\n\n6)  重新提交应用\n\n```bash\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode client \\\n./examples/jars/spark-examples_2.12-3.0.0.jar \\\n10\n```\n\n- `--deploy-mode client`：以客户端模式执行，控制台显示执行结果\n\n查看`http://spark1:18080/`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328155109.png)\n\n\n\n\n\n## 9. K8S & Mesos 模式\n\nMesos 是 Apache 下的开源分布式资源管理框架，它被称为是分布式系统的内核，在Twitter 得到广泛使用，管理着 Twitter 超过 30,0000 台服务器上的应用部署，但是在国内，依然使用着传统的 Hadoop 大数据框架，所以国内使用 Mesos 框架的并不多， 但是原理其实都差不多，这里我们就不做过多讲解了。 \n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328155457.png)\n\n容器化部署是目前业界很流行的一项技术，基于 Docker 镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是 Kubernetes（k8s），而 Spark也在最近的版本中支持了 k8s 部署模式。这里我也不做过多的讲解。给个链接大家自己感受一下：https://spark.apache.org/docs/latest/running-on-kubernetes.html\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328155556.png)\n\n\n\n## 10. Windows 模式\n\n在自己学习时，每次都需要启动虚拟机，启动集群，这是一个比较繁琐的过程，并且会占大量的系统资源，导致系统执行变慢，不仅仅影响学习效果，也影响学习进度，Spark 非常暖心地提供了可以在 windows 系统下启动本地集群的方式，这样，在不使用虚拟机的情况下，也能学习 Spark 的基本使用，摸摸哒！\n\n在学习Spark时，一般情况下都会采用 windows 系统的集群来学习 Spark。\n\n\n\n### 10.1 解压缩文件\n\n将文件 spark-3.0.0-bin-hadoop3.2.tgz 解压缩到无中文无空格的路径中\n\n\n\n### 10.2 启动本地环境\n\n1)  执行解压缩文件路径下 bin 目录中的 spark-shell.cmd 文件，启动 Spark 本地环境\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328160258.png)\n\n\n\n2)  在 bin 目录中创建 input 目录，并添加 word.txt 文件,  在word.txt中输入以下内容\n\n```tex\nHello Scala\nHello Spark\nHello Scala\nHello Spark\n```\n\n在命令行中输入脚本代码\n\n```scala\nsc.textFile(\"input/word.txt\").flatMap(_.split(\" \")).map((_,1)).reduceByKey(_+_).collect\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328160521.png)\n\n\n\n### 10.3 命令行提交应用\n\n在 bin 目录下打开cmd，输入\n\n```bash\nspark-submit --class org.apache.spark.examples.SparkPi --master local[2] ../examples/jars/spark-examples_2.12-3.0.0.jar 10\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328161045.png)\n\n\n\n## 11. 部署模式对比\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210328161149.png)\n\n\n\n## 12. 端口号\n\n- Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）\n- Spark Master 内部通信服务端口号：7077\n- Standalone 模式下，Spark Master Web 端口号：8080（资源）\n- Spark 历史服务器端口号：18080\n- Hadoop YARN 任务运行情况查看端口号：8088\n\n\n\n","tags":["Spark"],"categories":["Spark"]},{"title":"Scala常见操作总结","url":"/2021/03/26/204025/","content":"\nScala常见操作总结\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n## 1. groupBy\n\ngroupBy方法定义\n\n```scala\ndef groupBy [K] (f: (A) ⇒ K): Map[K, Traversable[A]]\n```\n\n（1）案例1\n\n```scala\nval birds = List(\"Golden Eagle\", \"Gyrfalcon\", \"American Robin\", \"Mountain BlueBird\", \"Mountain-Hawk Eagle\")\nval groupedByFirstLetter = birds.groupBy(_.charAt(0))\nprintln(groupedByFirstLetter)\n```\n\n输出\n\n```tex\nMap(M -> List(Mountain BlueBird, Mountain-Hawk Eagle), G -> List(Golden Eagle, Gyrfalcon), A -> List(American Robin))\n```\n\n（2）案例2\n\n```scala\nval cats = List(\"Tiger\", \"Lion\", \"Puma\", \"Leopard\", \"Jaguar\", \"Cheetah\", \"Bobcat\")\nval groupedByLength = cats.groupBy(_.length)\nprintln(groupedByLength)\n```\n\n输出\n\n```tex\nMap(5 -> List(Tiger), 4 -> List(Lion, Puma), 7 -> List(Leopard, Cheetah), 6 -> List(Jaguar, Bobcat))\n```\n\n（3）案例3\n\n```scala\nval raptors = List(\"Golden Eagle\", \"Bald Eagle\", \"Prairie Falcon\", \"Peregrine Falcon\", \"Harpy Eagle\", \"Red Kite\")\nval kinds = raptors.groupBy {\n    case bird if bird.contains(\"Eagle\") => \"eagle\"\n    case bird if bird.contains(\"Falcon\") => \"falcon\"\n    case _ => \"unknown\"\n}\nprintln(kinds)\n```\n\n输出\n\n```tex\nMap(eagle -> List(Golden Eagle, Bald Eagle, Harpy Eagle), falcon -> List(Prairie Falcon, Peregrine Falcon), unknown -> List(Red Kite))\n```\n\n（4）案例4\n\n```scala\nval strList=List(\"aaa\",\"bbb\",\"ccc\",\"bbb\",\"ccc\")\nval results: Map[String, List[String]] = strList.groupBy(w => w)\nprintln(results)\n```\n\n输出\n\n```tex\nMap(ccc -> List(ccc, ccc), bbb -> List(bbb, bbb), aaa -> List(aaa))\n```\n\n\n\n（5）案例5\n\n```scala\nval words = List(\"one\", \"two\", \"one\", \"three\", \"four\", \"two\", \"one\")\nval counts = words.groupBy(w => w).mapValues(_.size)\nprintln(counts)\n```\n\n输出\n\n```tex\nMap(one -> 3, three -> 1, four -> 1, two -> 2)\n```\n\n（6）案例6\n\n```scala\nval numbers = List(1,4,5,1,6,5,2,8,1,9,2,1)\n//map 每个元素返回(x._1, x._2.length)\nval results: Map[Int, Int] = numbers.groupBy(x => x).map { x => (x._1, x._2.length) }\nprintln(results)\n```\n\n输出\n\n```tex\nMap(5 -> 2, 1 -> 4, 6 -> 1, 9 -> 1, 2 -> 2, 8 -> 1, 4 -> 1)\n```\n\n\n\n\n\n\n\n","tags":["scala"],"categories":["scala"]},{"title":"Scala中==、eq与equals的区别","url":"/2021/03/26/181800/","content":"\n\n\n推荐先了解一下[hashCode、identityHashCode、equals和==的原理](hashCode、identityHashCode、equals和==的原理详解)，再来看Scala中==、eq与equals的区别\n\n<!-- more -->\n\n\n\n根据[官方API](http://www.scala-lang.org/api/current/#scala.AnyRef)的定义：\n\n- final def ==(arg0: Any): Boolean\n  The expression x == that is equivalent to if (x eq null) that eq null else x.equals(that).\n- final def eq(arg0: AnyRef): Boolean\n  Tests whether the argument (that) is a reference to the receiver object (this).\n- def equals(arg0: Any): Boolean\n  The equality method for reference types.\n\n简言之，equals方法是检查值是否相等，而eq方法检查的是引用是否相等。\n\n对于`==`来说，所以如果比较的对象是null，那么`==`调用的是eq，不是`null`则调用的是equals。\n\n示例1\n\n```scala\nval c=new String(\"aa\");\nval d=new String(\"aa\");\n\nprintln(c,d) //(aa,aa)\nprintln(c==d) //true\nprintln(c.equals(d)) //true\nprintln(c.eq(d)) //false\n```\n\n可以看出，对于不是null的对象，`==`和equals是一样的，这一点和java不同，java中的`==`对于引用类型来说，比较永远是两个对象的内存地址是否相等。\n\n在Java中如果要对两个对象进行值比较，那么必须要实现equals 和hashCode方法。而在scala中为开发者提供了`case class`，默认实现了equals 和hashCode方法。\n\n示例2\n\n```scala\nobject T2 {\n\n  def main(args: Array[String]): Unit = {\n    //普通类\n    val a1=new A(1,\"abc\")\n    val a2=new A(1,\"abc\")\n    println(a1==a2) //false\n    println(a1.equals(a2))//false\n    println(a1.eq(a2))//false\n\n    //样例类\n    val b1=new B(2,\"efg\")\n    val b2=new B(2,\"efg\")\n    println(b1==b2) //true\n    println(b1.equals(b2))//true\n    println(b1.eq(b2))//false\n  }\n}\n//普通类\nclass A(_a:Int,_b:String){\n  var a:Int=_a\n  var b:String=_b\n}\n//样例类\ncase class B(_a:Int,_b:String){\n  var a:Int=_a\n  var b:String=_b\n}\n```\n\n可以看到，对于样例类，默认实现了equals 和hashCode，所以`==`和`equals`都是true，而eq比较的永远都是内存地址是否相等，所以输出false。\n\n\n\n\n\n【参考资料】\n\n[Scala中==,eq与equals的区别](http://www.cnblogs.com/woople/p/6839310.html)","tags":["scala"],"categories":["scala"]},{"title":"一、Spark快速入门","url":"/2021/03/26/170016/","content":"\nSpark快速入门\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. Spark概述\n\n### 1.1 Spark 是什么\n\nSpark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。\n\n### 1.2 Spark and Hadoop\n\n**Hadoop** \n\nHadoop 是由 java 语言编写的，在分布式服务器集群上存储海量数据并运行分布式\n分析应用的开源框架\n\n- 作为 Hadoop **分布式文件系统**，HDFS 处于 Hadoop 生态圈的最下层，存储着所有的 数 据 ，支持着Hadoop 的 所 有 服 务 。 它 的 理 论 基 础 源 于 Google 的TheGoogleFileSystem 这篇论文，它是 GFS 的开源实现。\n- MapReduce 是一种编程模型，Hadoop 根据 Google 的 MapReduce 论文将其实现，作为Hadoop 的**分布式计算模型**，是 Hadoop 的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了 HDFS 的分布式存储和 MapReduce 的分布式计算，Hadoop 在处理海量数据时，性能横向扩展变得非常容易。\n- HBase 是对 Google 的 Bigtable 的开源实现，但又和 Bigtable 存在许多不同之处。HBase 是一个基于 HDFS 的**分布式数据库**，擅长实时地随机读/写超大规模数据集。它也是 Hadoop 非常重要的组件。\n\n**Spark**\n\n- Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎\n- Spark Core 中提供了 Spark 最基础与最核心的功能\n- Spark  SQL 是 Spark 用来操作结构化数据的组件。通过 Spark  SQL，用户可以使用SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。\n- Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API。\n\n由上面的信息可以获知，Spark 出现的时间相对较晚，并且主要功能主要是**用于数据计算**，所以其实 Spark 一直被认为是 Hadoop  框架的升级版。\n\n\n\n### 1.3 Spark and MR\n\nHadoop 的 MR 框架和 Spark 框架都是数据处理框架，那么我们在使用时如何选择呢？\n\nHadoop 的 MR 框架设计的初衷是一次性数据计算，所谓的一次性数据计算，就是框架在处理的时候，会从存储设备中读取数据，进行逻辑操作，然后将处理的结果重新存储到介质中。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326175103.png)\n\n而Spark框架把数据处理的中间结果放入内存中，为下一次的计算提供了便利\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326175346.png)\n\n\n\n- Hadoop MapReduce 由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题。所以 Spark 应运而生，**Spark就是在传统的MapReduce计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的 RDD 计算模型**。\n- 机器学习中 ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。 MR这种模式不太合适，即使多 MR 串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR 显然不擅长。而Spark 所基于的 scala 语言恰恰擅长函数的处理。\n- Spark 是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集（Resilient Distributed Datasets），提供了比 MapReduce 丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。\n- **Spark 和 Hadoop 的根本差异是多个作业之间的数据通信问题**  : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘。\n- Spark  Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式。\n- Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互。\n- Spark 的缓存机制比 HDFS 的缓存机制高效。\n\n经过上面的比较，我们可以看出在绝大多数的数据计算场景中，Spark 确实会比 MapReduce更有优势。但是 Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败。此时， MapReduce 其实是一个更好的选择，所以 Spark并不能完全替代 MR。\n\n\n\n### 1.4 Spark核心模块\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326175900.png)\n\n- Spark Core\n  Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib 都是在 Spark Core 的基础上进行扩展的\n- Spark SQL\n  Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。\n- Spark Streaming\n  Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API。\n- Spark MLlib\n  MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。\n- Spark GraphX\n  GraphX 是 Spark 面向图计算提供的框架与算法库。\n\n\n\n## 2. Spark快速上手\n\n接下来，就让咱们走进 Spark 的世界，了解一下它是如何带领我们完成数据处理的。\n\n### 2.1 增加 Scala 插件\n\nSpark 由 Scala 语言开发的，所以本课件接下来的开发所使用的语言也为 Scala，咱们当前使用的 Spark 版本为 3.0.0，默认采用的 Scala 编译版本为 2.12，所以后续开发时。我们依然采用这个版本。开发前请保证 IDEA 开发工具中含有 Scala 开发插件\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326180258.png)\n\n\n\n### 2.2 增加依赖关系\n\n修改 Maven 项目中的 POM 文件，增加 Spark 框架的依赖关系。 这里基于 Spark3.0 版本，使用时请注意对应版本。\n\n```xml\n<dependencies>\n    <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-core_2.12</artifactId>\n        <version>3.0.0</version>\n    </dependency>\n</dependencies>\n```\n\n### 2.3 WordCount\n\n为了能直观地感受 Spark 框架的效果，接下来我们实现一个大数据学科中最常见的教学案例 WordCount\n\n核心思想：**缺什么补什么**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326181437.png)\n\n代码如下：\n\n```scala\npackage com.layne.spark.core.wc\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject Spark01_WordCount {\n\n  def main(args: Array[String]): Unit = {\n\n    // Application\n    // Spark框架\n    // TODO 建立和Spark框架的连接\n    val sparConf = new SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n    val sc = new SparkContext(sparConf)\n\n    // TODO 执行业务操作\n    // 1. 读取文件，拿到一行一行的数据\n    val lines: RDD[String] = sc.textFile(\"datas\")\n\n    // 2. 将一行数据进行拆分，形成一个一个的单词（分词）\n    //    扁平化：将整体拆分成个体的操作\n    //   \"hello world\" => hello, world, hello, world\n    val words: RDD[String] = lines.flatMap(_.split(\" \"))\n\n    // 3. 将数据根据单词进行分组，便于统计\n    //    (hello, hello, hello), (world, world)\n    val wordGroup: RDD[(String, Iterable[String])] = words.groupBy(word => word)\n\n    // 4. 对分组后的数据进行转换\n    //    (hello, hello, hello), (world, world)\n    //    (hello, 3), (world, 2)\n    val wordToCount = wordGroup.map {\n          //前面的=>和match等等省略了\n      case ( word, list ) => {\n        (word, list.size)\n      }\n    }\n\n    // 5. 将转换结果采集到控制台打印出来\n    val array: Array[(String, Int)] = wordToCount.collect()\n    array.foreach(println)\n\n    // TODO 关闭连接\n    sc.stop()\n\n  }\n}\n```\n\n\n\n实现方式2\n\n```scala\npackage com.layne.spark.core.wc\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject Spark02_1WordCount{\n\n  def main(args: Array[String]): Unit = {\n\n    // Application\n    // Spark框架\n    // TODO 建立和Spark框架的连接\n    // JDBC : Connection\n    val sparConf = new SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n    val sc = new SparkContext(sparConf)\n\n    // TODO 执行业务操作\n\n    // 1. 读取文件，获取一行一行的数据\n    //    hello world\n    val lines: RDD[String] = sc.textFile(\"datas\")\n\n    // 2. 将一行数据进行拆分，形成一个一个的单词（分词）\n    //    扁平化：将整体拆分成个体的操作\n    //   \"hello world\" => hello, world, hello, world\n    val words: RDD[String] = lines.flatMap(_.split(\" \"))\n\n    //=============改进1\n    // 3. 将单词进行结构的转换,方便统计\n    // word => (word, 1)\n    val wordToOne = words.map(word=>(word,1))\n\n    val wordGroup: RDD[(String, Iterable[(String, Int)])] = wordToOne.groupBy(t => t._1)\n\n    //============= 改进2\n    // 4. 对分组后的数据进行转换\n    //    (hello, hello, hello), (world, world)\n    //    (hello, 3), (world, 2)\n    val wordToCount = wordGroup.map {\n      //前面的=>和match等等省略了\n      case ( word, list ) => {\n        val result: (String, Int) = list.reduce(\n          (t1, t2) => {\n            (t1._1, t1._2 + t2._2)\n          }\n        )\n        result\n      }\n    }\n\n    // 5. 将转换结果采集到控制台打印出来\n    val array: Array[(String, Int)] = wordToCount.collect()\n    array.foreach(println)\n\n    // TODO 关闭连接\n    sc.stop()\n\n  }\n}\n\n```\n\n实现方式3\n\n```scala\npackage com.layne.spark.core.wc\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject Spark02_2WordCount {\n\n  def main(args: Array[String]): Unit = {\n\n    // Application\n    // Spark框架\n    // TODO 建立和Spark框架的连接\n    // JDBC : Connection\n    val sparConf = new SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n    val sc = new SparkContext(sparConf)\n\n    // TODO 执行业务操作\n\n    // 1. 读取文件，获取一行一行的数据\n    //    hello world\n    val lines: RDD[String] = sc.textFile(\"datas\")\n\n    // 2. 将一行数据进行拆分，形成一个一个的单词（分词）\n    //    扁平化：将整体拆分成个体的操作\n    //   \"hello world\" => hello, world, hello, world\n    val words: RDD[String] = lines.flatMap(_.split(\" \"))\n\n    //=============改进1\n    // 3. 将单词进行结构的转换,方便统计\n    // word => (word, 1)\n    val wordToOne = words.map(word => (word, 1))\n\n\n    //Spark框架提供了更多的功能，可以将分组和聚合使用一个方法实现\n\n    // 4. 将转换后的数据进行分组聚合\n    // 相同key的value进行聚合操作\n    //reduceByKey会寻找相同key的数据，当找到这样的两条记录时会对其value(分别记为x,y)做(x,y) => x+y的处理，即只保留求和之后的数据作为value。反复执行这个操作直至每个key只留下一条记录。\n    val wordToSum: RDD[(String, Int)] = wordToOne.reduceByKey(_ + _)\n    //等同于val wordToSum: RDD[(String, Int)] = wordToOne.reduceByKey((x: Int, y: Int) => x + y)\n\n    // 5. 将转换结果采集到控制台打印出来\n    val array: Array[(String, Int)] = wordToSum.collect()\n    array.foreach(println)\n\n    // TODO 关闭连接\n    sc.stop()\n\n  }\n}\n\n```\n\n\n\n执行过程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326210620.png)\n\n\n\n执行过程中，会产生大量的执行日志，如果为了能够更好的查看程序的执行结果，可以在项目的 resources 目录中创建 log4j.properties 文件，并添加日志配置信息：\n\n```properties\n# ERROR级别，即只有错误的时候才会打印，平时就打印了\nlog4j.rootCategory=ERROR, console \nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n# Set the default spark-shell log level to ERROR. When running the spark-shell,the\n# log level for this class is used to overwrite the root logger's log level, so that\n# the user can have different defaults for the shell and regular Spark apps.\nlog4j.logger.org.apache.spark.repl.Main=ERROR\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.spark_project.jetty=ERROR\nlog4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR\nlog4j.logger.org.apache.parquet=ERROR\nlog4j.logger.parquet=ERROR\n# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support\nlog4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL\nlog4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR\n```\n\n\n\n### 2.4 异常处理\n\n如果本机操作系统是 Windows，在程序中使用了 Hadoop 相关的东西，比如写入文件到HDFS，则会遇到如下异常：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326214453.png)\n\n出现这个问题的原因，并不是程序的错误，而是 windows 系统用到了 hadoop 相关的服务，解决办法是通过配置关联到 windows 的系统依赖就可以了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210326214650.png)\n\n把这个路径添加到系统环境变量里面\n\n","tags":["Spark"],"categories":["Spark"]},{"title":"八、Scala编程之递归思想","url":"/2021/03/26/141800/","content":"\nScala编程之递归思想\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. 函数式编程(递归思想)\n\nScala 是运行在 Java 虚拟机（Java Virtual Machine）之上，因此具有如下特点:\n1) 轻松实现和丰富的 Java 类库互联互通。\n2) 它既支持面向对象的编程方式，又支持函数式编程。\n3) 它写出的程序像动态语言一样简洁，但事实上它确是严格意义上的静态语言。\n4) Scala 就像一位武林中的集大成者，将过去几十年计算机语言发展历史中的精萃集于一身，化繁为简，为程序员们提供了一种新的选择。设计者马丁·奥得斯基 希望程序员们将编程作为简洁，高效，令人愉快的工作。同时也让程序员们进行关于编程思想的新的思考。\n\n\n\n**Scala提倡函数式编程(递归思想)**\n\n先说下编程范式:\n1) 在所有的编程范式中，面向对象编程（Object-Oriented Programming）无疑是最大的赢家。\n2) 但其实面向对象编程并不是一种严格意义上的编程范式，严格意义上的编程范式分为：命令式编程（Imperative Programming）、函数式编程（Functional Programming）和逻辑式编程（Logic Programming）。面向对象编程只是上述几种范式的一个交叉产物，更多的还是继承了命令式编程的基\n因。\n3) 在传统的语言设计中，只有命令式编程得到了强调，那就是程序员要告诉计算机应该怎么做。而递归则通过灵巧的函数定义，告诉计算机做什么。因此在使用命令式编程思维的程序中，是现在多数程序采用的编程方式，递归出镜的几率很少，而在函数式编程中，大家可以随处见到递归的方式。\n\n\n\n## 2. 应用实例：递归时间测试\n\nscala中循环不建议使用while和do...while,而建议使用递归。\n\n案例：计算1-50的和\n\n常规的解决方式\n\n```scala\npackage recur\n\nimport java.text.SimpleDateFormat\nimport java.util.Date\n\nobject T1 {\n\n  def main(args: Array[String]): Unit = {\n    //传统方法完成 1-50 的求和任务\n    val now: Date = new Date()\n    val dateFormat: SimpleDateFormat =\n      new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n    val date = dateFormat.format(now)\n\n    println(\"date=\" + date) //输出时间\n    var res = BigInt(0)\n    var num = BigInt(1)\n    var maxVal = BigInt(99999999l) //BigInt(99999999l)[测试效率大数]\n    while (num <= maxVal) {\n      res += num\n      num += 1\n    }\n    //耗时18s\n    println(\"res=\" + res) //res=4999999950000000\n    //再一次输出时间\n    val now2: Date = new Date()\n    val date2 = dateFormat.format(now2)\n    println(\"date2=\" + date2) //输出时间\n  }\n}\n\n```\n\n函数式编程的重要思想就是尽量不要产生额外的影响,上面的代码就不符合函数式编程的思想, 下面我们看看使用函数式编程方式来解决(Scala提倡的方式)\n测试：看看递归的速度是否有影响?\n\n\n\n```scala\npackage recur\n\nimport java.text.SimpleDateFormat\nimport java.util.Date\n\nobject T2 {\n\n  def main(args: Array[String]): Unit = {\n    // 递归的方式来解决\n    //传统方法完成 1-50 的求和任务\n    val now: Date = new Date()\n    val dateFormat: SimpleDateFormat =\n      new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n    val date = dateFormat.format(now)\n    println(\"date=\" + date) //输出时间\n\n    def mx(num: BigInt, sum: BigInt): BigInt = {\n      if (num <= 99999999l) return mx(num + 1, sum + num)\n      else return sum\n    }\n\n    //测试\n    var num = BigInt(1)\n    var sum = BigInt(0)\n    var res = mx(num,sum)\n    //耗费16s\n    println(\"res=\" + res) //输出：res=4999999950000000\n\n    //再一次输出时间\n    val now2: Date = new Date()\n    val date2 = dateFormat.format(now2)\n    println(\"date2=\" + date2) //输出时间\n  }\n}\n```\n\n\n\n## 3. 应用案例：最大值\n\n求最大值\n\n\n\n```scala\n//大话java数据结构\ndef max(xs: List[Int]): Int = {\nif (xs.isEmpty)\n\tthrow new java.util.NoSuchElementException\nif (xs.size == 1)\n\txs.head\nelse if (xs.head > max(xs.tail)) xs.head else max(xs.tail)\n}\n```\n\n\n\n## 4. 应用案例：翻转字符串\n\n```scala\ndef reverse(xs: String): String =\n\tif (xs.length == 1) xs else reverse(xs.tail) + xs.head\n```\n\n\n\n## 5. 应用案例：求阶乘\n\n```scala\ndef factorial(n: Int): Int =\n\tif (n == 0) 1 else n * factorial(n - 1)\n```\n\n","tags":["scala"],"categories":["scala"]},{"title":"七、Scala之函数式编程（高级部分）","url":"/2021/03/26/103737/","content":"\nScala之函数式编程（高级部分）\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n## 1. 偏函数\n\n先看一个需求\n给你一个集合`val list = List(1, 2, 3, 4, \"abc\") `，请完成如下要求:\n1) 将集合list中的所有数字+1，并返回一个新的集合\n2) 要求忽略掉 非数字 的元素，即返回的 新的集合 形式为 (2, 3, 4, 5)\n\n**思路1-map+fliter方式**\n\n```scala\nval list = List(1, 2, 3, 4, \"abc\")\n//思路1,使用map+fliter的思路\ndef f1(n:Any): Boolean = {\nn.isInstanceOf[Int]\n}\ndef f2(n:Int): Int = {\nn + 1\n}\ndef f3(n:Any): Int ={\nn.asInstanceOf[Int]\n}\nval list2 = list.filter(f1).map(f3).map(f2)\nprintln(\"list2=\" + list2)\n```\n\n\n\n**思路2-模式匹配**\n\n```scala\ndef addOne( i : Any ): Any = {\n    i match {\n        case x: Int => x + 1\n        case _ =>  //什么都不做\n    }\n}\nval list = List(1, 2, 3, 4, \"abc\")\nval list2 = list.map(addOne)\nprintln(\"list2=\" + list2) //list2=List(2, 3, 4, 5, ()) ，多了一个()\n```\n\n可以看到，上面思路2的输出多了一个`()`\n\n\n\n**偏函数**\n\n1) 在对**符合某个条件**，而不是所有情况进行逻辑操作时，使用偏函数是一个不错的选择\n2) 将包在大括号内的一组case语句封装为函数，我们称之为偏函数，它**只对会作用于指定类型的参数或指定范围值的参数实施计算**，超出范围的值会忽略（未必会忽略，这取决于你打算怎样处理）\n3) 偏函数在Scala中是一个特质（trait）：PartialFunction\n\n**使用偏函数解决前面的问题**\n\n```scala\nval list = List(1, 2, 3, 4, \"abc\")\n//说明 定义一个偏函数\n//1. PartialFunction[Any, Int] 表示偏函数接收的参数类型是Any，返回类型是Int\n//2. isDefinedAt(x: Any)如果返回true,就会调用apply构建对象实例，如果是false，过滤掉\n//3. apply 构造器，对传入的值+1并返回\nval addOne3= new PartialFunction[Any, Int]  {\n    override def isDefinedAt(x: Any): Boolean = {\n        //或直接：x.isInstanceOf[Int]\n        if (x.isInstanceOf[Int])\n        true\n        else false\n    }\n\n    override def apply(v1: Any): Int = v1.asInstanceOf[Int]+1\n}\n//使用偏函数\n//说明：如果是使用偏函数，则不能使用map,应该用collect\nval list3 = list.collect(addOne3) //list3=List(2, 3, 4, 5)\nprintln(\"list3=\" + list3)\n```\n\n**偏函数的小结**\n\n1) 使用构建特质的实现类(使用的方式是PartialFunction的匿名子类)\n2)  PartialFunction 是个特质(看源码)\n3)  构建偏函数时，参数形式   [Any, Int]是泛型，第一个表示参数类型，第二个表示返回参数\n4) 当使用偏函数时，会遍历集合的所有元素，编译器执行流程时先执行`isDefinedAt()`如果为true ,就会执行 apply, 构建一个新的Int 对象返回\n5) 执行isDefinedAt() 为false 就过滤掉这个元素，即不构建新的Int对象.\n6) map函数不支持偏函数，因为map底层的机制就是所有循环遍历，无法过滤处理原来集合的元素\n7) collect函数支持偏函数\n\n**偏函数简化形式**\n\n声明偏函数，需要重写特质中的方法，有的时候会略显麻烦，而Scala其实提供了简单的方法\n\n1) 简化形式1\n\n```scala\ndef f2: PartialFunction[Any, Int] = {\n\tcase i: Int => i + 1 // case语句可以自动转换为偏函数\n}\nval list2 = List(1, 2, 3, 4,\"ABC\").collect(f2)\n```\n\n2) 简化形式2\n\n```scala\nval list3 = List(1, 2, 3, 4,\"ABC\").collect{ case i: Int => i + 1 }\nprintln(list3)\n```\n\n\n\n## 2. 作为参数的函数\n\n函数作为一个变量传入到了另一个函数中，那么该**作为参数的函数的类型**是：function1，即：(参数类型) => 返回类型\n\n```scala\ndef plus(x: Int) = 3 + x\n//说明 下面三种形式相同\nval result1 = Array(1, 2, 3, 4).map(plus(_))\nval result2 = Array(1, 2, 3, 4).map(plus)\nval result3 = Array(1, 2, 3, 4).map(plus _)\nprintln(result1.mkString(\",\"))//\"4,5,6,7\"\nprintln(result2.mkString(\",\"))//\"4,5,6,7\"\nprintln(result3.mkString(\",\")) //4,5,6,7\n\n//scala中，函数也是有类型的\n//plus _中的_是指：本身不执行这个方法，而是把这个方法的类型或引用拿出来\nprintln(\"plus的函数类型：\"+(plus _))\n//效果同上\nprintln(\"plus的函数类型：\"+plus)\n```\n\n1) `map(plus(_))` 中的`plus(_) `就是将plus这个函数当做一个参数传给了map，`_`这里代表从集合中遍历出来的一个元素，这个下换线和`plus _`中的下换线含义是不一样的\n2) `plus(_)` 这里也可以写成 plus ，表示对 Array(1,2,3,4) 遍历，将每次遍历的元素传给plus的 x\n3) 进行 3 + x 运算后，返回新的Int ，并加入到新的集合 result1中\n4) `def map[B, That](f: A => B) `的声明中的` f: A => B` 一个函数，A表示传入函数的参数类型，B表示传入函数的返回类型\n\n\n\n## 3. 匿名函数\n\n没有名字的函数就是匿名函数，可以通过函数表达式来设置匿名函数\n\n```scala\nval triple = (x: Double) => 3 * x\nprintln(triple(3))\n```\n\n说明\n1)` (x: Double) => 3 * x` 就是匿名函数\n2) `(x: Double)` 是形参列表， => 是规定语法表示后面是函数体，` 3 * x `就是函数体，如果有多行，可以 {} 换行写.\n3) triple 是指向匿名函数的变量。\n\n请编写一个匿名函数，可以返回2个整数的和，并输出该匿名函数的类型\n\n```scala\nval f1 = (n1: Int, n2: Int ) => {\n    println(\"匿名函数被调用\")\n    n1 + n2\n}\nprintln(\"f1类型=\" + f1)\nprintln(f1(10, 30))\n```\n\n\n\n\n\n## 4. 高阶函数\n\n能够接受函数作为参数的函数，叫做高阶函数 (higher-order function)。可使应用程序更加健壮。\n\n**高阶函数基本使用**\n\n```scala\n//test 就是一个高阶函数，它可以接收f: Double =>\nDouble\ndef test(f: Double => Double, n1: Double) = {\n    f(n1)\n}\n//sum 是接收一个Double,返回一个Double\ndef sum(d: Double): Double = {\n    d + d\n}\nval res = test(sum, 6.0)\nprintln(\"res=\" + res)//res=12.0\n```\n\n**高阶函数可以返回函数类型**\n\n```scala\n//minusxy返回了一个匿名函数，该匿名函数是 (y: Int) => x - y\ndef minusxy(x: Int) = {\n    (y: Int) => x - y //匿名函数\n}\n\nval result3 = minusxy(3)(5)\n/*\n    分步写\n    val result3 = minusxy(3)\n    result3(5)\n    println(result3)//输出函数，而不是输出值\n     */\nprintln(result3)//2\n```\n\n**高级函数案例的小结**\n\n说明: `def minusxy(x: Int) = (y: Int) => x - y`\n1) 函数名为 minusxy\n2) 该函数返回一个匿名函数`(y: Int) = > x -y`\n\n说明val result3 = minusxy(3)(5)\n1) minusxy(3)执行`minusxy(x: Int)`得到` (y: Int) => 3 - y` 这个匿名函\n2) minusxy(3)(5)执行` (y: Int) => x - y` 这个匿名函数\n3) 也可以分步执行:` val f1 = minusxy(3);   val res = f1(90)`\n\n\n\n## 5. 参数(类型)推断\n\n参数推断省去类型信息（在某些情况下[需要有应用场景]，参数类型是可以推断出来的，如\n\n```scala\nval list = List(1, 2, 3, 4)\nlist.map()//这里因为list中所有的数据都是Int，所以map中接收的参数自动推断为Int，同时也可以进行简写\n```\n\nmap中函数参数类型是可以推断的，同时也可以进行相应的简写。\n\n1) 参数类型是可以推断时，**可以省略参数类型**\n2) 当传入的函数，只有单个参数时，**可以省去括号**\n3) 如果变量只在`=>`右边只出现一次，可以用`_`来代替\n\n**应用案例**\n\n```scala\n//分别说明\nval list = List(1, 2, 3, 4)\n//1. 直接传入一个匿名函数：(x:Int)=>x + 1\nprintln(list.map((x:Int)=>x + 1)) //List(2, 3, 4, 5)\n\n//2.因为map已经推断出来接收的参数是Int，所以匿名函数中的Int可以去掉\nprintln(list.map((x)=>x + 1)) //List(2, 3, 4, 5)\n\n//3. 当传入的函数，只有单个参数时，()可以去掉\nprintln(list.map(x=>x + 1)) // List(2, 3, 4, 5)\n\n//4.因为x在 => 的右边只出现了一次，所以可以用_代替，同时左侧的就不需要了\nprintln(list.map(_ + 1)) //// List(2, 3, 4, 5)\n```\n\n1) map是一个高阶函数，因此也可以直接传入一个匿名函数，完成map\n2) 当遍历list时，参数类型是可以推断出来的，可以省略数据类型Int，即`list.map((x)=>x + 1)`\n3) 当传入的函数，只有单个参数时，可以省去括号，即`list.map(x=>x + 1)`\n4) 如果变量只在`=>`右边只出现一次，可以用`_`来代替`list.map(_ + 1)`\n\n再来看一个案例\n\n```scala\ndef f1(n1: Int, n2: Int): Int = {\n    n1 + n2\n}\n\nval list = List(1, 2, 3, 4)\n//reduce默认是reduceLift\nprintln(list.reduce(f1)) //10\n\n//写成匿名函数\nprintln(list.reduce((n1: Int, n2: Int) => n1 + n2))//10\n\n//因为map已经推断出来接收的参数是Int，所以匿名函数中的Int可以去掉\nprintln(list.reduce((n1, n2) => n1 + n2))//10\n\n//因为n1,n2在 => 的右边只出现了一次，所以可以用_代替，同时左侧的就不需要了\nprintln(list.reduce(_+_))//10\n```\n\n\n\n## 6. 闭包(closure)\n\n基本介绍：闭包就是一个函数和与其相关的引用环境组合的一个整体(实体)\n\n**案例演示**\n\n```scala\ndef minusxy(x: Int) = (y: Int) => x - y\n//f函数就是闭包.\nval f = minusxy(20)\nprintln(\"f(1)=\" + f(1)) // 19\nprintln(\"f(2)=\" + f(2)) // 18\n```\n\n代码小结\n1)   第1点`(y: Int) => x – y`\n返回的是一个匿名函数 ，因为该函数引用到到函数外的 x,那么  该函数和x整体形成一个闭包\n如：这里 `val f = minusxy(20)` 的f函数就是闭包\n2) 你可以这样理解，返回函数是一个对象，而x就是该对象的一个字段，他们共同形成一个闭包\n3) 当多次调用f时（可以理解多次调用闭包），发现使用的是同一个x, 所以x不变。\n4) 在使用闭包时，主要搞清楚返回函数引用了函数外的哪些变量，因为他们会组合成一个整体(实体),形成一个闭包\n\n**闭包的最佳实践**\n\n请编写一个程序，具体要求如下\n1) 编写一个函数 `makeSuffix(suffix: String)`  可以接收一个文件后缀名(比如.jpg)，并返回一个闭包\n2) 调用闭包，可以传入一个文件名，如果该文件名没有指定的后缀(比如`.jpg`) ,则返回 `文件名.jpg `, 如果已经有`.jpg`后缀，则返回原文件名。\n3) 要求使用闭包的方式完成\n4)提示： `String.endsWith(xx)`\n\n```scala\npackage myfunPack\nobject T8 {\n\n  def main(args: Array[String]): Unit = {\n\n    /*\n    请编写一个程序，具体要求如下\n    1.编写一个函数 makeSuffix(suffix: String)  可以接收一个文件后缀名(比如.jpg)，     并返回一个闭包\n    2.调用闭包，可以传入一个文件名，如果该文件名没有指定的后缀(比如.jpg) ,则返回 文件名.jpg , 如果已经有.jpg后缀，则返回原文件名。\n     比如 文件名 是 dog =>dog.jpg\n     比如  文件名 是 cat.jpg => cat.jpg\n    3.要求使用闭包的方式完成,提示：String.endsWith(xx)\n     */\n    \n    //使用并测试\n    val f = makeSuffix(\".jpg\")\n    println(f(\"dog.jpg\")) // dog.jpg\n    println(f(\"cat\")) // cat.jpg\n  }\n  def makeSuffix(suffix: String) = {\n    //返回一个匿名函数，回使用到suffix\n    (filename:String) => {\n      if (filename.endsWith(suffix)) {\n        filename\n      } else {\n        filename + suffix\n      }\n    }\n  }\n}\n```\n\n**体会闭包的好处**\n\n1)返回的匿名函数和 `makeSuffix (suffix string) `的 suffix 变量 组合成一个闭包，因为 返回的函数引用到suffix这个变量\n\n2)我们体会一下闭包的好处，如果使用传统的方法，也可以轻松实现这个功能，但是传统方法需要每次都传入 后缀名，比如 .jpg ,而闭包因为可以保留上次引用的某个值，所以我们传入一次就可以反复使用。\n\n\n\n## 7. 函数柯里化(curry)\n\n1) 函数编程中，**接受多个参数的函数都可以转化为接受单个参数的函数**，这个转化过程就叫柯里化\n\n2) 柯里化就是证明了函数只需要一个参数而已。其实我们刚才的学习过程中，已经涉及到了柯里化操作。\n\n3) 不用设立柯里化存在的意义这样的命题。柯里化就是以函数为主体这种思想发展的必然产生的结果。(即：柯里化是面向函数思想的必然产生结果)\n\n**函数柯里化快速入门**\n\n编写一个函数，接收两个整数，可以返回两个数的乘积，要求:\n1) 使用常规的方式完成\n2) 使用闭包的方式完成\n3) 使用函数柯里化完成\n注意观察编程方式的变化。\n\n```scala\n//说明\ndef mul(x: Int, y: Int) = x * y\nprintln(mul(10, 10))\ndef mulCurry(x: Int) = (y: Int) => x * y\nprintln(mulCurry(10)(9))\ndef mulCurry2(x: Int)(y:Int) = x * y\nprintln(mulCurry2(10)(8))\n```\n\n\n\n**函数柯里化最佳实践**\n\n比较两个字符串在忽略大小写的情况下是否相等，注意，这里是两个任务：\n1) 全部转大写（或小写）\n2) 比较是否相等\n针对这两个操作，我们用一个函数去处理的思想，其实也变成了两个函数处理的思想（柯里化）\n\n方式1: 简单的方式,使用一个函数完成.\n\n```scala\ndef eq2(s1: String)(s2: String): Boolean = {\n    s1.toLowerCase == s2.toLowerCase\n}\n```\n\n方式2：使用稍微高级的用法(隐式类)：形式为 str.方法()\n\n```scala\npackage myfunPack\n\nobject T9 {\n\n  def main(args: Array[String]): Unit = {\n    def eq(s1: String, s2: String): Boolean = {\n      s1.equals(s2)\n    }\n\n    implicit class TestEq(s: String) {\n      //将字符串比较分解成两个任务完成\n      //1. checkEq 完成转换大小写\n      //2. f函数完成比较任务\n      def checkEq(ss: String)(f: (String, String) => Boolean): Boolean = {\n        f(s.toLowerCase, ss.toLowerCase)\n      }\n    }\n\n    val str1=\"hello\"\n    println(str1.checkEq(\"HELLO\")(eq))  //true\n  }\n}\n```\n\n看一个上面的简写形式\n\n```scala\npackage myfunPack\n\nobject T9 {\n\n  def main(args: Array[String]): Unit = {\n\n    implicit class TestEq(s: String) {\n      //将字符串比较分解成两个任务完成\n      //1. checkEq 完成转换大小写\n      //2. f函数完成比较任务\n      def checkEq(ss: String)(f: (String, String) => Boolean): Boolean = {\n        f(s.toLowerCase, ss.toLowerCase)\n      }\n    }\n\n    val str1 = \"hello\"\n\n    str1.checkEq(\"HELLO\")((s1: String, s2: String) => s1.equals(s2))\n\n    //因为比较的时候会自动推断String类型，所以String可以去掉\n    str1.checkEq(\"HELLO\")((s1, s2) => s1.equals(s2))\n\n    //因为参数s1,s2在=>的右边只出现了一次，所以可以用_代替，同时=>左侧就可以省略不写了\n    str1.checkEq(\"HELLO\")(_.equals(_))\n  }\n}\n\n```\n\n\n\n## 8. 控制抽象\n\n**看一个需求**\n如何实现将一段代码(从形式上看)，作为参数传递给高阶函数，在高阶函数内部执行这段代码. 其使用的形式如 `breakable{} `。\n\n```scala\npackage myfunPack\n\nimport scala.util.control.Breaks.{break, breakable}\n\nobject T10 {\n\n  def main(args: Array[String]): Unit = {\n    var n = 10\n    breakable {\n      while (n <= 20) {\n        n += 1\n        if (n == 18) {\n          break()\n        }\n      }\n    }\n  }\n}\n```\n\n**控制抽象基本介绍**\n\n控制抽象是这样的函数，满足如下条件\n1) 参数是函数\n2) **函数参数没有输入值也没有返回值**\n\n```scala\npackage myfunPack\n\nobject T11 {\n\n  def main(args: Array[String]): Unit = {\n    //myRunInThread 就是一个抽象控制\n    //因为 myRunInThread 是没有输入， 也没有输出的函数 f1: () => Unit\n    def myRunInThread(f1: () => Unit) = {\n      new Thread {\n        override def run(): Unit = {\n          f1() //只写了 f1\n        }\n      }.start()\n    }\n\n    //myRunInThread(匿名函数)，写入多行是()用{}代替\n    myRunInThread {\n      () =>\n        println(\"干活咯！5秒完成...\")\n        Thread.sleep(5000)\n        println(\"干完咯！\")\n\n    }\n\n\n    //简写形式  ：f1: () => Unit 简写为 f1: () => Unit\n    def myRunInThread2(f1:  => Unit) = {\n      new Thread {\n        override def run(): Unit = {\n          f1 //只写了 f1\n        }\n      }.start()\n    }\n\n    //对于没有输入，也没有返回值函数，可以简写成如下形式，即() => 可以省略\n    myRunInThread2 {\n      println(\"干活咯！5秒完成...~~~\")\n      Thread.sleep(5000)\n      println(\"干完咯！~~~\")\n    }\n  }\n}\n```\n\n**进阶用法：实现类似while的until函数**\n\n```scala\nvar x=10\nwhile(x>0){\n    x -= 1\n    println(\"x=\"+x)\n}\n```\n\n用抽象控制实现\n\n```scala\npackage myfunPack\n\nobject T12 {\n\n  def main(args: Array[String]): Unit = {\n    var x = 10\n\n    //说明\n    //1 函数名为 until , 实现了类似 while循环的效果\n    //2. condition: => Boolean 是后一个没有输入值，返回Boolean类型函数\n    //3. block: => Unit 没有输入值，也没有返回值的韩\n    def mywhile(condition: => Boolean)(block: => Unit): Unit = {\n      //类似while循环，递归\n      if(condition) {\n        block // x= 9 ,x = 8 x =7 ....\n        mywhile(condition)(block)\n      }\n\n    }\n\n    //第一个匿名函数：()=> x>0 简写为 x>0\n    //第二个匿名函数简写{}里面的代码块\n    mywhile(x > 0) {\n      x -= 1\n      println(\"x=\" + x )\n    }\n  }\n}\n```\n\n\n\n","tags":["scala"],"categories":["scala"]},{"title":"六、Scala之模式匹配","url":"/2021/03/25/204706/","content":"\nScala之模式匹配\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. match\n\nScala中的模式匹配类似于Java中的switch语法，但是更加强大。\n\n模式匹配语法中，采用match关键字声明，每个分支采用case关键字进行声明，当需要匹配时，会从第一个case分支开始，如果匹配成功，那么执行对应的逻辑代码，如果匹配不成功，继续执行下一个分支进行判断。如果所有case都不匹配，那么会执行case _ 分支，类似于Java中default语句。\n\n**Java Switch的简单回顾**\n\n```scala\n// Java\nint i = 1;\nswitch ( i ) {\ncase 0 :\n\tbreak;\ncase 1 :\n\tbreak;\ndefault :\n\tbreak;\n}\n```\n\n**Scala的模式匹配**\n\n```scala\n// 模式匹配，类似于Java的switch语法\nval oper = '#'\nval n1 = 20\nval n2 = 10\nvar res = 0\noper match {\n    case '+' => res = n1 + n2\n    case '-' => res = n1 - n2\n    case '*' => res = n1 * n2\n    case '/' => res = n1 / n2\n    case _ => println(\"oper error\")\n}\nprintln(\"res=\" + res)\n```\n\n**match的细节和注意事项**\n\n1) 如果所有case都不匹配，那么会执行case _ 分支，类似于Java中default语句\n2) 如果所有case都不匹配，又没有写case _ 分支，那么会抛出MatchError\n3) 每个case中，不用break语句，**自动中断case**\n4) 可以在match中使用其它类型，而不仅仅是字符\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210325205655.png)\n\n5) => 等价于 java swtich 的 `:`\n6) => 后面的代码块到下一个 case， 是作为一个整体执行，可以使用{} 扩起来，也可以不扩。 \n\n\n\n## 2. 守卫\n\n如果想要表达**匹配某个范围的数据**，就需要在模式匹配中增加条件守卫\n\n```scala\nfor (ch <- \"+-3!\") {\n    var sign = 0\n    var digit = 0\n    ch match {\n        case '+' => sign = 1\n        case '-' => sign = -1\n        // 说明..\n        case _ if ch.toString.equals(\"3\") => digit = 3\n        case _ => sign = 2\n    }\n println(\"ch=\" + ch + \" \" + \"sign=\" + sign + \" \" + \"digit=\" + digit)\n```\n\n输出\n\n```tex\nch=+ sign=1 digit=0\nch=- sign=-1 digit=0\nch=3 sign=0 digit=3\nch=! sign=2 digit=0\n```\n\n\n\n再来看一个例子\n\n```scala\nfor (ch <- \"+-3!\") {\n    var sign = 0\n    var digit = 0\n    ch match {\n        case _ => digit = 3 //匹配到这个，后面的就不执行了\n        case '+' => sign = 1\n        case '-' => sign = -1\n    }\n    println(\"ch=\" + ch + \" \" + \"sign=\" + sign + \" \" + \"digit=\" + digit)\n}\n```\n\n输出：\n\n```tex\nch=+ sign=0 digit=3\nch=- sign=0 digit=3\nch=3 sign=0 digit=3\nch=! sign=0 digit=3\n```\n\n\n\n## 3. 模式中的变量\n\n如果在case关键字后跟变量名，那么match前表达式的值会赋给那个变量\n\n```scala\nval ch = 'V'\nch match {\n    case '+' => println(\"ok~\")\n    case mychar => println(\"ok~\" + mychar) //这一个执行，输出：ok~V\n    case _ => println(\"ok~~\")\n}\n```\n\nmatch是一个表达式，因此有返回值\n\n```scala\nval ch1 = '+'\nval res = ch1 match {\n    case '+' => ch1 + \"hello\"\n    case '-' => println(\"12311\")\n}\nprintln(res)//输出 +hello\n```\n\n\n\n\n\n## 4. 类型匹配\n\n可以匹配对象的任意类型，这样做避免了使用isInstanceOf和asInstanceOf方法\n\n```scala\n// 类型匹配, obj 可能有如下的类型\nval a = 7\nval obj = if (a == 1) 1\nelse if (a == 2) \"2\"\nelse if (a == 3) BigInt(3)\nelse if (a == 4) Map(\"aa\" -> 1)\nelse if (a == 5) Map(1 -> \"aa\")\nelse if (a == 6) Array(1, 2, 3)\nelse if (a == 7) Array(\"aa\", 1)\nelse if (a == 8) Array(\"aa\")\n\n//1. 根据Object的类型来匹配\n// a:Int是一个变量，如果匹配到了，会把obj赋值给a\nval result = obj match {\n    case a: Int => a\n    case b: Map[String, Int] => \"对象是一个字符串-数字的Map集合\"\n    case c: Map[Int, String] => \"对象是一个数字-字符串的Map集合\"\n    case d: Array[String] => \"对象是一个字符串数组\"\n    case e: Array[Int] => \"对象是一个数字数组\"\n    case f: BigInt => Int.MaxValue\n    case _ => \"啥也不是\"\n}\nprintln(result) //输出：啥也不是\n```\n\n类型匹配注意事项\n\n1) Map[String, Int] 和Map[Int, String]是两种不同的类型，其它类推。\n2) 在进行类型匹配时，**编译器会预先检测是否有可能的匹配**，如果没有则报错.\n\n```scala\nval obj = 10\n// val obj=Map(\"hell0\"->12)\nval result = obj match {\n    case a: Int => a\n    case b: Map[String, Int] => \"Map集合\"\n    case _ => \"啥也不是\"\n}\n//说明：编译器报错\n```\n\n3) 一个说明:\n\n```scala\nval result = obj match {\ncase i : Int => i\n} \n```\n\n上述代码中的 `case i : Int => i `表示 将 `i = obj` (其它类推)，然后再判断类型\n\n4) 如果 `case _` 出现在match 中间，则表示隐藏变量名，即不使用，而不是表示默认匹配，如下所示\n\n```scala\n// 类型匹配, obj 可能有如下的类型\nval a = 7\nval obj = if(a == 1) 1\nelse if(a == 2) \"2\"\nelse if(a == 3) BigInt(3)\nelse if(a == 4) Map(\"aa\" -> 1)\nelse if(a == 5) Map(1 -> \"aa\")\nelse if(a == 6) Array(1, 2, 3)\nelse if(a == 7) Array(\"aa\", 1)\nelse if(a == 8) Array(\"aa\")\nval result = obj match {\n    case a : Int => a\n    case _ : BigInt => Int.MaxValue //看这里!\n    case b : Map[String, Int] => \"对象是一个字符串-数字的Map集合\"\n    case c : Map[Int, String] => \"对象是一个数字-字符串的Map集合\"\n    case d : Array[String] => \"对象是一个字符串数组\"\n    case e : Array[Int] => \"对象是一个数字数组\"\n    case _ => \"啥也不是\"\n}\nprintln(result)\n```\n\n## 5. 匹配数组\n\n1) Array(0) 匹配只有一个元素且为0的数组。\n2) Array(x,y) 匹配数组有两个元素，并将两个元素赋值为x和y。当然可以依次类推Array(x,y,z) 匹配数组有3个元素的等等....\n3) `Array(0,_*) `匹配数组以0开始\n\n应用案例\n\n```scala\nfor (arr <- Array(Array(0), Array(1, 0), Array(0, 1, 0),\n                  Array(1, 1, 0), Array(1, 1, 0, 1))) {\n    val result = arr match {\n        case Array(0) => \"0\"\n        case Array(x, y) => x + \"=\" + y\n        case Array(0, _*) => \"以0开头和数组\"\n        case _ => \"什么集合都不是\"\n    }\n    println(\"result = \" + result)\n}\n```\n\n输出\n\n```tex\nresult = 0\nresult = 1=0\nresult = 以0开头和数组\nresult = 什么集合都不是\nresult = 什么集合都不是\n```\n\n\n\n## 6. 匹配列表\n\n案例\n\n```scala\nfor (list <- Array(List(0), List(1, 0), List(0, 0, 0), List(1, 0, 0))) {\n    val result = list match {\n        case 0 :: Nil => \"0\" //返回只有一个0的列表\n        case x :: y :: Nil => x + \" \" + y //匹配两个元素列表\n        case 0 :: tail => \"0 ...\" //以0开头，后面有任意个元素，tail表示后面有任意\n        case _ => \"something else\"\n    }\n    println(result)\n}\n```\n\n输出\n\n```tex\n0\n1 0\n0 ...\nsomething else\n```\n\n请思考，如果要匹配 List(88) 这样的只含有一个元素的列表,并原值返回.应该怎么写?\n\n```scala\nfor (list <- Array(List(99), List(88), List(0, 0, 0), List(1, 0, 0))) {\n    val result = list match {\n        case 88 :: Nil => \"88\" //返回只有一个88的列表\n        case x :: y :: Nil => x + \" \" + y //匹配两个元素列表\n        case 0 :: tail => \"0 ...\" //以0开头，后面有任意个元素，tail表示后面有任意\n        case _ => \"something else\"\n    }\n    println(result)\n}\n```\n\n输出\n\n```tex\nsomething else\n88\n0 ...\nsomething else\n```\n\n\n\n## 7. 匹配元组\n\n案例\n\n```scala\n// 元组匹配\n// 元组匹配\nfor (pair <- Array((0, 1),(0,1,1), (1, 0), (1, 1), (1, 0, 2))) {\n    val result = pair match { //\n        case (0, _) => \"0 ...\" //匹配以0开头的二元组，但是第二个元素我舍弃，以后我不用它\n        case (y, 0) => y //匹配一个二元组，但是第二个元素必须是0\n        case _ => \"other\" //.\n    }\n    println(result)\n}\n```\n\n输出\n\n```tex\n0 ...\nother\n1\nother\nother\n```\n\n\n\n## 8. 对象匹配\n\n对象匹配，什么才算是匹配呢？，规则如下:\n1) case中的对象的unapply方法( 对象提取器 )返回Some集合则为匹配成功\n2) 返回none集合则为匹配失败\n\n应用案例1\n\n```scala\npackage dataStructure.mapPack\n\nobject T9 {\n\n  def main(args: Array[String]): Unit = {\n\n    object Square {\n      //Option也是一个集合\n      //unapply对象提取器，返回Option[Double]类型\n      def unapply(z: Double): Option[Double] = Some(math.sqrt(z))\n\n      def apply(z: Double): Double = z * z\n    }\n    // 模式匹配使用：\n    val number: Double = 36.0\n    number match {\n      //当匹配到case Square(n) 时，会调用Square的unapply方法，这时n会传给unapply的参数z\n      //如果unapply方法返回的是Some(6)，则表示匹配成功，同时将这个8 赋给 Square(n)里面的n\n      case Square(n) => println(n) //输出6.0\n      case _ => println(\"nothing matched\")\n    }\n  }\n}\n```\n\n应用案例1的小结\n1) 构建对象时apply会被调用 ，比如 `val n1 = Square(5)`\n2) 当将 Square(n) 写在 case 后时，如`case Square(n) => xxx`，会默认调用unapply 方法(对象提取器)\n3)  number 会被 传递给`def unapply(z: Double)` 的 z 形参\n4)  如果返回的是Some集合，则unapply提取器返回的结果会返回给 n 这个形参\n5) case中对象的unapply方法(提取器)返回some集合则为匹配成功\n6) 返回none集合则为匹配失败\n\n**应用案例2**\n\n```scala\npackage dataStructure.mapPack\n\nobject T11 {\n\n  def main(args: Array[String]): Unit = {\n    object Names {\n      def unapplySeq(str: String): Option[Seq[String]] = {\n        if (str.contains(\",\")) Some(str.split(\",\"))\n        else None\n      }\n    }\n    val namesString = \"Alice,Bob,Thomas\"\n    //说明\n    namesString match {\n      case Names(first, second, third) => {\n        println(\"the string contains three people's names\")\n        // 打印字符串\n        println(s\"$first $second $third\")\n      }\n      case _ => println(\"nothing matched\")\n    }\n  }\n}\n\n```\n\n输出\n\n```tex\nthe string contains three people's names\nAlice Bob Thomas\n```\n\n应用案例2的小结\n1) 当case 后面的对象提取器方法的参数为多个，则会默认调用`def unapplySeq()` 方法\n2) 如果unapplySeq返回是Some，获取其中的值,判断得到的sequence中的元素的个数是否是三个,如果是三个，则把三个元素分别取出，赋值给first，second和third\n3) 其它的规则不变.\n\n\n\n## 9. 变量声明中的模式\n\nmatch中每一个case都可以单独提取出来，意思是一样的.\n\n```scala\nval (x, y) = (1, 2) //相等于定义了两个变量\nval (q, r) = BigInt(10) /% 3  //说明  q = BigInt(10) / 3 ，r = BigInt(10) % 3\nval arr = Array(1, 7, 2, 9)\nval Array(first, second, _*) = arr // 提出arr的前两个元素\nprintln(first, second) //(1,7)\n```\n\n\n\n## 10. for表达式中的模式\n\nfor循环也可以进行模式匹配\n\n```scala\nval map = Map(\"A\" -> 1, \"B\" -> 0, \"C\" -> 3)\nfor ((k, v) <- map) {\n    println(k + \" -> \" + v)\n}\nprintln(\"=================\")\n//说明 匹配val为0的键值对\nfor ((k, 0) <- map) {\n    println(k + \" --> \" + 0)\n}\nprintln(\"=================\")\n//说明 匹配val为0的键值对\nfor ((k, v) <- map if v == 0) {\n    println(k + \" ---> \" + v)\n}\n```\n\n输出\n\n```tex\nA -> 1\nB -> 0\nC -> 3\n=================\nB --> 0\n=================\nB ---> 0\n```\n\n\n\n\n\n## 11. 样例(模板)类\n\n**引入**\n\n如果一个类中没有任何内容，那么后面的{}可以省略，样例类也是如此\n\n比如\n\n```scala\nclass Dog{\n}\n//等同于\nclass Dog\n```\n\n\n\n**样例类快速入门**\n\n```scala\nabstract class Amount\ncase class Dollar(value: Double) extends Amount\ncase class Currency(value: Double, unit: String) extends Amount\ncase object NoAmount extends Amount\n```\n\n\n\n说明: 这里的 Dollar，Currencry, NoAmount  是样例类。\n\n**基本介绍**\n1) 样例类仍然是类\n2) 样例类用case关键字进行声明。\n3) 样例类是为模式匹配而优化的类\n4) 构造器中的每一个参数都成为val——除非它被显式地声明为var（不建议这样做）\n5) 在样例类对应的伴生对象中提供apply方法让，你不用new关键字就能构造出相应的对象\n6) 提供unapply方法让模式匹配可以工作\n7) 将自动生成toString、equals、hashCode和copy方法(有点类似模板类，直接给生成，供程序员使用)\n8) 除上述外，样例类和其他类完全一样。你可以添加方法和字段，扩展它们\n\n\n\n**样例类最佳实践1:**\n\n当我们有一个类型为Amount的对象时，可以用模式匹配来匹配他的类型，并将属性值绑定到变量(即：把样例类对象的属性值提取到某个变量,该功能有用)\n\n```scala\npackage dataStructure.mapPack\n\nobject T12 {\n\n  def main(args: Array[String]): Unit = {\n    for (amt <- Array(Dollar(1000.0), Currency(1000.0, \"RMB\"), NoAmount)) {\n      val result = amt match {\n        //说明\n        case Dollar(v) => \"$\" + v\n        //说明\n        case Currency(v, u) => v + \" \" + u\n        case NoAmount => \"\"\n      }\n      println(amt + \": \" + result)\n    }\n  }\n}\nabstract class Amount\ncase class Dollar(value: Double) extends Amount\ncase class Currency(value: Double, unit: String) extends Amount\ncase object NoAmount extends Amount\n\n```\n\n输出\n\n```tex\nDollar(1000.0): $1000.0\nCurrency(1000.0,RMB): 1000.0 RMB\nNoAmount: \n```\n\n**样例类最佳实践2:**\n\n样例类的copy方法和带名参数\n\ncopy创建一个与现有对象值相同的新对象，并可以通过带名参数来修改某些属性。\n\n```scala\nobject T13 {\n\n  def main(args: Array[String]): Unit = {\n    val amt = Currency(29.95, \"RMB\")\n    val amt1 = amt.copy() //创建了一个新的对象，但是属性值一样\n    val amt2 = amt.copy(value = 19.95) //创建了一个新对象，但是修改了货币单位\n    val amt3 = amt.copy(unit = \"英镑\")//..\n    println(amt)\n    println(amt2)\n    println(amt3)\n  }\n}\ncase class Currency(value: Double, unit: String) extends Amount\n```\n\n输出\n\n```tex\nCurrency(29.95,RMB)\nCurrency(19.95,RMB)\nCurrency(29.95,英镑)\n```\n\n\n\n## 12. case语句的中置(缀)表达式\n\n什么是中置表达式？\n\n`1 + 2`，这就是一个中置表达式。可以在case语句中使用中置表示法，比如可以匹配一个List序列\n\n```scala\nList(1, 3, 5, 9) match { //修改并测试\n    //1.两个元素间::叫中置表达式,至少first，second两个匹配才行.注意：这里的名字随意\n    //2.first 匹配第一个 second 匹配第二个, rest 匹配剩余部分List(5, 9)\n    case first :: second :: rest => {\n        println(first +\",\"+ second +\",\"+ rest.length) //1,3,2\n        println(rest) //List(5, 9)\n    }\n    case _ => println(\"匹配不到...\")\n}\n```\n\n\n\n## 13. 匹配嵌套结构【案例说明】\n\n操作原理类似于正则表达式\n\n**最佳实践案例-商品捆绑打折出售**\n\n现在有一些商品，请使用Scala设计相关的样例类，完成商品捆绑打折出售。要求\n1) 商品捆绑可以是单个商品，也可以是多个商品。\n2) 打折时按照折扣x元进行设计.\n3) 能够统计出所有捆绑商品打折后的最终价格\n\n（1）创建样例类\n\n```scala\nabstract class Item // 项\ncase class Book(description: String, price: Double) extends Item\n//Bundle 捆 ， discount: Double 折扣 ， item: Item* ,\ncase class Bundle(description: String, discount: Double, item: Item*) extends Item\n```\n\n（2）匹配嵌套结构(就是Bundle的对象)\n\n```scala\n//给出案例表示有一捆数，单本漫画（40-10） +文学作品(两本书)（80+30-20）= 30 + 90 = 120.0\nval sale = Bundle(\"书籍\", 10,  Book(\"漫画\", 40), Bundle(\"文学作品\", 20, Book(\"《阳关》\", 80), Book(\"《围城》\", 30))) \n```\n\n（3）知识点1-description绑定到第一个Book的描述\n\n请思考：如何取出“漫画”\n\n```scala\n//使用case语句，得到\"漫画\"\nval res = sale match  {\n    //如果我们进行对象匹配时，不想接受某些值，则使用_ 忽略即可，_* 表示所有\n    case Bundle(_, _, Book(desc, _), _*) => desc\n}\nprintln(\"res=\"+res) //res=漫画\n```\n\n（4）知识点2-通过`@`表示法将嵌套的值绑定到变量。_\\*绑定剩余Item到rest\n\n```scala\n//得到一个元祖\nval result2 = sale match {\n    //@ Book(_, _)直接返回整个对象\n    case Bundle(_, _, art @ Book(_, _), rest @ _*) => (art, rest)\n}\n\nprintln(result2)\nprintln(\"art =\" + result2._1)//art =Book(漫画,40.0)\n//外面一层有WrappedArray\nprintln(\"rest=\" + result2._2)//rest=WrappedArray(Bundle(文学作品,20.0,WrappedArray(Book(《阳关》,80.0), Book(《围城》,30.0))))\n```\n\n（5）知识点3-不使用_\\*绑定剩余Item到rest\n\n```scala\nval result3 = sale match {\n    //说明因为没有使用 _* 即明确说明没有多个Bundle,所以返回的rest，就不是WrappedArray了。\n    case Bundle(_, _, art @ Book(_, _), rest) => (art, rest)\n}\nprintln(result3)\nprintln(\"art =\" + result3._1) //art =Book(漫画,40.0)\n//外面一层没有WrappedArray了\nprintln(\"rest=\" + result3._2) //rest=Bundle(文学作品,20.0,WrappedArray(Book(《阳关》,80.0), Book(《围城》,30.0)))\n```\n\n\n\n**最佳实践案例-商品捆绑打折出售**\n\n现在有一些商品，请使用Scala设计相关的样例类，完成商品可以捆绑打折出售。\n要求\n1) 商品捆绑可以是单个商品，也可以是多个商品。\n2) 打折时按照折扣xx元进行设计.\n3) 能够统计出所有捆绑商品打折后的最终价格\n\n```scala\ndef price(it: Item): Double = {\n    it match {\n        //如果是一本书，则只关心价格\n        case Book(_, p) => p\n        //如果是一个Bundle，则获取折扣disc和后面的东西\n        //its.map(price) 指 再次调用price方法，递归的去计算，从而获取所有的价格和折扣\n        //计算完了之后再求和，并减去折扣\n        case Bundle(_, disc, its @ _*) => its.map(price).sum - disc\n    }\n}\n```\n\n最终代码\n\n```scala\npackage dataStructure.lizi\n\nobject T1 {\n\n  def main(args: Array[String]): Unit = {\n\n    def price(it: Item): Double = {\n      it match {\n        case Book(_, p) => p\n        //生成一个新的集合,_是将its中每个循环的元素传递到price中it中。递归操作,分析一个简单的流程\n        case Bundle(_, disc, its@_*) => its.map(price _).sum - disc\n      }\n    }\n\n    val sale = Bundle(\"书籍\", 10, Book(\"漫画\", 40), Bundle(\"文学作品\", 20, Book(\"《阳关》\", 80), Book(\"《围城》\", 30)))\n\n    println(\"price: \"+price(sale))//120.0，答案正确\n  }\n}\n\nabstract class Item // 项\ncase class Book(description: String, price: Double) extends Item\n\n//Bundle 捆 ， discount: Double 折扣 ， item: Item* ,\ncase class Bundle(description: String, discount: Double, item: Item*) extends Item\n```\n\n","tags":["scala"],"categories":["scala"]},{"title":"hashCode、identityHashCode、equals和==的原理详解","url":"/2021/03/25/111145/","content":"\nhashCode、identityHashCode、equals和==的原理详解\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. hashCode概念\n\nhashCode是jdk根据对象的地址算出来的一个int数字，即对象的哈希码值，代表了该对象在内存中的存储位置。\n\n我们都知道hashCode()方法是顶级类Object类的提供的一个方法，所有的类都可以进行对hashCode方法重写。\n\n我们也知道在比较一个对象中的内容是否相同时往往会重写equals方法，值得注意的是，重写equals方法的同时必须也要重写hashCode方法，多次调用一个对象的hashCode方法必须返回同一个数字，这也是必须遵守的规范，不然会造必须存在的危害。\n\n\n\n## 2. hash冲突\n\n当两个对象equals相同，hashCode规定也必须相同，但反过来就不一定。两个对象对应一个hashCode，但equal却不一定相等。这就是传说中的hash冲突的场景。\n\nHashMap是以hashCode取模数组形式存放值的，那两个对象hashCode一样会不会造成前一个对象的值覆盖呢？答案是不会，因为它采用了另外一种链表数据结构来解决hash冲突的情况，即使两个对象的hashCode一样，它们会放到当前数组索引位置的链表中。所以，即使有相同的hashCode，equal却不一定相等。\n\n\n\n## 3. hashCode设计\n\nHashSet通过HashMap来实现的，用来存储不重复数据的，怎么判断里面的对象是否重复呢？判断对象是否重复即是判断对象里面的属性是否都一样，这时必须是重写了equals方法去比较对象的里面所有的值，而不是比较引用地址，比较引用地址它们永远都不相等，除非是同一个对象。通过equals比较的过程性能是非常不佳的，所以有了hashCode这个设计，可以先通过比较对象的hashCode是否一样确定是不是同一个对象，如果hashCode不一样这时肯定就不是同一个对象，反之如果hashCode一样而且equals或者==也一样这肯定就是同一个对象。所以先比较对象的hashCode再比较equals或者==，这样效率会明显提升。\n\n假如我们重写了equals而不重写hashCode方法，多个对象属性值一样的它们的hashCode肯定是不一样的，这时作为key在put到map中的时候，就会有多个这样的key，而达不到对象作为key的场景，同样也达不到HashSet去重的效果。因为HashSet通过HashMap来实现的，HashMap的Key是通过hashcode实现的。\n\n\n\n## 4. identityHashCode\n\nidentityHashCode是System里面提供的本地方法\n\n```java\n/**\n * Returns the same hash code for the given object as\n * would be returned by the default method hashCode(),\n * whether or not the given object's class overrides\n * hashCode().\n * The hash code for the null reference is zero.\n *\n * @param x object for which the hashCode is to be calculated\n * @return  the hashCode\n * @since   JDK1.1\n */\npublic static native int identityHashCode(Object x);\n```\n\nidentityHashCode和hashCode的区别是，identityHashCode会返回对象的hashCode，**而不管对象是否重写了hashCode方法**。\n\n看一个例子：\n\n```scala\npublic static void main(String[] args) {\n    String str1 = new String(\"abc\");\n    String str2 = new String(\"abc\");\n    System.out.println(\"str1 hashCode: \" + str1.hashCode());\n    System.out.println(\"str2 hashCode: \" + str2.hashCode());\n    System.out.println(\"str1 identityHashCode: \" + System.identityHashCode(str1));\n    System.out.println(\"str2 identityHashCode: \" + System.identityHashCode(str2));\n\n    User user = new User(\"test\", 1);\n    System.out.println(\"user hashCode: \" + user.hashCode());\n    System.out.println(\"user identityHashCode: \" + System.identityHashCode(user));\n}\n```\n\n输出结果：\n\n```tex\nstr1 hashCode: 96354\nstr2 hashCode: 96354\nstr1 identityHashCode: 1173230247\nstr2 identityHashCode: 856419764\nuser hashCode: 621009875\nuser identityHashCode: 621009875\n```\n\n结果分析：\n\n1、str1和str2的hashCode是相同的，是因为String类重写了hashCode方法，**它根据String的值来确定hashCode的值**，所以只要值一样，hashCode就会一样。\n\n2、str1和str2的identityHashCode不一样，虽然String重写了hashCode方法，identityHashCode永远返回根据对象物理内存地址产生的hash值，所以每个String对象的物理地址不一样，identityHashCode也会不一样。\n\n3、User对象没重写hashCode方法，所以hashCode和identityHashCode返回的值一样。\n\n经过我的测试，ArrayList也重写了Hashcode方法，其HashCode根据ArrayList中的内容计算得到，只要两个ArrayList中的内容一样，其HashCode就一致\n\n```java\npublic static void main(String[] args) {\n    ArrayList<String> a=new ArrayList<String>();\n    ArrayList<String> b=new ArrayList<String>();\n    a.add(\"hello\");\n    a.add(\"world\");\n    b.add(\"hello\");\n    b.add(\"world\");\n    System.out.println(a.hashCode());//-1107615551\n    System.out.println(b.hashCode());//-1107615551\n}\n```\n\n输出：\n\n```java\n-1107615551\n-1107615551\n```\n\n\n\n\n\n**结论**\n\nhashCode方法可以被重写并返回重写后的值，identityHashCode会返回对象的hash值而不管对象是否重写了hashCode方法。\n\n\n\n\n\n## 5. == 和 equals\n\n可以先参考一下[Java简单数据类型和封装类中的equals和\"==\"](https://wxler.github.io/2021/03/06/160554/)\n\njava中的数据类型，可分为两种：\n\n1. 基本数据类型，也称原始数据类型。byte,shrot,char,int,long,float,double,boolean（存储在内存中的堆栈（以后简称栈））\n   - **他们之间的比较，应用双等号（==)，比较 的是他们的值**。\n2. 引用类型（类，复合数据类型）（在栈中仅仅是存储引用类型的变量的地址，而其本身则存储在堆中）\n   - **当他们用（==）进行比较的，比较的是他们在内存中的存放地址，（即栈中的内容是否相等）所以，除非是同一个new出来的对象，他们的比较后的 结果为true，否则比较后的结果为false。重写后的equals操作表示的两个变量是否是对同一个对象的引用（即堆中的内容是否相等）**\n\n\n\n这里问题来了，对于引用类型，`==`底层比较的是hashcode吗？\n\n就拿上面的例子来说\n\n```scala\npublic static void main(String[] args) {\n    ArrayList<String> a=new ArrayList<String>();\n    ArrayList<String> b=new ArrayList<String>();\n    a.add(\"hello\");\n    a.add(\"world\");\n    b.add(\"hello\");\n    b.add(\"world\");\n    System.out.println(a.hashCode());//-1107615551\n    System.out.println(b.hashCode());//-1107615551\n    System.out.println(a==b);\n}\n```\n\n输出结果：\n\n```java\n-1107615551\n-1107615551\nfalse\n```\n\n\n\n可以看到，两个hashcode相等的对象，`==`结果为false，所以，引用类型的`==`等于等层实现并不是hashcode。\n\n那是什么呢？肯定是identityHashCode，无论对象是否重写了hashCode方法，它返回的都是对象的地址\n\n\n\n再来看equals，equals是Object中的方法，与`==`不同，equals()是一个方法，我们可以推测知道，**它其中的实现所使用的肯定是==运算符**。再进一步的思考，equals()本意不正是`==`运算符进行对象比较时候的作用吗。那么，既然是两者有同样的作用，为什么还要弄出一个equals()方法来呢？因为==运算符不允许我们进行覆盖，为了实现某些功能（比如判断两个对象中的内容是否相等），我们必须对equals方法进行重写才能实现，所以java引入了equals方法。如果对象没有实现equals方法，equals的效果等同于`==`。\n\n来看一个例子，验证一下：\n\n```java\nclass UserA{\n\tString name;\n\tpublic UserA(String name){\n\t\tthis.name=name;\n\t}\n}\n\nclass UserB{\n\tString name;\n\tpublic UserB(String name){\n\t\tthis.name=name;\n\t}\n\t@Override\n\tpublic boolean equals(Object obj){\n\t\tUserB userb=(UserB)obj;\n\t\tif(this.name.equals(userb.name))\n\t\t\treturn true;\n\t\telse return false;\n\t}\n}\n\npublic class Test6 {\n\n\tpublic static void main(String[] args) {\n\t\tUserA a1=new UserA(\"layne\");\n\t\tUserA a2=new UserA(\"layne\");\n\t\t\n\t\tUserB b1=new UserB(\"wxler\");\n\t\tUserB b2=new UserB(\"wxler\");\n\t\t\n\t\tSystem.out.println(a1==a2);//false\n\t\tSystem.out.println(a1.equals(a2));//false\n\t\t\n\t\tSystem.out.println(b1==b2);//false\n\t\tSystem.out.println(b1.equals(b2));//true\n\t}\n\n}\n\n```\n\n可以看到，当一个类没有重写equals方法时，equals和`==`其实是一样的，比较的都是内存地址是否一致。在重写了equals方法之后，equals和`==`就不一样了，此时，equals比较的是两个对象的内容是否相等，`==`比较的是两个对象的内存地址是否相等。\n\n\n\n上面也提到过，如果一个类重写了equals方法，必然也会重写hashCode方法，否则可能造成不必要的错误。另外，当一个类重写hashCode方法之后，基本上都会重写equals方法，**这两者只要重写一个，另一个必将会重写，这样才能保持一致性**。\n\n```java\n\tpublic static void main(String[] args) {\n\t\tArrayList<String> a=new ArrayList<String>();\n\t\tArrayList<String> b=new ArrayList<String>();\n\t\ta.add(\"hello\");\n\t\ta.add(\"world\");\n\t\tb.add(\"hello\");\n\t\tb.add(\"world\");\n\n\t\tSystem.out.println(a==b);//false\n\t\tSystem.out.println(a.equals(b));//true\n\t}\n```\n\n上面，ArrayList即重写了hashcode方法，也重写了equals方法。\n\n\n\n\n\n\n\n\n\n","tags":["Java"],"categories":["Java"]},{"title":"五、Scala中的数据结构","url":"/2021/03/24/162939/","content":"\nScala中的数据结构\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. 可变集合与不可变集合\n\n\n\n1) Scala同时支持不可变集合和可变集合，不可变集合可以安全的并发访问\n\n2) 两个主要的包：\n\n- 不可变集合：scala.collection.immutable\n- 可变集合：  scala.collection.mutable\n\n3) **Scala默认采用不可变集合，对于几乎所有的集合类，Scala都同时提供了可变(mutable)和不可变(immutable)的版本**\n\n4) Scala的集合有三大类：序列Seq、集Set、映射Map，所有的集合都扩展自Iterable特质。在Scala中，集合有可变（mutable）和不可变（immutable）两种类型。 \n\n\n\n来看一个表\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210324212434.png)\n\nvar和val的可变与不可变：指的是声明的变量(mySet)是可不可以改变\n\n- var：它指向的内存地址（即指针）可以变，但是这里跟上面的比喻有些出入，如果对变量重新赋值，就相当于把这个名字给了别人，原来那个人就没有名字了。一段时间后，就会被当成垃圾回收掉\n- val：它指向的内存地址（即指针）不可变。即一个人他不可以改名，从生到死都只能用这个名字。\n\n而集合的可变与不可变：指的是对象本身可不可以改变。对象本身是一段内存地址，在scala中我们是不能直接访问的，只能通过mySet这个变量来访问\n\n- 可变对象：它有add，move，clear等方法，它的那一段内存地址存储的内容和大小是可以改变的。\n- 不可变对象：它没有add，move，clear等方法，它的那一段内存地址从声明之后就不能改变。【即一个不可变对象，里面所有的内容都不允许改变，和String一个道理】\n- 如果将var跟不可变对象结合使用，在使用\"+\"等连接操作时，实际上是重新创建了一个新的对象，原来的对象已经废弃了。（结合java中String理解）\n\n```scala\nvar mySet=Set(\"a\",\"b\"); //声明一个可变的mySet变量，可以想象mySet指向的内存地址是A0001\nmySet.add(\"c\"); //报错，因为mySet是一个不可变对象，它没有add方法\nmySet=Set(\"a\",\"b\",\"d\"); //正常执行，因为用var什么，mySet指针可以改变。这时候mySet只需的内存地址是A0002\n \nval mySet=Set(\"a\",\"b\");//声明一个可变的mySet变量，可以想象mySet指向的内存地址是A0001，且不可改变\nmySet.add(\"c\"); //报错，因为mySet是一个不可变对象，它没有add方法\nmySet=Set(\"a\",\"b\",\"d\"); //报错，因为使用val声明，mySet指针不能改变\n \n \nimport scala.collection.mutable.Set  //导入可变集合包\n \nvar mySet=Set(\"a\",\"b\");\nmySet.add(\"c\"); //正常执行，因为mySet是一个可变对象，可以调用它的add方法改变对象\nmySet=Set(\"a\",\"b\",\"d\"); //正常执行，因为用var什么，mySet指针可以改变\n \nval mySet=Set(\"a\",\"b\");\nmySet.add(\"c\"); //正常执行，因为mySet是一个可变对象，可以调用它的add方法改变对象\nmySet=Set(\"a\",\"b\",\"d\"); //报错，因为使用val声明，mySet指针不能改变\n```\n\n\n\n\n\n在scala中集合主要在三个包里面：scala.collection， scala.collection.immutable和scala.collection.mutable。\n\nscala中引入不可变集合是为了方便程序的使用并减少在程序中的未知风险。如果一个集合被定义为不可变的，那么我们在使用的过程中就可以指定该集合是不会变化的，可以放心使用。\n\n**scala.collection层次**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210324170818.png)\n\n\n\n**scala.collection.immutable不可变集合继承层次**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210324170845.png)\n\n\n\n1. Set、Map是Java中也有的集合，和java中的用法非常相似，但是有一点不同\n2. Seq是Java没有的，我们发现List归属到Seq了，因此这里的List就和java不是同一个概念了\n3. 我们前面的for循环如`for( i <- 1 to 3)` ，就是IndexedSeq 下的Vector\n4. String也是属于IndexeSeq，其实就是Char的集合\n5. 我们发现经典的数据结构比如Queue 和 Stack被归属到LinearSeq\n6. 大家注意Scala中的Map体系有一个SortedMap，说明Scala的Map可以支持排序\n7. IndexSeq 和 LinearSeq 的区别[IndexSeq是通过索引来查找和定位，因此速度快，比如String就是一个索引\n   集合，通过索引即可定位]，[LineaSeq 是线型的，即有头尾的概念，这种数据结构一般是通过遍历来查找，它的价值在于应用到一些具体的应用场景]\n\n**scala.collection.mutable可变集合继承层次**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210324170916.png)\n\n## 2. 数组-定长数组(声明泛型)\n\n**第一种方式定义数组**\n\n这里的数组等同于Java中的数组，中括号的类型就是数组的类型\n\n```scala\npackage dataStructure.arrayPack\n\nobject T1 {\n\n  def main(args: Array[String]): Unit = {\n    val arr01 = new Array[Int](4)\n    println(arr01.length)\n    println(\"arr01(0)=\" + arr01(0))\n    for (i <- arr01) {\n      println(i)\n    }\n    println(\"--------------------\")\n    //赋值,集合元素采用小括号访问\n    arr01(3) = 10\n    for (i <- arr01) {\n      println(i)\n    }\n  }\n}\n\n```\n\n反编译，看重点那几行\n\n```java\npublic final class T1$ {\n  public static T1$ MODULE$;\n  \n  public void main(String[] args) {\n    int[] arr01 = new int[4];\n    scala.Predef$.MODULE$.println(BoxesRunTime.boxToInteger(arr01.length));\n    scala.Predef$.MODULE$.println((new StringBuilder(9)).append(\"arr01(0)=\").append(arr01[0]).toString());\n    (new ArrayOps.ofInt(scala.Predef$.MODULE$.intArrayOps(arr01))).foreach((Function1)T1$::$anonfun$main$1);\n    scala.Predef$.MODULE$.println(\"--------------------\");\n    arr01[3] = 10;\n    (new ArrayOps.ofInt(scala.Predef$.MODULE$.intArrayOps(arr01))).foreach((Function1)T1$::$anonfun$main$2);\n  }\n  \n  private T1$() {\n    MODULE$ = this;\n  }\n}\n```\n\n第二种方式定义数组\n\n```scala\npackage dataStructure.arrayPack\n\nobject T2 {\n\n  def main(args: Array[String]): Unit = {\n    val arr1 = Array(1, 2)////通过scala的apply方法创建\n    var arr02 = Array(1, 3, \"xxx\")//通过scala的apply方法创建\n    for (i <- arr02) {\n      println(i)\n    }\n    println(arr02(2))\n  }\n}\n\n```\n\n上面代码的反编译\n\n```scala\npublic final class T2$ {\n  public static T2$ MODULE$;\n  \n  public void main(String[] args) {\n    int[] arr1 = { 1, 2 };\n    Object[] arr02 = (Object[])scala.Array$.MODULE$.apply((Seq)scala.Predef$.MODULE$.genericWrapArray(new Object[] { BoxesRunTime.boxToInteger(1), BoxesRunTime.boxToInteger(3), \"xxx\" }, ), scala.reflect.ClassTag$.MODULE$.Any());\n    scala.Predef$.MODULE$.genericArrayOps(arr02).foreach(T2$::$anonfun$main$1$adapted);\n    scala.Predef$.MODULE$.println(arr02[2]);\n  }\n  \n  private T2$() {\n    MODULE$ = this;\n  }\n}\n\n```\n\n\n\n## 3. 数组-变长数组(声明泛型)\n\n```scala\npackage dataStructure.arrayPack\n\nimport scala.collection.mutable.ArrayBuffer\n\nobject T3 {\n\n  def main(args: Array[String]): Unit = {\n    //定义/声明\n    val arr2 = ArrayBuffer[Int]()\n    //追加值/元素\n    arr2.append(7)\n    //重新赋值\n    arr2(0) = 5\n    arr2.append(90,3)\n    //删除\n    arr2.remove(1)\n    println(arr2)//输出ArrayBuffer(5, 3)\n\n  }\n}\n\n```\n\n反编译\n\n```java\npublic final class T3$ {\n  public static T3$ MODULE$;\n  \n  public void main(String[] args) {\n    ArrayBuffer arr2 = (ArrayBuffer)scala.collection.mutable.ArrayBuffer$.MODULE$.apply((Seq)scala.collection.immutable.Nil$.MODULE$);\n    arr2.append((Seq)scala.Predef$.MODULE$.wrapIntArray(new int[] { 7 }));\n    arr2.update(0, BoxesRunTime.boxToInteger(5));\n    arr2.append((Seq)scala.Predef$.MODULE$.wrapIntArray(new int[] { 90, 3 }));\n    arr2.remove(1);\n    scala.Predef$.MODULE$.println(arr2);\n  }\n  \n  private T3$() {\n    MODULE$ = this;\n  }\n}\n\n```\n\n另外一个例子\n\n```scala\npackage dataStructure.arrayPack\n\nimport scala.collection.mutable.ArrayBuffer\n\nobject T3 {\n\n  def main(args: Array[String]): Unit = {\n    val arr01 = ArrayBuffer[Any](3, 2, 5)//调用apply方法创建对象\n    println(\"arr01(1)=\" + arr01(1))\n    for (i <- arr01) {\n      println(i)\n    }\n    println(arr01.length) //3\n    println(\"arr01.hash=\" + arr01.hashCode())\n    arr01.append(90.0,13)\n    println(\"arr01.hash=\" + arr01.hashCode())\n    arr01(1) = 89 //修改\n    println(\"--------------------------\")\n    for (i <- arr01) {\n      println(i)\n    }\n    //删除\n    arr01.remove(0)\n    println(\"--------------------------\")\n    for (i <- arr01) {\n      println(i)\n    }\n    println(\"最新的长度=\" + arr01.length)//4\n\n  }\n}\n\n```\n\n再来看一个小例子\n\n```scala\nval a=new ArrayBuffer[Int](1)//直接new一个对象\nval b=ArrayBuffer[Int](1,3,5)//通过apply方法创建对象\n```\n\n反编译为：\n\n```java\nArrayBuffer a = new ArrayBuffer(1);\nArrayBuffer b = (ArrayBuffer)scala.collection.mutable.ArrayBuffer$.MODULE$.apply((Seq)scala.Predef$.MODULE$.wrapIntArray(new int[] { 1, 3, 5 }));\n```\n\n说明\n\n1)  ArrayBuffer是变长数组，类似java的ArrayList\n2) ` val arr2 = ArrayBuffer[Int]()` 也是使用的apply方法构建对象\n3) `def append(elems: A*) { appendAll(elems) }` 接收的是可变参数\n4)每append一次，arr2在底层会重新分配空间，进行扩容，arr2的内存地址会发生变化，也就成为新的ArrayBuffer\n\n```scala\n//定义/声明\nval arr2 = ArrayBuffer[Int]()\n//追加值/元素\narr2.append(7)\n//重新赋值\narr2(0) = 7\n```\n\n这点类似于java的ArrayList，每append一次，底层会重新分配空间，进行扩容，内存地址会发生变化，也就会成为新的ArrayBuffer\n\n\n\n## 4. 定长数组与变长数组的转换\n\n```tex\narr1.toBuffer  //定长数组转可变数组\narr2.toArray  //可变数组转定长数组\n```\n\n说明：\n1) arr2.toArray 返回结果才是一个定长数组，arr2本身没有变化\n2) arr1.toBuffer返回结果才是一个可变数组，arr1本身没有变化\n\n\n\n```scala\nval arr2 = ArrayBuffer[Int]()\n// 追加值\narr2.append(1, 2, 3)\nprintln(arr2) //ArrayBuffer(1,2,3)\nval newArr = arr2.toArray;\nprintln(newArr)//[I@1d251891\nval newArr2 = newArr.toBuffer\nnewArr2.append(123)\nprintln(newArr2)//ArrayBuffer(1, 2, 3, 123)\n```\n\n\n\n## 5. 多维数组\n\n```scala\n//定义\nval arr = Array.ofDim[Double](3,4)\n//说明：二维数组中有三个一维数组，每个一维数组中有四个元素\n\n//赋值\narr(1)(1) = 11.11\n```\n\n示例：\n\n```scala\nval array1 = Array.ofDim[Int](3, 4)\narray1(1)(1) = 9\nfor (item <- array1) {\n    for (item2 <- item) {\n        print(item2 + \"\\t\")\n    }\n    println()\n}\nprintln(\"===================\")\nfor (i <- 0 to array1.length - 1) {\n    for (j <- 0 to array1(i).length - 1) {\n        printf(\"arr[%d][%d]=%d\\t\", i, j, array1(i)(j))\n    }\n    println()\n}\n```\n\n\n\n## 6. 数组-Scala数组与Java的List的互转\n\n```scala\npackage Test\nimport java.util\nimport scala.collection.mutable\nimport scala.collection.mutable.ArrayBuffer\n\nobject AA {\n\n  def main(args: Array[String]): Unit = {\n\n    //创建一个Scala的可变数组\n    val arrBuffer = ArrayBuffer[Int](1, 2)\n    //将Scala的数组转化为Java的List\n    import scala.collection.JavaConverters.bufferAsJavaList\n    val javaList: util.List[Int] = bufferAsJavaList(arrBuffer)\n    javaList.add(12)\n    println(javaList) //输出结果为：[1, 2, 12]\n    //将Java的List 转化为Scala的数组\n    import scala.collection.JavaConverters.asScalaBuffer\n    val scalaBuffer: mutable.Buffer[Int] = asScalaBuffer(javaList)\n    scalaBuffer.append(13)\n    println(scalaBuffer) //输出结果为ArrayBuffer(1, 2, 12, 13)\n  }\n}\n\n```\n\n\n\n## 7. 元组Tuple-元组的基本使用\n\n元组也是可以理解为一个容器，可以存放各种相同或不同类型的数据。说的简单点，就是将多个无关的数据封装为一个整体，称为元组，最多的特点灵活，对数据没有过多的约束。\n\n注意：元组中最大只能有22个元素\n\n```scala\npackage dataStructure.tuplePack\n\nobject T1 {\n\n  def main(args: Array[String]): Unit = {\n    val tuple1 = (1, 2, 3, \"hello\", 4)\n    println(tuple1)\n  }\n}\n```\n\n上面代码的反编译\n\n```scala\npublic final class T1$ {\n  public static T1$ MODULE$;\n  \n  public void main(String[] args) {\n    Tuple5 tuple1 = new Tuple5(BoxesRunTime.boxToInteger(1), BoxesRunTime.boxToInteger(2), BoxesRunTime.boxToInteger(3), \"hello\", BoxesRunTime.boxToInteger(4));\n    scala.Predef$.MODULE$.println(tuple1);\n  }\n  \n  private T1$() {\n    MODULE$ = this;\n  }\n}\n```\n\n再来看一个案例\n\n```scala\n//1. 通过apply方法创建元组对象\nval tuple1 = (1, 2, 3, \"hello\", 4)\nprintln(tuple1) //(1,2,3,hello,4)\n\n//2. 通过apply方法创建元组对象\nval tuple2 = Tuple5[Int,Int,Int,Int,String](1,4,5,6,\"lala\")//或者写为val tuple2 =Tuple5(1,2,3,4,5)\nprintln(tuple2)//(1,4,5,6,lala)\n\n//3. 通过new的方式创建元组对象\nval tuple3=new Tuple5[Int,Int,Int,Int,String](1,2,3,4,\"ac\")//或者写为val tuple2=new Tuple5(1,2,3,4,\"ac\")\nprintln(tuple3) //(1,2,3,4,ac)\n```\n\n说明\n\n1)  tuple1 的类型是 Tuple5类 是scala特有的类型\n2)  tuple1 的类型取决于 t1 后面有多少个元素, 有对应关系，比如 4个元素对应 Tuple4\n3) 给大家看一个Tuple5 类的定义,大家就了然了\n\n```scala\nfinal case class Tuple5[+T1, +T2, +T3, +T4, +T5](_1: T1, _2: T2, _3: T3, _4: T4, _5: T5)\nextends Product5[T1, T2, T3, T4, T5]\n{\noverride def toString() = \"(\" + _1 + \",\" + _2 + \",\" + _3 + \",\" + _4 + \",\" + _5 + \")\"\n}\n```\n\n4) 元组中最大只能有22个元素 即 `Tuple1...Tuple22`\n\n\n\n**元组数据的访问**\n\n访问元组中的数据,可以采用顺序号（_顺序号），也可以通过索引（productElement）访问。\n\n```scala\nval t1 = (1, \"a\", \"b\", true, 2)\nprintln(t1._1) //访问元组的第一个元素 ，从1开始\nprintln(t1.productElement(0)) // 访问元组的第一个元素，从0开始\n//println(t1(0))错误的写法\n```\n\n\n\n**元组数据的遍历**\n\n```scala\nval tuple1 = (1, 2, 3, \"hello\", 4)\nfor (item <- tuple1.productIterator)\n\tprintln(item)\n```\n\n\n\n## 8. 列表 List\n\nScala中的List 和Java List 不一样，在Java中List是一个接口，真正存放数据是ArrayList，而Scala的List可以直接存放数据，就是一个object，**默认情况下Scala的List是不可变的**，List属于序列Seq。\n\nList源码的定义\n\n```scala\n//定义在package object scala，因为Package scala 这个包可以直接用，而scala._\nval List = scala.collection.immutable.List\n```\n\nList定义在`package object scala`，所以`Package scala` 这个包可以直接使用，而`scala._`是Scala语言自动引入的包，所以我们不用再引入任何包，就可以使用List。\n\n**创建List**\n\n```scala\npackage dataStructure.listPack\n\nobject T1 {\n\n  def main(args: Array[String]): Unit = {\n\n    //方法1\n    val list01 = List(1, 2, 3,\"OK\") //通过apply方法创建一个元素，自动推断类型为Any\n    println(list01)\n    //var list01=new List[Int]()错误，\n    //var list01=new List()//错误，说明List不能用new一个对象\n\n    val list02: List[Int] = List(1) //原理方式同一，自动推断类型为Int\n    println(list02)\n\n    val list03 = Nil  //空集合 \n    println(list03)\n\n    //var list04 = List[Int]()也可\n    val list04=List() //等同于val list04:List[Nothing]=List(),也等于 val list04=Nil\n    println(list04)\n\n  }\n}\n\n```\n\n小节\n\n1) List默认为不可变的集合\n2) List 在 scala包对象声明的,因此不需要引入其它包也可以使用\n3) 源码定义`val List = scala.collection.immutable.List`\n4) List 中可以放任何数据类型，比如 arr1的类型为 List[Any]\n5) 如果希望得到一个空列表，可以使用Nil对象, 在 scala包对象声明的,因此不需要引入其它包也可以使用\n\n**访问List元素**\n\n```scala\nval value1 = list1(1) // 1是索引，表示取出第2个元素.\nprintln(value1)\n```\n\n**List元素的追加**\n\n**默认scala提供的集合都是不可变的**，增加元素都会产生一个新的集合。（和String拼接原理类似）\n\n方式1-在列表的最后增加数据\n\n```scala\nvar list1 = List(1, 2, 3, \"abc\")\n// :+运算符表示在列表的最后增加数据\nval list2 = list1 :+ 4\nprintln(list1) //list1没有变化 List(1, 2, 3, abc)\nprintln(list2) //新的列表结果是 [1, 2, 3, \"abc\", 4]\n```\n\n方式2-在列表的最前面增加数据\n\n```scala\nvar list1 = List(1, 2, 3, \"abc\")\n// :+运算符表示在列表的最后增加数据\nval list2 = 4 +: list1\nprintln(list1) //list1没有变化 List(1, 2, 3, abc)\nprintln(list2) //新的列表结果是 List(4, 1, 2, 3, abc)\n```\n\n方式3- `::`的使用，这个符号应该是List的特有符号\n1) 符号`::`表示向集合中  新建集合添加元素。\n2) 运算时，集合对象一定要放置在最右边\n3) 运算规则，从右向左。\n4) `::: `运算符是将集合中的每一个元素加入到空集合中去，即`:::`操作的对象时集合，而`::`操作的对象是一个个具体的元素值\n\n```scala\nval list1 = List(1, 2, 3, \"abc\")\nval list5 = 4 :: 5 :: 6 :: list1 :: Nil\nprintln(list5) //输出： List(4, 5, 6, List(1, 2, 3, abc))\n\nval list7 = 4 :: 5 :: 6 :: list1 ::: Nil\nprintln(list7) //输出：List(4, 5, 6, 1, 2, 3, abc)\n\nval list8=list1:::list7\nprintln(list8)//List(1, 2, 3, abc, 4, 5, 6, 1, 2, 3, abc)\n```\n\n\n\n## 9. 列表 ListBuffer\n\nListBuffer是可变的list集合，可以添加，删除元素，ListBuffer属于序列（seq）\n\n添加元素\n\n```scala\npackage dataStructure.arrayPack\n\nimport scala.collection.mutable.ArrayBuffer\n\nobject T10 {\n\n  def main(args: Array[String]): Unit = {\n    val arr1=ArrayBuffer(1,2)\n    println(\"arr1 hashcode:\"+System.identityHashCode(arr1))\n    arr1 += 3 //1、添加元素，不会创建新的对象\n    println(\"arr1 hashcode:\"+System.identityHashCode(arr1))\n    arr1.append(4,5) //2、添加元素，不会创建新的对象\n    println(\"arr1 hashcode:\"+System.identityHashCode(arr1))\n    println(arr1) //ArrayBuffer(1, 2, 3, 4, 5)\n    \n    var arr2=arr1 :+ 4 //3、添加元素，会新建一个ArrayBuffer对象\n    var arr3= -7 +: arr2  //4、添加元素，会新建一个ArrayBuffer对象\n    println(arr3) //ArrayBuffer(-7, 1, 2, 3, 4, 5, 4)\n\n  }\n}\n\n```\n\n合并两个ListBuffer\n\n```scala\nval lst0 = ListBuffer(1,2,3)\nval lst1 = ListBuffer(4,5,6)\nlst0 ++= lst1 //不能用:::合并\nprintln(lst0) //ListBuffer(1, 2, 3, 4, 5, 6)\n```\n\n更新ListBuffer中元素\n\n```scala\nval a = ArrayBuffer(1, 2, 3)\na(1)=999\nprintln(a) //ArrayBuffer(1, 999, 3)\n```\n\n删除ListBuffer中元素\n\n```scala\nval a = ArrayBuffer(1, 2, 3)\na.remove(1)\nprintln(a) //ArrayBuffer(1, 3)\n```\n\n\n\n\n\n\n\n## 10. 集合通用符号和方法\n\n\n\n可变元素通用符号和方法\n\n```tex\n通用方法：append remove\n通用符号：+= 、 +: 、 :+、 ++=、()作为下标\n其中，+= 、++= 不用创建新的对象，而 +: 、 :+ 会创建新的对象\n```\n\n\n\n不可变元素通用符号和方法\n\n```tex\n通用方法：无\n通用符号：+: 、 :+、 ()作为下标\n```\n\n\n\n特例：\n\n- 元祖Tuple不能使用()作为下标\n- `::`、`:::` 是列表List的特殊使用方式，其它的集合不能用\n\n\n\n## 11. 队列\n\n1) 队列是一个有序列表，在底层可以用数组或是链表来实现。\n2) 其输入和输出要遵循先入先出的原则。即：先存入队列的数据，要先取出。后存入的要后取出\n3) 在Scala中，由设计者直接给我们提供队列类型使用。\n4) 在scala中, 有 `scala.collection.mutable.Queue` 和 `scala.collection.immutable.Queue` , 一般来说，**我们在开发中通常使用可变集合中的队列**。 \n\n\n\n创建队列\n\n```scala\nimport scala.collection.mutable\n//说明: 这里的Int是泛型，表示q1队列只能存放Int类型\n//如果希望q1可以存放其它类型，则使用 Any 即可。\nval q1 = new mutable.Queue[Int]() //或  val q1 = new mutable.Queue[Int]()\nval q2 = mutable.Queue(55,77)//通过apply方法创建对象\nprintln(q1)//Queue()\nprintln(q2)//Queue(55, 77)\n```\n\n向队列中追加单个元素和集合\n\n```scala\nimport scala.collection.mutable\nval q1 = new mutable.Queue[Int]\nq1 += 20 //追加单个元素\nprintln(q1) //Queue(20)\nq1 ++= List(2,4,6) //追加List\nprintln(q1)//Queue(20, 2, 4, 6)\n\nq1 ++= mutable.Queue(55,77) //追加Queue\nprintln(q1) //Queue(20, 2, 4, 6, 55, 77)\n```\n\n按照队尾进（将数据添加到队列的最后），队头出 来进出队列\n\n```scala\nimport scala.collection.mutable\nval q1 = new mutable.Queue[Int]//\nq1 += 12\nq1 += 34\nq1 ++= List(2,9)\nval num=q1.dequeue() //队列头出\nprintln(num)//12\nprintln(q1)//Queue(34, 2, 9)\nq1.enqueue(20,60) //队列尾进\nprintln(q1)//Queue(34, 2, 9, 20, 60)\n```\n\n**也可以有双端队列的用**\n\n返回队列的第一个元素\n\n```scala\nprintln(q1.head)\n```\n\n返回队列最后一个元素\n\n```scala\nprintln(q1.last)\n```\n\n\n返回除了第一个以外剩余的元素， 可以级联使用，这个在递归时使用较多。\n\n```scala\nprintln(q1.tail)\nprintln(q1.tail.tail)\n```\n\n\n\n\n\n\n\n## 12. 重载运算符\n\n补充一个小知识点\n\n```scala\npackage dataStructure.queuePack\n\nobject T3 {\n  def main(args: Array[String]): Unit = {\n    //补充操作符重载...\n    val cat = new Cat\n    println(cat.age) //10\n    cat += 9\n    println(cat.age) //19\n  }\n}\n\nclass Cat {\n  var age: Int = 10\n  def +=(n:Int): Unit = {\n    this.age += n\n    println(\"xxx\")\n  }\n}\n\n```\n\n\n\n\n\n## 13. 映射 Map\n\n**Java中的Map回顾**\n\nHashMap 是一个散列表(数组+链表)，它存储的内容是键值对(key-value)映射，Java中的HashMap是无序的，key不能重复。\n\n```java\n//只要类型上有<String,Integer>就行,后面的要不要都行\nHashMap<String,Integer> hm = new HashMap();\nhm.put(\"no1\", 100);\nhm.put(\"no2\", 200);\nhm.put(\"no3\", 300);\nhm.put(\"no4\", 400);\nSystem.out.println(hm);\nSystem.out.println(hm.get(\"no2\"));\n```\n\n**Scala中的Map介绍**\n\n1) Scala中的Map 和Java类似，也是一个散列表，它存储的内容也是键值对(key-value)映射，**Scala中不可变的Map是有序的**，可变的Map是无序的。\n2) Scala中，有可变Map (scala.collection.mutable.Map) 和 不可变Map(scala.collection.immutable.Map) \n\n### 13.1 创建map的四种方式\n\n**方式1-构造不可变映射**\n\nScala中的不可变Map是有序，构建Map中的元素底层是Tuple2类型。\n\n```scala\nval map1 = Map(\"Alice\" -> 10, \"Bob\" -> 20, \"Kotlin\" -> \"北京\")//->为Map的独特写法\nprintln(map1)//Map(Alice -> 10, Bob -> 20, Kotlin -> 北京)\n```\n\n小结\n\n1. 从输出的结果看到，输出顺序和声明顺序一致\n2. 构建Map集合中，集合中的元素其实是Tuple2类型\n3. 默认情况下（即没有引入其它包的情况下）,Map是不可变map\n4. 为什么说Map中的元素是Tuple2 类型 [反编译或看对应的apply]\n\n\n\n**方式2-构造可变映射**\n\n```scala\n//需要指定可变Map的包\nval map2 = scala.collection.mutable.Map(\"Alice\" -> 10, \"Bob\" -> 20, \"Kotlin\" -> 30)\nprintln(map2)//Map(Bob -> 20, Kotlin -> 30, Alice -> 10)\n```\n\n从输出的结果看到，输出顺序和声明顺序不一致\n\n\n\n**方式3-创建空的映射**\n\n```scala\nval map3 = new scala.collection.mutable.HashMap[String, Int]\nprintln(map3)//Map()\n```\n\n**方式4-对偶元组**\n\n即创建包含键值对的二元组， 和第一种方式等价，只是形式上不同而已。\n\n对偶元组 就是只含有两个数据的元组。\n\n```scala\nimport scala.collection.mutable\nval map4 = mutable.Map( (\"A\", 1), (\"B\", 2), (\"C\", 3),(\"D\", 30) )\nprintln(\"map4=\" + map4)//map4=Map(D -> 30, A -> 1, C -> 3, B -> 2)\nprintln(map4(\"A\"))//1\n```\n\n\n\n### 13.2 Map-取值\n\n**方式1-使用map(key)**\n\n```scala\nval value1 = map2(\"Alice\")\nprintln(value1)\n```\n\n说明:\n1) 如果key存在，则返回对应的值\n2) 如果key不存在，则抛出异常[java.util.NoSuchElementException]\n3) 在Java中,如果key不存在则返回null\n\n\n\n**方式2-使用contains方法检查是否存在key**\n\n```scala\n// 返回Boolean\n// 1.如果key存在，则返回true\n// 2.如果key不存在，则返回false\nmap4.contains(\"B\")\n```\n\n使用containts先判断在取值，可以防止异常，并加入相应的处理逻辑\n\n```scala\nval map4 = mutable.Map( (\"A\", 1), (\"B\", 2), (\"C\", 3),(\"D\", 30.9) )\nif( map4.contains(\"B\") ) {\nprintln(\"key存在 值= \" + map4(\"B\"))\n} else {\nprintln(\"key不存在\")\n}\n```\n\n**方式3-使用map.get(key).get取值**\n\n通过 `映射.get(键)` 这样的调用返回一个Option对象，要么是Some，要么是None\n\n```scala\nimport scala.collection.mutable\nvar map4 = mutable.Map( (\"A\", 1), (\"B\", \"北京\"), (\"C\", 3) )\nprintln(map4.get(\"A\")) //Some(1)\nprintln(map4.get(\"A\").get) //1\n```\n\n说明和小结:\n\n1) map.get方法会将数据进行包装\n2) 如果 map.get(key) key存在返回some,如果key不存在，则返回None\n3)  如果 map.get(key).get  key存在，返回key对应的值,否则，抛出异常java.util.NoSuchElementException: None.get\n\n\n\n**方式4-使用map4.getOrElse()取值**\n\ngetOrElse 方法 : `def getOrElse[V1 >: V](key: K, default: => V1)`\n说明：\n1) 如果key存在，返回key对应的值。\n2) 如果key不存在，返回默认值。在java中底层有很多类似的操作。\n\n如何选择取值方式建议\n1) 如果我们确定map有这个key ,则应当使用map(key), 速度快\n2) 如果我们不能确定map是否有key ,而且有不同的业务逻辑，使用map.contains() 先判断在加入逻辑\n3) 如果只是简单的希望得到一个值，使用`map4.getOrElse(\"ip\",\"127.0.0.1\")`\n\n```scala\nimport scala.collection.mutable\nval map4 = mutable.Map( (\"A\", 1), (\"B\", \"北京\"), (\"C\", 3) )\nprintln(map4.getOrElse(\"A\",\"默认\"))//1\nprintln(map4.getOrElse(\"AAA\",\"默认\"))//默认\n```\n\n\n\n### 13.3 更新map的元素\n\n```scala\nval map4 = mutable.Map( (\"A\", 1), (\"B\", \"北京\"), (\"C\", 3) )\nmap4(\"AA\") = 20\nprintln(map4)\n```\n\n说明:\n1) map 是可变的，才能修改，否则报错\n2) 如果key存在：则修改对应的值, **key不存在,等价于添加一个key-val**\n\n\n\n### 13.4 添加map元素\n\n**方式1-增加单个元素**\n\n```scala\nimport scala.collection.mutable\nval map4 = mutable.Map( (\"A\", 1), (\"B\", \"北京\"), (\"C\", 3) )\nmap4 += ( \"D\" -> 4 ) //方式1\nmap4 += ( \"B\" -> 50 ) //如果增加的key存在会覆盖以前的val值\nmap4.+=( \"E\" -> 999 ) //方式2\nprintln(map4)  //Map(D -> 4, A -> 1, C -> 3, E -> 999, B -> 50)\n```\n\n**方式2-增加多个元素**\n\n```scala\nimport scala.collection.mutable\nval map4 = mutable.Map( (\"A\", 1), (\"B\", \"北京\"), (\"C\", 3) )\nval map5 = map4 + (\"E\"->1, \"F\"->3)\nmap4 += (\"EE\"->1, \"FF\"->3)\nprintln(map4)//Map(A -> 1, C -> 3, EE -> 1, B -> 北京, FF -> 3)\nprintln(map5)//Map(A -> 1, C -> 3, F -> 3, E -> 1, B -> 北京)\n```\n\n\n\n### 13.5 删除map元素\n\n```scala\nimport scala.collection.mutable\nval map4 = mutable.Map( (\"A\", 1), (\"B\", \"北京\"), (\"C\", 3),(\"D\",555) )\nmap4 -= (\"A\", \"B\") //删除多个元素\nmap4 -= \"D\" //删除一个元素\nprintln(\"map4=\" + map4)\n```\n\n1) \"A\",\"B\" 就是要删除的key, 可以写多个.\n2) 如果key存在，就删除，如果key不存在，也不会报错.\n\n\n\n### 13.6 对map遍历\n\n对map的元素(元组Tuple2对象 )进行遍历的方式很多，具体如下:\n\n```scala\nimport scala.collection.mutable\nval map1 = mutable.Map((\"A\", 1), (\"B\", \"北京\"), (\"C\", 3))\nfor ((k, v) <- map1) println(k + \" is mapped to \" + v)\nfor (v <- map1.keys) println(v)\nfor (v <- map1.values) println(v)\nfor (v <- map1) println(v) //v是Tuple\n```\n\n说明\n1.每遍历一次，返回的元素是Tuple2\n2.取出的时候，可以按照元组的方式来取\n\n\n\n\n\n## 14. 集 Set\n\n集是不重复元素的结合。集不保留顺序，默认是以哈希集实现\n\n**Java中Set的回顾**\n\njava中，HashSet是实现`Set<E>`接口的一个实体类，数据是以哈希表的形式存放的，里面的不能包含重复数据。Set接口是一种不包含重复元素的 collection，HashSet中的数据也是没有顺序的。\n\n```java\nHashSet hs = new HashSet<String>();\nhs.add(\"jack\");\nhs.add(\"tom\");\nhs.add(\"jack\");\nhs.add(\"jack2\");\nSystem.out.println(hs);\n```\n\n**Scala中Set的说明**\n\n默认情况下，Scala 使用的是不可变集合，如果你想使用可变集合，需要引用`scala.collection.mutable.Set` 包\n\n**集 Set-创建**\n\nSet不可变集合的创建\n\n```scala\nval set = Set(1, 2, 3) //不可变\nprintln(set)\n```\n\nSet可变集合的创建\n\n```scala\nimport scala.collection.mutable\nval set01 = Set(1,2,4,\"abc\") //不可变\nprintln(set01)\nval set02 = mutable.Set(1,2,4,\"abc\") //可变\nprintln(set02)\n```\n\n**可变集合的元素添加**\n\n```scala\nmutableSet.add(4) //方式1\nmutableSet += 6  //方式2\nmutableSet.+=(5) //方式3\nmutableSet +=(5,9,8) //方式4 添加集合\n```\n\n说明：如果添加的对象已经存在，则不会重复添加，也不会报错\n\n**可变集合的元素删除**\n\n```scala\nval set02 = mutable.Set(1,2,4,\"abc\")\nset02 -= 2 // 操作符形式\nset02.remove(\"abc\") // 方法的形式，scala的Set可以直接删除值\nprintln(set02)\n```\n\n说明：说明：如果删除的对象不存在，则不生效，也不会报错\n\n**集 Set-遍历**\n\n```scala\nval set02 = mutable.Set(1, 2, 4, \"abc\")\nfor(x <- set02) {\nprintln(x)\n}\n```\n\n**集 Set-更多操作**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210325151134.png)\n\n\n\n\n\n## 15. Set和Map通用符号\n\n`+=`即可以添加一个元素，也可以添加一个集合\n\n在Set中\n\n```scala\nmutableSet.add(4) //方式1\nmutableSet += 6  //方式2\nmutableSet.+=(5) //方式3\nmutableSet +=(5,9,8) //方式4 添加集合\n```\n\n在Map中\n\n```scala\nimport scala.collection.mutable\nval map4 = mutable.Map( (\"A\", 1), (\"B\", \"北京\"), (\"C\", 3) )\nmap4 += ( \"D\" -> 4 ) //方式1\nmap4 += ( \"B\" -> 50 ) //如果增加的key存在会覆盖以前的val值\nmap4.+=( \"E\" -> 999 ) //方式2\nmap4 += (\"EE\"->1, \"FF\"->3)\n```\n\n\n\n\n\n## 16. 集合元素的映射-map映射操作\n\n看一个实际需求\n\n要求：请将List(3,5,7) 中的所有元素都 \\* 2 ，将其结果放到一个新的集合中返回，即返回一个新的List(6,10,14), 请编写程序实现\n\n用传统的方法解决\n\n```scala\nval list1 = List(3, 5, 7)\nvar list2 = List[Int]()\nfor (item <- list1) { //遍历\nlist2 = list2 :+ item * 2\n}\nprintln(list2)\n```\n\n**map映射操作**\n\n上面提出的问题，其实就是一个关于集合元素映射操作的问题。\n在Scala中可以通过map映射操作来解决：将集合中的每一个元素通过指定功能（函数）映射（转换）成新的结果集合这里其实就是所谓的将函数作为参数传递给另外一个函数,这是函数式编程的特点\n\n以HashSet为例说明\n\n`def map[B](f: (A) ⇒ B): HashSet[B]`   //map函数的签名\n1) 这个就是map映射函数集合类型都有\n2) `[B]` 是泛型\n3) map 是一个高阶函数(可以接受一个函数的函数，就是高阶函数)，可以接收函数 `f: (A) => B` 后面详解(先简单介绍下.)\n4) `HashSet[B]` 就是返回的新的集合\n\n**使用map映射函数来解决**\n\n```scala\nval list1 = List(3, 5, 7)\ndef f1(n1: Int): Int = {\n    2 * n1\n}\nval list2 = list1.map(f1)\nprintln(list2)//List(6, 10, 14)\n```\n\n\n\n为了进一步理解，我们在举一个高阶函数的案例\n\n```scala\n  def main(args: Array[String]): Unit = {\n    test2(sayOK)\n  }\n\n  def test2(f: () => Unit) = {\n    f()\n  }\n\n  def sayOK() = {\n    println(\"sayOKKK...\")\n  }\n```\n\n高阶函数基本使用\n\n```scala\ndef main(args: Array[String]): Unit = {\n    val res = test(sum, 6.0) //res=12.0\n    println(\"res=\" + res)\n}\ndef test(f: Double => Double, n1: Double) = {\n    f(n1)\n}\ndef sum(d: Double): Double = {\n    d + d\n}\n```\n\n\n\n**深刻理解map映射函数的机制-模拟实现**\n\n```scala\npackage dataStructure.mapPack\n\nobject T4 {\n  def main(args: Array[String]): Unit = {\n    val list1 = List(3, 5, 7)\n\n    def f1(n1: Int): Int = {\n      println(\"xxx\")\n      2 * n1\n    }\n\n    val list2 = list1.map(f1)\n    println(list2) //List(6, 10, 14)\n    val myList = MyList()\n    val myList2 = myList.map(f1)\n    println(\"myList2=\" + myList2) //myList2=List(6, 10, 14)\n    println(\"myList=\" + myList.list1) //myList=List(3, 5, 7)\n  }\n}\n\nclass MyList {\n  var list1 = List(3, 5, 7)\n  var list2 = List[Int]()\n\n  def map(f: Int => Int): List[Int] = {\n    for (item <- list1) {\n      list2 = list2 :+ f(item)\n    }\n    list2\n  }\n}\n\nobject MyList {\n  def apply(): MyList = new MyList()\n}\n```\n\n\n\n一个练习\n\n请将 `val names = List(\"Alice\", \"Bob\", \"Nick\")` 中的所有单词，全部转成字母大写，返回到新的List集合中\n\n```scala\nval names = List(\"Alice\", \"Bob\", \"Nick\")\ndef upper(s:String): String = {\n    s.toUpperCase\n}\nval names2 = names.map(upper)\nprintln(\"names=\" + names2) //names=List(ALICE, BOB, NICK)\n```\n\n\n\n## 17. flatmap映射\n\nflatmap映射：flat即压扁，压平，扁平化映射\n\nflatmap：flat即压扁，压平，扁平化，效果就是将集合中的**每个元素的子元素映射到某个函数并返回新的集合**。\n\n看一个案例：\n\n```scala\nval names = List(\"Alice\", \"Bob\", \"Nick\")\ndef upper( s : String ) : String = {\n    s. toUpperCase\n}\n//注意：每个字符串也是char集合\nprintln(names.flatMap(upper)) //List(A, L, I, C, E, B, O, B, N, I, C, K)\n```\n\n\n\n## 18. 集合元素的过滤-filter\n\nfilter：将符合要求的数据(筛选)放置到新的集合中\n\n应用案例：将  `val names = List(\"Alice\", \"Bob\", \"Nick\")` 集合中首字母为'A'的筛选到新的集合。\n\n```scala\nval names = List(\"Alice\", \"Bob\", \"Nick\")\ndef startA(s:String): Boolean = {\n    s.startsWith(\"A\")\n}\nval names2 = names.filter(startA)\nprintln(\"names=\" + names2) //names=List(Alice)\n```\n\n\n\n## 19. 化简 reduce\n\n看一个需求:\nval list = List(1, 20, 30, 4 ,5) , 求出list的和.\n\n化简：**将二元函数引用于集合中的函数**。\n\n上面的问题当然可以使用遍历list方法来解决，这里我们使用scala的化简方式来完成\n\n```scala\nval list = List(1, 20, 30, 4, 5)\ndef sum(n1: Int, n2: Int): Int = {\n    n1 + n2\n}\nval res = list.reduceLeft(sum)\nprintln(\"res=\" + res) //res=60\n```\n\n说明\n1) `def reduceLeft[B >: A](@deprecatedName('f) op: (B, A) => B): B`\n2) reduceLeft(f) 接收的函数需要的形式为 `op: (B, A) => B): B`\n3) reduceleft(f) 的运行规则是 从左边开始执行将得到的结果返回给第一个参数\n4)  然后继续和下一个元素运行，将得到的结果继续返回给第一个参数，继续和下一个元素运行\n5) 即: `((((1 + 2)  + 3) + 4) + 5) = 15`\n\n`reduceLefft(_ + _)`这个函数的执行逻辑如图:\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210325163052.png)\n\n`reduceRight(_ - _)`反之同理\n\n练习一下\n\n1) 分析下面的代码输出什么结果\n\n```scala\nval list = List(1, 2, 3, 4 ,5)\ndef minus( num1 : Int, num2 : Int ): Int = {\n    num1 - num2\n}\nprintln(list.reduceLeft(minus)) // 输出 -13\nprintln(list.reduceRight(minus)) //输出 3\nprintln(list.reduce(minus)) //输出-13\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210325163605.png)\n\n2) 使用化简的方法求出 List(3,4,2,7,5) 最小的值\n\n```scala\nval list = List[Int](3,4,2,7,5)\ndef min( num1 : Int, num2 : Int ): Int = {\n    if(num1>num2)\n    num2\n    else num1\n}\nprintln(list.reduceLeft(min)) // 输出 2\nprintln(list.reduceRight(min)) //输出 2\nprintln(list.reduce(min)) //输出2\n```\n\n\n\n## 20. 折叠 fold\n\nfold函数将上一步返回的值作为函数的第一个参数继续传递参与运算，直到list中的所有元素被遍历。\n\n1) 可以把reduceLeft看做简化版的foldLeft。如何理解:\n\n```scala\ndef reduceLeft[B >: A](@deprecatedName('f) op: (B, A) => B): B =\nif (isEmpty) throw new UnsupportedOperationException(\"empty.reduceLeft\")\nelse tail.foldLeft[B](head)(op)\n```\n\n大家可以看到，reduceLeft就是调用的`foldLeft[B](head)`，并且是默认从集合的head元素开始操作的。\n\n2) 相关函数：fold，foldLeft，foldRight，可以参考reduce的相关方法理解\n\n应用案例\n\n```scala\n// 折叠\nval list = List(1, 2, 3, 4)\ndef minus( num1 : Int, num2 : Int ): Int = {\n    num1 - num2\n}\n// 函数的柯里化\nprintln(list.foldLeft(5)(minus)) //输出 -5，相当于把5加在List的第一个位置\nprintln(list.foldRight(5)(minus)) // 输出3，相当于把5加在List的最后一个位置\n```\n\n说明\n\n折叠的原理和化简的运行机制几乎一样，`list.foldLeft(5)(minus)`可以理解为\n\n```scala\nval list = List(5,1,2,3,4)\ndef minus( num1 : Int, num2 : Int ): Int = {\n    num1 - num2\n}\nprintln(list.reduceLeft(minus))//-5\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210325174525.png)\n\n`list.foldRight(5)(minus)`可以理解为\n\n```scala\nval list = List(1,2,3,4,5)\ndef minus( num1 : Int, num2 : Int ): Int = {\n    num1 - num2\n}\nprintln(list.reduceRight(minus))//3\n```\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210325174556.png)\n\n\n\nfoldLeft和foldRight 缩写方法分别是：`/:`和`:\\`\n\n```scala\nval list4 = List(1, 9, 2, 8)\ndef minus(num1: Int, num2: Int): Int = {\n    num1 - num2\n}\nvar i6 = (1 /: list4) (minus) // 等价=> list4.foldLeft(1)(minus)\nprintln(i6) // 输出-19\ni6 = (100 /: list4) (minus) //等价=> list4.foldLeft(100)(minus)\nprintln(i6) // 输出 80\ni6 = (list4 :\\ 10) (minus) // list4.foldRight(10)(minus)\nprintln(i6) // 输出-4\n```\n\n\n\n## 21. 扫描 scan\n\n扫描，即对某个集合的所有元素做fold操作，但是会把**产生的所有中间结果**放置于一个集合中保存\n\n应用实例1\n\n```scala\ndef minus( num1 : Int, num2 : Int ) : Int = {\n    num1 - num2\n}\n//(1 to 5) 等同于 List(1,2,3,4,5)\nval i8 = (1 to 5).scanLeft(5)(minus) ////IndexedSeq[Int]\nprintln(i8)//输出 Vector(5, 4, 2, -1, -5, -10)\n```\n\n说明：\n\n1. 把5放在List的第一位，此时List为`（5,1,2,3,4,5)`\n2. 把5放入Vector中，此时Vector(5)\n3. 执行5-1=4，放入Vector，此时Vector(5,4)\n4. 执行4-2=2，放入Vector，此时Vector(5,4,2)\n5. 依次类推，直到执行-5-5=-10，此时 Vector(5, 4, 2, -1, -5, -10)\n\n应用案例2\n\n```scala\ndef minus( num1 : Int, num2 : Int ) : Int = {\n    num1 - num2\n}\nval i9 = (1 to 5).scanRight(5)(minus) //IndexedSeq[Int]\nprintln(i9)//输出 Vector(-2, 3, -1, 4, 0, 5)\n```\n\n说明\n\n1. 把5放在List的最后一位，此时List为`（1,2,3,4,5,5)`\n2. 把5放入Vector中，此时Vector(5)\n3. 执行5-5=0，使用头插法插入Vertor，此时Vector(0,5)\n4. 执行4-0=4，使用头插法插入Vertor，此时Vector(4,0,5)\n5. 执行3-4=-1，使用头插法插入Vertor，此时Vector(-1,4,0,5)\n6. 依次类推，直到执行1-3=-2，此时 Vector(-2, 3, -1, 4, 0, 5)\n\n\n\n## 22. 集合应用案例\n\n**案例1**\n\n一个变量sentence\n\n```scala\nval sentence = \"AAAAAAAAAABBBBBBBBCCCCCDDDDDDD\"\n```\n\n将sentence 中各个字符，通过foldLeft存放到 一个ArrayBuffer中\n目的：理解flodLeft的用法.\n\n```scala\nval sentence = \"AAAAAAAAAABBBBBBBBCCCCCDDDDDDD\"\n//初始值是arr，后面的c一个个变量\ndef putArry( arr : ArrayBuffer[Char], c : Char ): ArrayBuffer[Char] = {\n    arr.append(c)\n    arr\n}\n//创建val arr = ArrayBuffer[Char]()\nval arr = ArrayBuffer[Char]()\nsentence.foldLeft(arr)(putArry)\nprintln(arr)\n```\n\n\n\n**案例2**\n\n一个变量sentence\n\n```scala\nval sentence = \"AAAAAAAAAABBBBBBBBCCCCCDDDDDDD\"\n```\n\n使用映射集合，统计一句话中，各个字母出现的次数\n提示：`Map[Char, Int]()`\n\n```scala\nimport scala.collection.mutable.Map\nval sentence = \"AAAAAAAAAABBBBBBBBCCCCCDD\"\ndef charCount( map : Map[Char, Int], c : Char ): Map[Char, Int] = {\n    map += (c -> (map.getOrElse(c, 0) + 1))\n    map\n}\nval map2 = sentence.foldLeft(Map[Char, Int]())(charCount)\nprintln(map2) //Map(A -> 10, B -> 8, C -> 5, D -> 2)\n```\n\n\n\n## 23. 扩展-拉链(合并)\n\n在开发中，当我们需要将两个集合进行 对偶元组合并，可以使用拉链。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210325201740.png)\n\n```scala\n// 拉链\nval list1 = List(1, 2 ,3)\nval list2 = List(4, 5, 6)\nval list3 = list1.zip(list2) // (1,4),(2,5),(3,6)\nprintln(\"list3=\" + list3)\n```\n\n注意事项\n1) 拉链的本质就是两个集合的合并操作，合并后每个元素是一个 对偶元组。\n2) 操作的规则下图:\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210325201833.png)\n\n3) 如果两个集合个数不对应，会造成数据丢失。\n4) 集合不限于List, 也可以是其它集合比如 Array\n5) 如果要取出合并后的各个对偶元组的数据，可以遍历\n\n```scala\nfor(item<-list3){\nprint(item._1 + \" \" + item._2) //取出时，按照元组的方式取出即可  \n}\n```\n\n\n\n## 24. 扩展-迭代器\n\n通过iterator方法从集合获得一个迭代器，通过while循环和for表达式对集合进行遍历(学习使用迭代器来遍历)\n\n```scala\nval iterator = List(1, 2, 3, 4, 5).iterator // 得到迭代器\nprintln(\"--------遍历方式1 -----------------\")\nwhile (iterator.hasNext) {\n    println(iterator.next())\n}\nprintln(\"--------遍历方式2 for -----------------\")\nfor(enum <- iterator) {\n    println(enum) //\n}\n```\n\n1)  iterator 的构建实际是 AbstractIterator 的一个匿名子类，该子类提供了\n\n```scala\ndef iterator: Iterator[A] = new AbstractIterator[A] {\nvar these = self\ndef hasNext: Boolean = !these.isEmpty\ndef next(): A =\n```\n\n2) 该AbstractIterator 子类提供了  hasNext next 等方法.\n\n3) 因此，我们可以使用 while的方式，使用hasNext next 方法变量\n\n\n\n## 25. 扩展-流 Stream\n\nstream是一个集合。这个集合，可以用于存放无穷多个元素，但是这无穷个元素并不会一次性生产出来，而是需要用到多大的区间，就会动态的生产，末尾元素遵循lazy规则(即：要使用结果才进行计算的) 。\n\n案例:\n\n```scala\ndef numsForm(n: BigInt) : Stream[BigInt] = n #:: numsForm(n + 1)\nval stream1 = numsForm(1)\n```\n\n说明\n1) Stream 集合存放的数据类型是`BigInt`\n2) numsForm 是自定义的一个函数，函数名是程序员指定的。\n3) 创建的集合的第一个元素是 n , 后续元素生成的规则是 n + 1\n4) 后续元素生成的规则是可以程序员指定的 ，比如 numsForm( n * 4)...\n\n使用tail，会动态的向stream集合按规则生成新的元素\n\n```scala\n//创建Stream\ndef numsForm(n: BigInt) : Stream[BigInt] = n #:: numsForm(n + 1)\nval stream1 = numsForm(1)\nprintln(stream1) //Stream(1, ?)\n//取出第一个元素\nprintln(\"head=\" + stream1.head) //head=1\nprintln(stream1.tail) //Stream(2, ?)\nprintln(stream1) //Stream(1, 2, ?)\n```\n\n如果使用流集合，就不能使用last属性，如果使用last集合就会进行无限循环\n\n**使用map映射stream的元素并进行一些计算**\n\n```scala\n//创建Stream\ndef numsForm(n: BigInt) : Stream[BigInt] = n #:: numsForm(n + 1)\ndef multi(x:BigInt) : BigInt = {\n    x * x\n}\nval stream2=numsForm(5).map(multi)\nprintln(stream2) //Stream(25, ?)\nstream2.tail\nprintln(stream2) //Stream(25, 36, ?)\n```\n\n\n\n\n\n## 26. 扩展-视图 View\n\nStream的懒加载特性，也可以对其他集合应用view方法来得到类似的效果，具有如下特点：\n\n1) view方法产出一个总是被懒执行的集合。\n2) view不会缓存数据，每次都要重新计算，比如遍历View时。\n\n应用案例\n请找到1-100 中，数字倒序排列 和它本身相同的所有数。(1 2, 11, 22, 33 ...)\n\n```scala\ndef multiple(num: Int): Int = {\n    num\n}\n\ndef eq(i: Int): Boolean = {\n    i.toString.equals(i.toString.reverse)\n}\n\n//说明: 没有使用view\nval viewSquares1 = (1 to 100)\n.map(multiple)\n.filter(eq)\nprintln(viewSquares1) //Vector(1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 22, 33, 44, 55, 66, 77, 88, 99)\n//for (x <- viewSquares1) {}\n//使用view\nval viewSquares2 = (1 to 100)\n.view\n.map(multiple)\n.filter(eq)\nprintln(viewSquares2) //SeqViewMF(...)\nfor (item <- viewSquares2)\n    print(item + \" \")\n```\n\n\n\n## 27. 扩展-线程安全的集合\n\n所有线程安全的集合都是以Synchronized开头的集合\n\n```scala\nSynchronizedBuffer\nSynchronizedMap\nSynchronizedPriorityQueue\nSynchronizedQueue\nSynchronizedSet\nSynchronizedStack\n```\n\n\n\n## 28. 扩展-并行集合\n\n**parallel(并行)**\n\n1) Scala为了充分使用多核CPU，提供了并行集合（有别于前面的串行集合），用于多核环境的并行计算。\n2) 主要用到的算法有：\nDivide and conquer : 分治算法，Scala通过splitters(分解器)，combiners（组合器）等抽象层来实现，主要原理是将计算工作分解很多任务，分发给一些处理器去完成，并将它们处理结果合并返回\n\nWork stealin算法【学数学】，主要用于任务调度负载均衡（load balancing），通俗点完成自己的所有任务之后，发现其他人还有活没干完，主动（或被安排）帮他人一起干，这样达到尽早干完的目的\n\n**应用案例**\n\n打印1~5\n\n```scala\n(1 to 5).foreach(println(_))\nprintln(\"-----------\")\n(1 to 5).par.foreach(println(_)) //输出的顺序不是从1到5\n```\n\n查看并行集合中元素访问的线程\n\n```scala\nval result1 = (0 to 100).map{case _ => Thread.currentThread.getName}\nval result2 = (0 to 100).par.map{case _ => Thread.currentThread.getName}\nprintln(result1)\nprintln(result2)\n```\n\n\n\n## 29. 扩展-操作符\n\n这部分内容没有必要刻意去理解和记忆，语法使用的多了，自然就会熟练的使用，该部分内容了解一下即可。\n\n操作符扩展\n1) 如果想在变量名、类名等定义中使用语法关键字（保留字），可以配合反引号反引号\n\n```scala\nval `val` = 42\n```\n\n2) 中置操作符：A 操作符 B 等同于 A.操作符(B)\n\n```scala\nval n1 = 1\nval n2 = 2\nval r1 = n1 + n2\nval r2 = n1.+(n2) //看Int的源码，n1本身不改变\nprintln(\"r1=\" + r1 + \" r2=\" + r2)\nval dog = new Dog\ndog.+(90)\ndog + 10\nprint(dog.age) // 101\n```\n\n3) 后置操作符：A操作符 等同于 `A.操作符`，如果操作符定义的时候不带()则调用时不能加括号\n\n```scala\nobject T9 {\n\n  def main(args: Array[String]): Unit = {\n    // 操作符\n    val oper = new Operate\n    println(oper++) //123\n    println(oper.++) //123\n  }\n}\n\nclass Operate {\n  //定义函数/方法的时候，省略的()\n  def ++ = \"123\"\n}\n```\n\n4) 前置操作符，`+、-、！、~`等操作符A等同于A.unary_操作符\n\n```scala\nobject T9 {\n\n  def main(args: Array[String]): Unit = {\n    // 操作符\n    val oper = new Operate\n    !oper //前置运算符\n  }\n}\n\nclass Operate {\n  // 声明前置运算符\n  //unary ：一元运算符\n  def unary_! = println(\"!!!!!!!\")\n}\n```\n\n5) 赋值操作符，`A 操作符= B` 等同于 `A = A 操作符 B`  ，比如 A += B 等价 A = A + B","tags":["scala"],"categories":["scala"]},{"title":"四、Scala之隐世转换与隐世参数","url":"/2021/03/24/152651/","content":"\n\n\nScala之隐世转换与隐世参数\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n## 1. 隐世转换入门\n\n先看一段代码，引出隐式转换的实际需要=>指定某些数据类型的相互转化\n\n```scala\nobject Scala01 {\ndef main(args: Array[String]): Unit = {\n    val num : Int = 3.5 //?错 高精度->低精度\n    println(num)\n}\n}\n```\n\n**隐式函数基本介绍**\n\n隐式转换函数是以implicit关键字声明的带有单个参数的函数。这种函数将会自动应用，将值从一种类型转换为另一种类型\n\n使用隐式函数可以优雅的解决数据类型转换，以前面的案例进行说明\n\n```scala\npackage implicitPack\n\nobject T1 {\n\n  def main(args: Array[String]): Unit = {\n    implicit def f1(d: Double): Int = {\n      d.toInt\n    }\n    implicit def f2(l: Long): Int = {\n      l.toInt\n    }\n    val num: Int = 3.5\n    println(num)\n    val num2: Int = 4.5\n    println(num2)\n    val num3: Int = 20l\n  }\n}\n\n```\n\n**隐式转换的注意事项和细节**\n\n1) 隐式转换函数的函数名可以是任意的，隐式转换与函数名称无关，只与函数签名（函数参数类型和返回值类型）有关。\n\n2) 隐式函数可以有多个(即：隐式函数列表)，但是需要保证在当前环境下，只有一个隐式函数能被识别\n\n```scala\n//在当前环境中，不能存在满足条件的多个隐式函数\nimplicit def a(d: Double) = d.toInt\nimplicit def b(d: Double) = d.toInt\nval i1: Int = 3.5 //（X）在转换时，识别出有两个方法可以被使用，就不确定调用哪一个，所以出错\nprintln(i1)\n```\n\n\n\n## 2. 隐式方法\n\n如果需要为一个类增加一个方法，可以通过隐式转换来实现。（动态增加功能）比如想为MySQL类增加一个delete方法\n\n**分析解决方案**\n\n在当前程序中，如果想要给MySQL类增加功能是非常简单的，但是在实际项目中，如果想要增加新的功能就会需要改变源代码，这是很难接受的。而且违背了软件开发的OCP开发原则 (闭合原则 open close priceple)。在这种情况下，可以通过隐式转换函数给类动态添加功能。\n\n使用隐式转换方式动态的给MySQL类增加delete方法：\n\n```scala\npackage implicitPack\n\nobject T2 {\n\n  def main(args: Array[String]): Unit = {\n    //这个形参就是自动匹配用的\n    implicit def addDelete(mysql: MySQL): DB = {////隐世方法不一定写在main方法里，只要在作用域范围内能找到就行\n      new DB //\n    }\n\n    val mysql = new MySQL\n    mysql.insert()//输出insert\n    mysql.delete()//输出delete\n  }\n}\n\nclass MySQL {\n  def insert(): Unit = {\n    println(\"insert\")\n  }\n}\n\nclass DB {\n  def delete(): Unit = {\n    println(\"delete\")\n  }\n}\n```\n\n这个代码经过反编译后为：\n\n```java\npublic final class T2$ {\n  public static T2$ MODULE$;\n  \n  private static final DB addDelete$1(MySQL mysql) {\n    return new DB();\n  }\n  \n  public void main(String[] args) {\n    MySQL mysql = new MySQL();\n    mysql.insert();\n    addDelete$1(mysql).delete();\n  }\n  \n  private T2$() {\n    MODULE$ = this;\n  }\n}\n\n```\n\n\n\n## 3. 隐式值\n\n隐式值也叫隐式变量，**将某个形参变量标记为implicit**，所以编译器会在方法省略隐式参数的情况下去搜索作用域内的隐式值作为缺省参数\n\n应用案例\n\n```scala\npackage implicitPack\n\nobject T3 {\n\n  def main(args: Array[String]): Unit = {\n    implicit val str1: String = \"jack\" //隐世值不一定写在main方法里，只要在作用域范围内能找到就行\n    def hello(implicit name: String): Unit = {\n      println(name + \" hello\")\n    }\n    hello //调用.不带()，输出jack hello\n  }\n}\n\n```\n\n使用隐式值时，不要出现模棱两可的现象，如下所示\n\n```scala\npackage implicitPack\n\nobject T4 {\n  def main(args: Array[String]): Unit = {\n    // 隐式变量（值）\n    implicit val name: String = \"Scala\"\n    implicit val name1: String = \"World\"\n    def hello(implicit content: String = \"jack\"): Unit = {\n      println(\"Hello \" + content)\n    } //调用hello\n    hello\n  }\n}\n```\n\n会保存，信息如下：\n\n```scala\nambiguous implicit values:\n both value name1 of type String\n and value name of type String\n match expected type String\n    hello\n```\n\n## 4. 隐式类\n\n在scala2.10后提供了隐式类，可以使用implicit声明类，隐式类的非常强大，同样可以扩展类的功能，比前面使用隐式转换丰富类库功能更加的方便，在集合中隐式类会发挥重要的作用。\n\n隐式类使用有如下几个特点：\n1) **其所带的构造参数有且只能有一个**\n2) 隐式类必须被定义在“类”或“伴生对象”或“包对象”里，即**隐式类不能是顶级的**(top-level  objects)，否则，就作用在整个程序中了，这显然是不可取的。\n3) 隐式类不能是case class（case class在后续介绍 样例类）\n4) 作用域内不能有与之相同名称的标识符\n\n**应用案例**\n\n```scala\npackage implicitPack\n\nobject T5 {\n  def main(args: Array[String]): Unit = {\n    //DB1会对应生成隐式类\n    implicit class DB1(val m: MySQL1) {\n      def addSuffix(): String = {\n        m + \" scala\"\n      }\n    }\n    val mysql1 = new MySQL1\n    mysql1.sayOk()//输出：sayOK\n    println(mysql1.addSuffix())//输出：implicitPack.MySQL1@3c5a99da scala\n  }\n}\n\nclass MySQL1 {\n  def sayOk(): Unit = {\n    println(\"sayOk\")\n  }\n}\n\n```\n\n该代码反编译为：\n\n```java\n//1\npackage implicitPack;\n\npublic final class T5$ {\n  public static T5$ MODULE$;\n  \n  private static final T5$DB1$1 DB1$2(MySQL1 m) {\n    return new T5$DB1$1(m);\n  }\n  \n  public void main(String[] args) {\n    MySQL1 mysql1 = new MySQL1();\n    mysql1.sayOk();\n    scala.Predef$.MODULE$.println(DB1$2(mysql1).addSuffix());\n  }\n  \n  private T5$() {\n    MODULE$ = this;\n  }\n}\n\n//2\npackage implicitPack;\n\nimport scala.Predef$;\nimport scala.Predef$any2stringadd$;\n\npublic class T5$DB1$1 {\n  private final MySQL1 m;\n  \n  public MySQL1 m() {\n    return this.m;\n  }\n  \n  public T5$DB1$1(MySQL1 m) {}\n  \n  public String addSuffix() {\n    return Predef$any2stringadd$.MODULE$.$plus$extension(Predef$.MODULE$.any2stringadd(m()), \" scala\");\n  }\n}\n\n//3\npackage implicitPack;\n\nimport scala.Predef$;\nimport scala.reflect.ScalaSignature;\n\n@ScalaSignature(bytes = \"\\006\\001]1Aa\\001\\003\\001\\017!)a\\002\\001C\\001\\037!)!\\003\\001C\\001'\\t1Q*_*R\\031FR\\021!B\\001\\rS6\\004H.[2jiB\\0137m[\\002\\001'\\t\\001\\001\\002\\005\\002\\n\\0315\\t!BC\\001\\f\\003\\025\\0318-\\0317b\\023\\ti!B\\001\\004B]f\\024VMZ\\001\\007y%t\\027\\016\\036 \\025\\003A\\001\\\"!\\005\\001\\016\\003\\021\\tQa]1z\\037.$\\022\\001\\006\\t\\003\\023UI!A\\006\\006\\003\\tUs\\027\\016\\036\")\npublic class MySQL1 {\n  public void sayOk() {\n    Predef$.MODULE$.println(\"sayOk\");\n  }\n}\n\n```\n\n编译器优先级：传值 > 隐式值 > 默认值，但是不能有二义性\n\n```scala\npackage implicitPack\n\nobject TT {\n\n  def main(args: Array[String]): Unit = {\n    implicit val name:String=\"aaa\"\n    def hello(implicit content:String=\"jack\"): Unit ={\n      println(\"hello:\"+content)\n    }\n    def hello2(implicit content:Int=3): Unit ={\n      println(\"hello2:\"+content)\n    }\n    hello//隐式值方式，输出hello:aaa\n    hello(\"bbb\")//传值方式，输出hello:bbb\n    hello2//默认方式，输出hello:3\n\n  }\n}\n```\n\n\n\n\n\n\n\n\n\n## 5. 隐式的转换时机\n\n1) 当方法中的参数的类型与目标类型不一致时\n\n```scala\npackage implicitPack\n\nobject T6 {\n\n  def main(args: Array[String]): Unit = {\n    implicit def f1(d:Double):Int={\n      d.toInt\n    }\n    def test1(n1:Int){\n      println(\"OK\")\n    }\n    test1(10.5)//隐式转换\n\n  }\n}\n\n```\n\n\n\n2) 当对象调用所在类中不存在的方法或成员时，编译器会自动将对象进行隐式转换（根据类型）\n\n\n\n## 6. 隐式解析机制\n\n即编译器是如何查找到缺失信息的，解析具有以下两种规则：\n1) 首先会在当前代码作用域下查找隐式实体（隐式方法、隐式类、隐式对象）。(一般是这种情况)\n2) 如果第一条规则查找隐式实体失败，会继续在隐式参数的类型的作用域里查找。类型的作用域是指与该类型相关联的全部伴生模块，一个隐式实体的类型T它的查找范围如下(第二种情况范围广且复杂在使用时，应当尽量避免出现，**了解一下就行**)：\na)  如果T被定义为T with A with B with C，那么A,B,C都是T的部分，在T的隐式解析过程中，它们的伴生对象都会被搜索。\nb)  如果T是参数化类型，那么类型参数和与类型参数相关联的部分都算作T的部分，比如List[String]的隐式搜索会搜索List的伴生对象和String的伴生对象。\nc)  如果T是一个单例类型p.T，即T是属于某个p对象内，那么这个p对象也会被搜索。\nd)  如果T是个类型注入`S#T`，那么S和T都会被搜索。\n\n\n\n\n\n## 7. 隐式转换的前提\n\n在进行隐式转换时，需要遵守两个基本的前提：\n1) 不能存在二义性\n2) 隐式操作不能嵌套使用 \n\n如:隐式转换函数\n\n```scala\n  def main(args: Array[String]): Unit = {\n    implicit def f1(d:Double):Int={\n      d.toInt\n      val num:Int=3.5// 隐式操作不能嵌套使用 \n    }\n    \n  }\n```\n\n\n\n","tags":["scala"],"categories":["scala"]},{"title":"三、Scala之面向对象编程","url":"/2021/03/23/110855/","content":"\nScala之面向对象编程\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n\n\n## 1. 类与对象\n\n**定义类**\n\n```tex\n[修饰符] class 类名 {\n\t类体\n}\n```\n\n1) scala语法中，类并不声明为public，**所有这些类都具有公有可见性**(即默认就是public),[修饰符在后面再详解].\n2) 一个Scala源文件可以包含多个类.\n\n**属性/成员变量**\n\n1) 属性的定义语法同变量，示例：`[访问修饰符] var 属性名称 [：类型] = 属性值`\n2) 属性的定义类型可以为任意类型，包含值类型或引用类型\n3) Scala中声明一个属性，**必须显示的初始化**，然后根据初始化数据的类型自动推断，属性类型可以省略(这点和Java不同)。\n4) 如果赋值为null，则一定要加类型，因为不加类型, 那么该属性的类型就是Null类型\n\n```scala\nclass Person {\n    var age : Int = 10\n    var sal = 8090.9  //给属性赋初值，省略类型，会自动推导\n    var Name  = null  // Name 是Null类型\n    var address : String = null // address 是String类型\n}\n```\n\n5) 如果在定义属性时，如果用var修饰，则可暂时不赋值，也可以使用符号`_`下划线)，让系统分配默认值。**val类型的成员变量，必须要自己手动初始化，不能使用符号`_`(下划线)**\n\n```scala\nclass Person{\n    var age:Int=_\n    var name:String=_\n}\nclass Person1(){\n  val name: String =\"小明\"//必须手动初始化\n  val age: Int =10\n}\n```\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323164729.png)\n\n*java中的boolean初始值也是false*\n\n6) 不同对象的属性是独立，互不影响，一个对象对属性的更改，不影响另外一个。\n\n属性的高级部分和构造器(构造方法/函数)相关，我们把属性高级部分放到构造器那里讲解。\n\n\n\n**如何创建对象**\n\n```tex\nval | var 对象名 [：类型]  = new 类型()\n```\n\n1) 如果我们不希望改变对象的引用(即：内存地址), 应该声明为val 性质的，否则声明为var, **scala设计者推荐使用val** ,因为一般来说，在程序中，我们只是改变对象属性的值，而不是改变对象的引用。\n2) scala在声明对象变量时，可以根据创建对象的类型自动推断，所以类型声明可以省略，**但当类型和后面new对象类型有继承关系即多态时**，就必须写了\n\n\n\n**访问属性**：对象名.属性名;\n\n\n\n**方法**\n\nScala中的方法其实就是函数，声明规则请参考函数式编程中的函数声明。\n\n```tex\ndef 方法名(参数列表) [：返回值类型] = {\n\t方法体\n}\n```\n\n1)  当我们scala开始执行时，先在栈区开辟一个main栈。main栈是最后被销毁\n2)  当scala程序在执行到一个方法时，总会开一个新的栈。\n3)  每个栈是独立的空间，变量（基本数据类型）是独立的，相互不影响（引用类型除外）\n4)  当方法执行完毕后，该方法开辟的栈就会被jvm机回收。\n\n\n\n## 2. 构造器\n\nJava构造器基本语法\n\n```tex\n[修饰符] 方法名(参数列表){\n\t构造方法体\n}\n```\n\n在Java中一个类可以定义多个不同的构造方法，构造方法重载\n\n- 如果程序员没有定义构造方法，系统会自动给类生成一个默认无参构造方法(也叫默认构造器)，比如 `Person (){}`\n- 一旦定义了自己的构造方法（构造器）,默认的构造方法就覆盖了，就不能再使用默认的无参构造方法，除非显示的定义一下,即:  `Person(){};`\n\n\n\n**Scala构造器**\n\n和Java一样，Scala构造对象也需要调用构造方法，并且可以有任意多个构造方法（即scala中构造器也支持重载）。\nScala类的构造器包括： 主构造器 和 辅助构造器\n\n基本语法\n\n```tex\nclass 类名(形参列表) {  // 主构造器\n\t// 类体\ndef  this(形参列表) {  // 辅助构造器\n }\ndef  this(形参列表) {  //辅助构造器可以有多个...\n }\n}\n```\n\n辅助构造器可以有多个，编译器通过不同参数来区分\n\n**Scala构造器注意事项和细节**\n\n1) Scala构造器作用是完成对新对象的初始化，构造器没有返回值。\n2) 主构造器的声明直接放置于类名之后 [反编译]\n3) 主构造器会执行类定义中的所有语句，这里可以体会到Scala的函数式编程和面向对象编程融合在一起，即：**构造器也是方法**（函数），传递参数和使用方法和前面的函数部分内容没有区别\n\n```scala\ndef main(args: Array[String]): Unit = {\n    val ab=new AB //会输出ABABAB\n}\n\nclass AB{\n    var cd=3\n    if(cd==3)//Scala的类中可以写任意代码\n    \tprintln(\"ABABAB\")\n}\n```\n\n4) 如果主构造器无参数，小括号可省略，构建对象时调用的构造方法的小括号也可以省略\n\n```scala\nclass AA{\n\n}\nvar a1=new AA\nvar a2=new AA()\n```\n\n5) 辅助构造器名称为this（这个和Java是不一样的），多个辅助构造器通过不同参数列表进行区分， **在底层就是构造器重载**。\n\n```scala\ndef main(args: Array[String]): Unit = {\n    val p=new Person(\"小明\")\n}\n\nclass Person() {\n    var name: String = _\n    var age: Int = _\n\n    def this(name: String) {\n        //辅助构造器无论是直接或间接，最终都一定要调用主构造器，执行主构造器的逻辑,而且需要放在辅助构造器的第一行\n        this() //直接调用主构造器\n        this.name = name\n    }\n\n    def this(name: String, age: Int) {\n        this() //直接调用主构造器\n        this.name = name\n        this.age = age\n    }\n\n    def this(age: Int) {\n        this(\"匿名\") //间接调用主构造器,因为 def this(name : String) 中调用了主构造器!\n        this.age = age\n    }\n}\n```\n\n6) 如果想让主构造器变成私有的，可以在()之前加上private，这样用户只能通过辅助构造器来构造对象了【反编译查看】\n\n```scala\nobject Test6 {\n  def main(args: Array[String]): Unit = {\n    val p=new Person(\"wxl\")\n  }\n}\n\nclass Person private(){\n  var name: String = _\n  var age: Int = _\n\n  def this(name: String) {\n    this() \n    this.name = name\n  }\n}\n```\n\n7) **辅助构造器的声明不能和主构造器的声明一致**，会发生错误(即构造器名重复)\n\n\n\n## 3. 属性高级\n\n\n\n（1）Scala类的主构造器的形参未用任何修饰符修饰，那么这个参数是局部变量\n\nscala示例：\n\n```scala\n////1. 如果主构造器是Worker(inName:String)，那么abc这个类的一个局部变量\nclass Worker1(abc: String) {a\n  var name = abc\n  var age:Int=_\n\n  def this(inName: String, age: Int) {\n    this(inName)\n    var t=abc\n    this.age = age\n  }\n```\n\nscala反编译如下，可以看到，abc这个主构造器的参数没有类似于get的方法，**因此外界不能访问**\n\n```java\npublic class Worker1 {\n  private final String abc;\n  \n  private String name;\n  \n  private int age;\n  \n  public Worker1(String abc) {\n    this.name = abc;\n  }\n  \n  public String name() {\n    return this.name;\n  }\n  \n  public void name_$eq(String x$1) {\n    this.name = x$1;\n  }\n  \n  public int age() {\n    return this.age;\n  }\n  \n  public void age_$eq(int x$1) {\n    this.age = x$1;\n  }\n  \n  public Worker1(String inName, int age) {\n    this(inName);\n    String t = this.abc;\n    age_$eq(age);\n  }\n}\n\n```\n\n（2）Scala类的主构造器的形参用var修饰，那么该参数是这个类的成员变量，且提供了get和set功能，外界可读可写\n\n```scala\n//2. 如果主构造器是Worker(var abc:String)，那么abc是这个类的成员函数，提供了get和set功能，外界可读可写\nclass Worker2(var abc: String) {\n  var name = abc\n  var age:Int=_\n\n  def this(inName: String, age: Int) {\n    this(inName)\n    var t=abc\n    this.age = age\n  }\n}\n```\n\nscala反编译为java\n\n```java\npublic class Worker2 {\n  private String abc;\n  \n  public String abc() {\n    return this.abc;\n  }\n  \n  public void abc_$eq(String x$1) {\n    this.abc = x$1;\n  }\n  \n  private String name = abc();\n  \n  private int age;\n  \n  public String name() {\n    return this.name;\n  }\n  \n  public void name_$eq(String x$1) {\n    this.name = x$1;\n  }\n  \n  public int age() {\n    return this.age;\n  }\n  \n  public void age_$eq(int x$1) {\n    this.age = x$1;\n  }\n  \n  public Worker2(String abc) {}\n  \n  public Worker2(String inName, int age) {\n    this(inName);\n    String t = abc();\n    age_$eq(age);\n  }\n}\n```\n\n（3）Scala类的主构造器的形参用val修饰，那么该参数是这个类的成员变量，且只提供了get，外界只能读\n\n```scala\n//3. 如果主构造器是Worker(val abc:String)，那么abc是这个类的成员函数，提供了set功能，外界只可读\nclass Worker3(val abc: String) {\n  var name = abc\n  var age:Int=_\n\n  def this(inName: String, age: Int) {\n    this(inName)\n    var t=abc\n    this.age = age\n  }\n}\n```\n\nscala反编译为java\n\n```java\npublic class Worker3 {\n  private final String abc;\n  \n  private String name;\n  \n  private int age;\n  \n  public String abc() {\n    return this.abc;\n  }\n  \n  public Worker3(String abc) {\n    this.name = abc;\n  }\n  \n  public String name() {\n    return this.name;\n  }\n  \n  public void name_$eq(String x$1) {\n    this.name = x$1;\n  }\n  \n  public int age() {\n    return this.age;\n  }\n  \n  public void age_$eq(int x$1) {\n    this.age = x$1;\n  }\n  \n  public Worker3(String inName, int age) {\n    this(inName);\n    String t = abc();\n    age_$eq(age);\n  }\n}\n```\n\n\n\n\n\n(4）如果类的成员变量使用val关键字声明，则该属性必须手动初始化，不能用下划线初始化 ，并且类中默认只提供了get方法，所以只能读不能写【案例+反编译】\n\n```scala\nclass Person1(){\n  val name: String =\"小明\"\n  val age: Int =10\n}\n```\n\nscala反编译成java代码，可以看到，没有提供set方法\n\n```java\npublic class Person1 {\n  private final String name = \";\n  \n  public String name() {\n    return this.name;\n  }\n  \n  private final int age = 10;\n  \n  public int age() {\n    return this.age;\n  }\n}\n```\n\n\n\n(5) 如果类的成员变量使用var关键字声明，则该可以用下划线初始化，并且类中默认提供了get和set方法，所以可读可写\n\nscala语言\n\n```scala\n  class Person() {\n    var name: String = _\n    var age: Int = _\n  }\n```\n\nscala反编译为java\n\n```scala\npublic class Person {\n  private String name;\n  \n  private int age;\n  \n  public String name() {\n    return this.name;\n  }\n  \n  public void name_$eq(String x$1) {\n    this.name = x$1;\n  }\n  \n  public int age() {\n    return this.age;\n  }\n  \n  public void age_$eq(int x$1) {\n    this.age = x$1;\n  }\n```\n\n\n\n（6)JavaBeans规范定义了Java的属性是像getXxx（）和setXxx（）的方法。许多Java工具（框架）都依赖这个命名习惯。为了Java的互操作性。将Scala字段加`@BeanProperty`时，这样会自动生成规范的 setXxx/getXxx 方法。这时可以使用 对象.setXxx() 和 对象.getXxx() 来调用属性。\n\nscala示例\n\n```scala\nclass Worker4(val abc: String) {\n  var name = abc\n  @BeanProperty var age:Int=_\n}\n```\n\n反编译为java\n\n```java\npublic class Worker4 {\n  private final String abc;\n  \n  private String name;\n  \n  private int age;\n  \n  public String abc() {\n    return this.abc;\n  }\n  \n  public Worker4(String abc) {\n    this.name = abc;\n  }\n  \n  public String name() {\n    return this.name;\n  }\n  \n  public void name_$eq(String x$1) {\n    this.name = x$1;\n  }\n  \n  public int age() {\n    return this.age;\n  }\n  \n  public void age_$eq(int x$1) {\n    this.age = x$1;\n  }\n  \n  public int getAge() {\n    return age();\n  }\n  \n  public void setAge(int x$1) {\n    age_$eq(x$1);\n  }\n}\n\n```\n\n\n\n## 4. 对象创建的流程\n\n```scala\nclass Person {\nvar age: Short = 90\nvar name: String = _\ndef this(n: String, a: Int) {\nthis()\nthis.name = n\nthis.age = a\n}}\nvar p : Person = new Person(\"小倩\",20)\n```\n\n流程分析(面试题-写出)\n1) 加载类的信息(属性信息，方法信息)\n2) 在内存中(堆)开辟空间\n3) 使用父类的构造器(主和辅助)进行初始\n4) 使用主构造器对属性进行初始化 【age:90, naem nul】\n5) 使用辅助构造器对属性进行初始化 【 age:20, naem 小倩 】\n6) 将开辟的对象的地址赋给 p这个引用\n\n\n\n\n\n\n\n## 5. 包\n\n### 5.1 Scala包的特点概述\n\nJava包的三大作用\n1) 区分相同名字的类\n2) 当类很多时,可以很好的管理类\n3) 控制访问范围\n\nJava打包语法：如 `package com.atguigu;`\n\nJava如何引入包：如 `import java.awt.*;`\n\n**Scala包的基本介绍**\n\n和Java一样，Scala中管理项目可以使用包，但Scala中的包的功能更加强大，使用也相对复杂些，下面我们学习Scala包的使用和注意事项。\n\n```scala\npackage com.atguigu.chapter02.xh\nclass Cat {\n}\npackage com.atguigu.chapter02.xm\nclass Cat {\n}\n//使用不同包下相同的类\nvar cat1 = new com.atguigu.chapter02.xh.Cat()\nprintln(\"cat1\" + cat1)\nvar cat2 = new com.atguigu.chapter02.xm.Cat()\nprintln(\"cat2\" + cat2)\n```\n\n\n\n基本语法：`package 包名`，和java相同\n\nScala包的三大作用(和Java一样)\n1) 区分相同名字的类\n2) 当类很多时,可以很好的管理类\n3) 控制访问范围\n\n\n\n**命名规范**：`com.公司名.项目名.业务模块名`\n\n比如：\n\n```scala\ncom.atguigu.oa.model  \ncom.atguigu.oa.controller\ncom.sina.edu.user\ncom.sohu.bank.order \n```\n\n\n\n**Scala会自动引入的常用包**（不需要自己再手动引入）\n\n```tex\njava.lang.* （这个java也会自动引入）\nscala._ ，如scala.List就不用手动引入就可以使用，但是其子包不能使用，比如scala.io.*里面的内容就不能使用\nscala.Predef._ (一般很多的隐式转换都在该包下)，如Predef.println等等常用方法可以使用\n```\n\n\n\n**Scala中包名和源码所在的文件目录可以不一致**，但是编译后的字节码文件（.class文件）路径和包名会保持一致(这个工作由编译器完成)。\n\n\n\n### 5.2 Scala包注意事项和使用细节\n\n1）scala进行package 打包时，可以有如下形式。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323212429.png)\n\n**上面三种打包方式完全等价**\n\n2) 包也可以像嵌套类那样嵌套使用（包中有包）, 这个在前面的第三种打包方式已经讲过了，在使用第三种方式时的好处是：程序员可以在同一个文件中，将类(class / object)、trait 创建在不同的包中，这样就非常灵活了。\n\n3) 作用域原则：可以直接向上访问。即: **Scala中子包中直接访问父包中的内容**, 大括号体现作用域。\n(提示：Java中子包使用父包的类，需要import)。在子包和父包 类重名时，默认采用就近原则，如果希望指定使用某个类，则带上包名即可。\n\n```scala\npackage com.atguigu {\n\n//这个类就是在com.atguigu包下\nclass User {\n}\n\n//这个类也在com.atguigu包下\nobject Monster {\n}\n\nclass Dog {\n}\npackage scala {\n\n//这个类就是在com.atguigu.scala包下\nclass User {\n}\n\n//这个Test 类对象\nobject Test {\n  def main(args: Array[String]): Unit = {\n    //子类可以直接访问父类的内容\n    var dog = new Dog()\n    println(\"dog=\" + dog)\n    //在子包和父包 类重名时，默认采用就近原则.\n    var u = new User()\n    println(\"u=\" + u)\n    //在子包和父包 类重名时，如果希望指定使用某个类，则带上包名即可\n    var u2 = new com.atguigu.User()\n    println(\"u2=\" + u2)\n  }\n}\n```\n\n4) 父包要访问子包的内容时，需要import对应的类等\n\n```scala\npackage com.atguigu {\n  //引入在com.atguigu 包中希望使用到子包的类Tiger ,因此需要引入.\n\n  import com.atguigu.scala.Tiger\n\n  //这个类就是在com.atguigu包下\n  class User {\n  }\n  package scala {\n\n    //Tiger 在 com.atguigu.scala 包中\n    class Tiger {}\n\n  }\n\n  object Test2 {\n    def main(args: Array[String]): Unit = {\n      //如果要在父包使用到子包的类，需要import\n      import com.atguigu.scala.Tiger\n      val tiger = new Tiger()\n      println(\"tiger=\" + tiger)\n    }\n  }\n\n}\n```\n\n5) 可以在同一个.scala文件中，声明多个并列的package(建议嵌套的pakage不要超过3层)\n\n6) 包名可以相对也可以绝对，比如，访问BeanProperty的绝对路径是：`_root_. scala.beans.BeanProperty` ，在一般情况下：我们使用相对路径来引入包，只有当包名冲突时，使用绝对路径来处理。\n\n```scala\nclass Manager(var name: String) {\n  //第一种形式\n  //@BeanProperty var age: Int = _\n  //第二种形式, 和第一种一样，都是相对路径引入\n  //@scala.beans.BeanProperty var age: Int = _\n  //第三种形式, 是绝对路径引入，可以解决包名冲突\n  @_root_.scala.beans.BeanProperty var age: Int = _\n}\n\nobject TestBean {\n  def main(args: Array[String]): Unit = {\n    val m = new Manager(\"jack\")\n    println(\"m=\" + m)\n  }\n}\n```\n\n\n\n\n\n### 5.3 包对象\n\n基本介绍：包可以包含类、对象和特质trait，但不能包含函数/方法或变量的定义。这是Java虚拟机的局限。\n\n```scala\npackage Test {\n\n  package scala {\n    //在包里面直接写方法或者定义变量就报错\n    var name = \"\" //报错\n    def test1 () {//报错\n\n    }\n  }\n}\n```\n\n为了弥补这一点不足，scala提供了包对象的概念来解决这个问题。\n\n```scala\npackage com.atguigu {\n\n  //每个包都可以有一个包对象。你需要在父包(com.atguigu)中定义它,\n  package object scala {\n    var name = \"jack\"\n\n    def sayOk(): Unit = {\n      println(\"package object sayOk!\")\n    }\n  }\n   \n  //这个要和包对象名一致\n  package scala {\n\n\n    object TestObj {\n      def main(args: Array[String]): Unit = {\n\n        //可以使用包对象中的变量和方法\n        println(\"name=\" + name)\n        sayOk() //这个sayOk 就是包对象scala中声明的sayOk\n      }\n    }\n\n  }\n\n}\n```\n\n包对象的注意事项\n\n\n\n1) 每个包都可以有一个包对象，而且只能有一个。你需要在父包中定义它。\n\n2) 在包对象中可以定义变量和方法，在包对象中定义的变量和方法就可以在对应的子包中使用\n\n3) 包对象名称需要和子包名一致，一般用来对包的功能补充\n\n\n\n### 5.4 包的可见性\n\njava提供四种访问控制修饰符号控制方法和变量的访问权限（范围）:\n1) 公开级别:用public 修饰,对外公开\n2) 受保护级别:用protected修饰,对子类和同一个包中的类公开\n3) 默认级别:没有修饰符号,向同一个包的类公开.\n4) 私有级别:用private修饰,只有类本身可以访问,不对外公开.\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323222615.png)\n\nJava访问修饰符使用注意事项\n1) 修饰符可以用来修饰类中的属性，成员方法以及类\n2) 只有默认的和public才能修饰类！，并且遵循上述访问权限的特点。\n\n**Scala中包的可见性**\n\n在Java中，访问权限分为: public，private，protected和默认。在Scala中，你可以通过类似的修饰符达到同样的效果。但是使用上有区别。\n\n```scala\npackage testA\n\nobject Testvisit {\n  def main(args: Array[String]): Unit = {\n    val  c = new Clerk()\n    c.showInfo()\n    Clerk.test(c)\n    //c.sal不能访问\n    //Clerk.sal不能访问\n  }}\nclass Clerk {\n  var name : String = \"jack\"\n  private var sal : Double = 9999.9\n  def showInfo(): Unit = {\n    println(\" name \" + name + \" sal= \" + sal)\n  }}\nobject Clerk {\n  def test(c: Clerk): Unit = {\n    //这里体现出在伴生对象中，可以访问c.sal\n    println(\"test() name=\" + c.name + \" sal= \" + c.sal)\n  }\n}\n```\n\n\n\n1) 当属性访问权限为默认时，从底层看属性是private的，但是因为提供了xxx_$eq()[类似setter]/xxx()[类似getter] 方法，因此从使用效果看是任何地方都可以访问)\n\n2) 当方法访问权限为默认时，默认为public访问权限\n\n3) private为私有权限，只在类的内部和伴生对象中可用\n\n4) protected为受保护权限，scala中受保护权限比Java中更严格，只能子类访问，同包无法访问 (编译器)\n\n5) 在scala中没有public关键字,即不能用public显式的修饰属性和方法。\n\n6) 包访问权限（表示属性和方法有了限制。**同时包也有了限制**），这点和Java不一样，体现出Scala包使用的灵活性。\n\n下面的实例，相当于在private的基础上，扩大了访问权限\n\n```scala\npackage testA.layne\n\nobject Testvisit1 {\n  def main(args: Array[String]): Unit = {\n    val  c = new Clerk()\n    //c.showInfo()加了private，方法就不能访问\n    val sal=c.sal\n    println(sal) //输出9999.9\n\n  }}\nclass Clerk {\n  var name : String = \"jack\"\n  //这里我们增加了一个包的访问权限，即在当前layne包下，sal属性可以访问\n  private[layne] var sal : Double = 9999.9\n  private def showInfo(): Unit = {\n    println(\" name \" + name + \" sal= \" + sal)\n  }\n}\n```\n\n\n\n### 5.5 包的引入\n\nScala引入包也是使用import, 基本的原理和机制和Java一样，但是Scala中的import功能更加强大，也更灵活。\n\n因为Scala语言源自于Java，所以java.lang包中的类会自动引入到当前环境中，而Scala中的scala包和Predef包的类也会自动引入到当前环境中，即起其下面的类可以直接使用。\n\n如果想要把其他包中的类引入到当前环境中，需要使用import\n\n**Scala引入包的细节和注意事项**\n\n1) 在Scala中，import语句可以出现在任何地方，并不仅限于文件顶部，import语句的作用一直延伸到包含该语句的块末尾。这种语法的好处是：在需要时在引入包，缩小import 包的作用范围，提高效率。\n\n```scala\nclass User {\n    import scala.beans.BeanProperty\n    @BeanProperty var  name : String = \"\"\n}\nclass Dog {\n    @BeanProperty var  name : String = \"\" //这里不可以使用上面引入的BeanProperty\n}\n```\n\n2) Java中如果想要导入包中所有的类，可以通过通配符\\*，Scala中采用下 _  \n\n```scala\n//引入scala.io中所有的类\nimport scala.io._\n```\n\n3) 如果不想要某个包中全部的类，而是其中的几个类，可以采用选取器(大括号\n\n```scala\ndef test(): Unit = {\n    import scala.collection.mutable.{HashMap, HashSet}\n    var map = new HashMap()\n    var set = new HashSet()\n}\n```\n\n4) 如果引入的多个包中含有相同的类，那么可以将不需要的类进行重命名进行区分，这个就是重命名。\n\n```scala\nimport java.util.{ HashMap=>JavaHashMap, List}\nimport scala.collection.mutable._\nvar map = new HashMap() // 此时的HashMap指向的是scala中的HashMap\nvar map1 = new JavaHashMap(); // 此时使用的java中hashMap的别名\n```\n\n5) 如果某个冲突的类根本就不会用到，那么这个类可以直接隐藏掉。\n\n```scala\nimport java.util.{ HashMap=>_, _} // 含义为 引入java.util包的所有类，但是忽略HahsMap类.\nvar map = new HashMap() // 此时的HashMap指向的是scala中的HashMap, 而且idea工具，的提示也不会显示java.util的HashMaple \n```\n\n\n\n## 6. 封装、继承、多态\n\n### 6.1 封装\n\n**Scala封装的注意事项和细节**\n\n1) Scala中为了简化代码的开发，当声明属性时，本身就自动提供了对应setter/getter方法，如果属性声明为private的，那么自动生成的setter/getter方法也是private的，如果属性省略访问权限修饰符，那么自动生成的setter/getter方法是public的。\n\n2) 因此我们如果只是对一个属性进行简单的set和get ，只要声明一下该属性(属性使用默认访问修饰符) 不用写专门的getset，默认会创建，访问时，直接对象.变量。这样也是为了保持访问一致性\n\n3) 从形式上看 dog.food 直接访问属性，其实底层仍然是访问的方法,  看一下反编译的代码就明白\n\n4) 有了上面的特性，目前很多新的框架，在进行反射时，也支持对属性的直接反射\n\n### 6.2 继承\n\nJava继承的简单回顾 `class 子类名 extends 父类名 { 类体 }`，子类继承父类的属性和方法\n\n（1）和Java一样，Scala也支持类的单继承\n\nScala继承的基本语法\n\n```tex\nclass 子类名 extends 父类名  { 类体 }\n```\n\n示例：\n\n```scala\nclass Person {\n  var name: String = _\n  var age: Int = _\n\n  def showInfo(): Unit = {\n    println(\"学生信息如下：\")\n    println(\"名字：\" + this.name)\n  }\n}\n\nclass Student extends Person {\n  def studying(): Unit = {\n    println(this.name + \"学习 scala中....\")\n  }\n}\n```\n\n（2）子类继承了所有的属性，只是私有的属性不能直接访问，需要通过公共的方法去访问\n\n```scala\nobject Extends02 {\n  def main(args: Array[String]): Unit =\n\n  val sub = new Sub()\n  sub.sayOk()\n}\n}\n\nclass Base {\n  var n1: Int = 1\n  protected var n2: Int = 2\n  private var n3: Int = 3\n\n  def test100(): Unit = {\n    println(\"base 100\")\n  }\n\n  protected def test200(): Unit = {\n    println(\"base 200\")\n  }\n\n  private def test300(): Unit = {\n    println(\"base 300\")\n  }\n}\n\nclass Sub extends Base {\n  def sayOk(): Unit = {\n    this.n1 = 20\n    this.n2 = 40\n    println(\"范围\" + this.n1 + this.n2)\n  }\n}\n```\n\n（3）scala明确规定，重写一个非抽象方法需要用override修饰符，调用超类的方法使用super关键字\n\n```scala\nclass Person {\n  var name: String = \"tom\"\n\n  def printName() {\n    println(\"Person printName() \" + name)\n  }\n}\n\nclass Emp extends Person {\n  //这里需要显式的使用override\n  override def printName() {\n    println(\"Emp printName() \" + name)\n    super.printName()\n  }\n}\n```\n\n### 6.3 类型检查和转换\n\n要测试某个对象是否属于某个给定的类，可以用isInstanceOf方法。用asInstanceOf方法将引用转换为子类的引用。classOf获取对象的类名。\n\n```scala\npackage testA.layne\n\nobject T3 {\n\n  def main(args: Array[String]): Unit = {\n    // 获取对象类型\n    println(classOf[String])//class java.lang.String\n    println(classOf[Emp])//class testA.layne.Emp\n    val s = \"zhangsan\"\n\n    //这种是Java中反射方式得到类型\n    println(s.getClass.getName)//java.lang.String\n    println(s.isInstanceOf[String])///true\n    //将s显示转换成String\n    println(s.asInstanceOf[String])//zhangsan\n    var p = new Person\n    val e = new Emp\n    p = e //将子类对象赋给父类。\n    p.name = \"layne\"\n    println(e.name)//layne\n    p.asInstanceOf[Emp].printName()//调用子类emp中的方法\n  }\n\n}\n\nclass Person {\n  var name: String = \"tom\"\n\n  def printName() {\n    println(\"Person printName() \" + name)\n  }\n}\n\nclass Emp extends Person {\n  //这里需要显式的使用override\n  override def printName() {\n    println(\"Emp printName() \" + name)\n    super.printName()\n  }\n}\n```\n\n- classOf[String]就如同Java的 String.class 。\n- obj.isInstanceOf[T]就如同Java的obj instanceof T 判断obj是不是T类型。\n- obj.asInstanceOf[T]就如同Java的(T)obj 将obj强转成T类型。\n\n\n\n### 6.4 超类的构造\n\n在Java中，创建子类对象时，子类的构造器总是去调用一个父类的构造器(显式或者隐式调用)。\n\n```java\nclass A {\n  public A () {\n    System.out.println(\"A()\");\n  }\n  public A (String name) {\n    System.out.println(\"A(String name)\" + name);\n  }\n}\n\nclass B extends A {\n  public B () {\n    //这里会隐式调用super(); 就是无参的父类构造器A()\n    System.out.println(\"B()\");\n  }\n  public B (String name) {\n    super (name);\n    System.out.println(\"B(String name)\" + name);\n  }\n}\n```\n\n在Scala中，类有一个主构器和任意数量的辅助构造器，而每个辅助构造器都必须先调用主构造器(也可以是间接调用)，这点在前面我们说过了\n\n```scala\nclass Person {\n  var name = \"zhangsan\"\n  println(\"Person...\")\n}\n\nclass Emp extends Person {\n  println(\"Emp ....\")\n\n  def this(name: String) {\n    this // 必须调用主构造器，这里的主构造器指的是Emp\n    this.name = name\n    println(\"Emp 辅助构造器~\")\n  }\n}\n```\n\nScala中，只有主构造器可以调用父类的构造器。辅助构造器不能直接调用父类的构造器。在Scala的构造器中，你不能调用super(params) \n\n```scala\nclass Person(name: String) { //父类的构造器\n}\nclass Emp (name: String) extends Person(name) {// 将子类参数传递给父类构造器,这种写法√\n    // super(name)  (×) 没有这种语法\n    def  this() {\n    \tsuper(\"abc\") // (×)不能在辅助构造器中调用父类的构造器\n\t}\n}\n```\n\n举一个例子\n\n```scala\npackage testA.layne\n\nobject T4 {\n\n  def main(args: Array[String]): Unit = {\n    val stu:PersonA=new Student(\"Layne\")\n    stu.printName()\n  }\n}\n\nclass PersonA(pname:String) {\n  var name: String = \"tom\"\n  println(\"Welcome to PersonA：\"+pname)\n\n  def printName() {\n    println(\"Person printName() \" + name)\n  }\n}\n\nclass Student(studentname:String) extends PersonA(studentname){\n\n  var sno:Int=20\n\n  println(\"Welcome to Student：\"+studentname)\n\n  override def printName(): Unit ={\n    println(\"Student priceName()\"+name)\n    super.printName()\n  }\n}\n```\n\n输出\n\n```tex\nWelcome to PersonA：Layne\nWelcome to Student：Layne\nStudent priceName()tom\nPerson printName() tom\n```\n\n### 6.5 覆盖属性/方法\n\n在Scala中，子类改写父类的字段，我们称为覆写/重写字段。覆写字段需使用override修饰。\n\n现在，回顾一下java的动态绑定，下面可以看到，对象a都是调用的子类方法\n\n```java\npackage testA.layne;\n\npublic class T5 {\n    public static void main(String[] args) {\n        A a = new B();\n        System.out.println(a.sum());  //40\n        System.out.println(a.sum1()); //30\n    }\n}\n\nclass A {\n    public int i = 10;\n    public int sum() {\n        return getI() + 10;\n    }\n    public int sum1() {\n        return i + 10;\n    }\n    public int getI() {\n        return i;\n    }\n}\nclass B extends A {\n    public int i = 20;\n    public int sum() {\n        return i + 20;\n    }\n    public int getI() {\n        return i;\n    }\n    public int sum1() {\n        return i + 10;\n    }\n}\n```\n\n来看一个scala覆盖字段的例子\n\n```scala\npackage testA.layne\n\nobject T6 {\n\n  def main(args: Array[String]): Unit = {\n    val obj : AA = new BB()\n    val obj2 : BB = new BB()\n    println(obj.age)//20\n    println(obj.age)//20\n  }\n}\n\nclass AA {\n  val age : Int = 10\n}\n\nclass BB extends AA {\n  override val age : Int = 20\n}\n\n```\n\n- def只能重写另一个def(即：方法只能重写另一个方法)\n- val只能重写另一个val 属性 或 重写不带参数的def\n\n```scala\n//判断1\n//代码正确吗?不正确，重写时类型一定要保持一致\nclass AAAA {\n  var name: String = \"\"\n}\nclass BBBB extends AAAA {\n  override  val name: String = \"jj\" //类型没保持一致\n}\n\n//判断2\n//代码正确吗?不正确，用var修饰的变量不能被重写\nclass AAAA {\n  var name: String = \"\"\n}\nclass BBBB extends AAAA {\n  override  var name: String = \"jj\"\n}\n\n//判断3，正确，val变量可以重新一个不带参数的def\nclass A {\ndef sal(): Int = {\n\treturn 10\n}}\nclass B extends A {\noverride val sal : Int = 0 //val变量可以重新一个不带参数的def\n}\n```\n\nvar只能重写另一个抽象的var属性\n\n```scala\nabstract class A03{\n  val age:Int=10\n  var name:String\n}\nclass B03 extends A03{\n  override val age:Int=20\n  var name:String=_\n}\n```\n\n抽象属性：声明未初始化的变量就是抽象的属性,抽象属性在抽象类\n\nvar重写抽象的var属性小结：\n\n1. 一个属性没有初始化，那么这个属性就是抽象属性\n2. 抽象属性在编译成字节码文件时，属性并不会声明，但是会自动生成抽象方法，所以类必须声明为抽象类\n\n上面的代码反编译为java代码\n\n```scala\npublic class ABC {\n  private int a = 10;\n  \n  public int a() {\n    return this.a;\n  }\n  \n  public void a_$eq(int x$1) {\n    this.a = x$1;\n  }\n  \n  private final int b = 20;\n  \n  public int b() {\n    return this.b;\n  }\n    \npublic class B03 extends A03 {\n  private final int age = 20;\n  \n  private String name;\n  \n  public int age() {\n    return this.age;\n  }\n  \n  public String name() {\n    return this.name;\n  }\n  \n  public void name_$eq(String x$1) {\n    this.name = x$1;\n  }\n}\n  \n```\n\n3. 如果是覆写一个父类的抽象属性，那么override 关键字可省略 [原因：父类的抽象属性，生成的是抽象方法，因此就不涉及到方法重写的概念，因此override可省略，当然也可以写override]\n\n\n\n\n\n### 6.6 抽象类\n\n抽象类基本语法\n\n```scala\nabstract class Person() { // 抽象类\n    var name: String // 抽象字段, 没有初始化\n    def printName // 抽象方法, 没有方法体\n}\n```\n\n说明：抽象类的价值更多是在于设计，是设计者设计好后，让子类继承并实现抽象类(即：实现抽象类的抽象方法)\n\nScala抽象类使用的注意事项和细节讨论\n\n1) 抽象类不能被实例\n2) 抽象类不一定要包含abstract方法。也就是说，抽象类可以没有abstract方法\n3) 一旦类包含了抽象方法或者抽象属性，则这个类必须声明为abstract\n4) 抽象方法不能有主体，不允许使用abstract修饰。\n5) 如果一个类继承了抽象类，则它必须实现抽象类的所有抽象方法和抽象属性，除非它自己也声明为abstract类。（java里面只要实现抽象方法就行了）\n6)抽象方法和抽象属性不能使用private、final 来修饰，因为这些关键字都是和重写/实现相违背的。\n7) 抽象类中可以有实现的方法\n8) 子类重写抽象方法不需要override，写上也不会错\n\n**匿名子类**\n\n和Java一样，可以通过包含带有定义或重写的代码块的方式创建一个匿名的子类.\n\n回顾-java匿名子类使用\n\n```java\nabstract class A2 {\n  abstract public void cry();\n}\n\nA2 obj = new A2() {\n  @Override\n  public void cry() {\n    System.out.println(\"okook!\");\n  }\n};\n```\n\nscala匿名子类\n\n```scala\nabstract class Monster {\n  var name: String\n\n  def cry()\n}\n\nvar monster = new Monster {\n  override var name: String = \"牛魔王\"\n\n  override def cry(): Unit = {\n    println(\"牛魔王哼哼叫唤..\")\n  }\n}\n```\n\n**继承层级**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210322223800.png)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210324101314.png)\n\n\n\n\n\n1) 在scala中，所有其他类都是AnyRef的子类，类似Java的Object。\n2) AnyVal和AnyRef都扩展自Any类。Any类是根节点\n3) Any中定义了isInstanceOf、asInstanceOf方法，以及哈希方法等。\n4) Null类型的唯一实例就是null对象。可以将null赋值给任何引用，但不能赋值给值类型的变量。\n5) Nothing类型没有实例，它是任何类型的子类型，可以交给一个变量和函数。它对于泛型结构是有用处的，举例：空列表Nil的类型是List[Nothing]，它是List[T]的子类型，T可以是任何类。\n\n\n\n\n\n\n\n\n\n## 7. 静态概念与伴生对象\n\n回顾下Java的静态概念\n\n```tex\npublic static 返回值类型  方法名(参数列表) {方法体}\n静态属性...\n```\n\nJava中静态方法并不是通过对象调用的，而是通过类对象调用的，所以静态操作并不是面向对象的。\n\n**Scala中静态的概念-伴生对象**\n\nScala语言是完全面向对象(万物皆对象)的语言，所以并没有静态的操作(即在Scala中没有静态的概念)。但是为了能够和Java语言交互(因为Java中有静态概念)，就产生了一种特殊的对象来模拟类对象，我们称之为类的**伴生对象**。这个类的所有静态内容都可以放置在它的伴生对象中声明和调用\n\n**伴生对象的快速入门**\n\n```scala\nclass ScalaPerson {//class ScalaPerson 是伴生类\n\tvar name : String = _\n}\nobject ScalaPerson {//object ScalaPerson 是伴生对象\n\tvar sex : Boolean = true\n}\nprintln(ScalaPerson.sex)//可以直接输出\n```\n\n1) Scala中伴生对象采用object关键字声明，伴生对象中声明的全是 \"静态\"内容，**可以通过伴生对象名称直接调用**。\n2) **伴生对象对应的类称之为伴生类**，伴生对象的名称应该和伴生类名一致。\n3) 伴生对象中的属性和方法都可以**通过伴生对象名**(类名)直接调用访问，伴生类可以直接使用伴生对象中的属性和方法【看第(5)的反编译】\n\n```scala\nobject Test12 {\n\n  def main(args: Array[String]): Unit = {\n\n    println(ScalaPerson1.sex)//伴生对象中的sex通过伴生对象名访问\n    val s=new ScalaPerson1 //name只能通过new出来的对象访问\n    println(s.name)\n      \n    val a=ScalaPerson1//这种方式也是通过伴生对象访问，只不过赋值给了一个变量\n    println(a.sex)\n  }\n}\n\nobject ScalaPerson1 {//object ScalaPerson 是伴生对象\n  var sex : Boolean = true\n}\nclass ScalaPerson1 {//class ScalaPerson 是伴生类\n  var name : String = _\n}\n```\n\n\n\n4) 从语法角度来讲，所谓的伴生对象其实就是类的静态方法和成员的集合\n\n5) 从技术角度来讲，scala还是没有生成静态的内容，**只不过是将伴生对象生成了一个新的类，实现属性和方法的调用**。[反编译看源码]\n\n将（3）中scala反编译为java\n\n```scala\npublic class ScalaPerson1 {\n  private String name;\n  \n  public static void sex_$eq(boolean paramBoolean) {\n    ScalaPerson1$.MODULE$.sex_$eq(paramBoolean);\n  }\n  \n  public static boolean sex() {\n    return ScalaPerson1$.MODULE$.sex();\n  }\n  \n  public String name() {\n    return this.name;\n  }\n  \n  public void name_$eq(String x$1) {\n    this.name = x$1;\n  }\n}\n\npublic final class ScalaPerson1$ {\n  public static ScalaPerson1$ MODULE$;\n  \n  private boolean sex;\n  \n  public boolean sex() {\n    return this.sex;\n  }\n  \n  public void sex_$eq(boolean x$1) {\n    this.sex = x$1;\n  }\n  \n  private ScalaPerson1$() {\n    MODULE$ = this;\n    this.sex = true;\n  }\n}\n```\n\n\n\n6) 从底层原理看，伴生对象实现静态特性是依赖于 public static final MODULE$ 实现的。\n\n7) 如果 class A 独立存在，那么A就是一个类， 如果 object A 独立存在，那么A就是一个\"静态\"性质的对象[即**类对象**], 此时在 object A中声明的属性和方法可以通过 `A.属性` 和 `A.方法` 来实现调用\n\n8）**如果伴生对象和伴生类同时出现**，伴生对象的声明应该和伴生类的声明在同一个源码文件中(如果不在同一\n个文件中会运行错误，会提示类已存在的错误)，但是如果没有伴生类，也就没有所谓的伴生对象了，所以放在哪里就无所谓了。\n\n9) 当一个文件中，存在伴生类和伴生对象时，文件的图标会发生变化\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323200911.png)\n\n\n\n**伴生对象-apply方法**\n\n先来看几个例子：\n\n示例1\n\n```scala\nobject Test15 {\n\n  def main(args: Array[String]): Unit = {\n    val t1 = new TestA\n    t1(\"layne1\")  //会输出 TestA apply method called:layne1\n  }\n}\n\nclass TestA{\n  def apply(param:String){\n    println(\"TestA apply method called:\" + param)\n  }\n}\n```\n\n示例2\n\n```scala\nobject Test16 {\n\n  def main(args: Array[String]): Unit = {\n    TestB(\"layne2\")//会输出 TestB apply method called:layne2\n  }\n}\n\nobject TestB{\n  def apply(param:String){\n    println(\"TestB apply method called:\" + param)\n  }\n}\n\n```\n\n可以看出，apply方法调用：用括号传递给**类对象**或**伴生对象**一个或多个参数时，Scala会在相应的类或对象中查找方法名为apply且参数列表与传入的参数一致的方法，并用传入的参数来**调用该apply方法**，并获取apply方法的返回值。\n\n另外，apply方法也支持重载。\n\n在Scala中，我们把所有类的构造方法以apply方法的形式定义在它的伴生对象当中，这样伴生对象的方法就会自动被调用，调用就会生成类对象。\n\n示例3\n\n```scala\npackage Test\n\nobject Test17 {\n\n  def main(args: Array[String]): Unit = {\n    val mycar=Car(\"tom\")\n    mycar.info()//输出Car name is tom\n  }\n}\n\nclass Car(name:String){\n  def info(){\n    println(\"Car name is \" + name)\n  }\n}\n\nobject Car{\n  def apply(name:String) = new Car(name) //调用伴生类Car的构造方法\n  //def apply(name:String):Car = new Car(name) //这个写法同上\n}\n```\n\nScala之所以用apply方法是为了保持它的对象和函数之间使用的一致性，因为Scala融合了面向对象和函数式两种编程风格，它是一种混合式的编程。\n\n- 面向对象调用：对象.方法\n- 函数式调用：函数(参数)\n\nScala中函数式的调用可以转化成对象调用，同理对象调用也可以被转化成函数的调用。\n\n```scala\n//示例3里面\nval mycar=Car(\"tom\")//函数式调用,前提是定义了apply方法\nmycar.info()//函数式调用转化为 面向对象调用\n\n//在示例1里面\nval t1 = new TestA \nt1(\"layne1\")  //对象调用转化为函数式调用，前提是定义了apply方法\n```\n\n\n\n\n\n## 8 接口&trait（特征）\n\n### 8.1 scala中的接口与trait\n\n**回顾Java接口**\n\n```tex\n//声明接口\ninterface 接口名\n//实现接口\nclass 类名 implements 接口名1，接口2\n```\n\n1) 在Java中, 一个类可以实现多个接口。\n2) 在Java中，接口之间支持多继承\n3) 接口中属性都是常量\n4) 接口中的方法都是抽象的\n\n**Scala接口**\n\n从面向对象来看，接口并不属于面向对象的范畴，Scala是纯面向对象的语言，在Scala中，没有接口。\n\nScala语言中，采用特质trait（特征）来代替接口的概念，也就是说，多个类具有相同的特征（特征）时，就可以将这个特质（特征）独立出来，采用关键字trait声明。 理解trait 等价于(interface + abstract class)\n\n**trait 的声明**\n\n```tex\ntrait 特质名 {\n\ttrait体\n}\n```\n\nSerializable就是scala的一个特质：\n\n```scala\nobject T1 extends Serializable {\n    \n}\n```\n\n在scala中，java中的接口可以当做特质使用。\n\n\n\n### 8.2 trait 的使用\n\n一个类具有某种特质（特征），就意味着这个类满足了这个特质（特征）的所有要素，所以在使用时，也采用了extends关键字，如果有多个特质或存在父类，那么需要采用with关键字连接\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210324103437.png)\n\n可以把特质可以看作是对继承的一种补充，Scala的继承是单继承，也就是一个类最多只能有一个父类，这种单继承的机制可保证类的纯洁性，比c++中的多继承机制简洁。但对子类功能的扩展有一定影响，所以我们认为: Scala引入trait特征，第一可以替代Java的接口,  第二个也是对单继承机制的一种补充。\n\n1) Scala提供了特质（trait），特质可以同时拥有抽象方法和具体方法，一个类可以实现/继承多个特质。\n\n```scala\npackage traitPackage\n\nobject T1 {\n\n  def main(args: Array[String]): Unit = {\n    val a:Trait01=new AA\n    a.sayOK()\n    a.sayHello()\n    val trait01Obj=new Trait01 {\n      override def sayOK(): Unit = {\n        println(\"trait01Obj sayOk\")\n      }\n    }\n    trait01Obj.sayHello()\n    trait01Obj.sayOK()\n  }\n}\n\ntrait Trait01{\n  def sayOK()\n  def sayHello():Unit={\n    println(\"Trait01 sayHello\")\n  }\n}\n\nclass AA extends Trait01{\n  override def sayOK(): Unit ={\n    println(\"AA sayOk\")\n  }\n}\n```\n\n上面的代码反编译为java\n\n```java\npublic interface Trait01 {\n  static void $init$(Trait01 $this) {}\n  \n  default void sayHello() {\n    Predef$.MODULE$.println(\"Trait01 sayHello\");\n  }\n  \n  void sayOK();\n}\n\npublic class AA implements Trait01 {\n  public void sayHello() {\n    Trait01.sayHello$(this);\n  }\n  \n  public AA() {\n    Trait01.$init$(this);\n  }\n  \n  public void sayOK() {\n    Predef$.MODULE$.println(\"AA sayOk\");\n  }\n}\n\n```\n\n2) 特质中没有实现的方法就是抽象方法。类通过extends继承特质，通过with可以继承多个特质\n\n3) 所有的java接口都可以当做Scala特质使用\n\n```scala\ntrait Logger {\n  def log(msg: String)\n}\n\nclass Console extends Logger with Cloneable with Serializable {\n  def log(msg: String) {\n    println(msg)\n  }\n}\n```\n\n4) 和Java中的接口不太一样的是特质中的方法并不一定是抽象的，也可以有非抽象方法(即：实现了的方法)。\n\n```scala\ntrait Operate {\n  def insert( id : Int ): Unit = {\n    println(\"保存数据=\"+id)\n  }\n}\ntrait DB extends Operate {\n  override  def insert( id : Int ): Unit = {\n    print(\"向数据库中\")\n    super .insert(id)\n  }\n}\n\nclass MySQL extends DB {\n}\n```\n\n\n\n### 8.3 带有特质的对象\n\n1) 除了可以在类声明时继承特质以外，**还可以在构建对象时混入特质**，扩展目标类的功能\n2) 此种方式也可以应用于对抽象类功能进行扩展\n3) 动态混入是Scala特有的方式（java没有动态混入），可在不修改类声明/定义的情况下，扩展类的功能，非常的灵活，耦合性低 。\n4) 动态混入可以在不影响原有的继承关系的基础上，给指定的类扩展功能。\n\n```scala\npackage traitPackage\n\nobject T2 {\n\n  def main(args: Array[String]): Unit = {\n    var oracle = new OracleDB with Operate3\n    oracle.insert(999)\n    val mysql = new MySQL3 with Operate3 {\n      override def connect(): Int = {\n        println(\"hello mysql connect\")\n        1+1\n      }\n    }\n    mysql.insert(4)\n    mysql.connect()\n\n    \n  }\n}\n\ntrait Operate3 {\n  def insert(id: Int): Unit = {\n    println(\"插入数据 = \" + id)\n  }\n}\n\nclass OracleDB {\n}\n\nabstract class MySQL3 {\n  def connect():Int\n}\n```\n\n**在Scala中创建对象共有几种方式**\n\n1. new 对象\n2. apply 创建\n3. 匿名子类方式\n4. 动态混入\n\n\n\n### 8.4 叠加特质\n\n> 叠加特指是本质上也是动态混入\n\n构建对象的同时如果混入多个特质，称之为叠加特质，仔细看下面的代码注释，理解叠加特质的含义\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210324121243.png)\n\n```scala\npackage traitPackage\n\nobject T3 {\n\n  def main(args: Array[String]): Unit = {\n    // 1.Scala在叠加特质的时候，从左到右叠加，即对于mysql1来说先DB4，在File4\n    val mysql1 = new MySQL4 with DB4 with File4\n    /*\n    * mysql这里输出\n    * Operate4...\n      Data4\n      DB4\n      File4\n    * */\n    //val mysql2 = new MySQL4 with File4 with DB4\n    /*\n    * mysql2这里输出\n    * Operate4...\n      Data4\n      File4\n      DB4\n    * */\n\n    //在动态混入时，果调用super，并不是表示调用父特质的方法，而是向前面（左边）继续查找特质，如果找不到，才会去父特质查找\n    //下面输出： (File4)向文件(DB4)向数据库(Data4)插入数据 = 888\n    mysql1.insert (888)//最先执行File4里面，在调用super是DB4里面的\n  }\n}\n\ntrait Operate4 {\n  println(\"Operate4...\")\n\n  def insert(id: Int)\n}\n\ntrait Data4 extends Operate4 {\n  println(\"Data4\")\n\n  override def insert(id: Int): Unit = {\n    println(\"(Data4)插入数据 = \" + id)\n  }\n}\n\ntrait DB4 extends Data4 {\n  println(\"DB4\")\n\n  override def insert(id: Int): Unit = {\n    print(\"(DB4)向数据库\")\n    super.insert(id)\n  }\n}\n\ntrait File4 extends Data4 {\n  println(\"File4\")\n\n  override def insert(id: Int): Unit = {\n    print(\"(File4)向文件\")\n    super.insert(id)\n  }\n}\n\nclass MySQL4 {}\n\n```\n\n在特质中重写抽象方法特例\n\n看下面的代码\n\n```scala\ntrait Operate5 {\n  def insert(id : Int)\n}\ntrait File5 extends Operate5 {\n  def insert( id : Int ): Unit = {\n    println(\"将数据保存到文件中..\")\n    super.insert(id)\n  }\n}\n```\n\n运行代码，编译无法通过，报错信息为：\n\n```tex\nmethod insert in trait Operate5 is accessed from super. It may not be abstract unless it is overridden by a member declared `abstract' and `override'\n \tsuper.insert(id)\n```\n\n因为我们在特质File5中调用super的insert方法，而Operate5的该方法却没有具体实现，因此报错\n\n**解决方法**\n\n方式1 : 去掉 super()...\n\n方式2: 声明 abstract override，编译通过\n\n```scala\ntrait Operate5 {\n  def insert(id : Int)\n}\ntrait File5 extends Operate5 {\n  abstract override def insert( id : Int ): Unit = {\n    println(\"将数据保存到文件中..\")\n    super.insert(id)\n  }\n}\n```\n\n理解 abstract override 的小技巧分享：\n可以这里理解，当我们给某个方法增加了`abstract override` 后，就是明确的告诉编译器，该方法确实是重写了父特质的抽象方法，但是重写后，该方法仍然是一个抽象方法（因为没有完全的实现，需要其它特质继续实现[**通过混入顺序**]）\n\n```scala\npackage traitPackage\n\nobject T4 {\n\n  def main(args: Array[String]): Unit = {\n\n\n    val mysql5 = new MySQL5 with DB5  with File5\n    mysql5.insert(123)\n  }\n}\n\ntrait Operate5 {\n  def insert(id : Int)\n}\ntrait File5 extends Operate5 {\n  abstract override def insert( id : Int ): Unit = {\n    println(\"(File5)将数据保存到文件中..\")\n    super.insert(id)\n  }\n}\ntrait DB5 extends  Operate5 {\n  def insert( id : Int ): Unit = {\n    println(\"(DB5)将数据保存到数据库中..\")\n  }\n}\nclass MySQL5 {}\n```\n\n输出：\n\n```tex\n(File5)将数据保存到文件中..\n(DB5)将数据保存到数据库中..\n```\n\n结合上面的代码，看4个案例\n\n```scala\nvar mysql2 = new MySQL5  with DB5 //ok\nmysql2.insert(100)\nvar mysql3 = new MySQL5  with File5 //error\nmysql2.insert(100)\nvar mysql4 = new MySQL5 with File5 with DB5// error\nmysql4.insert(100)\nvar mysql4 = new MySQL5 with DB5 with File5// ok\nmysql4.insert(100)\n```\n\n\n\n\n\n### 8.5 特质中的字段和方法\n\n富接口：即该特质中既有抽象方法，又有非抽象方法\n\n```scala\ntrait Operate {\n    def insert( id : Int ) //抽象\n    def pageQuery(pageno:Int, pagesize:Int): Unit = { //实现\n    \tprintln(\"分页查询\")\n    }\n}\n```\n\n**特质中的具体字段**\n\n特质中可以定义具体字段，如果初始化了就是具体字段，如果不初始化就是抽象字段。混入该特质的类就具有了该字段，字段不是继承，而是直接加入类，成为自己的字段。\n\n**特质中的抽象字段**\n\n特质中未被初始化的字段在具体的子类中必须被重写。\n\n\n\n### 8.6 特质构造顺序\n\n特质也是有构造器的，构造器中的内容由“字段的初始化”和一些其他语句构成。具体实现请参考“特质叠加”。\n\n第一种特质构造顺序(声明类的同时混入特质)\n1) 调用当前类的超类构造器\n2) 第一个特质的父特质构造器\n3) 第一个特质构造器\n4) 第二个特质构造器的父特质构造器, 如果已经执行过，就不再执行\n5) 第二个特质构造器\n6) .......重复4,5的步骤(如果有第3个，第4个特质)\n7) 当前类构造器 \n\n```scala\npackage traitPackage\n\nobject T5 {\n\n  def main(args: Array[String]): Unit = {\n    val ff1 = new FF()\n    println(ff1)\n    val ff2 = new KK()\n    println(ff2)\n  }\n}\n\ntrait AA {\n  println(\"A...\")\n}\n\ntrait BB extends AA {\n  println(\"B....\")\n}\n\ntrait CC extends BB {\n  println(\"C....\")\n}\n\ntrait DD extends BB {\n  println(\"D....\")\n}\n\nclass EE {\n  println(\"E...\")\n}\n\nclass FF extends EE with CC with DD {\n  println(\"F....\")\n}\n\nclass KK extends EE {\n  println(\"K....\")\n}\n```\n\n输出：\n\n```scala\nE...\nA...\nB....\nC....\nD....\nF....\ntraitPackage.FF@1b40d5f0\nE...\nK....\ntraitPackage.KK@ea4a92b\n```\n\n\n\n第2种特质构造顺序(在构建对象时，动态混入特质)\n1) 调用当前类的超类构造器\n2) 当前类构造器\n3) 第一个特质构造器的父特质构造器\n4) 第一个特质构造器.\n5) 第二个特质构造器的父特质构造器, 如果已经执行过，就不再执行\n6) 第二个特质构造器\n7) .......重复5,6的步骤(如果有第3个，第4个特质)\n8) 当前类构造器\n\n```scala\npackage traitPackage\n\nobject T6 {\n\n  def main(args: Array[String]): Unit = {\n    val  f=new MB with F3 with F2\n    \n  }\n}\n\ntrait  F1{\n  println(\"F1...\")\n}\n\ntrait  F2 extends F1{\n  println(\"F2...\")\n}\n\ntrait  F3 extends F1{\n  println(\"F3...\")\n}\n\nclass MA{\n  println(\"MA...\")\n}\n\nclass MB extends MA{\n  println(\"MB....\")\n}\n```\n\n输出\n\n```tex\nMA...\nMB....\nF1...\nF3...\nF2...\n```\n\n分析两种方式对构造顺序的影响\n第1种方式实际是构建类对象, 在混入特质时，该对象还没有创建。\n第2种方式实际是构造匿名子类，可以理解成在混入特质时，对象已经创建了\n\n\n\n### 8.7 扩展类的特质\n\n特质可以继承类，以用来拓展该类的一些功能\n\n```scala\ntrait LoggedException extends Exception{\n    def log(): Unit ={\n        println(getMessage()) // 方法来自于Exception类\n    }\n}\n```\n\n所有混入该特质的类，会自动成为那个特质所继承的超类的子类\n\n```scala\ntrait LoggedException extends Exception {\n  def log(): Unit = {\n    println(getMessage()) // 方法来自于Exception类\n  }\n}\n\n//因为LoggedException继承了Exception\n//而UnhappyException继承了LoggedException\n//所以UnhappyException是Excpetion的子类\nclass UnhappyException extends LoggedException {\n  // 已经是Exception的子类了，所以可以重写方法\n  override def getMessage = \"错误消息！\"\n}\n```\n\n如果混入该特质的类，已经继承了另一个类(A类)，则要求A类是特质超类的子类，否则就会出现了多继承现象，发生错误。\n\n```scala\npackage  traitPackage\n\nobject T7{\n\n  def main(args: Array[String]): Unit = {\n  }\n}\n\ntrait LoggedException extends Exception {\n  def log(): Unit = {\n    println(getMessage()) // 方法来自于Exception类\n  }\n}\n\nclass UnhappyException extends LoggedException {\n  // 已经是Exception的子类了，所以可以重写方法\n  override def getMessage = \"错误消息！\"\n}\n\n//正确：因为IndexOutOfBoundsException是LoggedException特质的超累Exception的子类\nclass UnhappyException2 extends IndexOutOfBoundsException with LoggedException{\n  override def getMessage = \"错误消息！\"\n}\n\nclass CCC{\n}\n\n//错误：因为CCC不是LoggedException特质的超累Exception的子类\nclass UnhappyException3 extends CCC with LoggedException{\n\n}\n\n```\n\n这个错误信息如下：\n\n```tex\nillegal inheritance; superclass CCC\n is not a subclass of the superclass Exception\n of the mixin trait LoggedException\nclass UnhappyException3 extends CCC with LoggedException{\n```\n\n当然，如果该特质没有超类，则就没有这个限制了。即如果LoggedException没有超类，则对CCC没有任何限制了\n\n```scala\nclass CCC{\n}\n\ntrait  AAA{\n\n}\ntrait  BBB{\n  \n}\nclass  UnhappyException4 extends CCC with AAA with BBB{\n\n}\n```\n\n对于上面这个例子来说，要么CCC、AAA、BBB有相同的父类，要么没有父类\n\n\n\n### 8.8 自身类型(selftype)\n\n自身类型：主要是为了解决特质的循环依赖问题，同时可以确保特质在不扩展某个类的情况下，依然可以做到限制混入该特质的类的类型。\n\n所谓的循环依赖是指：两个类互相依赖，互相是对象父类或子类\n\n```scala\n//Logger就是自身类型特质(selftype)，当这里做了自身类型后，那么就\n//相当于 trait Logger extends Exception，要求混入该特质的类也是Exception子类\ntrait Logger {\n  // 明确告诉编译器，我就是Exception,如果没有这句话，下面的getMessage不能调用\n  this: Exception =>\n  def log(): Unit ={\n    // 既然我就是Exception, 那么就可以调用其中的方法\n    println(getMessage)\n  }\n}\nclass Console extends  Logger {} //对吗? 不对，Logger这个自身类型特质(selftype)，要求混入该特质的类也是Exception子类\nclass Console extends Exception with Logger//对吗?正确，先继承了Eception,再混入该特质\n```\n\n\n\n## 2.9 嵌套类\n\n### 9.1 嵌套类的使用\n\n在Scala中，你几乎可以在任何语法结构中内嵌任何语法结构。如在类中可以再定义一个类，这样的类是嵌套类，其他语法结构也是一样。嵌套类类似于Java中的内部类。\n\n**Java内部类的分类**\n\n从定义在外部类的成员位置上来看，\n1) 成员内部类（没用static修饰）\n2) 和静态内部类（使用static修饰），\n定义在外部类局部位置上（比如方法内）来看：\n1) 分为局部内部类（有类名）\n2) 匿名内部类（没有类名）\n\n这里我们就回顾一下成员内部类和静态内部类。\n\n![img](https://gitee.com/wxler/blogimg/raw/master/imgs/20210324141129.png)\n\n\n\n**Scala嵌套类的使用1**\n\n请编写程序，定义Scala 的成员内部类和静态内部类，并创建相应的对象实例。\n\n```scala\nclass ScalaOuterClass {\n  class ScalaInnerClass { //成员内部类\n  }\n}\nobject ScalaOuterClass {  //伴生对象\n  class ScalaStaticInnerClass { //静态内部类\n  }\n}\nval outer1 : ScalaOuterClass = new ScalaOuterClass();\nval outer2 : ScalaOuterClass = new ScalaOuterClass();\n// Scala创建内部类的方式和Java不一样，将new关键字放置在前，使用  对象.内部类  的方式创建\nval inner1 = new outer1.ScalaInnerClass()\nval inner2 = new outer2.ScalaInnerClass()\n//创建静态内部类对象\nval staticInner = new\n    ScalaOuterClass.ScalaStaticInnerClass()\nprintln(staticInner)\n```\n\n**Scala嵌套类的使用2**\n\n请编写程序，在内部类中访问外部类的属性\n\n方式1\n内部类如果想要访问外部类的属性，可以通过外部类对象访问。即：访问方式：外部类名.this.属性名\n\n```scala\nclass ScalaOuterClass {\n  var name: String = \"scott\"\n  private var sal: Double = 1.2\n\n  class ScalaInnerClass { //成员内部类\n    def info() = {\n      // 访问方式：外部类名.this.属性名\n      // 怎么理解 ScalaOuterClass.this 就相当于是 ScalaOuterClass 这个外部类的一个实例,\n      // 然后通过 ScalaOuterClass.this 实例对象去访问 name 属性\n      // 只是这种写法比较特别，学习java的同学可能更容易理解 ScalaOuterClass.class 的写法.\n      println(\"name = \" + ScalaOuterClass.this.name\n        + \" age =\" + ScalaOuterClass.this.sal)\n    }\n  }\n\n}\n\nobject ScalaOuterClass { //伴生对象\n  class ScalaStaticInnerClass { //静态内部类\n  }\n\n}\n\n```\n\n**方式2**\n\n内部类如果想要访问外部类的属性，也可以通过外部类别名访问(推荐)。即：访问方式：外部类名别名.属性名   【外部类名.this  等价 外部类名别名】\n\n```scala\nclass ScalaOuterClass {\n  myOuter => //这样写，你可以理解成这样写，myOuter就是代表外部类的一个对象.\n  class ScalaInnerClass { //成员内部类\n    def info() = {\n      println(\"name = \" + ScalaOuterClass.this.name\n        + \" age =\" + ScalaOuterClass.this.sal)\n      println(\"-----------------------------------\")\n      println(\"name = \" + myOuter.name\n        + \" age =\" + myOuter.sal)\n    }\n  }\n  // 当给外部指定别名时，需要将外部类的属性放到别名后.\n  var name: String = \"scott\"\n  private var sal: Double = 1.2\n}\n```\n\n\n\n### 9.2 类型投影\n\n先看一段代码，引出类型投影\n\n```scala\npackage traitPackage\n\nobject T10 {\n\n  def main(args: Array[String]): Unit = {\n    val outer1: ScalaOuterClass3 = new ScalaOuterClass3();\n    val outer2: ScalaOuterClass3 = new ScalaOuterClass3();\n    val inner1 = new outer1.ScalaInnerClass3()\n    val inner2 = new outer2.ScalaInnerClass3()\n    inner1.test(inner1) // ok, 因为 需要outer1.ScalaInner\n    inner2.test(inner2) // ok, 因为 需要outer2.ScalaInner\n    //在默认情况下，scala的内部类的实例和创建该内部类实例的外部对象关联\n    inner1.test(inner2) // error, outer1需要outer1的那个内部类\n  }\n}\n\n\nclass ScalaOuterClass3 {\n  myOuter =>\n\n  class ScalaInnerClass3 { //成员内部类\n    def test(ic: ScalaInnerClass3): Unit = {\n      System.out.println(ic)\n    }\n  }\n\n}\n```\n\n1. Java中的内部类从属于外部类,因此在java中 inner1.test(inner2) 就可以，因为是按类型来匹配的。\n2. Scala中内部类从属于外部类的对象，所以外部类的对象不一样，创建出来的内部类也不一样，无法互换使用\n3. 比如你使用ideal 看一下在inner1.test()的形参上，它提示的类型是 outer1.ScalaInnerClass\n\n**解决方式-使用类型投影**\n\n类型投影是指：在方法声明上，如果使用  `外部类#内部类`  的方式，表示忽略内部类的对象关系，等同于Java中内部类的语法操作，我们将这种方式称之为 类型投影（即：忽略对象的创建方式，只考虑类型）。\n\n把上面的代码改为如下所示的代码，inner1.test(inner2)就不报错了\n\n```scala\nclass ScalaOuterClass3 {\n  myOuter =>\n\n  class ScalaInnerClass3 { //成员内部类\n    def test(ic: ScalaOuterClass3#ScalaInnerClass3): Unit = {\n      System.out.println(ic)\n    }\n  }\n}\n```\n\n","tags":["scala"],"categories":["scala"]},{"title":"二、Scala之函数式编程（基础部分）","url":"/2021/03/22/223515/","content":"\n在scala中，函数式编程和面向对象编程融合在一起，学习函数式编程式需要oop的知识，同样学习oop需要函数式编程的基础。\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n## 1. 函数式编程介绍\n\n在学习Scala中将方法、函数、函数式编程和面向对象编程明确一下：\n\n1) 在scala中，方法和函数几乎可以等同(比如他们的定义、使用、运行机制都一样的)，只是函数的使用方式更加的灵活多样。\n2) 函数式编程是从编程方式(范式)的角度来谈的，可以这样理解：函数式编程把函数当做一等公民，充分利用函数、 支持的函数的多种使用方式。\n比如：**在Scala当中，函数是一等公民，像变量一样，既可以作为函数的参数使用，也可以将函数赋值给一个变量，函数的创建不用依赖于类或者对象**。而在Java当中，函数的创建则要依赖于类、抽象类或者接口。\n3) 面向对象编程是以对象为基础的编程方式。\n4) 在scala中函数式编程和面向对象编程融合在一起了 。\n\n\n\n在学习Scala中将方法、函数、函数式编程和面向对象编程关系分析图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323111615.png)\n\n\n\n- \"函数式编程\"是一种\"编程范式\"（programming paradigm）。\n- 它属于\"结构化编程\"的一种，主要思想是把运算过程尽量写成一系列嵌套的函数调用。\n- 函数式编程中，将函数也当做数据类型，因此可以接受函数当作输入（参数）和输出（返回值）。\n- 函数式编程中，最重要的就是函数。\n\n\n\n## 2. 函数的定义和实现机制\n\n**基本语法**\n\n```tex\ndef 函数名 ([参数名: 参数类型], ...)[[: 返回值类型] =] {\n    语句...\n    return 返回值\n}\n```\n\n1) 函数声明关键字为def  (definition)\n2) `[参数名: 参数类型], ...`：表示函数的输入(就是参数列表), 可以没有。 如果有，多个参数使用逗号间隔\n3) 函数中的语句：表示为了实现某一功能代码块\n4) 函数可以有返回值，也可以没有\n5) 返回值形式1:    `: 返回值类型  = `\n6) 返回值形式2:    `=`  **表示返回值类型不确定，使用类型推导完成**\n7) 返回值形式3:   **全部去掉**，即去掉`[[: 返回值类型] =]`，表示没有返回值，return 不生效\n\n```scala\nobject Test2 {\n\n  def main(args: Array[String]): Unit = {\n    println(test)//输出()，即和Unit的效果一样\n  }\n\n  def test {\n    println(\"test\")\n  }\n}\n```\n\n\n\n8) 如果没有return ,**默认以执行到最后一行的结果作为返回值**\n快速入门案例\n\n\n\n**函数-调用机制**\n\n为了让大家更好的理解函数调用机制, 看1个案例，并画出示意图，这个很重要，比如getSum 计算两个数的和,并返回结果。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323131913.png)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323131856.png)\n\n栈：**先进后出**\n\n\n\n**函数-递归调用**\n\n一个函数在函数体内又调用了本身，我们称为递归调用\n\n1) 程序执行一个函数时，就创建一个新的受保护的独立空间(新函数栈)\n2) 函数的局部变量是独立的，不会相互影响\n3) 递归必须向退出递归的条件逼近，否则就是无限递归，死龟了\n4) 当一个函数执行完毕，或者遇到return，就会返回，遵守谁调用，就将结果返回给谁。\n\n\n\n## 3. 函数注意事项和细节\n\n1) 函数的形参列表可以是多个, 如果函数没有形参，调用时 可以不带()\n\n2) 形参列表和返回值列表的数据类型可以是值类型和引用类型。\n\n3) Scala中的函数可以根据函数体**最后一行代码自行推断函数返回值类型**。那么在这种情况下，return关键字可以省略\n\n4) 因为Scala可以自行推断，所以在省略return关键字的场合，返回值类型也可以省略\n\n5) 如果函数明确使用return关键字，那么函数返回就不能使用自行推断了，这时要明确写成 `: 返回类型 =`  \n\n```scala\n//正确\ndef getSum(n1: Int, n2: Int): Int = {\n//因为这里有明确的return , 这时 getSum 就不能省略 : Int = 的 Int了\n\treturn n1 + n2\n\n//正确    \ndef getSum(n1: Int, n2: Int): Int = {\n\tn1 + n2\n\n//正确\ndef getSum(n1: Int, n2: Int) = {\n    n1 + n2\n}\n    \n//错误，使用自动推断，不能再用return  \ndef getSum(n1: Int, n2: Int) = {\n    return n1 + n2\n}\n    \n//正确，但是无返回值，和Unit效果相同，println(getSum(1,2))会输出()\ndef getSum(n1: Int, n2: Int) {\n    n1 + n2\n}    \n\n//正确，但是无返回值，和Unit效果相同，println(getSum(1,2))会输出() \ndef getSum(n1: Int, n2: Int) {\n    return n1 + n2\n}\n```\n\n\n\n6) 如果函数明确声明无返回值（声明Unit），那么函数体中即使使用return关键字也不会有返回值\n\n```scala\n//正确，但是无返回值，println(getSum(1,2))会输出() \ndef getSum(n1: Int, n2: Int):Unit= {\n    return n1 + n2\n}\n```\n\n\n\n7) 如果明确函数无返回值或不确定返回值类型，那么返回值类型可以省略(**或声明为Any**)\n\n8) Scala语法中任何的语法结构都可以嵌套其他语法结构(灵活)，即：**函数中可以再声明/定义函数，类中可以再声明类 ，方法中可以再声明/定义方法**\n\n9) Scala函数的形参，在声明参数时，直接赋初始值(默认值)，这时调用函数时，如果没有指定实参，则会使用默认值。如果指定了实参，则实参会覆盖默认值。\n\n```scala\ndef sayOk(name : String = \"jack\"): String = {\n\treturn name + \" ok! \"\n}\n\nprintln(sayOk())//输出jack ok! \n//println(sayOk)//语法错误\n```\n\n10)如果函数存在多个参数，每一个参数都可以设定默认值，那么这个时候，传递的参数到底是覆盖默认值，还是赋值给没有默认值的参数，就不确定了(默认按照声明顺序[从左到右])。在这种情况下，可以采用带名参数 \n\n```scala\ndef main(args: Array[String]): Unit = {\n\n    mysqlCon(user=\"Layne\")\n    //f6(\"v2\")//语法错误\n    f6(p2=\"v2\")\n}\n\ndef mysqlCon(add:String = \"localhost\",port : Int = 3306,\n             user: String = \"root\", pwd : String = \"root\"): Unit = {\n    println(\"add=\" + add)\n    println(\"port=\" + port)\n    println(\"user=\" + user)\n    println(\"pwd=\" + pwd)\n}\n\ndef f6 ( p1 : String = \"v1\", p2 : String ) {\n    println(p1 + p2);\n}\n```\n\n\n\n11) **scala函数的形参默认是val的**，因此不能在函数中进行修改\n\n12)递归函数未执行之前是无法推断出来结果类型，在使用时必须有明确的返回值类型\n\n```scala\ndef f8(n: Int) = { //? 错误，递归不能使用类型推断，必须指定返回的数据类型\n\tif(n <= 0)\n\t1\n\telse\n\tn * f8(n - 1)\n}\n```\n\n\n\n13) Scala函数支持可变参数\n\n```scala\n//支持0到多个参数\ndef sum(args :Int*) : Int = {\n}\n//支持1到多个参数\ndef sum(n1: Int, args:  Int*) : Int  = {\n}\n```\n\n说明:\n(1) args 是集合, 通过 for循环 可以访问到各个值。\n(2) **可变参数需要写在形参列表的最后**\n\n```scala\nobject Test {\n   def main(args: Array[String]) {\n        printStrings(\"Runoob\", \"Scala\", \"Python\");\n   }\n   def printStrings( args:String* ) = {\n      var i : Int = 0;\n      for( arg <- args ){\n         println(\"Arg value[\" + i + \"] = \" + arg );\n         i = i + 1;\n      }\n   }\n}\n```\n\n输出\n\n```tex\nArg value[0] = Runoob\nArg value[1] = Scala\nArg value[2] = Python\n```\n\n\n\n14）①函数没有参数时，在定义函数时()可以省略\n\n②函数没有参数时，在调用函数时()可以省略\n\n③函数没有参数时，如果在定义时()省略，则在调用时不能再有()\n\n④函数没有参数时，如果在定义时()没有省略，则在调用既可以有()，也可以没有()\n\n⑤函数在定义时，如果其内容只有一行代码，则{}可以省略\n\n```scala\npackage Test\n\nobject Test3 {\n\n  def main(args: Array[String]): Unit = {\n    println(test1)//2. 函数没有参数时，在调用函数时()可以省略\n    //println(test1())//报错  //3. 函数没有参数时，如果在定义时()省略，则在调用时不能再有()\n    println(test2)//2. 函数没有参数时，在调用函数时()可以省略\n    println(test2())//4. 函数没有参数时，如果在定义时()没有省略，则在调用既可以有()，也可以没有()\n    println(test3)\n  }\n  def test1={ //1. 函数没有参数时，()可以省略\n    \"aaa\"\n  }\n\n  def test2()={\n    \"bbb\"\n  }\n\n  def test3=\"ccc\"//函数在定义时，如果其内容只有一行代码，则{}可以省略\n\n}\n\n```\n\n输出\n\n```scala\naaa\nbbb\nbbb\nccc\n```\n\n\n\n## 4. 过程\n\n将函数的返回类型为Unit的函数称之为过程(procedure)，如果明确函数没有返回值，那么等号可以省略\n\n```scala\n//test4就是一个过程\ndef test4:Unit={\n    println(\"Hello\")\n}\n//test5也是一个过程\ndef test5{\n    println(\"Hello\")\n}\n```\n\n1) 注意区分: 如果函数声明时没有返回值类型，但是有 = 号，可以进行类型推断最后一行代码。这时这个函数实际是有返回值的，该函数并不是过程。\n2) 开发工具的自动代码补全功能，**虽然会自动加上Unit**，但是考虑到Scala语言的简单，灵活，最好不加.\n\n\n\n## 5. 惰性函数\n\n惰性计算（尽可能延迟表达式求值）是许多函数式编程语言的特性。惰性集合在需要时提供其元素，无需预先计算它们，这带来了一些好处。首先，您可以将耗时的计算推迟到绝对需要的时候。其次，您可以创造无限个集合，只有它们继续收到请求，才可以继续提供元素。函数的惰性使用让您能够得到更高效的代码。Java 并没有为惰性提供原生支持，Scala提供了。\n\nJava实现懒加载的代码\n\n```java\npublic class LazyDemo {\nprivate String property; //属性也可能是一个数据库连接，文件等资源\npublic String getProperty() {\nif (property == null) {//如果没有初始化过，那么进行初始化\nproperty = initProperty();\n}\nreturn property;\n}\nprivate String initProperty() {\nreturn \"property\";\n}\n}\n//比如常用的单例模式懒汉式实现时就使用了上面类似的思路实现\n```\n\n**Scala惰性函数**\n\n当函数返回值被声明为lazy时，函数的执行将被推迟，直到我们首次对此取值，该函数才会执行。这种函数我们称之为惰性函数，在Java的某些框架代码中称之为懒加载(延迟加载)。\n\n```scala\ndef main(args: Array[String]): Unit = {\n    lazy val res = sum(10, 20)\n    println(\"-----------------\")\n    println(\"res=\" + res) //在要使用res 的时候，sum才执行\n}\n\ndef sum(n1: Int, n2: Int): Int = {\n    println(\"sum() 执行了..\")\n    return n1 + n2\n}\n```\n\n输出\n\n```tex\n-----------------\nsum() 执行了..\nres=30\n```\n\n1) lazy 不能修饰 var 类型的变量\n2) 在调用函数时，加了 lazy ,会导致函数的执行被推迟。我们在声明一个变量时，如果给声明了 lazy ,那么变量值得分配也会推迟。 比如 `lazy val i = 10`，只有在使用变量`i`时，才会给它分配值\n\n\n\n## 6. 异常\n\n- Scala提供try和catch块来处理异常。try块用于包含可能出错的代码。catch块用于处理try块中发生的异常。可以根据需要在程序中有任意数量的try...catch块。\n- 语法处理上和Java类似，但是又不尽相同\n\n```java\ntry {\n    // 可疑代码\n    int i = 0;\n    int b = 10;\n    int c = b / i; // 执行代码时，会抛出ArithmeticException异常\n} catch(Exception e)  {\n    e.printStackTrace();\n}finally {\n    // 最终要执行的代码\n    System.out.println(\"java finally\");\n}\n```\n\n1) java语言按照try—catch-catch...—finally的方式来处理异常\n2) 不管有没有异常捕获，都会执行finally, 因此通常可以在finally代码块中释放资源\n3) 可以有多个catch，分别捕获对应的异常，**这时需要把范围小的异常类写在前面**，把范围大的异常类写在后面，否则编译错误。会提示 \"Exception 'java.lang.xxxxxx'has already been caught\"\n\n**Scala异常处理举例**\n\n```scala\ntry {\n    val r = 10 / 0\n} catch {\n    case ex: ArithmeticException=> println(\"捕获了除数为零的算数异常\")\n    case ex: Exception => println(\"捕获了异常\")\n} finally {\n    // 最终要执行的代码\n    println(\"scala finally...\")\n}\n```\n\n\n\n1) 我们将可疑代码封装在try块中。 在try块之后使用了一个catch处理程序来捕获异常。如果发生任何异常，catch处理程序将处理它，程序将不会异常终止。\n2) Scala的异常的工作机制和Java一样，但是Scala没有“checked(编译期)”异常，即**Scala没有编译异常这个概念**，异常都是在运行的时候捕获处理。\n\n3) 用throw关键字，抛出一个异常对象。所有异常都是Throwable的子类型。**throw表达式是有类型的，就是Nothing**，**因为Nothing是所有类型的子类型，所以throw表达式可以用在需要类型的地方**\n\n```scala\ndef main(args: Array[String]): Unit = {\n    val res = test()\n    println(res.toString)\n}\ndef test(): Nothing = {\n    throw new Exception(\"不对\")\n}\n```\n\n4) 在Scala里，借用了模式匹配的思想来做异常的匹配，因此，在catch的代码里，是一系列case子句来匹配异常。【前面案例可以看出这个特点, 模式匹配我们后面详解】。当匹配上后 => 有多条语句可以换行写，类似 java 的 switch case x: 代码块.\n5) 异常捕捉的机制与其他语言中一样，如果有异常发生，**catch子句是按次序捕捉的**。因此，在catch子句中，越具体的异常越要靠前，越普遍的异常越靠后，如果把越普遍的异常写在前，把具体的异常写在后，在scala中也不会报错，但这样是非常不好的编程风格\n\n6) finally子句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作，这点和Java一样。\n8) Scala提供了throws关键字来声明异常。可以使用方法定义声明异常。 它向调用者函数提供了此方法可能引发此异常的信息。 它有助于调用函数处理并将该代码包含在try-catch块中，以避免程序异常终止。在scala中，可以使用throws注释来声明异常\n\n```scala\ndef main(args: Array[String]): Unit = {\n    try{\n        f11()\n    }catch {\n        case ex: NumberFormatException=> println(\"捕捉了格式转换异常\")\n        case ex: Exception => println(\"捕获了异常\")\n    }\n    println(\"OK\")\n}\n\n@throws(classOf[NumberFormatException]) //等同于NumberFormatException.class\ndef f11() = {\n    \"abc\".toInt\n}\n```\n\n\n","tags":["scala"],"categories":["scala"]},{"title":"一、Scala基础之变量、运算符、程序流程控制","url":"/2021/03/22/223312/","content":"\n\n\nScala基础之变量、运算符、程序流程控制\n\n<!-- more -->\n\n\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 一、变量\n\n### 1.1 Scala变量使用说明\n\n变量声明基本语法\n\n```tex\nvar | val 变量名 [: 变量类型] = 变量值\n```\n\n注意事项\n1) 声明变量时，类型可以省略（编译器自动推导,即类型推导）\n\n2) 类型确定后，就不能修改，**说明Scala 是强数据类型语言**\n\n```scala\nvar a:Int=10\na=10.5 //报错，类型确定后就不能修改\n```\n\n3) 在声明/定义一个变量时，可以使用var 或者 val 来修饰， var 修饰的变量可改变，val 修饰的变量不可改 \n\n4) val修饰的变量在编译后，等同于加上final，通过反编译看下底层代码。\n\n5)  var 修饰的**对象引用**可以改变，val 修饰的则不可改变，但对象的状态或属性(值)却是可以改变的。(比如: 自定义对象、数组、集合等等) ，因此一般用val修饰对象。\n\n6) 变量声明时，需要初始值\n\n\n\n### 1.2 scala数据类型\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210322233141.png)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210322223724.png)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210322223800.png)\n\n1）Scala各类型数据有固定的表数范围和字段长度，不受具体OS的影响，以保证Scala程序的可移植性。\n\n2） Scala的整型 常量/字面量  默认为 Int 型，声明Long型 常量/字面量 须后加`l`或`L`\n\n3）Scala的浮点型常量默认为Double型，声明Float型常量，须后加`f`或`F`\n\n4）浮点型常量有两种表示形式\n十进制数形式：如：`5.12       512.0f        .512   (必须有小数点）`\n科学计数法形式:如：`5.12e2  = 5.12乘以10的2次方     5.12E-2  = 5.12除以10的2次方`\n\n\n\n### 1.3 Unit类型、Null类型和Nothing类型\n\n1) Null类只有一个实例对象，null，类似于Java中的null引用。**null可以赋值给任意引用类型(AnyRef)**，但是不能赋值给值类型(AnyVal: 比如 Int, Float, Char,Boolean, Long, Double, Byte, Short)。\n\n注意：Int被反编译为int，**因此Scala中的AnyVal都是值类型**\n\n```scala\nval a:StringOps=null\nval b:String=null//说明String是引用类型\nprintln(s\"a=${a},b=${b}\")//a=null,b=null\nval c:Int=null//报错\n```\n\n\n\n2) Unit类型用来标识过程，也就是没有明确返回值的函数。由此可见，Unit类似于Java里的void。Unit只有一个实例，()，这个实例也没有实质的意义\n\n```scala\n  def main(args: Array[String]): Unit = {\n    println(fun())//输出()\n  }\n\n  def fun():Unit={\n  }\n```\n\n3）Nothing，可以作为没有正常返回值的方法的返回类型，**非常直观的告诉你这个方法不会正常返回**，而且由于Nothing是其他任意类型的子类，他还能跟要求返回值的方法兼容。\n\n```scala\n  def main(args: Array[String]): Unit = {\n    test()//抛出异常\n  }\n\n  def test():Nothing={\n    throw new Exception(\"一个异常\")\n  }\n```\n\n\n\n### 1.4 值类型隐式转换\n\nScala程序在进行赋值或者运算时，精度小的类型自动转换为精度大的数据类型，这个就是自动类型转换(隐式转换)。\n\n数据类型按精度(容量)大小排序为\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210322234626.png)\n\n1) 有多种类型的数据混合运算时，系统首先自动将所有数据转换成容量最大的那种数据类型，然后再进行计算，如`5.6 + 10 = 》double`\n\n2) 当我们把精度(容量)大 的数据类型赋值给精度(容量)小 的数据类型时，就会报错，反之就会进行自动类型转换。\n\n3) (byte, short) 和 char之间不会相互自动转换。\n\n4) byte，short，char  他们三者可以计算，**在计算时首先转换为int类型**。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210322234827.png)\n\n5) 自动提升原则： 表达式结果的类型自动提升为 操作数中最大的类型\n\n\n\n### 1.5 强制类型转换\n\n自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转函数，但可能造成精度降低或溢出，格外要注意。\n\njava  : ` int num = (int)2.5`\nscala :  `var num : Int =  2.7.toInt`  （scala中所有的基本类型数据都是一个对象，这点和java不同）\n\n强转符号只针对于最近的操作数有效，往往会使用小括号提升优先级\n\n```scala\nval num1: Int = 10 * 3.5.toInt + 6 * 1.5.toInt  // 36\nval num2: Int = (10 * 3.5 + 6 * 1.5).toInt // 44\nprintln(num1 + \" \" + num2)\n```\n\n\n\n### 1.6 值类型和String类型的转换\n\n在程序开发中，我们经常需要将基本数据类型转成String 类型，或者将String类型转成基本数据类型。\n\n**基本类型转String类型**\n语法： 将基本类型的值+\"\" 即可\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210322235339.png)\n\n**String类型转基本数据类型**\n\n语法：通过基本类型的String的 toXxx方法即可\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210322235416.png)\n\n在将String 类型转成 基本数据类型时，要确保String类型能够转成有效的数据，比如 我们可以把 \"123\" , 转成一个整数，但是不能把 \"hello\" 转成一个整数。\n\n\n\n### 1.7 标识符的命名规范\n\n1) Scala 对各种变量、方法、函数等命名时使用的字符序列称为标识符\n2) 凡是自己可以起名字的地方都叫标识符\n\nScala中的标识符声明，基本和Java是一致的，**但是细节上会有所变化**。\n\n1) 首字符为字母，后续字符任意字母和数字，美元符号，可后接下划线_\n\n2) 数字不可以开头。\n\n3) 首字符为操作符(比如+ - \\* / )，后续字符也需跟操作符 ,至少一个\n\n4) 操作符(比如+-\\*/)不能在标识符中间和最后\n\n5) 用反引号\\`....\\`包括的任意字符串，即使是关键字(39个)也可以，如\n\n标识符举例说明\n\n```tex\n`public` //ok\nhello    // ok\nhello12 // ok\n1hello  // error\nh-b   // error\nx h   // error\nh_4   // ok\n_ab   // ok\nInt    // ok, 在scala中，Int 不是关键字，而是预定义标识符,可以用，但是不推荐\nFloat  // ok\n_   // 不可以，因为在scala中，_ 有很多其他的作用，因此不能使用\nAbc    // ok\n+*-   // ok\n+a  // error\n```\n\n一定要注意：\n\n```scala\nvar ++ = 10 //ok，这里++为变量名\nvar ++= 10 //error，因为+=是数学运算\n```\n\n\n\n## 二、运算符\n\n运算符是一种特殊的符号，用以表示数据的运算、赋值和比较等。\n1) 算术运算符\n2) 赋值运算符\n3) 比较运算符(关系运算符)\n4) 逻辑运算符\n5) 位运算符\n\n\n\n### 2.1 算术运算符\n\n算术运算符(arithmetic)是对**数值类型的变量进行运算的**，在Scala程序中使用的非常多。\n\n算数运算符的运算规则和Java一样。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323095641.png)\n\nScala中没有++、--操作符，需要通过+=、-=来实现同样的效果\n\n\n\n### 2.2 关系运算符(比较运算符)\n\n1) 关系运算符的结果都是boolean型，也就是要么是true，要么是false\n2) 关系表达式 经常用在 if结构的条件中或循环结构的条件中\n3) 关系运算符的使用和java一样\n\n4) 使用陷阱: 如果两个浮点数进行比较，应当保证数据类型一致\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323095710.png)\n\n\n\n```scala\nvar a = 9\nvar b = 8\nprintln(a>b)  // true\nprintln(a>=b) // true\nprintln(a<=b) // false\nprintln(a<b) // false\nprintln(a==b) // false\nprintln(a!=b) // true\nvar flag : Boolean = a>b  // true\n```\n\n\n\n### 2.3 逻辑运算符\n\n用于连接多个条件（一般来讲就是关系表达式），最终的结果也是一个Boolean值。\n\n假定变量 A 为 true，B 为 false\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323095903.png)\n\n\n\n### 2.4 赋值运算符\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323100007.png)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323100049.png)\n\n这部分的赋值运算涉及到二进制相关知识，其运行的规则和Java一样。\n\n\n\n### 2.5 位运算符\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323100228.png)\n\n说明: 位运算符的规则和Java一样\n\n**Scala不支持三目运算符** , 在Scala 中使用 if – else 的方式实现。\n\n```scala\nval num = 5 > 4 ? 5 : 4  //没有\nval num = if (5>4) 5 else 4\n```\n\n\n\n### 2.6 运算符优先级\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323100517.png)\n\n\n\n运算符的优先级和Java一样。\n\n小结运算符的优先级\n1.() []\n2.单目运算\n3.算术运算符\n4.移位运算\n5.比较运算符(关系运算符)\n6.位运算\n7.关系运算符\n8.赋值运算\n\n### 2.7 键盘输入语句\n\n```scala\nimport scala.io.StdIn\nobject InputDemo {\n\n  def main(args: Array[String]): Unit = {\n    var a:Int=StdIn.readInt()\n    var b:Double=StdIn.readDouble()\n    var c:String=StdIn.readLine()\n    println(a)\n    println(b)\n    println(c)\n  }\n}\n\n```\n\n\n\n### 2.8 多种print输出方式\n\n（1）普通输出方式：\n\n```scala\nobject HelloWorld {\n  def main(args: Array[String]): Unit = {\n    var i:Int = 10\n    val j:Int = 20\n    println(\"Hello World\")\n    println(i + j)\n  }\n}\n\n```\n\n（2）插值字符串输出方式：\n\n```scala\nobject HelloWorld {\n  def main(args: Array[String]): Unit = {\n    val name:String = \"hikari\"\n    val age:Int = 30\n    println(s\"username = ${name}, age = ${age}\")\n  }\n}\n\n```\n\n这样的好处是可以少些+号，避免字符串拼写错误出现。\n\n(3)格式化输出\n\n```scala\nobject HelloWorld {\n  def main(args: Array[String]): Unit = {\n    val name:String = \"hikari\"\n    val age:Int = 30\n    //print(s\"username = $name, age = $age\")\n    printf(\"name = %s age = %d\",name,age)\n  }\n}\n\n```\n\nprintf方法常用格式说明符列表\n\n%c 单个字符\n\n%d 十进制整数\n\n%f 十进制浮点数\n\n%o 八进制数\n\n%s 字符串\n\n%u 无符号十进制数\n\n%x 十六进制数\n\n%% 输出百分号%\n\n举例，“%s:%.2f”是格式字符串，其中“%s”表示输出字符串，“%.2f”表示输出浮点数时，限制2位有效小数，更改“%.”后面的数字，可以限制不同的小数位数输出\n\n\n\n## 三、程序流程控制\n\nScala语言中控制结构和Java语言中的控制结构基本相同，在不考虑特殊应用场景的情况下，代码书写方式以及理解方式都没有太大的区别。\n\n1) 顺序控制\n2) 分支控制\n3) 循环控制\n\n### 3.1 分支控制if-else\n\n和java完全一样\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323102131.png)\n\n\n\nScala中是没有三元运算符，因为可以这样简写\n\n```scala\n// Java\nint result = flg ? 1 : 0\n// Scala\nval result = if (flg) 1 else 0\n```\n\n**scala中的if-else是有返回值的**，因此，本身这个语言也不需要三元运算符了，并且可以写在同一行，类似 三元运算\n\n分支控制if-else 注意事项\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210323102330.png)\n\n上面输出if-else结构的返回值\n\n\n\n### 3.2 switch分支结构\n\n**在scala中没有switch**，而是使用**模式匹配**来处理。\n模式匹配涉及到的知识点较为综合，因此我们放在后面讲解：match-case\n\n\n\n### 3.3 for循环控制\n\nScala 也为for 循环这一常见的控制结构提供了非常多的特性，这些for 循环的特性被称为for 推导式（for comprehension）或for 表达式（for expression）\n\n（1）**范围数据循环方式1**\n\n```scala\nfor(i <- 1 to 3){\n\tprint(i + \" \")\n}\nprintln()\n```\n\n说明\n1) i 表示循环的变量， `<- `规定好 to 规定\n2) i 将会从 1-3 循环， **前后闭合**\n\n（2）**范围数据循环方式2**\n\n```scala\nfor(i <- 1 until 3) {\n\tprint(i + \" \")\n}\nprintln()\n```\n\n说明:\n1) 这种方式和前面的区别在于 i 是从1 到 3-1\n2) **前闭合后开的范围**，和java的arr.length() 类似`for (int i = 0; i < arr.lenght; i++){}`\n\n（3）**循环守卫**\n\n```scala\nfor(i <- 1 to 3 if i != 2) {\n\tprint(i + \" \")\n}\nprintln()\n```\n\n1) 循环守卫，即循环保护式（也称条件判断式，守卫）。保护式为true则进入循环体内部，为false则跳过，类似于`i==2`时执行continue\n2) 上面的代码等价\n\n```scala\nfor (i <- 1 to 3) {\nif (i != 2) {\n\tprintln(i+\"\")\n\t}\n}\n```\n\n（4）**引入变量**\n\n```scala\nfor(i <- 1 to 3; j = 4 - i) {\n\tprint(j + \" \")\n}\n```\n\n1) 没有关键字，所以范围后一定要加`；`来隔断逻辑\n2) 上面的代码等价\n\n```scala\nfor ( i <- 1 to 3) {\n\tval j = 4 –i\n\tprint(j+\"\")\n}\n```\n\n（5）**嵌套循环**\n\n```scala\nfor(i <- 1 to 3; j <- 1 to 3) {\n\tprintln(\" i =\" + i + \" j = \" + j)\n}\n```\n\n1) 没有关键字，所以范围后一定要加`；`来隔断逻辑\n2) 上面的代码等价\n\n```scala\nfor ( i <- 1 to 3) {\n    for ( j <- 1to 3){\n    \tprintln(i + \" \" + j + \" \")\n    }\n}\n```\n\n（6）**循环返回值**\n\n```scala\nval res = for(i <- 1 to 10) yield i\nprintln(res)//输出Vector(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n```\n\n1) 将遍历过程中处理的结果返回到一个新Vector集合中，使用yield关键字\n\n（7）**使用花括号{}代替小括号()**\n\n```scala\nfor(i <- 1 to 3; j =  i * 2) {\n\tprintln(\" i= \" + i + \" j= \" + j)\n}\n```\n\n可以写成\n\n```scala\nfor{\n    i <- 1 to 3\n    j = i * 2} {\n\tprintln(\" i= \" + i + \" j= \" + j)\n}\n```\n\n1) {}和()对于for表达式来说都可以\n2) **for 推导式有一个不成文的约定**：当for 推导式仅包含单一表达式时使用圆括号，当其包含多个表达式时使用大括号\n3) 当使用{} 来换行写表达式时，**分号就不用写了**\n\n（8）**注意事项和细节说明**\n\n1) scala 的for循环形式和java是较大差异，，但是基本的原理还是一样的。\n2) scala 的for循环的步长如何控制\n\n```scala\nfor(i <- Range(1,10,2))//Range是1至10左闭右开，步长为2\n    print(i+\" \") //依次输出1 3 5 7 9 \n```\n\n或者写成\n\n```scala\nfor(i <- 1 until 10) {\n    if(i%2!=0)//使用守卫控制步长\n    print(i+\" \")\n}\n```\n\n下面这种写法是错误的，一定要注意\n\n```scala\nfor(i <- 1 until 10) {\n    i+=2//因为i为val，是不允许更改的\n    print(i+\" \")\n}\n```\n\n\n\n### 3.4 while循环控制\n\n```tex\n循环变量初始化\nwhile (循环条件) {\n    循环体(语句)\n    循环变量迭代\n}\n```\n\n注意事项和细节说明\n1) 循环条件是返回一个布尔值的表达式\n2) while循环是先判断再执行语句\n3) 与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()\n4) 因为while中没有返回值，所以当要用该语句来计算并返回结果时，就不可避免的使用变量 ，**而变量需要声明在while循环的外部**，**那么就等同于循环的内部对外部的变量造成了影响**，所以不推荐使用，而是推荐使用for循环。\n\n\n\n### 3.5 do..while循环控制\n\n```tex\n循环变量初始化;\ndo{\n    循环体(语句)\n    循环变量迭代\n} while(循环条件)\n```\n\n注意事项和细节说明\n1) 循环条件是返回一个布尔值的表达式\n2) do..while循环是先执行，再判断\n3) 和while 一样，因为do…while中没有返回值,所以当要用该语句来计算并返回结果时,就不可避免的使用变量 ，而变量需要声明在do...while循环的外部，那么就等同于循环的内部对外部的变量造成了影响，所以不推荐使用，而是推荐使用for循环\n\n\n\n### 3.6 for或while循环的中断\n\nScala内置控制结构特地去掉了break和continue，是为了更好的适应函数化编程，推荐使用函数式的风格解决break和contine的功能，而不是一个关键字。\n\n（1）实现break的效果\n\n```scala\nimport scala.util.control.Breaks.{break, breakable}\n\nobject WhileBreak {\n  def main(args: Array[String]): Unit = {\n    breakable {\n      for (i <- 1 to 3) {\n        print(i+\" \") //依次输出1 2 \n        if (i == 2)\n          break();\n      }\n    }\n    println()\n    var n=1\n    breakable{\n      while(n<=20){\n        n+=1\n        //依次输出n=2,n=3,n=4,n=5,n=6,n=7,n=8,n=9,n=10,n=11,n=12,n=13,n=14,n=15,n=16,n=17,n=18\n        print(\"n=\"+n+\",\") \n        if(n==18)\n          break()\n      }\n    }\n  }\n}\n\n```\n\n（2）实现continue的效果\n\nScala内置控制结构特地也去掉了continue，是为了更好的适应函数化编程，可以使用if – else 或是 循环守卫实现continue的效果\n\n```scala\nfor (i <- 1 to 10 if (i != 2 && i != 3)) {\n\tprintln(\"i=\" + i)\n}\n```\n\n等同于\n\n```java\nfor(int i=0;i<=10;i++)\n\tif(i==2)\n\t\tcontinue\n    if(i==3)\n        continue\n    System.out.println(\"i=\" + i)\n```\n\n","tags":["scala"],"categories":["scala"]},{"title":"hadoop完全分布式集群搭建","url":"/2021/03/17/200228/","content":"\n\n\n强烈建议先看一遍[hadoop伪分布式集群搭建](https://blog.csdn.net/qq_37555071/article/details/115048384)，然后再按本文的**hadoop完全分布式集群搭建**进行配置。\n\n<!-- more -->\n\n## 集群规划\n\n四台服务器，分别为layne1~4，分布如下：\n\n- layne1作为NameNode服务器\n- layne2作为SecondaryNameNode和DataNode服务器\n- layne3、layne4作为DataNode服务器\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210321133419.png)\n\n\n\n## HDFS完全分布式搭建\n\n详细步骤如下：\n\n\n\n1、四台服务器之间互相均可以免密登录\n\n可参考我之前的博客[多台服务之间免密登陆](https://wxler.github.io/2021/02/23/101109/#11-%E5%A4%9A%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B9%8B%E9%97%B4%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95)\n\n\n\n2、四台服务器JDK安装并配置环境变量\n\n可参考[rpm安装jdk](https://wxler.github.io/2021/02/10/112210/#rpm%E5%AE%89%E8%A3%85jdk)\n\n\n\n3、先在layne1上配置好，然后将配置好的hadoop拷贝到layne2~layne4上，**这里所有配置都先在layne1上进行**。\n\n将hadoop安装包拷贝到layne1上并解压，然后进行如下配置：\n\n（1）配置hadoop-env.sh\n\n由于通过SSH远程启动进程的时候默认不会加载/etc/profile设置，JAVA_HOME变量就加载不到，需要手动指定。\n\n在`/opt/hadoop-2.6.5`下，输入`vim ./etc/hadoop/hadoop-env.sh`，找到JAVA_HOME所在的行，并改为`export JAVA_HOME=/usr/java/default`。\n\n```java\n[root@layne1 hadoop-2.6.5]# pwd\n/opt/hadoop-2.6.5\n[root@layne1 hadoop-2.6.5]# cd ./etc/hadoop/\n[root@layne1 hadoop]# vim hadoop-env.sh\n\n```\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210317200315.png)\n\n\n\n（2）修改slaves指定DataNode的位置\n\n```bash\n/opt/hadoop-2.6.5/etc/hadoop\n[root@layne1 hadoop]# vim slaves\nlayne2\nlayne3\nlayne4\n\n```\n\n \n\n（3）配置hdfs-site.xml\n\n指定SecondaryNameNode的位置\n\n```bash\n[root@layne1 hadoop]# pwd\n/opt/hadoop-2.6.5/etc/hadoop\n[root@layne1 hadoop]# vim hdfs-site.xml \n```\n\n修改为如下内容：\n\n```xml\n<configuration>\n  <!-- 指定block副本数 -->\n  <property>\n    <name>dfs.replication</name>\n    <value>2</value>\n  </property>\n  <!-- 指定secondarynamenode所在的位置 -->\n  <property>\n    <name>dfs.namenode.secondary.http-address</name>\n    <value>layne2:50090</value>\n  </property>\n</configuration>\n```\n\n\n\n（4）配置core-site.xml\n\n这里是配置NameNode的位置和hadoop的存储目录\n\n```bash\n[root@layne1 hadoop]# pwd\n/opt/hadoop-2.6.5/etc/hadoop\n[root@layne1 hadoop]# vim core-site.xml \n```\n\n修改为：\n\n```xml\n<configuration>\n  <!-- 指定访问HDFS的时候路径的默认前缀  /  hdfs://layne1:9000/ -->\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://layne1:9000</value>\n  </property>\n  <!-- 指定hadoop的临时目录位置-->\n  <property>\n    <name>hadoop.tmp.dir</name>\n    <value>/var/layne/hadoop/full</value>\n  </property>\n</configuration>\n\n```\n\n\n\n4、将第3步配置好的hadoop拷贝到layne2~node4上\n\n先将layne1上的hadoop目录打成压缩包\n\n```bash\n[root@layne1 opt]# pwd\n/opt\n[root@layne1 opt]# tar -zcvf hadoop-2.6.5.tar.gz hadoop-2.6.5/\n```\n\n将`/opt/hadoop-2.6.5.tar.gz` 拷贝到layne2、layne3、layne4的对应目录中，即分别执行：\n\n```bash\nscp hadoop-2.6.5.tar.gz layne2:/opt\nscp hadoop-2.6.5.tar.gz layne3:/opt\nscp hadoop-2.6.5.tar.gz layne4:/opt\n```\n\n然后，在layne2、layne3、layne4分别解压\n\n```bash\ntar -zxvf hadoop-2.6.5.tar.gz\n```\n\n最后，删除四台虚拟机上的hadoop-2.6.5.tar.gz\n\n```bash\nrm -f hadoop-2.6.5.tar.gz\n```\n\n\n\n5、配置环境变量\n\n在layne1上配置环境变量，在`/etc/profile`中最后一行加入：\n\n```bash\nexport JAVA_HOME=/usr/java/default\nexport ZOOKEEPER_HOME=/opt/zookeeper-3.4.6\nexport HADOOP_HOME=/opt/hadoop-2.6.5\nexport PATH=$PATH:$JAVA_HOME/bin:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n```\n\n然后，将`etx/profile`分别拷贝到layne2、layne3、layne4，再分别执行`source /etc/profile`使其立即生效。\n\n\n\n6、格式化并启动\n\n在layne1上执行：`hdfs  namenode  -format`\n启动即可(该命令在四台服务器上哪一台执行都可以)：`start-dfs.sh`\n\n在四台机器上分别输入jps即可显示以下信息：\n\n```tex\nlayne1上显示\n[root@layne1 current]# jps\n1232 NameNode\n1301 Jps\nlayne2上显示\n[root@layne2 opt]# jps\n1203 DataNode\n1256 SecondaryNameNode\n1295 Jps\nlayne3上显示\n[root@layne3 opt]# jps\n1300 DataNode\n1487 Jps\nlayne4上显示\n[root@layne4 /]# jps\n1189 DataNode\n1258 Jps\n\n```\n\n访问：http://layne1:50070，点击DataNodes一栏，会出现如下界面：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210317215029.png)\n\n\n\n7、上传文件\n\n```bash\n[root@layne4 apps]# hdfs dfs -mkdir -p /user/abc\n21/03/17 22:49:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[root@layne4 apps]# ls\njdk-8u221-linux-x64.rpm\n[root@layne4 apps]# hdfs dfs -put jdk-8u221-linux-x64.rpm /user/abc\n21/03/17 22:49:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n```\n\n上述命令将文件`jdk-8u221-linux-x64.rpm`上传到hadoop的`/user/abc`目录下，注意该目录不是Linux系统中的目录，这两者没关系。\n\n在界面中可以看到上传的文件：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210317225939.png)\n\n\n\n8、关闭hadoop集群\n\n执行`stop-dfs.sh`关闭hadoop集群，该命令在hadoop集群中任意一台虚拟机执行均可。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Hadoop"],"categories":["Hadoop"]},{"title":"hadoop伪分布式集群搭建","url":"/2021/03/16/191640/","content":"\n\n\n## 基础设施\n\n基础设施环境如下：\n\n- jdk 1.7+（提前设置好环境变量）\n- ssh自己和自己之间进行免密登陆，如在layne1上执行`ssh layne1`\n- 时间同步\n- 设置本机ip\n- 设置主机名\n\n\n\n可参考[Linux切换运行级别、关闭防火墙、禁用selinux、关闭sshd、时间同步、修改时区、拍摄快照、克隆操作、修改语言环境](https://wxler.github.io/2021/02/04/211133/)。\n\n<!-- more -->\n\n\n\n这里不得不提Linux系统远程执行和远程登陆的区别：\n\n- 远程执行：不需要用户交互，而是用户直接给出一个命令，直接在远程执行，不会加载 `/etc/profile`\n- 远程登陆：返回一个交互接口，返回接口`/bash`  会加载`/etc/profile`\n\n\n\n\n\n\n\n## 操作步骤\n\n我在主机名为layne1上搭建hadoop伪分布式集群，详细步骤如下\n\n1、配置免密钥\n\n```bash\nssh-keygen  -t  dsa  -P  ''  -f  ~/.ssh/id_dsa\ncat  ~/.ssh/id_dsa.pub  >  ~/.ssh/authorized_keys\n```\n\n- `id_dsa.pub` 存放每台服务器自己的公钥\n- `authorized_keys` 存放的也是服务器的公钥，不过除了自己的公钥外，也可以存放其它服务器的公钥。\n\n再执行`ssh layne1`，让其自己和自己之间进行免密登陆。\n\n\n\n2、上传hadoop的tar包`hadoop-2.6.5.tar.gz`到Linux系统的`/opt/apps`目录下\n\n\n\n3、解压`hadoop-2.6.5.tar.gz`到`/opt`目录\n\n```bash\n[root@layne1 apps]# tar -zxvf hadoop-2.6.5.tar.gz -C /opt\n```\n\n\n\n4、删除`hadoop-2.6.5/share/`下的doc目录，doc里面是一些页面和文档，在Linux上没用，删除以后我们把这个hadoop复制到其他服务器上速度比较快\n\n```bash\n[root@layne1 hadoop-2.6.5]# pwd\n/opt/hadoop-2.6.5\n[root@layne1 hadoop-2.6.5]# cd share\n[root@layne1 share]# ls\ndoc  hadoop\n[root@layne1 share]# rm -rf doc\n[root@layne1 share]# ls\nhadoop\n```\n\n\n\n5、添加hadoop环境变量\n\n将`HADOOP_HOME`以及`HADOOP_HOME/bin`和`HADOOP_HOME/sbin`添加到环境变量，在`/etc/profile`里最后一行添加：\n\n```bash\nexport HADOOP_HOME=/opt/hadoop-2.6.5\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n```\n\n再执行`source /etc/profile`使其立即生效。\n\n\n\n6、hadoop-env.sh配置\n\n由于通过SSH远程启动进程的时候默认不会加载`/etc/profile`设置，JAVA_HOME变量就加载不到，需要手动指定。\n\n在`/opt/hadoop-2.6.5`下，输入`vim ./etc/hadoop/hadoop-env.sh`，找到JAVA_HOME所在的行，并改为`export JAVA_HOME=/usr/java/default`。\n\n```java\n[root@layne1 hadoop-2.6.5]# pwd\n/opt/hadoop-2.6.5\n[root@layne1 hadoop-2.6.5]# cd ./etc/hadoop/\n[root@layne1 hadoop]# vim hadoop-env.sh\n\n```\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210317200315.png)\n\n\n\n\n\n7、配置core-site.xml\n\n```bash\n[root@layne1 hadoop]# pwd\n/opt/hadoop-2.6.5/etc/hadoop\n[root@layne1 hadoop]# vim core-site.xml \n```\n\n**这个文件指定的是namenode的访问**\n\n```xml\n<configuration>\n  <!-- 指定访问HDFS的时候路径的默认前缀  /  hdfs://layne1:9000/ -->\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://layne1:9000</value>\n  </property>\n  <!-- 指定hadoop的临时目录位置，它会给namenode、secondarynamenode以及datanode的存储目录指定前缀 -->\n  <property>\n    <name>hadoop.tmp.dir</name>\n    <value>/var/layne/hadoop/pseudo</value>\n  </property>\n</configuration>\n\n```\n\n\n\n配置文件拷贝后格式不美观，可以通过以下方式格式化：\n\n1. 在vim命令按ESC回报命令模式，把光标定位在`<configuration>`行首\n2. 输入`Ctrl+V`\n3. 按键盘上的下箭头按钮，直到`<configuration/>`\n4. 输入`:!xmllint -format -`，然后回车\n5. 删除`<configuration>`上一行多出的`<?xml version=\"1.0\"?>`\n\n值得一提的是，这些配置都可以在`hadoop-2.6.5\\share\\doc\\hadoop\\index.html`里面找到，最好用IE浏览器打开，否则可能不识别。\n\n在windows上用IE浏览器打开`hadoop-2.6.5\\share\\doc\\hadoop\\index.html`，点击进入core-default.xml\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210316205007.png)\n\n可以看到，`hadoop.tmp.dir`的默认配置为` /tmp/hadoop-${user.name} `，即在Linux的临时文件下保存，所以我们要修改配置\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210316205112.png)\n\n\n\n要记住：\n\n- `core-default.xml`中的所有配置都可以在`core-site.xml` 中进行配置。\n- `hdsf-default.xml`中的所有配置都可以在`hdfs-site.xml` 中进行配置。\n\n\n\n8、配置hdfs-site.xml\n\n```bash\n[root@layne1 hadoop]# pwd\n/opt/hadoop-2.6.5/etc/hadoop\n[root@layne1 hadoop]# vim hdfs-site.xml \n```\n\n加入以下内容：\n\n```xml\n<configuration>\n  <!-- 指定block副本数 -->\n  <property>\n<name>dfs.replication</name>\n<value>1</value>\n  </property>\n  <!-- 指定secondarynamenode所在的位置 -->\n  <property>\n<name>dfs.namenode.secondary.http-address</name>\n<value>layne1:50090</value>\n  </property>\n</configuration>\n\n```\n\n\n\n9、配置slaves\n\n这里是配置datanode结点\n\n```bash\n[root@layne1 hadoop]# pwd\n/opt/hadoop-2.6.5/etc/hadoop\n[root@layne1 hadoop]# vim slaves\nlayne1\n```\n\n即在`slaves`输入`layne1`。\n\n\n\n10、格式化hadoop\n\n下面可以看到，第7步配置的临时目录位置不存在\n\n```bash\n[root@layne1 hadoop]# ls /var/layne/hadoop/pseudo\nls: cannot access /var/layne/hadoop/pseudo: No such file or directory\n\n```\n\n现在输入\n\n```bash\nhdfs  namenode  -format\n```\n\n\n\n再次查看日志\n\n```bash\n[root@layne1 hadoop]# ls /var/layne/hadoop/pseudo\ndfs\n[root@layne1 hadoop]# cd /var/layne/hadoop/pseudo/dfs\n[root@layne1 dfs]# ls\nname\n[root@layne1 dfs]# cd name\n[root@layne1 name]# ls\ncurrent\n[root@layne1 name]# cd current\n[root@layne1 current]# ls\nfsimage_0000000000000000000  fsimage_0000000000000000000.md5  seen_txid  VERSION\n[root@layne1 current]# pwd\n/var/layne/hadoop/pseudo/dfs/name/current\n\n```\n\n\n\n11、启动hadoop\n\n输入以下命令启动hadoop\n\n```bash\nstart-dfs.sh\n```\n\n启动过程如下：\n\n```bash\n[root@layne1 current]# start-dfs.sh\n21/03/16 21:19:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nStarting namenodes on [layne1]\nlayne1: starting namenode, logging to /opt/hadoop-2.6.5/logs/hadoop-root-namenode-layne1.out\nlayne1: starting datanode, logging to /opt/hadoop-2.6.5/logs/hadoop-root-datanode-layne1.out\nStarting secondary namenodes [layne1]\nlayne1: starting secondarynamenode, logging to /opt/hadoop-2.6.5/logs/hadoop-root-secondarynamenode-layne1.out\n21/03/16 21:20:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n```\n\n\n\n12、查看hadoop进程\n\n输入jps\n\n```bash\n[root@layne1 current]# jps\n1623 SecondaryNameNode\n1467 DataNode\n1389 NameNode\n1741 Jps\n```\n\n\n\n说明进程都正常启动了，然后网页访问：\n\nhttp://layne1:50070\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210316212658.png)\n\n\n\n进入文件系统\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210316213025.png)\n\n\n\n下图可以看出，文件系统为空\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210316213104.png)\n\n\n\n13、上传文件\n\n我们试着上传一个文件\n\n```bash\n[root@layne1 apps]# ll\ntotal 387548\n-rw-r--r-- 1 root root 199635269 Mar 16 19:30 hadoop-2.6.5.tar.gz\n-rw-r--r-- 1 root root 179505388 Feb 23 13:34 jdk-8u221-linux-x64.rpm\n-rw-r--r-- 1 root root  17699306 Feb 23 13:34 zookeeper-3.4.6.tar.gz\n[root@layne1 apps]# hdfs dfs -put hadoop-2.6.5.tar.gz /\n21/03/16 21:29:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n```\n\n现在可以看到上传的文件\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210316213104.png)\n\n点击文件名称，可以看到该文件被分为两个block块，第一个block为128M（没有指定block，默认大小为128M）\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210316213303.png)\n\n然后，我们自己生成一个文件\n\n```bash\n[root@layne1 apps]# pwd\n/opt/apps\n[root@layne1 apps]# for i in `seq 100000`; do echo \"hello layne $i\" >> hh.txt; done\n```\n\n\n\n上传生成的`hh.txx`文件，文件block块大小为1048576字节，重复数为1：\n\n```bash\nhdfs dfs -D dfs.blocksize=1048576 -D dfs.replication=1 -put hh.txt /\n```\n\n再次刷新，就能看到上传的文件了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210316215355.png)\n\n从上图可以看出，刚刚上传的`hh.txx`文件块大小为1M，这是因为`1024x1024=1048576`，`dfs.blocksize`单位是字节，即bytes，`1KB=1024bytes`，所以`1024x1024bytes=1048576bytes=1024KB=1M`\n\n`-D dfs.replication=1`指定副本数为1，如果不指定，默认按照第8步`dfs.replication`配置的副本数。一般来说，可以将不重要的文件的副本数设置小一点。\n\n在上传文件时，`-D dfs.blocksize`和`-D dfs.replication`可以不指定，所以上传文件的格式为：\n\n```bash\nhdfs dfs -put 被上传的文件全路径名或相对路径名 放置的hdfs目录\n```\n\n比如，`hdfs dfs -put test.txt /a/b`，就是将当前目录下的test.txt文件，上传到hdfs的`a/b`目录下，这个前提是`a/b`目录一定要存在。\n\n\n\n14、查看hdfs中的文件\n\n```bash\n[root@layne1 apps]# hdfs dfs -ls /\n21/03/16 21:55:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nFound 2 items\n-rw-r--r--   1 root supergroup  199635269 2021-03-16 21:29 /hadoop-2.6.5.tar.gz\n-rw-r--r--   1 root supergroup    1788895 2021-03-16 21:53 /hh.txt\n```\n\n当然，也可以在浏览器中查看。\n\n\n\n15、查看hadoop存储目录\n\n```bash\n[root@layne1 dfs]# pwd\n/var/layne/hadoop/pseudo/dfs\n[root@layne1 dfs]# ls\ndata  name  namesecondary\n\n```\n\n查看生成的块\n\n```bash\n[root@layne1 subdir0]# pwd\n/var/layne/hadoop/pseudo/dfs/data/current/BP-603651742-192.168.218.51-1615900416149/current/finalized/subdir0/subdir0\n[root@layne1 subdir0]# ll -h\ntotal 194M\n-rw-r--r-- 1 root root 128M Mar 16 21:29 blk_1073741825\n-rw-r--r-- 1 root root 1.1M Mar 16 21:29 blk_1073741825_1001.meta\n-rw-r--r-- 1 root root  63M Mar 16 21:29 blk_1073741826\n-rw-r--r-- 1 root root 500K Mar 16 21:29 blk_1073741826_1002.meta\n-rw-r--r-- 1 root root 1.0M Mar 16 21:53 blk_1073741827\n-rw-r--r-- 1 root root 8.1K Mar 16 21:53 blk_1073741827_1003.meta\n-rw-r--r-- 1 root root 723K Mar 16 21:53 blk_1073741828\n-rw-r--r-- 1 root root 5.7K Mar 16 21:53 blk_1073741828_1004.meta\n\n```\n\n查看datanode相关信息\n\n```bash\n[root@layne1 current]# pwd\n/var/layne/hadoop/pseudo/dfs/data/current\n[root@layne1 current]# cat VERSION \n#Tue Mar 16 21:20:02 CST 2021\nstorageID=DS-bd5deff9-13b7-4c66-bf6f-b044da77d527\nclusterID=CID-18a01d3b-2057-4277-9220-3476626cd9a8\ncTime=0\ndatanodeUuid=6e4f5a59-386e-48d2-a4dd-aa4c36b723e0\nstorageType=DATA_NODE\nlayoutVersion=-56\n\n```\n\n\n\n16、关闭hadoop\n\n```bash\nstop-dfs.sh\n```\n\n","tags":["Hadoop"],"categories":["Hadoop"]},{"title":"一致性算法2PC、3PC、Paxos、Raft、ZAB","url":"/2021/03/08/203219/","content":"\n**一致性算法2PC、3PC、Paxos、Raft、ZAB**\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n## 简述\n\n一个分布式系统可能面临多个问题：\n\n1. 消息传递异步无序: 现实网络不是一个可靠的信道，存在消息延时、丢失，节点间消息传递做不到同步有序\n2. 节点宕机: 节点持续宕机，不会恢复\n3. 节点宕机恢复: 节点宕机一段时间后恢复，在分布式系统中最常见\n4. 网络分化: 网络链路中的某个部位出现问题\n5. 拜占庭将军问题: 在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的，所以所有的一致性算法的 **必要前提** 就是安全可靠的消息通道，发出的信号不会被篡改。\n\n> 拜占庭将军问题：是指 拜占庭帝国军队的将军们必须全体一致的决定是否攻击某一支敌军。问题是这些将军在地理上是分隔开来的，只能依靠通讯员进行传递命令，但是通讯员中存在叛徒，它们可以篡改消息，叛徒可以欺骗某些将军采取进攻行动；促成一个不是所有将军都同意的决定，如当将军们不希望进攻时促成进攻行动；或者迷惑某些将军，使他们无法做出决定。\n\n而为什么要去解决数据一致性的问题？你想想，如果将网上商城购物系统拆分成了订单和积分子系统，这两个子系统部署在不同的机器上了，万一在消息的传播过程中积分系统宕机了，总不能你这边下了订单却没加积分吧？你总得保证两边的数据需要一致吧？\n\n\n\n\n\n\n\n## 一、2PC（两阶段提交）\n\n> 千万不要吧PC理解成个人电脑了，其实他们是 phase-commit 的缩写，即阶段提交。\n\n2PC(tow phase commit)两阶段提交，所谓的两个阶段是指：第一阶段：`准备阶段`，第二阶段：`提交阶段`。\n\n> 在我们所需要解决的是在分布式系统中，**整个调用链中**，我们所有服务的数据处理要么都成功要么都失败，即所有服务的 **原子性问题** 。\n\n两阶段提交是一种保证分布式系统数据一致性协议，它本身是一致强一致性算法，现在很多**数据库**都是采用的两阶段提交协议来完成 **分布式事务** 的处理。\n\n在两阶段提交中，主要涉及到两个角色，分别是协调者和参与者。\n\n第一阶段：当要执行一个分布式事务的时候，事务发起者首先向协调者发起事务请求，然后协调者会给所有参与者发送 `prepare` 请求（其中包括事务内容）告诉参与者你们需要执行事务了，如果能执行我发的事务内容那么就先执行但不提交，执行后请给我回复。然后参与者收到 `prepare` 消息后，他们会开始执行事务（但不提交），并将 `Undo` 和 `Redo` 信息记入事务日志中，之后参与者就向协调者反馈是否准备好了。\n\n第二阶段：第二阶段主要是协调者根据参与者反馈的情况来决定接下来是否可以进行事务的提交操作，即提交事务或者回滚事务。\n\n比如这个时候 **所有的参与者** 都返回了准备好了的消息，这个时候就进行事务的提交，协调者此时会给所有的参与者发送 **`Commit` 请求** ，当参与者收到 `Commit` 请求的时候会执行前面执行的事务的 **提交操作** ，提交完毕之后将给协调者发送提交成功的响应。\n\n而如果在第一阶段并不是所有参与者都返回了准备好了的消息，那么此时协调者将会给所有参与者发送 **回滚事务的 `rollback` 请求**，参与者收到之后将会 **回滚它在第一阶段所做的事务处理** ，然后再将处理情况返回给协调者，最终协调者收到响应后便给事务发起者返回处理失败的结果。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210308210829.png)\n\n\n\n优点：原理简单，实现方便\n\n缺点：`单点问题`，`同步阻塞`，，`数据不一致`，`容错性不好`\n\n\n\n- **单点故障问题**，如果协调者挂了那么整个系统都处于不可用的状态了。\n- **阻塞问题**，在二阶段提交的过程中，整体链路所有的节点都在等待其他节点的响应，无法进行其他操作。这种同步阻塞极大的限制了分布式系统的性能。\n- **数据不一致问题**，比如当第二阶段，协调者只发送了一部分的 `commit` 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题。\n- **容错性不好**，二阶段提交协议没有设计较为完善的容错机制，任意一个节点是失败都会导致整个事务的失败。\n\n\n\n\n\n## 二、3PC\n\n三阶段提交（Three-phase commit），是二阶段提交（2PC）的改进版本。与两阶段提交不同的是，三阶段提交有两个改动点。\n\n1. `引入超时机制`。同时在协调者和参与者中都引入超时机制。\n2. 在第一阶段和第二阶段中插入一个`准备阶段`，保证了在最后提交阶段之前各参与节点的状态是一致的。也就是说，除了引入超时机制之外，**3PC把2PC的准备阶段再次一分为二**，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。\n\n> 3PC是弱一致性算法，因为它引入超时时间，系统中的所有数据副本经过一定时间后，最终才能能够达到一致的状态\n\n各个阶段的执行过程如下：\n\n1. **CanCommit阶段**：协调者向所有参与者发送 `CanCommit` 请求，参与者收到请求后会根据自身情况查看是否能执行事务，如果可以则返回 YES 响应并进入预备状态，否则返回 NO 。\n2. **PreCommit阶段**：协调者根据参与者返回的响应来决定是否可以进行下面的 `PreCommit` 操作。如果上面参与者返回的都是 YES，那么协调者将向所有参与者发送 `PreCommit` 预提交请求，**参与者收到预提交请求后，会进行事务的执行操作，并将 `Undo` 和 `Redo` 信息写入事务日志中** ，最后如果参与者顺利执行了事务则给协调者返回成功的响应。如果在第一阶段协调者收到了 **任何一个 NO** 的信息，或者 **在一定时间内** 并没有收到全部的参与者的响应，那么就会中断事务，它会向所有参与者发送中断请求（abort），参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，它也会中断事务。\n3. **DoCommit阶段**：这个阶段其实和 `2PC` 的第二阶段差不多，如果协调者收到了所有参与者在 `PreCommit` 阶段的 YES 响应，那么协调者将会给所有参与者发送 `DoCommit` 请求，**参与者收到 `DoCommit` 请求后则会进行事务的提交工作**，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。若协调者在 `PreCommit` 阶段 **收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应** ，那么就会进行中断请求的发送，参与者收到中断请求后则会 **通过上面记录的回滚日志** 来进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210308212105.png)\n\n上图是 `3PC` 成功运行时的流程图，可以看到 `3PC` 在很多地方进行了超时中断的处理，比如协调者在指定时间内为收到全部的确认消息则进行事务中断的处理，这样能 **减少同步阻塞的时间** 。还有需要注意的是，**`3PC` 在 `DoCommit` 阶段参与者如未收到协调者发送的提交事务的请求，也会在一定时间内进行事务的提交**。为什么这么做呢？是因为这个时候我们肯定**保证了在第一阶段所有的协调者全部返回了可以执行事务的响应**，这个时候我们有理由**相信其他系统都能进行事务的执行和提交**，所以**不管**协调者有没有发`DoCommit` 请求给参与者，进入第三阶段参与者都会进行事务的提交操作。\n\n总之，`3PC` 通过一系列的超时机制很好的缓解了阻塞问题，但是最重要的一致性并没有得到根本的解决，比如由于网络原因，协调者发送的abort中断请求没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort请求并执行回滚的参与者之间存在数据不一致的情况。\n\n\n\n所以，要解决一致性问题还需要靠 `Paxos` 算法\n\n\n\n## 三、Paxos 算法\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210308220007.png)\n\n该算法的提出者莱斯利·兰伯特在前面几篇论文中都不是以严谨的数学公式进行的。其实这个paxos算法也分成两阶段。\n\n`Paxos` 算法是基于**消息传递且具有高度容错特性的一致性算法**，是目前公认的解决分布式一致性问题最有效的算法之一，**其解决的问题就是在分布式系统中如何就某个值（决议）达成一致** 。这 个“值”可能是一个数据的某，也可能是一条LOG等；根据不同的应用环境这个“值”也不同。在 `Paxos` 中主要有三个角色，分别为 `Proposer提案者`、`Acceptor表决者`、`Learner学习者`。\n\n\n\n### 3.1 第一阶段：prepare 阶段\n\n\n\n`Proposer提案者`选择一个**具有全局唯一性的、递增的提案编号N**，即在整个集群中是唯一的编号 N，在**第一阶段是只将编号为N的Prepare请求发送给所有的表决者**。\n\n如果一个`Acceptor表决者`收到一个编号为N的Prepare请求，如果N小于它已经响应过的提案编号，则拒绝或不响应。若N大于该Acceptor已经响应过的所有Prepare请求的编号（maxN），那么Acceptor会将**以前接受过的最大编号**的提案作为响应反馈给 `Proposer` ，**同时该Acceptor承诺不再接受任何编号小于N的提案**。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210308224156.png)\n\n### 3.2 第二阶段：accept 阶段\n\n如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么此时 `Proposer` 会给所有的 `Acceptor` 发送真正的提案（你可以理解为第一阶段为试探），这个时候 `Proposer` 就会发送提案的内容和提案编号。\n\n表决者收到提案请求后会比较本身已经接受过的最大提案编号和该提案编号，如果该提案编号 **大于等于** 已经接受过的最大提案编号，那么就 `accept` 该提案（此时执行提案内容但不提交），随后将情况返回给 `Proposer` 。如果不满足则不回应或者返回 NO 。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210308224748.png)\n\n当 `Proposer` 收到超过半数的 `accept` ，那么它这个时候会向所有的 `acceptor` 发送提案的**提交请求**。需要注意的是，因为上述仅仅是超过半数的 `acceptor` 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要**向未批准的 `acceptor` 发送提案内容和提案编号并让它无条件执行和提交**，而对于前面已经批准过该提案的 `acceptor` 来说 **仅仅需要发送该提案的编号** ，让 `acceptor` 执行提交就行了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210308225001.png)\n\n而如果 `Proposer` 如果没有收到超过半数的 `accept` 那么它将会将 **递增** 该 `Proposal` 的编号，然后 **重新进入 `Prepare` 阶段** 。\n\n\n\nLearner学习被选定的第二阶段达成的某个值（决议），这里我们称之为Value。Learner学习方式有三种方案：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309095818.png)\n\n\n\n\n\n### 3.3 paxos 算法的死循环问题\n\n> 有的人说是活锁，我认为是死锁，因为比较两个proposer都没有完成Paxos的第二阶段，一直在重复自增提案编号，导致谁都无法最终完成提案，即造成两个进程争夺资源（这里提案编号资源），互相等待对方的资源，谁也不肯放弃，从而造成死锁。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309094653.png)\n\n\n\n选择一个主Proposer，并规定只有主Proposer才能提出议案。这样一来，只要主Proposer和过半的Acceptor能够正常进行网络通信，那么肯定会有一个提案被批准（第二阶段的accept），则可以解决死循环导致的活锁问题。\n\nPaxos是基于消息传递的具有高度容错性的分布式一致性算法。**Paxos算法引入了过半的概念，解决了2PC，3PC的太过保守的缺点，且使算法具有了很好的容错性**，另外**Paxos算法支持分布式节点角色之间的转换**，这极大避免了分布式单点问题的出现，因此Paxos算法既解决了无限等待问题，也解决了脑裂问题，是目前来说最优秀的分布式一致性算法。其中，Zookeeper的ZAB算法和Raft一致性算法都是基于Paxos的。\n\n\n### 3.4 举例说明\n\n假设：只有User1、User2、User3 三个人决定1+1等于几！\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309100031.png)\n\n\n\n**第一阶段**\n\n1. User1 提案编号为 1 并发送给User2和User3。\n\n因User2 和User3 并没有接受过小于编号为1的提案，所以它们可以接受该提议，并反馈给User1 不再接受小于编号1的提案。这时User1收到多数人的回复，将进入**第2阶段**。（如果收到的回复并不能形成多数人，那么将再次进入阶段1）\n\n2. User2 提案编号为2 ，并发送给User1和User3。‘\n\nUser1第一次收到提案，它并没有同意过小于编号为2的提议，所以它可以接受该提议。User3由于接受过User1编号为1的提案，但User2的提案编号 2 > 1 所以User3也可以同意User2的提议，并反馈不再接受小于2的提议。User2也收到多数人的回复，将进行**第2阶段**。\n\n3. User3提案编号为3 ，并发送给user1 和user2 .\n\n因user1收到user3编号为3的提案 > user2编号为2的提案，所以接受user3的提案。因user2收到User3编号为3的提案 > user1 编号为1的提案，所以接受user3的提案。至此user3也收到多数人回复，将进行第2阶段。\n\n**第二阶段**\n\n1. user1 发送编号为1的提议，提议内容为：1+1=1；并发送给user2和User3 。\n\n由于user2已经声明不再接受小于3的提案，所以拒绝user1的提案。由于User3已经声明不再接受小于2的提案，所以同样拒绝User1的提案。User1提议被多数人拒绝，再次进入阶段1\n\n2. user2 发送编号为2的提议，提议内容为：1+1=2；并发送给User1和User3\n\n由于User1已经声明不再接受小于3的提案，所以拒绝user2的提议。由于User3已经声明不再接受小于2的提案，该提案编号=2所以user3同意User2的提议。但User2并没有获得多数人的同意，所以`同样进行阶段1`.\n\n3. User3 发送编号为3的提议，提议内容为：1+1=3；并发送给User1和User2;\n\n由于 user1 声明不再接受小于3的提案，所以同意User3的提议。由于 user2 声明不再接受小于3的提案，所以同意User3的提议。\n\n至此最终User3可以获得多数人的同意。\n\n**虽然上面没有说自己同意自己，但我认为自己可以同意自己，毕竟如果有三台机器，总不可能让另外两台都同意吧，这不就成了在投票阶段3台决定Leader了。**\n\n\n\n## 四、Raft\n\nRaft 也是一个一致性算法，和 Paxos 目标相同。但他还有另一个名字：易于理解的一致性算法。也就是说，他的目标就是成为一个易于理解的一致性算法。以替代 Paxos 的晦涩难懂。\n\n\n\n**什么是 Raft 算法**\n\n首先说什么是 Raft 算法：**Raft 是一种为了管理复制日志的一致性算法**。\n\n什么是一致性呢？\n\n>  Raft 的论文这么说的：**一致性算法允许一组机器像一个整体一样工作，即使其中一些机器出现故障也能够继续工作下去**。\n\n这里的一致性针对分布式系统。\n\n**领导人选举**\n\nRaft 通过选举一个高贵的领导人，然后给予他全部的管理复制日志的责任来实现一致性。\n\n而每个 server 都可能会在 3 个身份之间切换：\n\n> 领导者 候选者 跟随者\n\n而影响他们身份变化的则是 选举。当所有服务器初始化的时候，都是 跟随者，这个时候需要一个 领导者，所有人都变成 候选者，直到有人成功当选 领导者。角色轮换如下图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309101121.png)\n\n而领导者也有宕机的时候，宕机后引发新的 选举，所以，整个集群在选举和正常运行之间切换，具体如下图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309101153.png)\n\n从上图可以看出，选举和正常运行之间切换，但请注意， 上图中的 term 3 有一个地方，后面没有跟着 正常运行 阶段，为什么呢?\n\n答：当一次选举失败（比如正巧每个人都投了自己），就执行一次 **加时赛**，每个 Server 会在一个随机的时间里重新投票，这样就能保证不冲突了。所以，当 term 3 选举失败，等了几十毫秒，执行 term 4 选举，并成功选举出领导人。\n\n接着，领导者**周期性**的向所有跟随者发送心跳包来维持自己的权威。如果一个跟随者在一段时间里没有接收到任何消息，也就是选举超时，那么他就会认为系统中没有可用的领导者,并且发起选举以选出新的领导者。\n\n要开始一次选举过程，跟随者先要增加自己的当前任期号并且转换到候选人状态。然后请求其他服务器为自己投票。那么会产生 3 种结果：\n\na. 自己成功当选 b. 其他的服务器成为领导者 c. 僵住，没有任何一个人成为领导者\n\n注意：\n\n每一个 server 最多在一个任期内投出一张选票（有任期号约束），先到先得。要求最多只能有一个人赢得选票。一旦成功，立即成为领导人，然后广播所有服务器停止投票阻止新得领导产生。僵住怎么办？Raft 通过使用随机选举超时时间（例如 150 - 300 毫秒）的方法将服务器打散投票。每个候选人在僵住的时候会随机从一个时间开始重新选举。以上，就是 Raft 所有关于领导选举的策略。\n\n**日志复制**\n\n一旦一个领导人被选举出来，他就开始为客户端提供服务。客户端发送日志给领导者，随后领导者将日志复制到其他的服务器。如果跟随者故障，领导者将会尝试重试。直到所有的跟随者都成功存储了所有日志。\n\n下图表示了当一个客户端发送一个日志给领导者，随后领导者复制给跟随者的整个过程\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309101559.png)\n\n4 个步骤：\n\n1. 客户端提交\n2. 复制数据到所有跟随者\n3. 跟随者回复 确认收到\n4. 领导者回复客户端和所有跟随者 确认提交。\n\n可以看到，直到第四步骤，整个事务才会达成。中间任何一个步骤发生故障，都不会影响日志一致性。\n\n\n\nRaft算法具备强一致、高可靠、高可用等优点，具体体现在：\n\n`强一致性`：虽然所有节点的数据并非实时一致，但Raft算法保证Leader节点的数据最全，同时所有请求都由Leader处理，所以在客户端角度看是强一致性的。\n\n`高可靠性`：Raft算法保证了Committed的日志不会被修改，State Matchine只应用Committed的日志，所以当客户端收到请求成功即代表数据不再改变。Committed日志在大多数节点上冗余存储，少于一半的磁盘故障数据不会丢失。\n\n`高可用性`：从Raft算法原理可以看出，选举和日志同步都只需要大多数的节点正常互联即可，所以少量节点故障或网络异常不会影响系统的可用性。即使Leader故障，在选举超时到期后，集群自发选举新Leader，无需人工干预，不可用时间极小。但Leader故障时存在重复数据问题，需要业务去重或幂等性保证。\n\n`高性能`：与必须将数据写到所有节点才能返回客户端成功的算法相比，Raft算法只需要大多数节点成功即可，少量节点处理缓慢不会延缓整体系统运行。\n\n\n\n## 五、ZAB\n\n作为一个优秀高效且可靠的分布式协调框架，`ZooKeeper` 在解决分布式数据一致性问题时并没有直接使用 `Paxos` ，而是专门定制了一致性协议叫做 `ZAB(ZooKeeper Automic Broadcast)` 原子广播协议，该协议能够很好地支持 **崩溃恢复** 。\n\n### 5.1 ZAB 中的三个角色\n\n ZAB 中三个主要的角色，Leader 领导者、Follower跟随者、Observer观察者 。\n\n- `Leader` ：集群中 **唯一的写请求处理者** ，能够发起投票（投票也是为了进行写请求）。\n- `Follower`：能够接收客户端的请求，如果是读请求则可以自己处理，**如果是写请求则要转发给 `Leader`** 。在选举过程中会参与投票，**有选举权和被选举权** 。\n- `Observer` ：就是没有选举权和被选举权的 `Follower` 。\n\n在 `ZAB` 协议中对 `zkServer`(即上面我们说的三个角色的总称) 还有两种模式的定义，分别是 **消息广播** 和 **崩溃恢复** 。\n\n\n\n### 5.2 ZXID和myid\n\n\n\n**ZooKeeper** 采用全局递增的事务 id 来标识，所有 proposal(提议)在被提出的时候加上了**ZooKeeper Transaction Id** 。ZXID是64位的Long类型，**这是保证事务的顺序一致性的关键**。ZXID中高32位表示纪元**epoch**，低32位表示事务标识**xid**。你可以认为zxid越大说明存储数据越新，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309202934.png)\n\n\n\n1. 每个leader都会具有不同的**epoch**值，表示一个纪元/朝代，用来标识 leader周期。每个新的选举开启时都会生成一个新的epoch，从1开始，每次选出新的Leader，epoch递增1，并会将该值更新到所有的zkServer的zxid的epoch。\n2. **xid**是一个依次递增的事务编号。数值越大说明数据越新，可以简单理解为递增的事务id。**每次epoch变化，都将低32位的序号重置**，这样保证了zxid的全局递增性。\n\n\n\n每个ZooKeeper服务器，都需要在数据文件夹下创建一个名为myid的文件，该文件包含整个ZooKeeper集群唯一的id（整数）。例如，某ZooKeeper集群包含三台服务器，hostname分别为zoo1、zoo2和zoo3，其myid分别为1、2和3，则在配置文件中其id与hostname必须一一对应，如下所示。在该配置文件中，`server.`后面的数据即为myid\n\n```tex\nserver.1=zoo1:2888:3888\nserver.2=zoo2:2888:3888\nserver.3=zoo3:2888:3888\n```\n\n\n\n\n\n### 5.3 历史队列\n\n每一个follower节点都会有一个**先进先出**（FIFO)的队列用来存放收到的事务请求，保证执行事务的顺序。所以：\n\n- 可靠提交由ZAB的事务一致性协议保证\n- 全局有序由TCP协议保证\n- 因果有序由follower的历史队列(history queue)保证\n\n\n\n### 5.4 消息广播模式\n\nZAB协议两种模式：消息广播模式和崩溃恢复模式。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309204307.png)\n\n\n\n说白了就是 `ZAB` 协议是如何处理写请求的，上面我们不是说只有 `Leader` 能处理写请求嘛？那么我们的 `Follower` 和 `Observer` 是不是也需要 **同步更新数据** 呢？总不能数据只在 `Leader` 中更新了，其他角色都没有得到更新吧。\n\n第一步肯定需要 `Leader` 将写请求 **广播** 出去呀，让 `Leader` 问问 `Followers` 是否同意更新，如果超过半数以上的同意那么就进行 `Follower` 和 `Observer` 的更新（和 `Paxos` 一样）。消息广播机制是通过如下图流程**保证事务的顺序一致性**的：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309205602.png)\n\n\n\n1. leader从客户端收到一个写请求\n2. leader生成一个新的事务并为这个事务生成一个唯一的ZXID\n3. leader将这个事务发送给所有的follows节点，将带有 zxid 的消息作为一个提案(proposal)分发给所有 follower。\n4. follower节点将收到的事务请求加入到历史队列(history queue)中，当 follower 接收到 proposal，先将 proposal 写到硬盘，写硬盘成功后再向 leader 回一个 ACK\n5. 当leader收到大多数follower（超过一半）的ack消息，leader会向follower发送commit请求（leader自身也要提交这个事务）\n6. 当follower收到commit请求时，会判断该事务的ZXID是不是比历史队列中的任何事务的ZXID都小，如果是则提交事务，如果不是则等待比它更小的事务的commit(保证顺序性)\n7. Leader将处理结果返回给客户端\n\n**过半写成功策略**：Leader节点接收到写请求后，这个Leader会将写请求广播给各个Server，各个Server会将该写请求加入历史队列，并向Leader发送ACK信息，当Leader收到一半以上的ACK消息后，说明该写操作可以执行。Leader会向各个server发送commit消息，各个server收到消息后执行commit操作。\n\n这里要注意以下几点：\n\n- Leader并不需要得到Observer的ACK，即Observer无投票权\n- Leader不需要得到所有Follower的ACK，只要收到过半的ACK即可，**同时Leader本身对自己有一个ACK**\n- Observer虽然无投票权，但仍须同步Leader的数据从而在处理读请求时可以返回尽可能新的数据\n\n另外，Follower/Observer也可以接受写请求，此时：\n\n- Follower/Observer接受写请求以后，不能直接处理，而需要将写请求转发给Leader处理\n- 除了多了一步请求转发，其它流程与直接写Leader无任何区别\n- Leader处理写请求是通过上面的消息广播模式，实质上最后所有的zkServer都要执行写操作，这样数据才会一致\n\n而对于读请求，Leader/Follower/Observer都可直接处理读请求，从本地内存中读取数据并返回给客户端即可。由于处理读请求不需要各个服务器之间的交互，因此Follower/Observer越多，整体可处理的读请求量越大，也即读性能越好。\n\n\n\n\n\n### 5.5 崩溃恢复模式\n\n\n\n恢复模式大致可以分为四个阶段：选举、发现、同步、广播。\n\n1. **选举阶段**（Leader election）：当leader崩溃后，集群进入选举阶段（下面会将如何选举Leader），开始选举出潜在的准 leader，然后进入下一个阶段。\n2. **发现阶段**（Discovery）：用于在从节点中发现最新的ZXID和事务日志。准Leader接收所有Follower发来各自的最新epoch值。Leader从中选出最大的epoch，基于此值加1，生成新的epoch分发给各个Follower。各个Follower收到全新的epoch后，返回ACK给Leader，带上各自最大的ZXID和历史提议日志。Leader选出最大的ZXID，并更新自身历史日志，此时Leader就用拥有了最新的提议历史。（注意：每次epoch变化时，ZXID的第32位从0开始计数）。\n3. **同步阶段**（Synchronization）：主要是利用 leader 前一阶段获得的最新提议历史，同步给集群中所有的Follower。只有当超过半数Follower同步成功，这个准Leader才能成为正式的Leader。这之后，follower 只会接收 zxid 比自己的 lastZxid 大的提议。\n4. 广播阶段（Broadcast）：集群恢复到广播模式，开始接受客户端的写请求。\n\n> 在发现阶段，或许有人会问：既然Leader被选为主节点，已经是集群里数据最新的了，为什么还要从节点中寻找最新事务呢？这是为了防止某些意外情况。所以这一阶段，Leader集思广益，接收所有Follower发来各自的最新epoch值。\n\n\n\n这里有两点要注意：\n\n（1）**确保已经被Leader提交的提案最终能够被所有的Follower提交**\n\n假设 `Leader (server2)` 发送 `commit` 请求（忘了请看上面的消息广播模式），他发送给了 `server3`，然后要发给 `server1` 的时候突然挂了。这个时候重新选举的时候我们如果把 `server1` 作为 `Leader` 的话，那么肯定会产生数据不一致性，因为 `server3` 肯定会提交刚刚 `server2` 发送的 `commit` 请求的提案，而 `server1` 根本没收到所以会丢弃。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309214242.png)\n\n那怎么解决呢？\n\n**这个时候 `server1` 已经不可能成为 `Leader` 了，因为 `server1` 和 `server3` 进行投票选举的时候会比较 `ZXID` ，而此时 `server3` 的 `ZXID` 肯定比 `server1` 的大了**（后面讲到选举机制时就明白了）。同理，只能由server3当Leader，server3当上Leader之后，在同步阶段，会将最新提议历史同步给集群中所有的Follower，这就保证数据一致性了。如果server2在某个时刻又重新恢复了，它作为`Follower` 的身份进入集群中，再向Leader同步当前最新提议和Zxid即可。\n\n（2）**确保跳过那些已经被丢弃的提案**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309215027.png)\n\n假设 `Leader (server2)` 此时同意了提案N1，自身提交了这个事务并且要发送给所有 `Follower` 要 `commit` 的请求，却在这个时候挂了，此时肯定要重新进行 `Leader` 的选举，假如此时选 `server1` 为 `Leader` （这无所谓，server1和server2都可以当选）。但是过了一会，这个 **挂掉的 `Leader` 又重新恢复了** ，此时它肯定会作为 `Follower` 的身份进入集群中，需要注意的是刚刚 `server2` 已经同意提交了提案N1，但其他 `server` 并没有收到它的 `commit` 信息，所以其他 `server` 不可能再提交这个提案N1了，这样就会出现数据不一致性问题了，所以 **该提案N1最终需要被抛弃掉** 。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309215338.png)\n\n\n\n### 5.6 脑裂问题\n\n脑裂问题：所谓的“脑裂”即“大脑分裂”，也就是本来一个“大脑”被拆分了两个或多个“大脑”。通俗的说，就是比如当你的 cluster 里面有两个节点，它们都知道在这个 cluster 里需要选举出一个 master。那么当它们两之间的通信完全没有问题的时候，就会达成共识，选出其中一个作为 master。但是如果它们之间的通信出了问题，那么两个结点都会觉得现在没有 master，所以每个都把自己选举成 master，于是 cluster 里面就会有两个 master。\n\nZAB为解决脑裂问题，要求集群内的节点数量为2N+1, 当网络分裂后，始终有一个集群的节点数量过半数，而另一个集群节点数量小于N+1（即小于半数）, 因为选主需要过半数节点同意，所以任何情况下集群中都不可能出现大于一个leader的情况。\n\n因此，有了过半机制，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题。\n\n\n\n\n\n【参考资料】\n\n1. https://snailclimb.gitee.io/javaguide/#/docs/system-design/distributed-system/zookeeper/zookeeper-plus\n2. https://mp.weixin.qq.com/s/b5mGEbn-FLb9vhOh1OpwIg\n\n\n\n","tags":["zookeeper","ZAB"],"categories":["zookeeper"]},{"title":"Java的System.out.println有多慢，你知道吗？","url":"/2021/03/06/201454/","content":"\n本文带你探索Java的System.out.println有多慢！\n<!-- more -->\n\n\n\n今天刷一道算法题提交后发现，运行时间Timeout，本以为是个在正常不过的现象，却让我发现了一个隐藏的”宝藏“。\n\n我设计的算法时间复杂度是$O(n)$，提交后竟然时间超时，难道还有比这更好的算法吗？通过查看已经通过的其他人的代码，发现其他人的设计思路和我差不多，有的甚至还不如我，但为什么他们能够通过，而我就不能呢？\n\n经过仔细检查和对比后发现，这完全是由`System.out.println`造成的。\n\n现在让我们进行试验，来探明其中的究竟。\n\n**实验一**：打印5000条数据到控制台，输出其运行的时间\n\n```java\nlong startTime=System.currentTimeMillis();\nfor(int i=0;i<5000;i++){\n    System.out.print(i+\" \");\n}\nlong endTime=System.currentTimeMillis();\nSystem.out.println(\"\\n总共花费\"+(endTime-startTime)+\"ms\");\n```\n\n输出：**<font color=\"red\">总共花费110ms</font>**\n\n**实验二**：通过StringBuilder，一次性打印这5000条数据\n\n```java\nlong startTime=System.currentTimeMillis();\nStringBuilder strBuf=new StringBuilder();\nfor(int i=0;i<5000;i++){\n    strBuf.append(i+\" \");\n}\nSystem.out.print(strBuf.toString());\nlong endTime=System.currentTimeMillis();\nSystem.out.println(\"\\n总共花费\"+(endTime-startTime)+\"ms\");\n```\n\n输出：**<font color=\"red\">总共花费11ms</font>**\n\n这时候你可能会说：“纳尼？实现同样的效果，实验二竟然比实验一快11倍”。\n\n是的，的确如此，现在你知道我的算法为什么那么慢了吧！\n\n那么，为什么会出现这个现象呢？经过查阅资料后发现，`System.out.println`底层执行的是write系统调用接口。我们知道，系统调用会涉及到用户态和内核态的转换，用户进程需要把其缓冲区中的数据拷贝到内核态中才能执行系统调用，执行完毕后还要返回到用户态，这就会产生一定的时间开销。所以实验一相当于执行了5000次系统调用，而实验二仅仅执行了一次系统调用，故而产生上面的结果。\n\n所以，要减少代码的执行时间，尽可能的减少`System.out.println`的执行次数是必要的，可通过实验二的方式输出需要的格式。\n\n到了这里，你可能会想Java的Scanner的执行效率如何？它会不会产生系统调用，让我们再做两个实验！\n\n**实验三**：依次输入5000个数据，输出其执行时间\n\n```java\nScanner sc=new Scanner(System.in);\nint len=sc.nextInt();\n//一定要从这里开始计时\nlong startTime=System.currentTimeMillis();\nint[] arr=new int[len];\nfor(int i=0;i<len;i++)\n    arr[i]=sc.nextInt();\nlong endTime=System.currentTimeMillis();\nSystem.out.println(\"\\n总共花费\"+(endTime-startTime)+\"ms\");\t\n```\n\n输入(限于篇幅，下面5000条没写完全)：\n\n```tex\n5000\n0 1 2 3 4 5 6 7 8 9 ... 4997 4998 4999 \n```\n\n输出：**<font color=\"red\">总共花费1076ms</font>**\n\n实验四：一次性读入5000条数据，输出其执行时间\n\n```java\nBufferedReader bufferedReader = new BufferedReader(new InputStreamReader(System.in));\nint len = Integer.parseInt(bufferedReader.readLine());\nlong startTime=System.currentTimeMillis();\nString[] readLine = bufferedReader.readLine().trim().split(\" \");\nint[] arr = Arrays.asList(readLine).stream().mapToInt(Integer::parseInt).toArray();\nbufferedReader.close();\nlong endTime=System.currentTimeMillis();\nSystem.out.println(\"\\n总共花费\"+(endTime-startTime)+\"ms\");\t\n```\n\n输入：同实验三。\n\n输出：**<font color=\"red\">总共花费2419ms</font>**\n\n你没有看错，一次性读入所有数据执行时间反而变得更长了。我认为，应该是实验四第五行的`Arrays.asList`函数导致其变慢，现在去掉实验四的第五行代码再次执行，输出**<font color=\"red\">总共花费739ms</font>**。这就符合我们的预期了，但其实也没有快多少是吧。\n\n但是，一般情况下，如果你采用实验四的方式输入算法的测试用例，你很可能会用到`Arrays.asList`将输入的数据转化为int数组，这样才能进行后续的逻辑运算。所以，在实现同等效果的情况下，实验三整体上更优。\n\n回到刚刚的问题，Java的Scanner会产生系统调用吗？答案是肯定会的，因为用户进程是无法直接操作硬件设备的，需要相应的系统调用间接操作。当你从键盘上输入数据后，用户进程会调用read这个系统调用接口，转入内核态从而读取键盘的数据。\n\n所以，当你的算法运行超时后，不要一味地去优化算法本身的逻辑，可能换一种输出方式，算法的执行时间就缩短了N倍。\n\n一般来说，按照一定的格式输出数据时，采用实验二的方式；输入数据时，仍然按照传统的实验三来输入，不要想着用实验四一次性读入所有的数据，除非你读取之后就是需要的格式，不用额外的转化。\n\n\n\n","tags":["Java"],"categories":["Java"]},{"title":"Java简单数据类型和封装类中的equals和\"==\"","url":"/2021/03/06/160554/","content":"\nJava简单数据类型和封装类中的equals和\"==\"\n<!-- more -->\n\n\n\n## 概述\n\njava中的数据类型，可分为两种：\n\n1. 基本数据类型，也称原始数据类型。byte,shrot,char,int,long,float,double,boolean（存储在内存中的堆栈（以后简称栈））\n\n   - **他们之间的比较，应用双等号（==)，比较 的是他们的值**。\n\n2. 引用类型（类，复合数据类型）（在栈中仅仅是存储引用类型的变量的地址，而其本身则存储在堆中）\n\n   - **当他们用（==）进行比较的，比较的是他们在内存中的存放地址，（即栈中的内容是否相等）所以，除非是同一个new出来的对象，他们的比较后的 结果为true，否则比较后的结果为false。重写后的equals操作表示的两个变量是否是对同一个对象的引用（即堆中的内容是否相等）**\n\n     \n\njava当中所有的类都是继承于Object这个超类的，在Object超类中的定义了一个equals的方法，这个方法的初始行为是用于检测一个对象是否等于另外一个对象，它将判断两个对象是否具有相同的引用（对象的内存地址即存放在栈中的地址，也是用==进行比较），如果两个对象的具有相同的引用，他们一定是相等的。然而，经常需要检测两个对象的内容是否相等，所以在一些类库中这个方法被重写（覆盖)掉了，String,Integer,Date在这些类中的equals有其自身的实现，而不再是比较类在栈内存中的存放地址了，而是比较该地址所指向的真实内容是否相等。\n\n\n\n## 简单数据类型和封装类中的equals和\"==\"\n\nJava为每一个简单数据类型提供了一个封装类，每个基本数据类型可以封装成对象类型。\n除int（Integer）和char（Character），其余类型首字母大写即成封装类类型名。double (Double)，float(Float),long(Long), short(Short),byte(Byte),boolean(Boolean)。\n\n **以int和Integer为例说明**\n\nJava中int和Integer区别如下：\n\n1. int是基本的数据类型,默认值可以为0\n2. Integer是int的封装类,默认值为null\n3. int和Integer都可以表示某一个数值\n\n**“equals”比较**\n\nInteger的equals(Object obj)方法，在equals(Object obj)方法中，会先判断参数中的对象obj是否是Integer同类型的对象，如果是则判断值（即地址指向的真实内容）是否相同，值相同则返回true，值不同则返回false，如果obj不是Integer类的对象，则返回false。\n需要注意的是：当参数是基本类型int时，编译器会给int自动装箱成Integer类，然后再进行比较。\n\n1. 基本类型(值类型)之间无法使用equals比较。\n2. equals参数为值类型，则参数会进行自动装箱为包装类型，之后请参见第3点。\n3.  equals参数为包装类型，则先比较是否为同类型，非同类型直接返回false，同类型再比较值。\n\n例如：\n\n1. `new Long(0).equals(0)` 为 false，equals参数默认为int类型，装箱为Integer类型，不同类型直接返回false\n2. `new Integer(500).equals(500)` 为 true，equals参数默认为int类型，装箱为Integer类型，相同类型再比较值返回true\n3. `new Integer(500).equals((short)500)` 为 false，equals参数为short类型，装箱为Short类型，不同类型直接返回false\n4. `new Long(0).equals(0L) `为 true，equals参数为long类型，装箱为Long类型，相同类型再比较值返回true\n\n**“==”比较**\n\n1. 基本类型之间互相比较：以值进行比较\n2. 一边是基本类型，一边是包装类型\n    1) 同类型的进行比较，如Integer 与int，Long与long进行==比较时，**会自动拆箱比较值**\n    2) 不同类型之间进行比较，则会自动拆箱，且会进行自动向上转型再比较值（低级向高级是隐式类型转换如：byte<short<int<long<float<double，**高级向低级必须强制类型转换**）\n3. 两边都是包装类型则直接比较引用地址，但是要注意IntegerCache除外。\n\n\n\n## IntegerCache 缓存\n\nJAVA的Integer有IntegerCache会缓存-128~127之间的对象。\n\n如：Integer x = 100，会调用Integer的valueOf()方法，这个方法就是返回一个Integer对象，但是在返回前，作了一个判断，判断要赋给对象的值是否在[-128,127]区间中，且IntegerCache（是Integer类的内部类，里面有一个Integer对象数组，用于存放已经存在的且范围在[-128,127]中的对象）中是否存在此对象，如果存在，则直接返回引用，否则，创建一个新对象返回。\n\n```java\nInteger i02 = 59;\nInteger i03 = Integer.valueOf(59);\nInteger i04 = new Integer(59);\n     \nSystem.out.println(i02 == i03);  //true 因为59位于缓存区间直接从缓存中获取\nSystem.out.println(i02 == i04);  //false\nSystem.out.println(i03 == i04);  //false\n```\n\n再看一些例子\n\n```java\nInteger i02 = 200;\nInteger i03 = Integer.valueOf(200);\nInteger i04 = new Integer(200);\n         \nSystem.out.println(i02 == i03);  //false 因为200超出缓存区间从新创建对象\nSystem.out.println(i02 == i04);  //false\nSystem.out.println(i03 == i04);  //false\n```\n\nint与Integer\n\n```java\nint a=55,b=201;\nInteger at=55,bt=201;\nSystem.out.println(a==at); //true 自动拆箱，和 IntegerCache 没关系\nSystem.out.println(b==bt); //同上\n```\n\n\n\n【参考资料】\n\n1. https://blog.csdn.net/qq_35807136/article/details/52189363\n2. https://www.cnblogs.com/mrhgw/p/10449391.html\n\n\n\n","tags":["Java"],"categories":["Java"]},{"title":"idea常见错误及解决方案","url":"/2021/03/04/232942/","content":"\n我见过的所有idea常见错误及解决方案。\n\n\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. idea右键java源代码目录看不到servlet选项\n\n\n\n在pom.xml, 添加servletjar包依赖，然后同步一下pom.xml的配置，右键java源代码目录就可以看到servlet选项了。\n\n```xml\n    <dependency>\n      <groupId>javax.servlet</groupId>\n      <artifactId>javax.servlet-api</artifactId>\n      <version>3.1.0</version>\n    </dependency>\n```\n\n\n\n\n\n## 2. idea创建项目后报错：找不到或无法加载主类\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210304233447.png)\n\n解决方法：\n\n下面配置中的jre，要选中系统中安装的jdk路径\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210304233552.png)\n\n\n\n\n\n## 3. IntelliJ IDEA新建Gradle项目启动404\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210304234503.png)\n\n参考：[https://www.tqwba.com/x_d/jishu/76473.html](https://www.tqwba.com/x_d/jishu/76473.html)\n\n[https://blog.csdn.net/YlanHds/article/details/79883300](https://blog.csdn.net/YlanHds/article/details/79883300)\n\n[https://www.pianshen.com/article/6692177453/](https://www.pianshen.com/article/6692177453/)\n\nfile->setting->build,exevution,development->maven->import 有个配置Store generated project files externally （在外部存储生成的项目文件）选中后，将不会在.idea目录中生成.iml文件和library文件。不选中将会生成。试了一下果然如此（很开心。。）\n\n按老外的说法就是选中后，有利于通过版本控制去共享。因为生成的文件共享出去毫无意义！ \n\n","tags":["idea"],"categories":["工具"]},{"title":"idea创建maven项目过慢终极解决方案","url":"/2021/03/04/222520/","content":"\nidea创建maven项目过慢终极解决方案。\n\n<!-- more -->\n\n\n\n通过idea创建maven项目的时候，需要等好长时间才能创建完毕，我摸索了很久，可以通过以下方式加快maven项目创建的速度。\n\n\n\n## 使用阿里云镜像\n\n在maven的`conf\\settings.xml`中新加一个阿里云的镜像地址：\n\n```xml\n<mirror>\n            <id>alimaven</id>\n            <mirrorOf>central</mirrorOf>\n            <name>aliyun maven</name>\n            <url>http://maven.aliyun.com/nexus/content/repositories/central/</url>\n</mirror>\n```\n\n注意，是在settings.xml文件中`<mirrors></mirrors>`结点中添加。\n\n\n\n## 配置DarchetypeCatalog\n\nidea在创建maven项目的时候，本质是执行`mvn archetype:generate`命令。该命令执行时，有一个`-DarchetypeCatalog`参数，可选值为：remote、internal、local，表示`archetype-catalog.xml`文件从哪里获取，这三个可选值的含义分别为：\n\n- `remote`：从maven官方的中央仓库获取\n- `internal`：从maven-archetype-plugin内置的目录获取\n- `local`：从本地获取，具体在哪个目录根据你配置maven本地仓库路径\n\nmaven默认从remote获取`archetype-catalog.xml`文件，即从 `http://repo1.maven.org/maven2/archetype-catalog.xml`下载。该文件大约为3~4M，下载速度很慢，导致创建maven项目时需要很长时间。\n\n解决方法如下：\n\n（1）点击菜单->File->Other Settings->Default Settings->Build,Execution,Deploy->Build Tools->Maven->Runner->VM Options 输入：\n\n```bash\n-DarchetypeCatalog=local\n```\n\n如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210304225256.png)\n\n然后点击apply，最后点击OK。\n\n注意，一定要从全局配置，不然只对某一项目有效。\n\n（2）从maven官方仓库中下载archetype-catalog.xml文件，将下列地址复制到浏览器地址栏（或其它下载工具）下载即可。\n\n```xml\nhttp://repo1.maven.org/maven2/archetype-catalog.xml\n```\n\n（3）archetype-catalog.xml文件放置的位置非常重要，假如你的maven本地仓库路径是默认的，没有进行修改，则需要放到`${user.home}/.m2/repository`下。\n\n我对maven本地仓库路径进行了修改，配置路径如下：\n\n```xml\n<localRepository>d:/maven/repository</localRepository>\n```\n\n所以，我把archetype-catalog.xml文件放在`D:\\maven\\repository`下面。\n\n网上有的人说放在`${user.home}/.m2`，有的说放在`C:\\Users\\del-berlin\\.m2\\repository\\org\\apache\\maven\\archetype\\archetype-catalog\\2.4\\`下，我试了都不对。\n\n（4）重启idea\n\n（5）如果archetype-catalog.xml文件放置的路径不对，会报如下错误：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210304230422.png)\n\n如果上述配置正确，则运行结果如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210304230847.png)\n\n2~3秒一个maven项目就创建好了，真的是非常快。\n\n\n\n\n\n\n\n","tags":["maven"],"categories":["工具"]},{"title":"史上最全的Zookeeper原理详解(万字长文)","url":"/2021/03/01/175946/","content":"\n我参考了几十篇文章，总结了里面最重要的部分，并增加了许多自己的思考和理解，完成了这篇博客。我认为这篇博客很全，里面的内容也通俗易懂，想要了解Zookeeper的原理，本文应该就够了。另外，所有的参考文章链接已放到本文末尾，有需要的读者可自行查阅。\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n## 1. ZooKeeper 介绍\n\n> 大家可以了解一下[Paxos的小岛(Island)](https://www.douban.com/note/208430424/)，以便更好的理解Zookeeper的概念\n\n### 1.1 什么是Zookeeper\n\n`ZooKeeper` 是一个开源的**分布式协调服务框架**，为分布式系统提供一致性服务。\n\n那么什么是分布式？什么是协调程序？和集群又有什么区别？\n\n举一个例子来说明，现在有一个网上商城购物系统，并发量太大单机系统承受不住，那我们可以多加几台服务器支持大并发量的访问需求，这个就是所谓的**`Cluster` 集群** 。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210308193345.png)\n\n如果我们将这个网上商城购物系统拆分成多个子系统，比如订单系统、积分系统、购物车系统等等，**然后将这些子系统部署在不同的服务器上** ，这个时候就是 **`Distributed` 分布式** 。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210308193736.png)\n\n对于集群来说，多加几台服务器就行（当然还得解决session共享，负载均衡等问题），而对于分布式来说，你首先需要将业务进行拆分，然后再加服务器，同时还要去解决分布式带来的一系列问题。比如各个分布式组件如何**协调**起来，如何减少各个系统之间的耦合度，如何处理分布式事务，如何去配置整个分布式系统，如何解决各分布式子系统的数据不一致问题等等。`ZooKeeper` 主要就是解决这些问题的。\n\n\n\n### 1.2 使用ZooKeeper的开源项目\n\n许多著名的开源项目用到了 ZooKeeper，比如：\n\n1. **Kafka** : ZooKeeper 主要为 Kafka 提供 Broker 和 Topic 的注册以及多个 Partition 的负载均衡等功能。\n2. **Hbase** : ZooKeeper 为 Hbase 提供确保整个集群只有一个 Master 以及保存和提供 regionserver 状态信息（是否在线）等功能。\n3. **Hadoop** : ZooKeeper 为 Namenode 提供高可用支持。\n4. **Dubbo**：阿里巴巴集团开源的分布式服务框架，它使用 ZooKeeper 来作为其命名服务，维护全局的服务地址列表。\n\n\n\n### 1.3 ZooKeeper的三种运行模式\n\nZooKeeper 有三种运行模式：单机模式、伪集群模式和集群模式。\n\n- 单机模式：这种模式一般适用于开发测试环境，一方面我们没有那么多机器资源，另外就是平时的开发调试并不需要极好的稳定性。\n- 集群模式：一个 ZooKeeper 集群通常由一组机器组成，一般 3 台以上就可以组成一个可用的 ZooKeeper 集群了。组成 ZooKeeper 集群的每台机器都会在内存中维护当前的服务器状态，并且每台机器之间都会互相保持通信。\n- 伪集群模式：这是一种特殊的集群模式，即集群的所有服务器都部署在一台机器上。当你手头上有一台比较好的机器，如果作为单机模式进行部署，就会浪费资源，这种情况下，ZooKeeper 允许你在一台机器上通过启动不同的端口来启动多个 ZooKeeper 服务实例，从而以集群的特性来对外服务。\n\n\n\n## 2. CAP和BASE理论\n\n一个分布式系统必然会存在一个问题：**因为分区容忍性（partition tolerance）的存在，就必定要求我们需要在系统可用性（availability）和数据一致性（consistency）中做出权衡** 。这就是著名的 `CAP` 定理。\n\n举个例子来说明，假如班级代表整个分布式系统，而学生是整个分布式系统中一个个独立的子系统。这个时候班里的小红小明偷偷谈恋爱被班里的小花发现了，小花欣喜若狂告诉了周围的人，然后小红小明谈恋爱的消息在班级里传播起来了。当在消息的传播（散布）过程中，你问班里一个同学的情况，如果他回答你不知道，那么说明整个班级系统出现了数据不一致的问题（因为小花已经知道这个消息了）。而如果他直接不回答你，因为现在消息还在班级里传播（为了保证一致性，需要所有人都知道才可提供服务），这个时候就出现了系统的可用性问题。\n\n这个例子中前者就是 `Eureka` 的处理方式，它保证了AP（可用性），后者就 `ZooKeeper` 的处理方式，它保证了CP（数据一致性）。\n\nCAP理论中，`P`（分区容忍性）是必然要满足的，因为毕竟是分布式，不能把所有的应用全放到一个服务器里面，这样服务器是吃不消的。所以，只能从AP（可用性）和CP（一致性）中找平衡。\n\n怎么个平衡法呢？在这种环境下出现了**BASE理论**：即使无法做到强一致性，但分布式系统可以根据自己的业务特点，采用适当的方式来使系统达到最终的一致性。BASE理论由：`Basically Avaliable` 基本可用、`Soft state` 软状态、`Eventually consistent` 最终一致性组成。\n\n- **基本可用(Basically Available)**：基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。例如，电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层在该页面只提供降级服务。\n- **软状态(Soft State)**： 软状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有多个副本，允许不同节点间副本同步的延时就是软状态的体现。\n- **最终一致性(Eventual Consistency)**： 最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。\n\n一句话概括就是：平时系统要求是基本可用，运行有可容忍的延迟状态，但是，无论如何经过一段时间的延迟后系统最终必须达成数据是一致的。\n\n> ACID 是传统数据库常用的设计理念，追求强一致性模型。BASE 支持的是大型分布式系统，通过牺牲强一致性获得高可用性。\n\n其实可能发现不管是CAP理论，还是BASE理论，他们都是理论，这些理论是需要算法来实现的，这些算法有2PC、3PC、Paxos、Raft、ZAB，它们所解决的问题全部都是：**在分布式环境下，怎么让系统尽可能的高可用，而且数据能最终能达到一致**。\n\n\n\n## 3. Zookeeper的特点\n\n> 该部分来源于[讲解 Zookeeper 的五个核心知识点](https://mp.weixin.qq.com/s?__biz=MzI4NjI1OTI4Nw==&mid=2247489891&idx=1&sn=eb7b6a4d4f2560df31eb41e10dc66264&chksm=ebdef85bdca9714d89dcd84894ce8c3af4c2ad5bab43760adfa30384fe7345c6d93fe55b47d5&mpshare=1&scene=24&srcid=0308ammHQgSRw3GGV7RWu4M3&sharer_sharetime=1615167268668&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&ascene=14&devicetype=android-29&version=2700153b&nettype=WIFI&abtest_cookie=AAACAA%3D%3D&lang=zh_CN&exportkey=AUeEnm4ZGyJa8Eg6QqeW7W8%3D&pass_ticket=He9OMc%2Bmhiuj111RUsXzzTJbt%2B9kiaQht3Dd7kCsxQpc8HgWiMvTeMy4aVZ1XSPB&wx_header=1 \" Zookeeper的特点\")。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309102817.png)\n\n\n\n1. **集群**：Zookeeper是一个领导者（Leader），多个跟随者（Follower）组成的集群。\n2. **高可用性**：集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。\n3. **全局数据一致**：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。\n4. **更新请求顺序进行**：来自同一个Client的更新请求按其发送顺序依次执行。\n5. **数据更新原子性**：一次数据更新要么成功，要么失败。\n6. **实时性**：在一定时间范围内，Client能读到最新数据。\n7. 从设计模式角度来看，zk是一个基于**观察者设计模式**的框架，它负责管理跟存储大家都关心的数据，然后接受观察者的注册，数据反生变化zk会通知在zk上注册的观察者做出反应。\n8. Zookeeper是一个**分布式协调系统**，满足CP性，跟SpringCloud中的Eureka满足AP不一样。\n\n\n\n## 4. 一致性协议之 ZAB\n\n> 推荐大家先了解其他的一致性算法，如2PC、3PC、Paxos、Raft，可参考[大数据中的 2PC、3PC、Paxos、Raft、ZAB](https://mp.weixin.qq.com/s/b5mGEbn-FLb9vhOh1OpwIg \"大数据中的 2PC、3PC、Paxos、Raft、ZAB\")。\n\n\n\n作为一个优秀高效且可靠的分布式协调框架，`ZooKeeper` 在解决分布式数据一致性问题时并没有直接使用 `Paxos` ，而是专门定制了一致性协议叫做 `ZAB(ZooKeeper Automic Broadcast)` 原子广播协议，该协议能够很好地支持 **崩溃恢复** 。\n\n### 4.1 ZAB 中的三个角色\n\n ZAB 中三个主要的角色，Leader 领导者、Follower跟随者、Observer观察者 。\n\n- `Leader` ：集群中 **唯一的写请求处理者** ，能够发起投票（投票也是为了进行写请求）。\n- `Follower`：能够接收客户端的请求，如果是读请求则可以自己处理，**如果是写请求则要转发给 `Leader`** 。在选举过程中会参与投票，**有选举权和被选举权** 。\n- `Observer` ：就是没有选举权和被选举权的 `Follower` 。\n\n在 `ZAB` 协议中对 `zkServer`(即上面我们说的三个角色的总称) 还有两种模式的定义，分别是 **消息广播** 和 **崩溃恢复** 。\n\n\n\n### 4.2 ZXID和myid\n\n\n\n**ZooKeeper** 采用全局递增的事务 id 来标识，所有 proposal(提议)在被提出的时候加上了**ZooKeeper Transaction Id** 。ZXID是64位的Long类型，**这是保证事务的顺序一致性的关键**。ZXID中高32位表示纪元**epoch**，低32位表示事务标识**xid**。你可以认为zxid越大说明存储数据越新，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309202934.png)\n\n\n\n1. 每个leader都会具有不同的**epoch**值，表示一个纪元/朝代，用来标识 leader周期。每个新的选举开启时都会生成一个新的epoch，从1开始，每次选出新的Leader，epoch递增1，并会将该值更新到所有的zkServer的zxid的epoch。\n2. **xid**是一个依次递增的事务编号。数值越大说明数据越新，可以简单理解为递增的事务id。**每次epoch变化，都将低32位的序号重置**，这样保证了zxid的全局递增性。\n\n\n\n每个ZooKeeper服务器，都需要在数据文件夹下创建一个名为myid的文件，该文件包含整个ZooKeeper集群唯一的id（整数）。例如，某ZooKeeper集群包含三台服务器，hostname分别为zoo1、zoo2和zoo3，其myid分别为1、2和3，则在配置文件中其id与hostname必须一一对应，如下所示。在该配置文件中，`server.`后面的数据即为myid\n\n```tex\nserver.1=zoo1:2888:3888\nserver.2=zoo2:2888:3888\nserver.3=zoo3:2888:3888\n```\n\n\n\n\n\n### 4.3 历史队列\n\n每一个follower节点都会有一个**先进先出**（FIFO)的队列用来存放收到的事务请求，保证执行事务的顺序。所以：\n\n- 可靠提交由ZAB的事务一致性协议保证\n- 全局有序由TCP协议保证\n- 因果有序由follower的历史队列(history queue)保证\n\n\n\n### 4.4 消息广播模式\n\nZAB协议两种模式：消息广播模式和崩溃恢复模式。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309204307.png)\n\n\n\n说白了就是 `ZAB` 协议是如何处理写请求的，上面我们不是说只有 `Leader` 能处理写请求嘛？那么我们的 `Follower` 和 `Observer` 是不是也需要 **同步更新数据** 呢？总不能数据只在 `Leader` 中更新了，其他角色都没有得到更新吧。\n\n第一步肯定需要 `Leader` 将写请求 **广播** 出去呀，让 `Leader` 问问 `Followers` 是否同意更新，如果超过半数以上的同意那么就进行 `Follower` 和 `Observer` 的更新（和 `Paxos` 一样）。消息广播机制是通过如下图流程**保证事务的顺序一致性**的：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309205602.png)\n\n\n\n1. leader从客户端收到一个写请求\n2. leader生成一个新的事务并为这个事务生成一个唯一的ZXID\n3. leader将这个事务发送给所有的follows节点，将带有 zxid 的消息作为一个提案(proposal)分发给所有 follower。\n4. follower节点将收到的事务请求加入到历史队列(history queue)中，当 follower 接收到 proposal，先将 proposal 写到硬盘，写硬盘成功后再向 leader 回一个 ACK\n5. 当leader收到大多数follower（超过一半）的ack消息，leader会向follower发送commit请求（leader自身也要提交这个事务）\n6. 当follower收到commit请求时，会判断该事务的ZXID是不是比历史队列中的任何事务的ZXID都小，如果是则提交事务，如果不是则等待比它更小的事务的commit(保证顺序性)\n7. Leader将处理结果返回给客户端\n\n**过半写成功策略**：Leader节点接收到写请求后，这个Leader会将写请求广播给各个Server，各个Server会将该写请求加入历史队列，并向Leader发送ACK信息，当Leader收到一半以上的ACK消息后，说明该写操作可以执行。Leader会向各个server发送commit消息，各个server收到消息后执行commit操作。\n\n这里要注意以下几点：\n\n- Leader并不需要得到Observer的ACK，即Observer无投票权\n- Leader不需要得到所有Follower的ACK，只要收到过半的ACK即可，**同时Leader本身对自己有一个ACK**\n- Observer虽然无投票权，但仍须同步Leader的数据从而在处理读请求时可以返回尽可能新的数据\n\n另外，Follower/Observer也可以接受写请求，此时：\n\n- Follower/Observer接受写请求以后，不能直接处理，而需要将写请求转发给Leader处理\n- 除了多了一步请求转发，其它流程与直接写Leader无任何区别\n- Leader处理写请求是通过上面的消息广播模式，实质上最后所有的zkServer都要执行写操作，这样数据才会一致\n\n而对于读请求，Leader/Follower/Observer都可直接处理读请求，从本地内存中读取数据并返回给客户端即可。由于处理读请求不需要各个服务器之间的交互，因此Follower/Observer越多，整体可处理的读请求量越大，也即读性能越好。\n\n\n\n\n\n### 4.5 崩溃恢复模式\n\n\n\n恢复模式大致可以分为四个阶段：选举、发现、同步、广播。\n\n1. **选举阶段**（Leader election）：当leader崩溃后，集群进入选举阶段（下面会将如何选举Leader），开始选举出潜在的准 leader，然后进入下一个阶段。\n2. **发现阶段**（Discovery）：用于在从节点中发现最新的ZXID和事务日志。准Leader接收所有Follower发来各自的最新epoch值。Leader从中选出最大的epoch，基于此值加1，生成新的epoch分发给各个Follower。各个Follower收到全新的epoch后，返回ACK给Leader，带上各自最大的ZXID和历史提议日志。Leader选出最大的ZXID，并更新自身历史日志，此时Leader就用拥有了最新的提议历史。（注意：每次epoch变化时，ZXID的第32位从0开始计数）。\n3. **同步阶段**（Synchronization）：主要是利用 leader 前一阶段获得的最新提议历史，同步给集群中所有的Follower。只有当超过半数Follower同步成功，这个准Leader才能成为正式的Leader。这之后，follower 只会接收 zxid 比自己的 lastZxid 大的提议。\n4. 广播阶段（Broadcast）：集群恢复到广播模式，开始接受客户端的写请求。\n\n> 在发现阶段，或许有人会问：既然Leader被选为主节点，已经是集群里数据最新的了，为什么还要从节点中寻找最新事务呢？这是为了防止某些意外情况。所以这一阶段，Leader集思广益，接收所有Follower发来各自的最新epoch值。\n\n\n\n这里有两点要注意：\n\n（1）**确保已经被Leader提交的提案最终能够被所有的Follower提交**\n\n假设 `Leader (server2)` 发送 `commit` 请求（忘了请看上面的消息广播模式），他发送给了 `server3`，然后要发给 `server1` 的时候突然挂了。这个时候重新选举的时候我们如果把 `server1` 作为 `Leader` 的话，那么肯定会产生数据不一致性，因为 `server3` 肯定会提交刚刚 `server2` 发送的 `commit` 请求的提案，而 `server1` 根本没收到所以会丢弃。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309214242.png)\n\n那怎么解决呢？\n\n**这个时候 `server1` 已经不可能成为 `Leader` 了，因为 `server1` 和 `server3` 进行投票选举的时候会比较 `ZXID` ，而此时 `server3` 的 `ZXID` 肯定比 `server1` 的大了**（后面讲到选举机制时就明白了）。同理，只能由server3当Leader，server3当上Leader之后，在同步阶段，会将最新提议历史同步给集群中所有的Follower，这就保证数据一致性了。如果server2在某个时刻又重新恢复了，它作为`Follower` 的身份进入集群中，再向Leader同步当前最新提议和Zxid即可。\n\n（2）**确保跳过那些已经被丢弃的提案**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309215027.png)\n\n假设 `Leader (server2)` 此时同意了提案N1，自身提交了这个事务并且要发送给所有 `Follower` 要 `commit` 的请求，却在这个时候挂了，此时肯定要重新进行 `Leader` 的选举，假如此时选 `server1` 为 `Leader` （这无所谓，server1和server2都可以当选）。但是过了一会，这个 **挂掉的 `Leader` 又重新恢复了** ，此时它肯定会作为 `Follower` 的身份进入集群中，需要注意的是刚刚 `server2` 已经同意提交了提案N1，但其他 `server` 并没有收到它的 `commit` 信息，所以其他 `server` 不可能再提交这个提案N1了，这样就会出现数据不一致性问题了，所以 **该提案N1最终需要被抛弃掉** 。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309215338.png)\n\n\n\n### 4.6 脑裂问题\n\n脑裂问题：所谓的“脑裂”即“大脑分裂”，也就是本来一个“大脑”被拆分了两个或多个“大脑”。通俗的说，就是比如当你的 cluster 里面有两个节点，它们都知道在这个 cluster 里需要选举出一个 master。那么当它们两之间的通信完全没有问题的时候，就会达成共识，选出其中一个作为 master。但是如果它们之间的通信出了问题，那么两个结点都会觉得现在没有 master，所以每个都把自己选举成 master，于是 cluster 里面就会有两个 master。\n\nZAB为解决脑裂问题，要求集群内的节点数量为2N+1, 当网络分裂后，始终有一个集群的节点数量过半数，而另一个集群节点数量小于N+1（即小于半数）, 因为选主需要过半数节点同意，所以任何情况下集群中都不可能出现大于一个leader的情况。\n\n因此，有了过半机制，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题。\n\n\n\n\n\n## 5. Zookeeper选举机制\n\n`Leader` 选举可以分为两个不同的阶段，第一个是我们提到的 `Leader` 宕机需要重新选举，第二则是当 `Zookeeper` 启动时需要进行系统的 `Leader` 初始化选举。下面是zkserver的几种状态：\n\n- **LOOKING** 不确定Leader状态。该状态下的服务器认为当前集群中没有Leader，会发起Leader选举。\n- **FOLLOWING** 跟随者状态。表明当前服务器角色是Follower，并且它知道Leader是谁。\n- **LEADING** 领导者状态。表明当前服务器角色是Leader，它会维护与Follower间的心跳。\n- **OBSERVING** 观察者状态。表明当前服务器角色是Observer，与Folower唯一的不同在于不参与选举，也不参与集群写操作时的投票。\n\n\n\n### 5.1 初始化Leader选举\n\n假设我们集群中有3台机器，那也就意味着我们需要2台同意（超过半数）。这里假设服务器1~3的myid分别为1,2,3，初始化Leader选举过程如下：\n\n1. 服务器 1 启动，发起一次选举。它会首先 **投票给自己** ，投票内容为`(myid, ZXID)`，因为初始化所以 `ZXID` 都为0，此时 `server1` 发出的投票为` (1, 0)`，即`myid`为1， `ZXID`为0。此时服务器 1 票数一票，不够半数以上，选举无法完成，服务器 1 状态保持为 LOOKING。\n2. 服务器 2 启动，再发起一次选举。服务器2首先也会将投票选给自己`(2, 0)`，并将投票信息广播出去（`server1`也会，只是它那时没有其他的服务器了），`server1` 在收到 `server2` 的投票信息后会将投票信息与自己的作比较。**首先它会比较 `ZXID` ，`ZXID` 大的优先为 `Leader`，如果相同则比较 `myid`，`myid` 大的优先作为 `Leader`**。所以，**此时`server1` 发现 `server2` 更适合做 `Leader`，它就会将自己的投票信息更改为`(2, 0)`然后再广播出去**，之后`server2` 收到之后发现和自己的一样无需做更改。此时，服务器1票数0票，服务器2票数2票，**投票已经超过半数**，确定 `server2` 为 `Leader`。服务器 1更改状态为 FOLLOWING，服务器 2 更改状态为 LEADING。\n3. 服务器 3 启动，发起一次选举。此时服务器 1，2已经不是 LOOKING 状态，它会直接以 `FOLLOWING` 的身份加入集群。\n\n\n\n### 5.2 运行时Leader选举\n\n运行时候如果Leader节点崩溃了会走崩溃恢复模式，新Leader选出前会暂停对外服务，大致可以分为四个阶段：选举、发现、同步、广播（见4.5节），此时Leader选举流程如下：\n\n1. Leader挂掉，剩下的两个 `Follower` 会将自己的状态 **从 `Following` 变为 `Looking` 状态** ，每个Server会发出一个投票，第一次都是投自己，其中投票内容为`(myid, ZXID)`，注意这里的 `zxid` 可能不是0了\n2. 收集来自各个服务器的投票\n3. 处理投票，处理逻辑：**优先比较ZXID，然后比较myid**\n4. 统计投票，只要超过半数的机器接收到同样的投票信息，就可以确定leader\n5. 改变服务器状态Looking变为Following或Leading\n6. 然后依次进入发现、同步、广播阶段\n\n举个例子来说明，假设集群有三台服务器，`Leader (server2)`挂掉了，只剩下server1和server3。 `server1` 给自己投票为(1,99)，然后广播给其他 `server`，`server3` 首先也会给自己投票(3,95)，然后也广播给其他 `server`。`server1` 和 `server3` 此时会收到彼此的投票信息，和一开始选举一样，他们也会比较自己的投票和收到的投票（`zxid` 大的优先，如果相同那么就 `myid` 大的优先）。这个时候 `server1` 收到了 `server3` 的投票发现没自己的合适故不变，`server3` 收到 `server1` 的投票结果后发现比自己的合适于是更改投票为(1,99)然后广播出去，最后 `server1` 收到了发现自己的投票已经超过半数就把自己设为 `Leader`，`server3` 也随之变为 `Follower`。\n\n\n\n\n\n## 6. Zookeeper数据模型\n\nZooKeeper 数据模型（Data model）采用层次化的多叉树形结构，每个节点上都可以存储数据，这些数据可以是数字、字符串或者是二级制序列。并且，每个节点还可以拥有 N 个子节点，最上层是根节点以`/`来代表。\n\n每个数据节点在 ZooKeeper 中被称为 **znode**，它是 ZooKeeper 中数据的最小单元。并且，每个 znode 都一个唯一的路径标识。由于**ZooKeeper 主要是用来协调服务的，而不是用来存储业务数据的**，这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为**1M**。\n\n和文件系统一样，我们能够自由的增加、删除**znode**，在一个**znode**下增加、删除子**znode**，唯一的不同在于**znode**是可以存储数据的。默认有四种类型的**znode**：\n\n1. **持久化目录节点 PERSISTENT**：客户端与zookeeper断开连接后，该节点依旧存在。\n2. **持久化顺序编号目录节点 PERSISTENT_SEQUENTIAL**：客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。\n3. **临时目录节点 EPHEMERAL**：客户端与zookeeper断开连接后，该节点被删除。\n4. **临时顺序编号目录节点 EPHEMERAL_SEQUENTIAL**：客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。\n\n在zookeeper客户端使用`get`命令可以查看znode的内容和状态信息：\n\n```bash\n[zk: localhost:2181(CONNECTED) 2] get /zk01\nupdateed02\ncZxid = 0x600000023\nctime = Mon Mar 01 21:20:26 CST 2021\nmZxid = 0xb0000000d\nmtime = Fri Mar 05 17:15:53 CST 2021\npZxid = 0xb00000018\ncversion = 5\ndataVersion = 7\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 10\nnumChildren = 3\n```\n\n下面我们来看一下每个 znode 状态信息究竟代表的是什么吧\n\n| znode 状态信息 | 解释                                                         |\n| -------------- | ------------------------------------------------------------ |\n| cZxid          | create ZXID，即该数据节点被创建时的事务 id                   |\n| ctime          | create time，znode 被创建的毫秒数(从1970 年开始)             |\n| mZxid          | modified ZXID，znode 最后更新的事务 id                       |\n| mtime          | modified time，znode 最后修改的毫秒数(从1970 年开始)         |\n| pZxid          | znode 最后更新子节点列表的事务 id，只有子节点列表变更才会更新 pZxid，子节点内容变更不会更新 |\n| cversion       | znode 子节点变化号，znode 子节点修改次数，子节点每次变化时值增加 1 |\n| dataVersion    | znode 数据变化号，节点创建时为 0，每更新一次节点内容(不管内容有无变化)该版本号的值增加 1 |\n| aclVersion     | znode 访问控制列表(ACL )版本号，表示该节点 ACL 信息变更次数  |\n| ephemeralOwner | 如果是临时节点，这个是 znode 拥有者的 sessionid。如果不是临时节，则 ephemeralOwner=0 |\n| dataLength     | znode 的数据长度                                             |\n| numChildren    | znode 子节点数量                                             |\n\n\n\n## 7. Zookeeper监听通知机制\n\n**Watcher** 监听机制是 **Zookeeper** 中非常重要的特性，我们基于  Zookeeper上创建的节点，可以对这些节点绑定**监听**事件，比如可以监听节点数据变更、节点删除、子节点状态变更等事件，通过这个事件机制，可以基于  **Zookeeper** 实现分布式锁、集群管理等多种功能，它有点类似于订阅的方式，即客户端向服务端 **注册** 指定的 `watcher` ，当服务端符合了 `watcher` 的某些事件或要求则会 **向客户端发送事件通知** ，客户端收到通知后找到自己定义的 `Watcher` 然后 **执行相应的回调方法** 。\n\n当客户端在Zookeeper上某个节点绑定监听事件后，如果该事件被触发，Zookeeper会通过回调函数的方式通知客户端，但是客户端只会收到一次通知。如果后续这个节点再次发生变化，那么之前设置 **Watcher** 的客户端不会再次收到消息（Watcher是一次性的操作），可以通过循环监听去达到永久监听效果。\n\nZooKeeper 的 Watcher 机制，总的来说可以分为三个过程：\n\n1. 客户端注册 Watcher，注册 watcher 有 3 种方式，getData、exists、getChildren。\n2. 服务器处理 Watcher 。\n3. 客户端回调 Watcher 客户端。\n\n监听通知机制的流程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309140525.png)\n\n1. 首先要有一个main()线程\n2. 在main线程中创建zkClient，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。\n3. 通过connect线程将注册的监听事件发送给Zookeeper。\n4. 在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。\n5. Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。\n6. listener线程内部调用了process()方法。\n\n\n\n## 8. Zookeeper会话（Session）\n\nSession 可以看作是 ZooKeeper 服务器与客户端的之间的一个 TCP 长连接，客户端与服务端之间的任何交互操作都和Session 息息相关，其中包含zookeeper的临时节点的生命周期、客户端请求执行以及Watcher通知机制等。\n\n> client 端连接 server 端默认的 2181 端口，也就是 session 会话。\n\n接下来，我们从**全局的会话状态变化**到**创建会话**再到**会话管理**三个方面来看看Zookeeper是如何处理会话相关的操作。\n\n### 8.1 会话状态\n\nsession会话状态有：\n\n- **connecting**：连接中，session 一旦建立，状态就是 connecting 状态，时间很短。\n- **connected**：已连接，连接成功之后的状态。\n- **closed**：已关闭，发生在 session 过期，一般由于网络故障客户端重连失败，服务器宕机或者客户端主动断开。\n\n客户端需要与服务端创建一个会话，这个时候客户端需要提供一个服务端地址列表，`host1 : port,host2: port ,host3:port` ，一般由地址管理器(HostProvider)管理，然后根据地址创建zookeeper对象。这个时候客户端的状态则变更为**CONNECTING**，同时客户端会根据上述的地址列表，按照顺序的方式获取IP来尝试建立网络连接，直到成功连接上服务器，这个时候客户端的状态就可以变更为**CONNECTED**。在Zookeeper服务端提供服务的过程中，有可能遇到网络波动等原因，导致客户端与服务端断开了连接，这个时候客户端会进行重新连接操作这个时候的状态为**CONNECTING**,当连接再次建立后，客户端的状态会再次更改为**CONNECTED**，也就是说只要在Zookeeper运行期间，客户端的状态总是能保持在**CONNECTING**或者是**CONNECTED**。当然在建立连接的过程中，如果出现了连接超时、权限检查失败或者是在建立连接的过程中，我们主动退出连接操作，这个时候客户端的状态都会变成**CLOSE**状态。\n\n### 8.2 会话ID的生成\n\n一个会话必须包含以下几个基本的属性：\n\n- **SessionID** : 会话的ID，用来唯一标识一个会话，每一次客户端建立连接的时候，Zookeeper服务端都会给其分配一个全局唯一的**sessionID**。在Zookeeper中，无论是哪台服务器为客户端分配的 `sessionID`，都务必保证全局唯一。\n- **Timeout**：一次会话的超时时间，客户端在构造Zookeeper实例的时候，会配置一个**sessionTimeOut**参数用于指定会话的超时的时间。Zookeeper服务端会按照连接的客户端发来的**TimeOut**参数来计算并确定超时的时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在超时规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。\n-  ExpirationTime：TimeOut是一个相对时间，而ExpirationTime则是在时间轴上的一个绝对过期时间。可能你也会想到，一个比较通用的计算方法就是：`ExpirationTime = CurrentTime + Timeout`。 这样算出来的时间最准确，但ZK可不是这么算的，下面会讲具体计算方式及这样做的原因。\n- **TickTime**：下一次会话超时的时间点，为了便于Zookeeper对会话实行分桶策略管理，同时也是为了高效低耗地实现会话的超时检查与清理，Zookeeper会为每个会话标记一个下次会话超时时间点。TickTime是一个13位的Long类型的数值，一般情况下这个值接近**TimeOut**，但是并不完全相等。\n- **isCloseing**：用来标记当前会话是否已经处于被关闭的状态。如果服务端检测到当前会话的超时时间已经到了，就会将isCloseing属性标记为已经关闭，这样以后即使再有这个会话的请求访问也不会被处理。\n\nSessionID作为一个全局唯一的标识，我们可以来探究下Zookeeper是如何保证Session会话在集群环境下依然能保证全局唯一性的。\n\n在**sessionTracker**初始化的时候，会调用**initializeNextSession**来生成sessionid，算法大概如下:\n\n```java\npublic static long initializeNextSession(long id ) {\n    long nextSid = 0;\n    nextSid = (System.currentTimeMillis() << 24) >> 8;\n    nextSid=nextSid|(id << 56);\n    return nextSid;\n}\n```\n\n从这段代码，我们可以看到session的创建大概分为以下几个步骤：\n\n**1. 获取当前时间的毫秒表示**\n\n我们假设当前System.currentTimeMills()获取的值是1380895182327，其64位二进制表示为:\n\n```tex\n00000000 00000000 00000001 01000001 10000011 11000100 01001101 11110111\n```\n\n**2. 接下来左移24位**，我们可以得到结果：\n\n```tex\n01000001 100000011 11000100 01001101 11110111 00000000 00000000 00000000\n```\n\n可以看到低位已经把高位补齐，剩下的低位都使用了0补齐。\n\n**3. 右移8位**，结果变成了：\n\n```tex\n00000000 01000001 100000011 11000100 01001101 11110111 00000000 00000000\n```\n\n**4. 计算机器码标识ID**：\n\n在initializeNextSession方法中，传入了一个id变量，这个变量就是当前zkServer的myid中配置的值，一般是一个整数，假设此时的值为2，转为64位二进制表示：\n\n```tex\n00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000010\n```\n\n此时发现高位几乎都是0，进行左移56位以后，得到值如下：\n\n```tex\n00000010 00000000 00000000 00000000 00000000 00000000 00000000 00000000\n```\n\n**5. 将前面第三步和第四步得到的结果进行 | 操作**，可以得到结果为：\n\n```tex\n00000010 01000001 10000011 11000100 01001101 11110111 00000000 00000000\n```\n\n这个时候我们可以得到一个集群中唯一的序列号ID，整个算法大概可以理解为，**先通过高8位确定zkServer所在的机器以后**，后面的56位按照当前毫秒进行随机，可以看出来当前的算法还是蛮严谨的，基本上看不出来什么明显的问题，但是其实也有问题的。我们可以看到，zk选择了当前机器时间内的毫秒作为基数，但是如果时间到了2022年4月8号以后， `System.currentTimeMillis ()`的值会是多少呢？\n\n```java\nCalendar calendar = Calendar.getInstance();\ncalendar.clear();\ncalendar.set(2022,5,1);\nlong millis = calendar.getTimeInMillis();\nSystem.out.println(Long.toBinaryString(millis));\n```\n\n输出：`0000000000000000000000011000000100011010110110000110110000000000`\n\n可以看到，输出结果前面有23个0，接着我们左移24位以后会发现，这个时候的值竟然是个负数。\n\n> 在java中最高位为1时表示负数，为0表示正整数\n\n为了保证不会出现负数的情况，可以将有符号移位换成无符号移位，解决方案如下:\n\n```java\npublic static long initializeNextSession(long id ) {\n    long nextSid = 0;\n    nextSid = (System.currentTimeMillis() << 24) >>> 8;\n    nextSid=nextSid|(id << 56);\n    return nextSid;\n}\n```\n\n上面`>>>`为无符号右移，当目标是负数时，**在移位时忽略符号位，空位都以0补齐，这样就保证了结果永远是正数**。\n\n### 8.3 SessionTracker与ClientCnxn\n\n**SessionTracker**是Zookeeper中的会话管理器，负责整个zk生命周期中会话的**创建**、**管理**和**清理**操作，而每一个会话在Sessiontracker内部都保留了如下三个数据结构，大体如下:\n\n```java\nprotected final ConcurrentHashMap<Long, SessionImpl> sessionsById =\n    new ConcurrentHashMap<Long, SessionImpl>();\nprivate final ConcurrentMap<Long, Integer> sessionsWithTimeout;\n```\n\n1. sessionsWithTimeout这是一个ConcurrentHashMap类型的数据结构，用来管理会话的超时时间，这个参数会被持久化到快照文件中去\n\n2. sessionsById是一个HashMap类型的数据结构，用于根据sessionId来管理session实体\n\n3. sessionsSets同样也是一个HashMap类型的数据结构，用来会话超时的时候进行归档，便于进行会话恢复和管理\n\n\n\n**ClientCnxn**是Zookeeper客户端的核心工作类，负责维护客户端与服务端之间的网络连接并进行一系列网络通信。\n\nClientCnxn内部又包含两个线程，**SendThread**是一个I/O线程，主要负责Zookeeper客户端和服务端之间的网络I/O通信，**EventThread**是一个事件线程，主要负责对服务端事件进行处理。\n\n- **SendThread**：SendThread维护了客户端与服务端之间的会话生命周期，其通过一定的周期频率向服务端发送一个PING包来实现心跳检测。此外，SendThread管理了客户端所有的请求发送和响应接受操作，其将上层客户端API操作转换成相应的请求协议并发送到服务端，并完成对同步调用的返回和异步调用的回调。同时，SendThread还负责将来自服务端的事件传递给EventThread去处理。\n- **EventThread**：EventThread负责客户端的事件处理，并触发客户端注册的Watcher监听。EventThread中有一个waitingEvents队列，用于临时存放那些需要被触发的Object，包括那些客户端注册的Watcher和异步接口中注册的回调器AsyncCallback。EventThread会不断地从waitingEvents队列中取出Object，识别出具体的类型，并分别调用process（Watcher）和processResult（AsyncCallback）接口方法来实现对事件的触发和回调。\n\nClientCnxn中有两个核心队列outgoingQueue和pendingQueue，分别代表客户端的请求发送队列和服务端响应的等待队列。\n\n- outgoing队列专门用于存储那些客户端需要发送到服务端的Packet集合\n- pending队列存储那些已经从客户端发送到服务端的，但是需要等待服务端响应的Packet结合\n\n\n\nclientCnxnSocket是底层Socket通信层，定义了Socket通信的接口，为了便于对底层Socket层进行扩展，例如使用Netty来实现和使用过NIO来实现。在Zookeeper中默认的实现是ClientCnxnSocketNIO，主要负责对请求的发送和响应的接收过程。\n\n**发送请求**\n\n在TCP连接正常情况下，从outgoingQueue队列中按照先进先出的顺序提取出一个可发送的Packet对象，同时生成一个客户端请求序号XID并将其设置到Packet请求头中，然后将其序列化后发送。\n\n请求发送完毕后，会立即将该Packet保存到pendingQueue队列中，以便等待服务端响应返回后进行相应的处理，处理完毕后返回给客户端\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309224615.jpg)\n\n**接收响应**\n\n客户端获取到来自服务端的完整响应数据后，根据不同的客户端请求类型，进行不同的处理：\n\n- 如果检测到当前客户端还未进行初始化，则说明当前客户端与服务端之间正在进行会话创建，那么就直接将接受到的ByteBuffer序列化成ConnectResponse对象\n- 如果当前客户端已经处于正常的会话周期，并且接受到的服务端响应是一个事件，那么就将接受到的ByteBuffer序列化成WatcherEvent对象，并将该事件放入待处理队列中\n- 如果是一个常规的请求响应（Create、GetData和Exist等操作请求），就会从pendingQueue队列中取出一个Packet，按照XID顺序将接受到的ByteBuffer序列化成响应的Response对象\n\n\n\n### 8.4 会话创建\n\n\n\n会话的创建的流程如下：\n\n1. client随机选一个服务端地址列表提供的地址，委托给`ClientCnxnSocket`去创建与zk之间的TCP长连接。\n2. SendThread会负责根据当前客户端的设置，构造出一个ConnectRequest请求，该请求代表了客户端试图与服务器创建一个会话。同时，Zookeeper客户端还会进一步将请求包装成网络IO的Packet对象，放入请求发送队列——outgoingQueue中去。\n3. 当客户端请求准备完毕后，ClientCnxnSocket从outgoingQueue中取出Packet对象，将其序列化成ByteBuffer后，向服务器进行发送。\n4. 服务端的SessionTracker为该会话分配一个sessionId，并发送响应。\n5. Client收到响应后，会首先判断当前的客户端状态是否是已初始化，如果尚未完成初始化，**那么就认为该响应一定是会话创建请求的响应**，直接交由readConnectResult方法来处理该请求。\n6. ClientCnxnSocket会对接受到的服务端响应进行反序列化，得到ConnectResponse对象，并从中获取到Zookeeper服务端分配的会话SessionId。\n7. 连接成功后，一方面需要通知SendThread线程，进一步对客户端进行会话参数设置，包括readTimeout和connectTimeout等，并更新客户端状态；另一方面，需要通知地址管理器HostProvider当前成功连接的服务器地址。\n8. 为了能够让上层应用感知到会话的成功创建，SendThread会生成一个事件SyncConnected-None，代表客户端与服务器会话创建成功，并将该事件传递给EventThread线程。\n9. EventThread线程收到事件后，会从ClientWatchManager管理器中查询出对应的Watcher，针对SyncConnected-None事件，那么就直接找出存储的Watcher，然后将其放到EventThread的waitingEvents队列中。\n10. EventThread不断地从waitingEvents队列中取出待处理的Watcher对象，然后直接调用该对象的process接口方法，以达到触发Watcher的目的。\n\n至此，Zookeeper客户端完整的一次会话创建过程已经全部完成了。\n\n\n\n### 8.5 会话超时管理\n\nSession是由ZK服务端来进行管理的，一个服务端可以为多个客户端服务，也就是说，有多个Session，那这些Session是怎么样被管理的呢？而分桶机制可以说就是其管理的一个手段。ZK服务端会维护着一个个\"桶\",然后把Session们分配到一个个的桶里面。而这个区分的维度，就是**ExpirationTime**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309163108.png)\n\n为什么要如此区分呢？因为ZK的服务端会在运行期间定时地对会话进行超时检测，如果不对Session进行维护的话，那在检测的时候岂不是要遍历所有的Session？这显然不是一个好办法，所以才以超时时间为维度来存放Session，这样在检测的时候，只需要扫描对应的桶就可以了。\n\n那这样的话，新的问题就来了：每个Session的超时时间是一个很分散的值，假设有1000个Session，很可能就会有1000个不同的超时时间，进而有1000个桶，这样有啥意义吗？因此zk的ExpirationTime 用了下面的计算方式：\n\n```tex\nExpirationTime = CurrentTime + SessionTimeout;\nExpirationTime = (ExpirationTime / ExpirationInterval + 1) * ExpirationInterval;\n```\n\n可以看到，最终得到的ExpirationTime是ExpirationInterval的**倍数**，而ExpirationInterval就是ZK服务端定时检查过期Session的频率，默认为2000毫秒。所以说，每个Session的ExpirationTime最后都是一个近似值，是ExpirationInterval的倍数，这样的话，ZK在进行扫描的时候，只需要扫描一个桶即可。\n\n>  另外让过期时间是ExpirationInterval的倍数还有一个好处就是，让检查时间和每个Session的过期时间在一个时间节点上。否则的话就会出现一个问题：ZK检查完毕的1毫秒后，就有一个Session新过期了，这种情况肯定是不好。\n\n为了便于理解，我们可以举几个例子，Zk默认的间隔时间是2000ms：\n\n- 比如我们计算出来一个sessionA在3000ms后过期，，那么其会坐落在`(3000/2000+1)*2000=5000ms`，放在4000ms这个key里。\n\n- 比如我们计算出来一个sessionB在1500ms后过期，那么其会坐落在`(1500/2000+1)*2000=3500ms`，放在2000ms这个key里。\n\n| 0    | 2000ms   | 4000ms   | 6000ms | 8000ms |\n| :--- | :------- | :------- | :----- | :----- |\n|      | sessionB | sessionA |        |        |\n\n这样线程就不用遍历所有的会话去逐一检查它们的过期时间了，有点妙。如果服务端检测到当前会话的超时时间已经到了，就会将**isCloseing**属性标记为已经关闭，这样以后即使再有这个会话的请求访问也不会被处理。\n\n\n\n### 8.5 会话激活\n\n在客户端与服务端完成连接之后生成过期时间，这个值并不是一直不变的，而是会随着客户端与服务端的交互来更新。**过期时间的更新，当然就伴随着Session在桶上的迁移**。过期时间计算的过程则是使用上面的公式，计算完新的超时时间以后，就可以放在桶相应位置上。激活的方式有：\n\n- 客户端每向服务端发送请求，包括读请求和写请求，都会触发一次激活，因为这预示着客户端处于活跃状态\n- 而如果客户端一直没有读写请求，那么它在TimeOut的三分之一时间内没有发送过请求的话，那么客户端会发送一次PING，来触发Session的激活。当然，如果客户端直接断开连接的话，那么TimeOut结束后就会被服务端扫描到然后进行清楚了\n\n除此之外，由于会话之间的激活是按照分桶策略进行保存的，因此我们可以利用此策略优化对于会话的超时检查，在Zookeeper中，会话超时检查也是由**SessionTracker**负责的，内部有一个线程专门进行会话的超时检查，只要依次的对每一个区块的会话进行检查。由于分桶是按照**ExpriationInterval** 的倍数来进行会话分布的，因此只要在这些时间点检查即可，这样可以减少检查的次数，并且批量清理会话，实现较高的效率。\n\n\n\n### 8.6 会话清理\n\n会话检查操作以后，当发现有超时的会话的时候，会进行会话清理操作，而Zookeeper中的会话清理操作，主要是以下几个步骤:\n\n1. 由于会话清理过程需要一定的时间，为了保证在清理的过程中，该会话不会再去接受和处理发来的请求，因此，在会话检查完毕后，**SessionTracker**会先将其会话的**isClose**标记为true，这样在会话清理期间接收到客户端的新请求也无法继续处理了。\n2. 发起关闭会话请求给`PrepRequestProcessor`，使其**在整个Zk集群里生效**。\n3. 收集需要清理的临时节点 ——通过sessionsWithTimeout和分桶策略找到超时的会话\n4. 当会话对应的临时节点列表找到后，Zookeeper会将列表中所有的节点变成删除节点的请求，并且丢给事物变更队列**OutStandingChanges**中，接着**FinalRequestProcessor**处理器会触发删除节点的操作，从内存数据库中删除。\n5. 当会话对应的临时节点被删除以后，就需要将会话从**SessionTracker**中移除了，主要从**SessionById**，**sessionsWithTimeOut**以及**sessionsSets**中将会话移除掉，当一切操作完成后，清理会话操作完成，这个时候将会关闭最终的连接**NioServerCnxn**。\n\n\n\n### 8.7 会话重连\n\n在Zookeeper运行过程中，也可能会出现会话断开后重连的情况，这个时候客户端会从连接列表中按照顺序的方式重新建立连接，直到连接上其中一台机器为止。这个时候可能出现两种状态，一种是正常的连接**CONNECTED**，这种情况是Zookeeper客户端在超时时间内连接上了服务端，此时sessionid不变；而超时以后才连接上服务端的话，这个时候的客户端会话状态则为**EXPIRED**，被视为非法会话。\n\n而在重连之前，可能因为其他原因导致的断开连接，即CONNECTION_LESS，会抛出异常**org.apache.zookeeper.KeeperException$ConnectionLossException**。此时，会话可能会出现两种情况：\n\n（1）会话失效：SESSION_EXPIRED\n\n会话失效一般发生在ConnectionLoss期间，客户端尝试开始重连，但是在超时时间以后，才与服务端建立连接的情况，这个时候服务端就会通知客户端当前会话已经失效，我们只能选择重新创建一个会话，进行数据的处理操作\n\n（2）会话转移：SESSION_MOVED\n\n会话转移也是在重连过程中常发生的一种情况，例如在断开连接之前，会话是在服务端A上，但是在断开连接重连以后，最终与服务端B重新恢复了会话，这种情况就称之为会话转移。而会话转移可能会带来一个新的问题，例如在断开连接之前，可能刚刚发送一个创建节点的请求，请求发送完毕后断开了，很短时间内再次重连上了另一台服务端，这个时候又发送了一个一样的创建节点请求，这个时候一样的事物请求可能会被执行了多次。因此在Zookeeper3.2版本开始，就有了会话转移的概念，并且封装了一个**SessionMovedExection**异常出来，在处理客户端请求之前，会检查一遍，请求的会话是不是当前服务端的，如果不存在当前服务端的会话，会直接抛出**SessionMovedExection**异常，当然这个时候客户端已经断开了连接，接受不到服务端的异常响应了。\n\n\n\n\n\n## 9. Zookeeper分布式锁\n\n> 本小节来自[漫画：如何用Zookeeper实现分布式锁？](https://mp.weixin.qq.com/s/u8QDlrDj3Rl1YjY4TyKMCA)\n\n**分布式锁**是雅虎研究员设计Zookeeper的初衷。利用Zookeeper的临时顺序节点，可以轻松实现分布式锁。\n\n### 9.1 获取锁\n\n首先，在Zookeeper当中创建一个持久节点ParentLock。当第一个客户端想要获得锁时，需要在ParentLock这个节点下面创建一个**临时顺序节点** Lock1。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175457.png)\n\n之后，Client1查找ParentLock下面所有的临时顺序节点并排序，判断自己所创建的节点Lock1是不是顺序最靠前的一个。如果是第一个节点，则成功获得锁。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175518.png)\n\n这时候，如果再有一个客户端 Client2 前来获取锁，则在ParentLock下载再创建一个临时顺序节点Lock2。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175541.png)\n\nClient2查找ParentLock下面所有的临时顺序节点并排序，判断自己所创建的节点Lock2是不是顺序最靠前的一个，结果发现节点Lock2并不是最小的。\n\n于是，Client2向排序仅比它靠前的节点Lock1注册**Watcher**，用于监听Lock1节点是否存在。这意味着Client2抢锁失败，进入了等待状态。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175609.png)\n\n这时候，如果又有一个客户端Client3前来获取锁，则在ParentLock下载再创建一个临时顺序节点Lock3。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175635.png)\n\n\n\nClient3查找ParentLock下面所有的临时顺序节点并排序，判断自己所创建的节点Lock3是不是顺序最靠前的一个，结果同样发现节点Lock3并不是最小的。\n\n于是，Client3向排序仅比它靠前的节点**Lock2**注册Watcher，用于监听Lock2节点是否存在。这意味着Client3同样抢锁失败，进入了等待状态。\n\n这样一来，Client1得到了锁，Client2监听了Lock1，Client3监听了Lock2。这恰恰形成了一个等待队列，很像是Java当中ReentrantLock所依赖的**AQS**（AbstractQueuedSynchronizer）。\n\n\n\n### 9.2 释放锁\n\n释放锁分为两种情况：\n\n**1.任务完成，客户端显示释放**\n\n当任务完成时，Client1会显示调用删除节点Lock1的指令。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175800.png)\n\n**2.任务执行过程中，客户端崩溃**\n\n获得锁的Client1在任务执行过程中，如果Duang的一声崩溃，则会断开与Zookeeper服务端的链接。根据临时节点的特性，相关联的节点Lock1会随之自动删除。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175823.png)\n\n\n\n由于Client2一直监听着Lock1的存在状态，当Lock1节点被删除，Client2会立刻收到通知。这时候Client2会再次查询ParentLock下面的所有节点，确认自己创建的节点Lock2是不是目前最小的节点。如果是最小，则Client2顺理成章获得了锁。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175847.png)\n\n同理，如果Client2也因为任务完成或者节点崩溃而删除了节点Lock2，那么Client3就会接到通知。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175913.png)\n\n最终，Client3成功得到了锁。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309175937.png)\n\n### 9.3 Zk和Redis分布式锁的比较\n\n下面的表格总结了Zookeeper和Redis分布式锁的优缺点：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309180014.png)\n\n有人说Zookeeper实现的分布式锁支持可重入，Redis实现的分布式锁不支持可重入，这是**错误的观点**。两者都可以在客户端实现可重入逻辑。\n\n> 什么是 “可重入”，可重入就是说某个线程已经获得某个锁，可以再次获取锁而不会出现死锁\n\n\n\n\n\n## 9. Zookeeper几个应用场景\n\n### 9.1 数据发布/订阅\n\n当某些数据由几个机器共享，且这些信息经常变化数据量还小的时候，这些数据就适合存储到ZK中。\n\n- **数据存储**：将数据存储到 Zookeeper 上的一个数据节点。\n- **数据获取**：应用在启动初始化节点从 Zookeeper 数据节点读取数据，并在该节点上注册一个数据变更 **Watcher**\n- **数据变更**：当变更数据时会更新 Zookeeper 对应节点数据，Zookeeper会将数据变更**通知**发到各客户端，客户端接到通知后重新读取变更后的数据即可。\n\n\n\n### 9.2 统一配置管理\n\n本质上，统一配置管理和数据发布/订阅是一样的。\n\n分布式环境下，配置文件的同步可以由Zookeeper来实现。\n\n1. 将配置文件写入Zookeeper的一个ZNode\n2. 各个客户端服务监听这个ZNode\n3. 一旦ZNode发生改变，Zookeeper将通知各个客户端服务\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309172415.png)\n\n### 9.3 统一集群管理\n\n\n\n可能我们会有这样的需求，我们需要了解整个集群中有多少机器在工作，我们想对及群众的每台机器的运行时状态进行数据采集，对集群中机器进行上下线操作等等。\n\n例如，集群机器监控：这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。 这种做法可行，但是存在两个比较明显的问题：\n\n1. 集群中机器有变动的时候，牵连修改的东西比较多。\n2. 有一定的延时。\n\n利用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统：\n\n1. 客户端在某个节点上注册一个Watcher，那么如果该节点的子节点变化了，会通知该客户端。\n2. 创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。\n\n如下图所示，监控系统在` /manage`节点上注册一个Watcher，如果` /manage`子节点列表有变动，监控系统就能够实时知道集群中机器的增减情况，至于后续处理就是监控系统的业务了。\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309232621.png)\n\n\n\n\n\n### 9.4 负载均衡\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309180848.png)\n\n多个相同的jar包在不同的服务器上开启相同的服务，可以通过nginx在服务端进行负载均衡的配置。也可以通过ZooKeeper在客户端进行负载均衡配置。\n\n1. 多个服务注册\n2. 客户端获取中间件地址集合\n3. 从集合中随机选一个服务执行任务\n\n**ZooKeeper负载均衡和Nginx负载均衡区别**：\n\n- **ZooKeeper**不存在单点问题，zab机制保证单点故障可重新选举一个leader只负责服务的注册与发现，不负责转发，减少一次数据交换（消费方与服务方直接通信），需要自己实现相应的负载均衡算法。\n- **Nginx**存在单点问题，单点负载高数据量大,需要通过 **KeepAlived** + **LVS** 备机实现高可用。每次负载，都充当一次中间人转发角色，增加网络负载量（消费方与服务方间接通信），自带负载均衡算法。\n\n\n\n### 9.5 命名服务\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210309181007.png)\n\n命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局唯一的路径，这个路径就可以作为一个名字，指向集群中某个具体的服务器，提供的服务的地址，或者一个远程的对象等等。\n\n阿里巴巴集团开源的分布式服务框架 Dubbo 中使用 ZooKeeper 来作为其命名服务，维护全局的服务地址列表。在 Dubbo 的实现中：\n\n- 服务提供者在启动的时候，向 ZooKeeper 上的指定节点` /dubbo/${serviceName}/providers` 目录下写入自己的 URL 地址，这个操作就完成了服务的发布。\n- 服务消费者启动的时候，订阅` /dubbo/${serviceName} /consumers` 目录下写入自己的 URL 地址。\n\n注意：所有向 ZooKeeper 上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。\n\n另外，Dubbo 还有针对服务粒度的监控，方法是订阅` /dubbo/${serviceName}` 目录下所有提供者和消费者的信息。\n\n\n\n另外，**分布式锁和选举也是Zookeeper的典型应用场景**。\n\n\n\n【参考资料】\n\n1. https://blog.csdn.net/weixin_44766402/article/details/92682593\n2. https://blog.csdn.net/u013679744/article/details/79222103\n3. https://blog.csdn.net/u013374645/article/details/93140148\n4. https://dbaplus.cn/news-141-1875-1.html\n5. https://mp.weixin.qq.com/s/b5mGEbn-FLb9vhOh1OpwIg\n6. https://mp.weixin.qq.com/s/W6QgmFTpXQ8EL-dVvLWsyg\n7. https://snailclimb.gitee.io/javaguide/#/docs/system-design/distributed-system/zookeeper/zookeeper-plus\n8. https://snailclimb.gitee.io/javaguide/#/docs/system-design/distributed-system/zookeeper/zookeeper-intro\n9. https://blog.csdn.net/u013679744/article/details/79222103\n10. https://www.cnblogs.com/raphael5200/p/5285583.html\n11. https://www.zhihu.com/question/20004877\n12. https://mp.weixin.qq.com/s/tInsv8fRVT1a-lE-uS1QEw\n13. https://zhuanlan.zhihu.com/p/158566353\n14. https://www.runoob.com/w3cnote/zookeeper-session.html\n15. https://segmentfault.com/a/1190000022193168\n16. https://mp.weixin.qq.com/s/8yHbaEsyiY1EdKenVcVEYA\n17. https://mp.weixin.qq.com/s/Ybt7M_uichWg5YX-9-tyhw\n18. https://mp.weixin.qq.com/s/4kjMJ0IKXP9T-cRCrf3uYQ\n19. https://mp.weixin.qq.com/s/HWjynP8-777EltcQtm3e3Q\n20. https://mp.weixin.qq.com/s/u8QDlrDj3Rl1YjY4TyKMCA\n21. https://mp.weixin.qq.com/s/Gs4rrF8wwRzF6EvyrF_o4A\n22. https://blog.csdn.net/yangguosb/article/details/80254240\n23. https://www.cnblogs.com/tommyli/p/3766189.html\n24. https://zhuanlan.zhihu.com/p/67654401\n\n\n\n","tags":["zookeeper"],"categories":["zookeeper"]},{"title":"git报错记录及解决方案","url":"/2021/02/28/151203/","content":"\n我见过的所有git报错及解决方案\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. OpenSSL SSL_connect: Connection was reset in connection to github.com:443\n\n\n\n**<font size=5 color=\"red\">Github报错OpenSSL SSL_connect: Connection was reset in connection to github.com:443终极解决方案</font>**\n\n今天在使用git命令进行push和pull时，出现如下报错：\n\n```bash\nfatal: unable to access 'https://github.com/wxler/test.git/': OpenSSL SSL_connect: Connection was reset in connection to github.com:443\n```\n我查了很多种方案，下面必有一个方法能够解决。\n\n### 方案一\n在git bash命令行中依次输入以下命令：\n\n```bash\ngit config --global http.sslBackend \"openssl\"\ngit config --global http.sslCAInfo \"C:\\Program Files\\Git\\mingw64\\ssl\\cert.pem\"\n```\n注意上面第二个命令，路径要换成git安装的路径。\n\n### 方案二\n如果你开启了VPN，很可能是因为代理的问题，这时候设置一下http.proxy就可以了。\n比如我用的VPN是shadow，先查看当前VPN代理使用的端口号，如下图所示，我的端口号是7890\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210301131617993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n点击上图的端口按钮，会出现如下提示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210301120241.png)\n\n此时，粘贴板内容为：\n\n```bash\nset http_proxy=http://127.0.0.1:7890\nset https_proxy=http://127.0.0.1:7890\n```\n\n所以，在git bash命令行中输入以下命令即可：\n\n```bash\ngit config --global http.proxy 127.0.0.1:7890\ngit config --global https.proxy 127.0.0.1:7890\n```\n\n如果你之前git中已经设置过上述配置，则使用如下命令取消再进行配置即可：\n\n```bash\ngit config --global --unset http.proxy\ngit config --global --unset https.proxy\n```\n\n下面是几个常用的git配置查看命令：\n\n```bash\ngit config --global http.proxy #查看git的http代理配置\ngit config --global https.proxy #查看git的https代理配置\ngit config --global -l #查看git的所有配置\n```\n\n参考：[https://stackoverflow.com/questions/49345357/fatal-unable-to-access-https-github-com-xxx-openssl-ssl-connect-ssl-error](https://stackoverflow.com/questions/49345357/fatal-unable-to-access-https-github-com-xxx-openssl-ssl-connect-ssl-error)\n\n\n\n\n\n### 方案三\n\n还有一个情况，是你的VNP代理服务器节点有问题，有时候更换一个结点就好了。\n![](https://img-blog.csdnimg.cn/img_convert/effabb538eba1d97cfb798e5293b5390.png)\n\n### 方案四\n打开一个新的git bash终端，就没问题了。这个可能是当前git的会话有关。\n\n\n如果以上所有方案都解决不了报错问题，则需要对症分析，请评论区留言！\n\n\n\n\n\n## 2. Failed to connect to 127.0.0.1 port 1080: Connection refused\n\n解决方案：[https://blog.csdn.net/weixin_41010198/article/details/87929622](https://blog.csdn.net/weixin_41010198/article/details/87929622)\n\n\n\n## 3. error: Your local changes to the following files would be overwritten by merge\n\n解决方案：[https://blog.csdn.net/nakiri_arisu/article/details/80259531](https://blog.csdn.net/nakiri_arisu/article/details/80259531)\n\n","tags":["git"],"categories":["工具"]},{"title":"git本地管理多个账户","url":"/2021/02/25/185259/","content":"\n我们可能会需要在一台电脑上以不同的github账户去使用git，这时就需要去解决如何管理本机上的多个ssh key的问题。\n<!-- more -->\n\n\n\n1、进入到`~/.ssh  `\n\n打开你的git bash , 输入命令：\n\n```bash\ncd  ~/.ssh \n```\n\n2、将`C:\\Users\\wang1\\.ssh`目录下的**id_rsa**和**id_rsa.pub**重新命名（如果有的话），防止被覆盖。比如，我命名为wxler_id_rsa和wxler_id_rsa.pub。\n\n3、生成 新的 ssh密钥\n\n生成第二个ssh key\n\n```bash\nssh-keygen -t rsa -C \"你的github注册邮箱地址\"\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225191011.png)\n\n- Enter file in which to save the key (/c/Users/wang1/.ssh/id_rsa)：提示你把新生成的id_rsa存放到哪里，此处默认会存放在C盘的用户名下的.ssh文件夹下（即你第一个github用户ssh key存放的目录）。这里我只填写了公钥和秘钥文件的名称，所以会在c/Users/wang1/.ssh/目录下生成两个文件：mylayne_id_rsa（秘钥文件）和mylayne_id_rsa.pub（公钥文件）。\n- Enter passphrase (empty for no passphrase):  提示你输入提交项目时输入的验证密码，这是为了防止别人随便在你的项目上push东西。如果你不想每次操作都输入密码，可以不填。\n\n\n\n4、添加新 ssh Key\n\n**默认SSH只会读取id_rsa，所以为了让SSH识别新的私钥，需要将其添加到SSH agent**。\n\n```bash\nssh-add ~/.ssh/mylayne_id_rsa\n```\n\n如果报错：Could not open a connection to your authentication agent，即无法连接到ssh agent\n可执行`ssh-agent bash`命令后再执行`ssh-add`命。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225191718.png)\n\n\n\n5、配置config文件\n\n查看.ssh文件夹中是否存在config文件，如果已存在则直接编辑config文件，命令：`vim config`。\n\n如果不存在则需要创建config文件，命令：`touch config`，再对config文件进行编辑。\n\n对config文件进行配置填写：\n\n```bash\nHost github.com  #把默认的常用的github Host设为github.com较好\n    HostName github.com\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/wxler_id_rsa\n    \nHost mylayne\n    HostName github.com\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/mylayne_id_rsa\n```\n\nconfig文件主要是为了提交远程仓库的时候，ssh 做区分用的。\n\n主要区分是通过host区分的，所以在以后的push和pull过程中，如果用的第一个，都是正常pull和push，但是对于第二个，要改成自己设置的别名mylayne。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225192443.png)\n\n\n\n5、测试是否配置成功\n\n```bash\nssh -T git@github.com\nssh -T git@mylayne\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225192632.png)\n\n出现如图的欢迎语则为配置成功。但是，执行`ssh -T git@mylayne`出现 permission denied 的错误。\n\n问题解决:  登录mylayne的github账户,  将ssh文件夹中的公钥（ mylayne_id_rsa.pub）添加到GitHub管理平台中，在GitHub的个人账户的设置中找到如下界面，title随便起一个，将公钥（ id_rsa.pub）文件中内容复制粘贴到key中，然后点击Add SSH key：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133430.png)\n\n这样就成功了，如下图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225195147.png)\n\n\n\n\n\n6、配置完成后，在连接Host不是github.com的github仓库时，远程库的地址要对应地做一些修改，比如现在添加mylayne帐号下的一个仓库testABC，则需要使用如下命令这样添加：\n\n```bash\ngit clone git@mylayne:mylayne/testABC.git\n```\n\n而并非原来的`git@github.com:wxler/testABC.git`。\n\n执行完上述命令后，再执行`git push`或`git pull`等操作就是在mylayne用户下操作，是不是很方便。\n\n\n\n\n\n【参考文档】\n\n [git:如何管理本机的多个ssh密钥](https://segmentfault.com/a/1190000005607713)\n\n[git本地管理多个密钥/账户(原创)](https://blog.csdn.net/qq_37210523/article/details/80993994?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs&dist_request_id=25343666-536b-4514-bf2c-bee59cc9d479&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs)\n\n[本地管理多个git账号](https://blog.csdn.net/walkstep/article/details/84824512)\n\n\n\n\n\n\n\n\n","tags":["git"],"categories":["工具"]},{"title":"在Github中进行pull request操作","url":"/2021/02/25/170421/","content":"\n本文介绍了如何在Github中进行pull request，如何成为github项目合作者，以及如何进行GitHub多人协作。\n\n<!-- more -->\n\n\n\n## 使用Github中的pull request\n\npull request是社会化编程的象征，通过这个功能，你可以参与到别人开发的项目中，并做出自己的贡献。pull request是自己修改源代码后，请求对方仓库采纳的一种行为，简称PR。\n\n下面具体说一下github中使用pull request的具体步骤：\n\n（1）fork想要pull request的项目\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225222811.png)\n\n点击fork按钮之后，你的github账户上会多出一个仓库。该仓库就是你本人的仓库，以后通过这个仓库进行pull request。\n\n（2）在fork的仓库上添加内容\n\n在自己github账户上，对刚刚fork的仓库添加自己的内容，可以直接在github网页上操作。当然，也可以通过相关工具`git clone`之后在本地上操作，之后再push到github上即可。\n\n（3）创建pull request\n\n在fork的仓库下，点击Pull requests，在里面再点击New pull request\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210228111033.png)\n\n这时，会进入一个新页面，有Base 和 Head 两个选项。**Base 是你希望提交变更的目标，Head 是目前包含你的变更的那个分支或仓库**。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210228111357.png)\n\n\n\n如果base是被你fork的仓库（即项目管理者的仓库），head是自己github账户上的仓库，点击Create pull request之后，就是把你本人的内容推送给项目管理者。\n\n如果base是自己github账户上的仓库，head是被你fork的仓库，**表示拉取项目管理者仓库最新的内容**。\n\n（4）提交pull request\n\n点击Create pull request之后，填写说明，然后再点一次Create pull request的按钮即可。**需要注意的是，在提交PR之前，一定要判断是否冲突**。如果有冲突，项目管理者（或合作者）需要解决冲突后才能合并PR。为了尽可能少的避免冲突，请务必在每一次创建PR之前，先在第（3）步里面拉取项目管理者仓库最新的内容，然后再创建和提交PR，这样就不会有冲突了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210228112346.png)\n\nPR 创建和提交后，管理者就要决定是否接受该 PR。如果接受了，PR提交的内容就同步到项目管理者的仓库中，这样所有的人就都可以看到。\n\n当然，**如果成为该项目的合作者，自己就可以点击上图的Merge pull request来合并提交的PR**（非合作者无法自己同意提交的PR），因此，合作者不需要管理者同意，就可以将提交的内容同步到项目管理者的仓库。如何成为合作者呢？请继续往下看。\n\n\n\n\n\n## 成为github项目合作者\n\ngithub项目合作者权限很大，可以合并提交的PR，也可以创建自己的分支或删除项目中的内容。也就是说，除了不能删除项目本身外，合作者拥有和管理者一样的权限。但是如果操作失误，处理起来很麻烦。所以，请熟练掌握github的使用后，再申请成为合作者。\n\n如果想成为github项目合作者，可以把自己github的用户名或者注册github的邮箱发给项目管理者，管理者邀请之后，Github会发一份邮件到你的邮箱中，你收到后点击接受即可，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225174426.png)\n\n成为github项目合作者以后，如果你提交了PR或通过其它方式提交了自己修改的内容，可以在管理者仓库下看到你的用户名，如下所示：\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210228113650.png)\n\n\n\n\n\n\n\n## GitHub多人协作\n\nGitHub多人协作是除了pull request之后另外一种协作方式。\n\n先看下这即几篇文章：\n\n1. [Git GitHub多人协作](https://segmentfault.com/a/1190000015798490)\n2. [[Git & GitHub] 怎么团队合作多人开发项目](https://blog.csdn.net/dietime1943/article/details/81391835?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_baidulandingword-0&spm=1001.2101.3001.4242)\n3. [多人共同使用一个github的仓库，实现版本管理，多人开发一个项目](https://blog.csdn.net/wjn19921104/article/details/80176251)\n\n大致知道什么是GitHub多人协作。\n\nGitHub多人协作的前提是：只要成为github该项目的合作者即可，不需要fork项目。\n\n成为github合作者后，就可通过git相关工具就可以操作`https://github.com/wxler/DailyAlgorithms.git`项目了。\n\nGitHub多人协作最容易出现冲突或其他的问题，如果在Github使用不熟练的情况下，请不要使用该方式。\n\n\n\n【参考资料】\n\n1. [github上fork原项目，如何将本地仓库代码更新到最新版本？](https://www.cnblogs.com/eyunhua/p/8463200.html)\n2. [github fork 别人的项目源作者更新后如何同步更新](https://blog.csdn.net/zhongzunfa/article/details/80344585)\n3. [如何使用github中的pull request功能？](https://blog.csdn.net/wangzi11111111/article/details/79861056)\n4. [Github中Pull Request操作](https://www.jianshu.com/p/a31a888ac46b)\n\n\n\n\n\n","tags":["git"],"categories":["工具"]},{"title":"Notepad++运行java代码，并设置自动格式化Java代码","url":"/2021/02/25/123719/","content":"\nNotepad++运行java代码，并设置自动格式化Java代码。\n<!-- more -->\n\n## Notepad++运行java代码\n\n1、下载插件\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225125342.png)\n\n2、在弹出的显示插件的窗口中勾上NppExec这个插件，再点install（因为我这里已经安装完了所以没有显示这个插件出来）\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225124224.png)\n\n如果显示安装失败，百度或Google上搜索NppExec插件。 在下载的压缩包里，把里面的文件夹的.dll文件解压到Notepad++文件夹下的plugins中。（注意，新建一个NppExec文件夹）\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225124427.png)\n\n3、安装后重启Notepad++，就能在Notepad++里看到下面这个东西：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225125414.png)\n\n点Execute，在下面的文本域中输入下面命令参数：\n\n```bash\nNPP_SAVE\nC:\\Program Files\\Java\\jdk1.8.0_77\\bin\\javac.exe -encoding UTF-8 \"$(FULL_CURRENT_PATH)\"\nC:\\Program Files\\Java\\jdk1.8.0_77\\bin\\java.exe  -cp \"$(CURRENT_DIRECTORY)\" \"$(NAME_PART)\"\n```\n\n*javac.exe和java.exe左侧是java的安装目录，在填写时一定要换成自己的java安装目录。*\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225124718.png)\n\n\n\n 点击save，自己起个编译命令的名称，最后点OK。\n\n4、按F6快捷键，点击OK，控制台出现代码运行的结果，就说明配置成功了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225124830.png)\n\n5、在NppExec中，打开Advanced Options配置信息\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225125247.png)\n\n6、配置菜单信息\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225125542.png)\n\n7、配置完成后重启notepad++\n\n8、点击-宏，可以看到刚配置的 java\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225125629.png)\n\n在宏下，点击管理快捷键\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225125654.png)\n\n\n\n9、点击插件命令，选择java，点击修改，我配置的是ctrl+1有冲突，把有冲突的快捷键清空就行。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225125758.png)\n\n10、以后写好java后直接 按ctrl+1可以直接编译并运行\n\n\n\n## Notepad++自动格式化Java代码\n\n1、进入[https://github.com/ywx/NppAStyle/releases](https://github.com/ywx/NppAStyle/releases)，找到Assets，下载与自己安装Notepad++位数相等的插件压缩包。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225130103.png)\n\n在这里查看自己的Notepad++版本\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210228143723.png)\n\n2、解压下载的压缩包，在Notepad++安装目录的 plugins 文件夹里新建一个NppAStyle文件夹，把 NppAStyle.dll 剪切该文件夹下。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225130416.png)\n\n3、重启Notepad++\n\n4、在插件下可看到NppAStyle，点击Options。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225130523.png)\n\n5、在右侧选择Java，在下面选中Replace Tab With Spaces，点击OK。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225130633.png)\n\n6、在宏下点击管理快捷键\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225130803.png)\n\n7、点击插件命令，找到Format Code配置快捷键。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210225130846.png)\n\n8、需要格式化代码的时候，按下快捷键 Alt + F ，或者 菜单栏：插件->NppAStyle->Format Code，代码就整齐了。\n\n\n\n\n\n\n\n","tags":["Notepad++"],"categories":["工具"]},{"title":"ZooKeeper实战篇-zk集群搭建、zkCli.sh操作、权限控制ACL、ZooKeeper JavaAPI使用","url":"/2021/02/23/101109/","content":"\n\n\n在看了[史上最全的Zookeeper原理详解(万字长文)](https://blog.csdn.net/qq_37555071/article/details/114609145 \"Zookeeper原理详解\")，了解Zookeeper的原理后，你是不是蠢蠢欲动想着手实践呢？这篇文章将手把手教你在Linux上搭建ZooKeeper集群，并调用相关API实现自己的Zookeeper应用。\n\n\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n\n\n\n\n\n\n## 1. Linux上搭建ZooKeeper集群\n\n在这之前，欢迎参考我之前的文章对虚拟机进行相关配置：[Linux切换运行级别、关闭防火墙、禁用selinux、关闭sshd、时间同步、修改时区、拍摄快照、克隆操作](https://mp.weixin.qq.com/s/kk1sY9EYajaEt_mevwmfIg)，否则后面可能会出现意想不到的错误。\n\n\n\n\n\n### 1.1 多台服务器之间免密登录\n\n为什么要实现多台服务器之间免密登录？\n\n因为zookeeper之间选举也好、投票也好，互相之间都会传递消息进行通信的，为了方便未来的管理，我们要实现多台服务器之间免密登陆。\n\n现在我们就实现在Linux上搭建ZooKeeper集群吧，下面先介绍授权的两个文件：\n\n- `id_dsa.pub` 存放每台服务器自己的公钥 \n- `authorized_keys` 存放的也是服务器的公钥，不过除了自己的公钥外，也可以存放其它服务器的公钥。\n\n\n\n下面我准备了四台虚拟机，主机名分别为layne1、layne2、layne3、layne4，实现四台服务器之间免密登录。\n\n1. 首先在每个服务器上产生自己的公钥，在每台服务器上执行以下命名，产生的公钥文件id_dsa.pub存放在`/root/.ssh`下：\n\n   ```bash\n   ssh-keygen  -t  dsa  -P  ''  -f  ~/.ssh/id_dsa\n   ```\n\n2. 在layne1上将其公钥写入到authorized_keys中\n\n   ```bash\n   cat  ~/.ssh/id_dsa.pub  >>  ~/.ssh/authorized_keys\n   ```\n\n3. 将layne1上的authorized_keys文件拷贝给layne2，在layne1上执行如下命令即可\n\n   ```bash\n   scp  ~/.ssh/authorized_keys   layne2:/root/.ssh/\n   ```\n\n4. 在layne2上将其公钥追加到authorized_keys中\n\n   ```bash\n   cat  ~/.ssh/id_dsa.pub  >>  ~/.ssh/authorized_keys\n   ```\n\n5. 将layne2上的authorized_keys文件拷贝给layne3\n\n   ```bash\n   scp  ~/.ssh/authorized_keys   layne3:/root/.ssh/\n   ```\n\n6. 在layne3上将其公钥追加到authorized_keys中\n\n   ```bash\n   cat  ~/.ssh/id_dsa.pub  >>  ~/.ssh/authorized_keys\n   ```\n\n7. 将layne3上的authorized_keys文件拷贝给layne4\n\n   ```bash\n   scp  ~/.ssh/authorized_keys   layne4:/root/.ssh/\n   ```\n\n8. 在layne4上将其公钥追加到authorized_keys中\n\n   ```bash\n   cat  ~/.ssh/id_dsa.pub  >>  ~/.ssh/authorized_keys\n   ```\n\n9. 将layne4的authorized_keys文件分别拷贝给layne1、layne2、layne3\n\n   ```bash\n   scp  ~/.ssh/authorized_keys   layne1:/root/.ssh/\n   scp  ~/.ssh/authorized_keys   layne2:/root/.ssh/\n   scp  ~/.ssh/authorized_keys   layne3:/root/.ssh/\n   ```\n\n至此，我们将完成了layne1~4虚拟机之间的免密登录。\n\n在任意虚拟机的shell命令行里，我们就可以通过`ssh 主机名`随意连接其他的虚拟机，而不需要输入密码。比如，在layne1上连接layne4，只需要输入以下命令：\n\n```bash\n[root@layne1 ~]# ssh layne4 # 第一次连接需要输入yes|no，不需要输入密码\nThe authenticity of host 'layne4 (192.168.218.54)' can't be established.\nRSA key fingerprint is 2d:4c:3c:0c:2a:0f:50:bc:a2:8d:c1:2f:8a:7d:63:c4.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'layne4,192.168.218.54' (RSA) to the list of known hosts.\nLast login: Tue Feb 23 10:29:52 2021 from 192.168.218.1\n[root@layne4 ~]# \n\n```\n\n\n\n### 1.2 ZooKeeper集群搭建\n\n在搭建ZooKeeper集群之前，先要在所有虚拟机上安装jdk，我之前的好多博客都详细描述了jdk的安装方法，这里就不介绍了，有需要的小伙伴，可参考[Linux上通过rpm安装jdk](https://wxler.github.io/2021/02/10/112210/#rpm%E5%AE%89%E8%A3%85jdk \"Linux中jdk安装方式\")。\n\n我安装的jdk版本是`jdk-8u221-linux-x64`，下面我在主机名为layne2、layne3、layne4的虚拟机搭建ZooKeeper集群。\n\n1. 在`https://zookeeper.apache.org/releases.html`上下载zookeeper的Linux压缩包。\n\n2. 将zookeeper的压缩包`zookeeper-3.4.6.tar.gz`上传到layne2上。\n\n3. 解压至`/opt`目录下\n\n   ```bash\n   tar -zxvf zookeeper-3.4.6.tar.gz -C /opt\n   ```\n\n4. 配置zookeeper的环境变量，执行`vim /etc/profile`，在末尾加入：\n\n   ```bash\n   export ZOOKEEPER_HOME=/opt/zookeeper-3.4.6\n   export PATH=$PATH:$ZOOKEEPER_HOME/bin\n   ```\n\n   然后执行`source /etc/profile`，让配置生效。\n\n5. 进入zookeeper的安装目录的conf下\n\n   ```bash\n   [root@layne2 apps]# cd $ZOOKEEPER_HOME/conf\n   [root@layne2 conf]# pwd\n   /opt/zookeeper-3.4.6/conf\n   ```\n\n6. 复制`zoo_sample.cfg`文件为`zoo.cfg`\n\n   ```bash\n   cp zoo_sample.cfg zoo.cfg\n   ```\n\n7. 先介绍`zoo.cfg`参数说明，然后再进行配置。\n\n   - `tickTime=2000` ：客户端与服务器或者服务器与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一次心跳，默认心跳时间为2000ms。通过心跳不仅能够用来监听机器的工作状态，还可以通过心跳来控制Flower跟Leader的通信时间。zookeeper的客户端和服务端之间也有和web开发里类似的session的概念，而zookeeper里最小的session过期时间通常是tickTime的两倍。\n   - `dataDir=/tmp/zookeeper`：用于保存 Zookeeper 中的数据，同时用于zookeeper集群的myid文件也存在这个文件夹里。默认路径为`/tmp/zookeeper`，最好不要使用`/tmp`，因为临时目录下，操作系统会定时清理里面的文件，可能会造成出乎意料的错误。\n   - `dataLogDir`：存放日志的目录。\n   - `clientPort=2181`：客户端连接zookeeper服务器的端口，zookeeper会监听这个端口，接收客户端的请求访问，这个端口默认是2181。\n   - `initLimit`：集群中的follower服务器(F)与leader服务器(L)之间**初始化连接时**最长能忍受多少个心跳时间间隔数。如果配置的是5，当已经超过 5 个心跳的时间（也就是 tickTime）长度后 ，ZooKeeper 服务器还没有收到客户端（即follower服务器，相对于 leader 而言的客户端）的返回信息，那么表明这个客户端连接失败，此时总的时间长度就是 5\\*2000=10秒。 如果在设定的时间段内，半数以上的跟随者未能完成同步（即初始时的选举），领导者便会宣布放弃领导地位，进行另一次的领导选举。如果zk集群环境数量确实很大，同步数据的时间会变长，因此这种情况下可以适当调大该参数。\n   - `syncLimit`：标识 Leader 与 Follower 之间请求和应答能容忍的最多心跳数，如果配置的是4，总的时间长度就是 4\\*2000=8 秒。如果 follower 在设置的时间内不能与leader 进行通信，那么此 follower 将被丢弃，此时所有关联到这个跟随者的客户端将连接到另外一个跟随着。\n   - `server.A=B：C：D`：其 中 A 是一个数字，表示这个是第几号服务器（和myid对应）；B 是这个服务器的ip地址（或主机名）；C 表示的是这个服务器与集群中的Leader服务器交换信息的端口（即follower与Leader交换信息的端口）；D表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口（选举时的端口）。如果是伪集群的配置方式，由于B都是一样的，所以不同的ZooKeeper实例通信端口号不能一样，要给C和D分配不同的端口号。  \n\n8. 根据上面的参数说明，对`zoo.cfg`进行配置，配置如下：\n\n   ```bash\n   # The number of milliseconds of each tick\n   tickTime=2000\n   # The number of ticks that the initial \n   # synchronization phase can take\n   initLimit=5\n   # The number of ticks that can pass between \n   # sending a request and getting an acknowledgement\n   syncLimit=2\n   # the directory where the snapshot is stored.\n   # do not use /tmp for storage, /tmp here is just \n   # example sakes.\n   dataDir=/opt/zookeeper-3.4.6/data\n   # zk log dir\n   dataLogDir=/var/log/zookeeper/datalog \n   # the port at which the clients will connect\n   clientPort=2181\n   # server list\n   server.1=layne2:2881:3881\n   server.2=layne3:2881:3881\n   server.3=layne4:2881:3881\n   ```\n\n   \n\n9. 创建`/var/log/zookeeper/datalog`和`/opt/zookeeper-3.4.6/data`目录\n\n   ```bash\n   [root@layne2 conf]# mkdir -p /var/log/zookeeper/datalog\n   [root@layne2 conf]# mkdir /opt/zookeeper-3.4.6/data\n   ```\n\n10. 在`/opt/zookeeper-3.4.6/data`目录下创建一个名为myid的文件，在myid中写下当前ZooKeeper的编号\n\n   ```bash\n   [root@layne2 data]# pwd\n   /opt/zookeeper-3.4.6/data\n   [root@layne2 data]# touch myid\n   [root@layne2 data]# echo 1 > /opt/zookeeper-3.4.6/data/myid\n   ```\n\n11. 将配置好Zookeeper拷贝到layne3、layne4上\n\n    ```bash\n    scp -r /opt/zookeeper-3.4.6/ layne3:/opt/\n    scp -r /opt/zookeeper-3.4.6/ layne4:/opt/\n    ```\n\n12. 在layne3和layne4上分别修改myid\n\n    ```bash\n    echo 2 > /opt/zookeeper-3.4.6/data/myid\n    echo 3 > /opt/zookeeper-3.4.6/data/myid\n    ```\n\n13. 在layne3和layne4配置Zookeeper的环境变量，并创建`/var/log/zookeeper/datalog`目录，参考步骤3和步骤8。\n\n14. 分别启动layne2、layne3、layne4上的ZooKeeper\n\n    ```bash\n    # 进入/opt/zookeeper-3.4.6/bin目录下操作\n    zkServer.sh start #启动zk\n    zkServer.sh stop  #停止zk\n    zkServer.sh status  #查看zk状态\n    zkCli.sh # 连接ZooKeeper客户端\n    ```\n\n15. 启动3台虚拟机上的ZooKeeper之后，如果报错`Will not attempt to authenticate using SASL`，报错原因是我们在实现多台服务器之间免密登陆的时候，**两台服务器之间一次都没有进行连接**。只要第一次连接之后，两台服务器之间才能免密登陆和授权。在layne2上执行下面三条命令，即可和layne3、layne4免密登陆。同理，layne3、layne4也如此。\n\n    ```bash\n    # 在layne2、layne3、layne4全部都要执行下述命令\n    ssh layne2 #自己也要和自己连接一次，即自己授权自己\n    exit\n    ssh layne3\n    exit\n    ssh layne4\n    exit\n    ```\n\n16. 再次尝试启动ZooKeeper，如果还报错，重启所有的虚拟机就好了。\n\n\n\n以上步骤就是搭建Zookeeper集群的完整过程，如果Zookeeper启动不了，或者是启动报错，可能是以下原因造成的：\n\n1. 没有创建zookeeper的日志目录`/var/log/zookeeper/datalog`。\n2. 没有在每个服务器的myid写入正确的编号。\n3. 没有用`ssh 主机名`在任意两台服务器之间进行第一次连接\n4. 如果不是上面3个原因，重启一下所有的虚拟机就好了。\n\n\n\n## 2. zkCli.sh客户端操作\n\nzkCli是 Zookeeper的一个简易客户端，下面讲解通过zkCli.sh客户端操作znode节点。\n\n### 2.1 打开客户端\n\n在Zookeeper服务端开启的情况下，运行客户端，使用命令：`zkCli.sh`\n\n若连接不同的主机，可使用命令：`zkCli.sh -server ip:port`，如`zkCli.sh -server 192.168.218.52:2181`，此处的端口是zoo.cfg配置文件中clientPort。\n\n连接客户端以后，可以使用`help`命令来查看客户端的操作\n\n```bash\n[zk: localhost:2181(CONNECTED) 1] help\nZooKeeper -server host:port cmd args\n\tstat path [watch]\n\tset path data [version]\n\tls path [watch]\n\tdelquota [-n|-b] path\n\tls2 path [watch]\n\tsetAcl path acl\n\tsetquota -n|-b val path\n\thistory \n\tredo cmdno\n\tprintwatches on|off\n\tdelete path [version]\n\tsync path\n\tlistquota path\n\trmr path\n\tget path [watch]\n\tcreate [-s] [-e] path data acl\n\taddauth scheme auth\n\tquit \n\tgetAcl path\n\tclose \n\tconnect host:port\n```\n\n### 2.2 创建节点\n\n使用create命令，可以创建一个Zookeeper节点，格式为：\n\n```bash\ncreate [-s] [-e] path data acl\n```\n\n- **[-s] [-e]**：-s 和 -e 都是可选的，-s 代表顺序节点， -e 代表临时节点，注意其中 -s 和 -e 可以同时使用，都不使用，则代表普通节点。需要注意的是，临时节点不能再创建子节点。\n- **path**：指定要创建节点的路径，比如 **/zk01**。\n- **data**：要在此节点存储的数据。\n- **acl**：访问权限相关，默认是 world，相当于全世界都能访问，请看后面第3节**Zookeeper权限控制ACL**。\n\n**①创建永久顺序节点**\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 5] create -s /zk01-seq 123 # 创建顺序节点\nCreated /zl01-seq0000000009\n```\n\n可以看到创建的zk01-seq节点后面添加了一串数字以示区别。\n\n**②创建临时顺序节点**\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 1] create -s -e /zk01-tmp-seq 1234\nCreated /zk01-tmp-seq0000000013\n```\n\n**③创建普通临时节点**\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 10] create -e /zk01-tmp 456\nCreated /zk01-tmp\n```\n\n临时节点在客户端会话结束后，就会自动删除，下面使用**quit**命令退出客户端\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 11] quit\nQuitting...\n2021-03-01 21:06:58,605 [myid:] - INFO  [main:ZooKeeper@684] - Session: 0x177ed505b850002 closed\n```\n\n再次使用客户端连接服务端，并使用ls / 命令查看根目录下的节点\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 0] ls /\n[zookeeper, zk01-seq0000000011]\n```\n\n可以看到根目录下已经不存在zk01-tmp临时节点了。\n\n**④创建普通永久节点**\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 2] create /zk01-permanent 123\nCreated /zk01-permanent\n```\n\n可以看到普通节点不同于顺序节点，不会自动在后面添加一串数字。\n\n### 2.3 读取节点\n\n与读取相关的命令ls、ls2、get和stat命令。\n\n**①ls命令**\n\nls 命令用于查看某个路径下的znode节点（只能查看第一级目录的所有子节点），格式为`ls path`，例如：\n\n- `ls /` \n- `ls /zookeeper`\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 3] ls / # 查看根目录下的znode节点\n[zookeeper, zk01-seq0000000011, zk01-tmp-seq0000000013, zk01-permanent]\n[zk: 192.168.218.52:2181(CONNECTED) 4] ls /zk01-permanent # 查看zk01-permanent目录下的znode节点\n[]\n```\n\n\n\n**②ls2命令**\n\nls2 命令也是用于查看某个路径下的znode节点，格式同ls，**但它能同时显示该路径节点的信息。**\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 5] ls2 / #查看根目录下的znode节点，并显示根节点的信息\n[zookeeper, zk01-seq0000000011, zk01-tmp-seq0000000013, zk01-permanent]\ncZxid = 0x0\nctime = Thu Jan 01 08:00:00 CST 1970\nmZxid = 0x0\nmtime = Thu Jan 01 08:00:00 CST 1970\npZxid = 0x600000022\ncversion = 26\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 0\nnumChildren = 4\n\n```\n\n\n\n**③get 命令**\n\nget 命令用于获取某个znode节点**数据和状态信息**。其格式为：\n\n```bash\nget path [watch]\n```\n\n- path：代表路径\n- [watch]：对该节点进行事件监听，该参数为可选参数。\n\n以下示例我们同时开启两个终端，对zk01节点进行监听：\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 9] create /zk01 zk01Content # 创建zk01普通持久节点\nCreated /zk01\n[zk: 192.168.218.52:2181(CONNECTED) 10] get /zk01 watch #在当前终端上对zk01事件监听\nzk01Content\ncZxid = 0x600000023\nctime = Mon Mar 01 21:20:26 CST 2021\nmZxid = 0x600000023\nmtime = Mon Mar 01 21:20:26 CST 2021\npZxid = 0x600000023\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 11\nnumChildren = 0\n[zk: localhost:2181(CONNECTED) 0] set /zk01 zk01ABC #在终端二上修改zk01节点的数据\ncZxid = 0x600000023\nctime = Mon Mar 01 21:20:26 CST 2021\nmZxid = 0x600000024\nmtime = Mon Mar 01 21:24:07 CST 2021\npZxid = 0x600000023\ncversion = 0\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 7\nnumChildren = 0\n```\n\n此时，会在第一个终端上会输出NodeDataChanged 事件：\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 11]\nWATCHER::\n\nWatchedEvent state:SyncConnected type:NodeDataChanged path:/zk01\n```\n\n\n\n**④stat 命令**\n\nstat 命令用于查看某个节点状态信息。该命令除了不输出节点的内容之后，输出的其他信息和get命令一致。\n\n其格式为：\n\n```bash\nget path [watch]\n```\n\n- path：代表路径\n- [watch]：对该节点进行事件监听，该参数为可选参数。\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 12] stat /zk01 #查看zk01节点的信息\ncZxid = 0x600000023\nctime = Mon Mar 01 21:20:26 CST 2021\nmZxid = 0x600000024\nmtime = Mon Mar 01 21:24:07 CST 2021\npZxid = 0x600000023\ncversion = 0\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 7\nnumChildren = 0\n```\n\n\n\n### 2.4 更新节点\n\n使用set命令，可以更新指定节点的数据内容，其格式：\n\n```bash\nset path data [version]\n```\n\n- **path**：节点路径。\n- **data**：需要存储的数据。\n- **[version]**：可选项，版本号(可用作乐观锁)。\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 13] get /zk01\nzk01ABC\ncZxid = 0x600000023\nctime = Mon Mar 01 21:20:26 CST 2021\nmZxid = 0x600000024\nmtime = Mon Mar 01 21:24:07 CST 2021\npZxid = 0x600000023\ncversion = 0\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 7\nnumChildren = 0\n```\n\n可以看到，zk01节点dataVersion为1，下面只有正确的版本号才能设置成功：\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 14] set /zk01 123 0 #带上版本号，只有正确的版本才能执行\nversion No is not valid : /zk01\n[zk: 192.168.218.52:2181(CONNECTED) 15] set /zk01 456 1 \ncZxid = 0x600000023\nctime = Mon Mar 01 21:20:26 CST 2021\nmZxid = 0x600000026\nmtime = Mon Mar 01 21:29:11 CST 2021\npZxid = 0x600000023\ncversion = 0\ndataVersion = 2\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 3\nnumChildren = 0\n[zk: 192.168.218.52:2181(CONNECTED) 16] set /zk01 789 3\nversion No is not valid : /zk01\n[zk: 192.168.218.52:2181(CONNECTED) 17] set /zk01 555  #不加版本号，均可以执行成功\ncZxid = 0x600000023\nctime = Mon Mar 01 21:20:26 CST 2021\nmZxid = 0x600000028\nmtime = Mon Mar 01 21:30:30 CST 2021\npZxid = 0x600000023\ncversion = 0\ndataVersion = 3\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 3\nnumChildren = 0\n\n```\n\n\n\n### 2.5 删除节点\n\ndelete 命令用于删除某节点。格式为：\n\n```bash\ndelete path [version]\n```\n\n- **path**：节点路径。\n- **[version]**：可选项，版本号（同 set 命令）。\n\n```bash\n[zk: 192.168.218.52:2181(CONNECTED) 18] delete /zk01-permanent #删除zk01-permanent节点\n```\n\n**若删除节点存在子节点，那么无法删除该节点，必须先删除子节点，再删除父节点。**\n\n\n\n## 3. Zookeeper 权限控制 ACL\n\nZookeeper 的 ACL（Access Control List，访问控制表）权限在生产环境是特别重要的，ACL 权限可以针对节点设置相关读写等权限，保障数据安全性。我们以zkCli.sh客户端为例，来说明zookeeper对ACL的设置。\n\nACL通过` [scheme:id:permissions]` 来构成权限列表。\n\n- **scheme**：代表采用的某种权限机制，包括 world、auth、digest、ip、super 几种。\n- **id**：代表允许访问的用户。\n- **permissions**：权限组合字符串，由 cdrwa 组成，其中每个字母代表支持不同权限， 创建权限 create(c)、删除权限 delete(d)、读权限 read(r)、写权限 write(w)、管理权限admin(a)。\n\n需要注意的是，zookeeper对权限的控制是znode级别的，不具有继承性，即子节点不继承父节点的权限。\n\nACL 命令有三个，分别是：\n\n- **getAcl 命令**：获取某个节点的 acl 权限信息。\n- **setAcl 命令**：设置某个节点的 acl 权限信息。\n- **addauth 命令**：输入认证授权信息，注册时输入明文密码，加密形式保存。\n\n### 3.1 world 实例\n\n这是默认方式，代表开放式权限。当创建一个新的节点(znode)，而又没有设置任何权限时，就是这个值，例如：\n\n```bash\n[zk: localhost:2181(CONNECTED) 51] create /node mynode\nCreated /node\n[zk: localhost:2181(CONNECTED) 52] getAcl /node\n'world,'anyone\n: cdrwa\n```\n\n可以看到，`/node`节点的ACL属于是world schema的，因为它没有设置ACL属性，这样任何人都可以访问这个节点。\n\n\n\n设置某一节点的权限命令为`setAcl`，语法格式为：\n\n```bash\nsetAcl <path> scheme:<id>:<acl> \n```\n\n>  setAcl命令中的id域是可忽略的，可以填任意值，或者空串，例如：`setAcl <path> auth::crdwa`。如果这个域是忽略的，会把所有已经授权的认证用户都加进来。\n\n如果要手工设置world schema，那么此时的id域只允许一个值，即anyone，格式如下：\n\n```bash\nsetAcl /node world:anyone:crdwa\n```\n\n\n\n下面，设置`/node`节点 permissions 权限部分为 crwa\n\n```bash\n[zk: localhost:2181(CONNECTED) 53] setAcl /node world:anyone:crwa\ncZxid = 0x60000002a\nctime = Mon Mar 01 21:58:20 CST 2021\nmZxid = 0x60000002a\nmtime = Mon Mar 01 21:58:20 CST 2021\npZxid = 0x60000002a\ncversion = 0\ndataVersion = 0\naclVersion = 1\nephemeralOwner = 0x0\ndataLength = 6\nnumChildren = 0\n[zk: localhost:2181(CONNECTED) 54] getAcl /node #再次查看权限，发现permissions部分已经改变。\n'world,'anyone\n: crwa\n```\n\n\n\n### 3.2 auth 实例\n\nauth 用于给用户授予权限，授权之前需要先创建用户。\n\n语法格式为：\n\n```bash\naddauth digest <user>:<password>\n```\n\n下面，给lucy用户授权`/node`节点的权限：\n\n```bash\n[zk: localhost:2181(CONNECTED) 6] setAcl /node auth:lucy:123456:cdrwa\nAcl is not valid : /node\n[zk: localhost:2181(CONNECTED) 7] addauth digest user1:123456  #只有授权之后的用户才能设置权限\n[zk: localhost:2181(CONNECTED) 8] setAcl /node auth:lucy:123456:cdrwa\ncZxid = 0x600000032\nctime = Mon Mar 01 22:12:43 CST 2021\nmZxid = 0x600000032\nmtime = Mon Mar 01 22:12:43 CST 2021\npZxid = 0x600000032\ncversion = 0\ndataVersion = 0\naclVersion = 1\nephemeralOwner = 0x0\ndataLength = 6\nnumChildren = 0\n[zk: localhost:2181(CONNECTED) 9] getAcl /node #查看权限，发现scheme和id部分已经改变\n'digest,'user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=\n: cdrwa\n```\n\n再来看一个例子，会有奇怪的现象发生：\n\n```bash\n[zk: localhost:2181(CONNECTED) 10] quit  #退出\nQuitting...\n2021-03-01 22:31:30,855 [myid:] - INFO  [main:ZooKeeper@684] - Session: 0x177ed505b850004 closed\n2021-03-01 22:31:30,856 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@512] - EventThread shut down\n[root@layne2 bin]# zkCli.sh  #连接客户端，重新生成session\n[zk: localhost:2181(CONNECTED) 0] addauth digest user1:123456\n[zk: localhost:2181(CONNECTED) 1] addauth digest user2:123456\n[zk: localhost:2181(CONNECTED) 2] addauth digest user3:123456\n[zk: localhost:2181(CONNECTED) 3] getAcl /node\n'digest,'user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=\n: cdrwa\n[zk: localhost:2181(CONNECTED) 4] setAcl /node auth:user2:crdwa\ncZxid = 0x600000032\nctime = Mon Mar 01 22:12:43 CST 2021\nmZxid = 0x600000032\nmtime = Mon Mar 01 22:12:43 CST 2021\npZxid = 0x600000032\ncversion = 0\ndataVersion = 0\naclVersion = 2\nephemeralOwner = 0x0\ndataLength = 6\nnumChildren = 0\n[zk: localhost:2181(CONNECTED) 5] getAcl /node\n'digest,'user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=\n: cdrwa\n'digest,'user2:hZG2W+NR7DCvADzOkGR6JGLqoTY=\n: cdrwa\n'digest,'user3:SzpfOOuDCdri8p4n7oIaFCZpXeE=\n: cdrwa\n\n```\n\n这个例子中，我们先添加了三个授权用户user1、user2、user3，然后通过setAcl设置ACL，命令中指定了id为user2，可以看到，最后通过getAcl查询出来的结果包含所有前面添加的三个认证用户。\n\n下面做几点总结（重要）：\n\n1. setAcl命令中的id值是无效的，当使用addauth命令授权多个用户后，再用setAcl设置ACL时，会把当前会话所有addauth的用户都被会加入到acl中。\n2. **通过addauth命令(`addauth digest <username>:<password>`)授权的用户只在当前会话(session)有效**。\n3. **setAcl命令设置权限后是永久式的**，即使当前会话退出也不会消失。\n4. 如果在当前会话中，用户没有通过addauth授权就用setAcl设置acl权限时会失败。\n5. 使用setAcl来设置acl权限后，经过addauth授权其它的用户，如果再使用setAcl设置权限 ，则会覆盖之前的acl权限信息，而且只会针对当前会话中的授权用户来设置acl权限。\n\n所以这种授权方式更倾向于用作测试开发环境，而不是产品环境中。\n\n### 3.3 digest 实例\n\n这就是最普通的`用户名:密码`的验证方式，在一般业务系统中最常用。其语法格式如下：\n\n```bash\nsetAcl <path> digest:<user>:<password(密文)>:<acl>\n```\n\n和auth实例相比，digest 实例的密码是经过sha1及base64处理的密文。\n\n密码可以通过如下shell的方式生成：\n\n```bash\necho -n <user>:<password> | openssl dgst -binary -sha1 | openssl base64\n```\n\n也可以通过zookeeper的库文件生成：\n\n```bash\n# 方式一：通过linux自带的命令工具生成密码的密文\n[root@layne2 bin]# echo -n user1:123456 | openssl dgst -binary -sha1 | openssl base64\nHYGa7IZRm2PUBFiFFu8xY2pPP/s=\n\n# 方式二：通过zookeeper的库文件生成密码的密文\n[root@layne2 bin]# java -cp /opt/zookeeper-3.4.6/zookeeper-3.4.6.jar:/opt/zookeeper-3.4.6/lib/slf4j-api-1.6.1.jar \\\n>   org.apache.zookeeper.server.auth.DigestAuthenticationProvider \\\n>   user1:123456\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\nuser1:123456->user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=\n```\n\n把上面输出的`HYGa7IZRm2PUBFiFFu8xY2pPP/s=`传递给diges实例下setAcl使用的password域。\n\n```bash\n[zk: localhost:2181(CONNECTED) 10] quit #退出\nQuitting...\n2021-03-01 22:31:30,855 [myid:] - INFO  [main:ZooKeeper@684] - Session: 0x177ed505b850004 closed\n2021-03-01 22:31:30,856 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@512] - EventThread shut down\n[root@layne2 bin]# zkCli.sh #连接客户端，重新生成session\n[zk: localhost:2181(CONNECTED) 0] getAcl /node\n'digest,'user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=\n: cdrwa\n'digest,'user2:hZG2W+NR7DCvADzOkGR6JGLqoTY=\n: cdrwa\n'digest,'user3:SzpfOOuDCdri8p4n7oIaFCZpXeE=\n: cdrwa\n[zk: localhost:2181(CONNECTED) 1] setAcl /node digest:user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=:rwdca\nAuthentication is not valid : /node   #digest scheme用户也必须经过授权\n[zk: localhost:2181(CONNECTED) 2] addauth digest user1:123abc456 #用户的密码一定是生成密文的密码才行                             \n[zk: localhost:2181(CONNECTED) 3] setAcl /node digest:user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=:rwdca\nAuthentication is not valid : /node\n[zk: localhost:2181(CONNECTED) 4] addauth digest user1:123456 #这次是生成密文的密码                               \n[zk: localhost:2181(CONNECTED) 5] setAcl /node digest:user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=:rwdca\ncZxid = 0x600000032\nctime = Mon Mar 01 22:12:43 CST 2021\nmZxid = 0x600000032\nmtime = Mon Mar 01 22:12:43 CST 2021\npZxid = 0x600000032\ncversion = 0\ndataVersion = 0\naclVersion = 3\nephemeralOwner = 0x0\ndataLength = 6\nnumChildren = 0\n[zk: localhost:2181(CONNECTED) 6] getAcl /node\n'digest,'user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=\n: cdrwa\n[zk: localhost:2181(CONNECTED) 7] addauth digest user2:123456\n[zk: localhost:2181(CONNECTED) 8] setAcl /node digest:user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=:rwdca   #授权user2用户后，再次用setAcl为user1设置权限，看是否同时给user2设置权限\ncZxid = 0x600000032\nctime = Mon Mar 01 22:12:43 CST 2021\nmZxid = 0x600000032\nmtime = Mon Mar 01 22:12:43 CST 2021\npZxid = 0x600000032\ncversion = 0\ndataVersion = 0\naclVersion = 4\nephemeralOwner = 0x0\ndataLength = 6\nnumChildren = 0\n[zk: localhost:2181(CONNECTED) 9] getAcl /node #可以看到，没有为user2设置权限\n'digest,'user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=\n: cdrwa\n[zk: localhost:2181(CONNECTED) 10] \n\n```\n\n和auth比较，digest有如下特性：\n\n1. 授权是针对单个特定用户。\n2. setAcl使用的密码不是明文，是sha1摘要值，无法反推出用户密码内容。\n\n### 3.4 IP 实例\n\n限制 IP 地址的访问权限，比如把权限设置给 IP 地址为 192.168.218.54 后，IP 为 192.168.218.52 已经没有访问权限。\n\nIP地址也可以为主机名。主机名可以是单个主机名，也可以是域名。IP可以是单个IP地址，也可以是IP地址段\n\n```bash\n[zk: localhost:2181(CONNECTED) 10] create /testnode tnode\nCreated /testnode\n[zk: localhost:2181(CONNECTED) 11] getAcl /testnode\n'world,'anyone\n: cdrwa\n[zk: localhost:2181(CONNECTED) 15] setAcl /testnode ip:192.168.218.54:cdrwa\ncZxid = 0x60000003e\nctime = Mon Mar 01 23:18:00 CST 2021\nmZxid = 0x60000003f\nmtime = Mon Mar 01 23:18:55 CST 2021\npZxid = 0x60000003e\ncversion = 0\ndataVersion = 1\naclVersion = 1\nephemeralOwner = 0x0\ndataLength = 23\nnumChildren = 0\n[zk: localhost:2181(CONNECTED) 16] getAcl /testnode                        \n'ip,'192.168.218.54\n: cdrwa\n[zk: localhost:2181(CONNECTED) 17] get /testnode                           \nAuthentication is not valid : /testnode\n\n```\n\n这时，通过192.168.218.54连接192.168.218.52中的zkCli.sh，就有访问权限：\n\n```bash\n[root@layne4 version-2]# zkCli.sh -server 192.168.218.52:2181\n[zk: 192.168.218.52:2181(CONNECTED) 0] getAcl /testnode\n'ip,'192.168.218.54\n: cdrwa\n[zk: 192.168.218.52:2181(CONNECTED) 1] get /testnode\ntnode\ncZxid = 0x60000003e\nctime = Mon Mar 01 23:18:00 CST 2021\nmZxid = 0x60000003f\nmtime = Mon Mar 01 23:18:55 CST 2021\npZxid = 0x60000003e\ncversion = 0\ndataVersion = 1\naclVersion = 1\nephemeralOwner = 0x0\ndataLength = 23\nnumChildren = 0\n\n```\n\n### 3.5 super用户\n\n设置一个超级用户，这个超级用户的设置必须在zookeeper内部，在zookeeper启动之前设置好。在这种scheme情况下，超级用户具有超级权限，可以做任何事情(cdrwa)，不需要授权。\n\n我们通过digest scheme方式只为user1设置d权限：\n\n```bash\n[zk: localhost:2181(CONNECTED) 23] setAcl /node digest:user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=:d \ncZxid = 0x600000032\nctime = Mon Mar 01 22:12:43 CST 2021\nmZxid = 0x600000032\nmtime = Mon Mar 01 22:12:43 CST 2021\npZxid = 0x600000032\ncversion = 0\ndataVersion = 0\naclVersion = 5\nephemeralOwner = 0x0\ndataLength = 6\nnumChildren = 0\n[zk: localhost:2181(CONNECTED) 24] get /node\nAuthentication is not valid : /node\n[zk: localhost:2181(CONNECTED) 25] getAcl /node\n'digest,'user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=\n: d\n[zk: localhost:2181(CONNECTED) 26] addauth digest root:123456\n[zk: localhost:2181(CONNECTED) 28] get /node                 \nAuthentication is not valid : /node\n```\n\n可以看到，usr1用户已经不能访问`/node`结点信息。同样的，root用户也不能。\n\n现在，我们为root用户添加为super用户：\n\n1、生成root用户的密文：\n\n```bash\n[root@layne2 bin]# echo -n root:123456 | openssl dgst -binary -sha1 | openssl base64\nu53OoA8hprX59uwFsvQBS3QuI00=\n```\n\n2、设置zookeeper环境变量SERVER_JVMFLAGS\n\n```bash\nexport SERVER_JVMFLAGS=\"-Dzookeeper.DigestAuthenticationProvider.superDigest=root:u53OoA8hprX59uwFsvQBS3QuI00=\"\n```\n\n3、重启zookeeper\n\n4、连接zkCli.sh客户端\n\n5、再次访问`/node`结点\n\n```bash\n[zk: localhost:2181(CONNECTED) 1] getAcl /node\n'digest,'user1:HYGa7IZRm2PUBFiFFu8xY2pPP/s=\n: d\n[zk: localhost:2181(CONNECTED) 2] get /node\nAuthentication is not valid : /node\n[zk: localhost:2181(CONNECTED) 3] addauth digest root:123456\n[zk: localhost:2181(CONNECTED) 4] get /node                 \nmynode\ncZxid = 0x600000032\nctime = Mon Mar 01 22:12:43 CST 2021\nmZxid = 0x600000032\nmtime = Mon Mar 01 22:12:43 CST 2021\npZxid = 0x600000032\ncversion = 0\ndataVersion = 0\naclVersion = 5\nephemeralOwner = 0x0\ndataLength = 6\nnumChildren = 0\n\n```\n\n可以看到，给root添加授权后，就能访问`/node`结点了，因为这时root在zookeeper集群里面被配置成了超级用户。\n\n在第2步，直接在Linux的bash命令行输入设置zookeeper环境变量SERVER_JVMFLAGS只对当前有效，如果Linux系统重启，就会失效。可以将该命令写入`/etc/profile`文件里，保证重启电脑后也不会失效。\n\n即在`/etc/profile`的最后一行加入下面的内容，并执行`source /etc/profile`让配置立即生效。\n\n```bash\nexport SERVER_JVMFLAGS=\"-Dzookeeper.DigestAuthenticationProvider.superDigest=root:u53OoA8hprX59uwFsvQBS3QuI00=\"\n```\n\n还有另一种方法设置Spuer用户，可以参考[ACL super 超级管理员](https://blog.csdn.net/u010900754/article/details/78498291 \"zookeeper设置super用户\")，我没有尝试，应该也可行。\n\n\n\n##  4. Zookeeper JAVA API的使用\n\n### 4.1 maven坐标\n\n```xml\n  <dependencies>\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>4.11</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.zookeeper</groupId>\n      <artifactId>zookeeper</artifactId>\n      <version>3.4.6</version>\n    </dependency>\n    <dependency>\n      <groupId>log4j</groupId>\n      <artifactId>log4j</artifactId>\n      <version>1.2.16</version>\n    </dependency>\n  </dependencies>\n```\n\n\n\n### 4.2 log4j配置\n\n```properties\nlog4j.rootLogger=INFO, stdout\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n\nlog4j.appender.logfile=org.apache.log4j.FileAppender\nlog4j.appender.logfile.File=target/zookeeperAPI.log\nlog4j.appender.logfile.layout=org.apache.log4j.PatternLayout\nlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n\n```\n\n### 4.3 连接Zookeeper\n\n```java\n// 用于等待 SyncConnected 事件触发后继续执行当前线程\nprivate CountDownLatch countDownLatch = new CountDownLatch(1);\n//session的实效时间\nprivate static final int SESSION_TIMEOUT = 30000;\n//创建Logger对象，按照文件log4j.properties中指定的格式输出日志到控制台\nprivate static final Logger LOGGER = LoggerFactory.getLogger(ZookeeperApiDemo.class);\n//zookeeper连接对象\nprivate ZooKeeper zooKeeper;\nprivate Watcher watcher = new Watcher() {\n    @Override\n    public void process(WatchedEvent event) {\n        if(Event.KeeperState.SyncConnected == event.getState()){\n            countDownLatch.countDown();\n            String msg=String.format(\"process info,eventType:%s,eventState:%s,eventPath:%s\",event.getType(),event.getState(),event.getPath());\n            LOGGER.info(msg);\n        }\n\n    }\n};\n\n@Before\npublic void connect() throws IOException {\n    zooKeeper = new ZooKeeper(\"192.168.218.52:2181,192.168.218.53:2181,192.168.218.54:2181\",\n                              SESSION_TIMEOUT,watcher);\n    try {\n        countDownLatch.await();\n        LOGGER.info(\"Zookeeper session establish success,sessionID=\"+Long.toHexString(zooKeeper.getSessionId()));\n    } catch (InterruptedException e) {\n        e.printStackTrace();\n        LOGGER.debug(\"Zookeeper session establish fail\");\n    }\n}\n\n@After\npublic void close(){\n    if(zooKeeper!=null){\n        try {\n            zooKeeper.close();\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\nZooKeeper构造函数的参数：\n\n- `connectionString` ：zookeeper主机（注意端口2181）\n- `sessionTimeout` ：会话超时（以毫秒为单位)\n- `watcher` ：实现“监视器”对象，zookeeper集合通过监视器对象返回连接状态。\n\n当new一个zookeeper对象后，zookeeper的连接过程可能会受到网络、zookeeper集群等各种问题的影响，连接的过程可能会比较慢。因此，为了提高程序的执行性能，可以在watcher监视器里面使用并发工具类CountDownLatch，这个工具类在初始化的时候指定一个int类型的值，通过调用countDown方法，这个值会减一，当减到0时，所有的await线程都会被叫醒。所以，每次在使用zookeeper之前，使用`countDownLatch.await()`来确保每次使用zookeeper对象之前，zookeeper客户端都能成功连接到集群。\n\n\n\n### 4.4 新增节点\n\n```java\n// 同步方式\ncreate(String path, byte[] data, List<ACL> acl, CreateMode createMode)\n// 异步方式\ncreate(String path, byte[] data, List<ACL> acl, CreateMode createMode,\nAsyncCallback.StringCallback callBack,Object ctx) \n```\n\n- `path`：znode路径\n- `data`：要存储在指定znode路径中的数据\n- `acl`：要创建的节点的访问控制列表。 zookeeper API提供了一个静态接口`ZooDefs.Ids`来获取一些基本的acl列表\n- `createMode`：节点的类型，这是一个枚举类型\n- `callBack`：异步回调接口\n- `ctx`：传递上下文参数\n\n下面创建临时结点`/zk001`\n\n```java\n    /**创建节点：\n     *CreateMode：\n     * PERSISTENT 普通持久节点\n     * PERSISTENT_SEQUENTIAL:顺序持久节点\n     * EPHEMERAL ：普通临时\n     * EPHEMERAL_SEQUENTIAL：顺序临时节点\n     * Access Control List: 访问控制列表\n     * OPEN_ACL_UNSAFE: ANYONE CAN VISIT\n     */\n    @Test\n    public void createNode(){\n        String result = null;\n        try {\n            result = zooKeeper.create(\"/zk001\",//节点的全路径\n                    \"zk001-data\".getBytes(),//节点中的数据->字节数据\n                    ZooDefs.Ids.OPEN_ACL_UNSAFE,//指定访问控制列表\n                    CreateMode.EPHEMERAL //指定创建节点的类型\n            );\n            Thread.sleep(10000);\n        } catch (Exception e) {\n           LOGGER.error(e.getMessage());\n        }\n       LOGGER.info(\"create node success,result={}\",result);\n    }\n```\n\n\n\n### 4.5 查看节点\n\n查询节点有两层，第一个是相当于zkCli的get，就是获取某个节点的内容。还有一个就是类似于ls，列出子节点。\n\n获取获取某个节点的内容可以通过zookeeper的getData方法，getData方法有多个重载，主要就是分为直接获取和异步获取，异步获取多了一个回掉，直接获取则直接返回获取的结果。\n\n```java\n// 同步方式\ngetData(String path, Watcher watcher, Stat stat)\n// 异步方式\ngetData(String path, Watcher watcher， AsyncCallback.DataCallback callBack，\nObject ctx)\n// 同步方式\ngetData(String path, boolean watch, Stat stat)\n// 异步方式\ngetData(String path, boolean watch， AsyncCallback.DataCallback callBack，\nObject ctx)    \n```\n\n- `path`：znode路径\n- `watcher`：使用新的注册的监视器，该参数允许传入null\n- `watch`：当watch为true时，则使用系统默认的Watcher，系统默认的Watcher是在zookeeper的构造函数中传递的Watcher。如果watch为false，则表明不注册Watcher。\n- `stat` ：返回znode的元数据\n- `callBack`：异步回调接口\n- `ctx`：传递上下文参数\n\n**同步方式获取某个节点的内容**\n\n```java\n    @Test\n    public void getNodeData(){\n        String result = null;\n        try {\n            //注意，是结点的全路径\n            byte[] data = zooKeeper.getData(\"/zk01\", null, null);\n            result = new String(data);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n            Assert.fail();\n        }\n        LOGGER.info(\"getNodeData={}\",result);\n    }\n```\n\n\n\n**异步方式获取某个节点的内容**\n\n这里要注意，一定要休眠，否则在看不到结果之前可能程序就停掉了。\n\n```java\n    @Test\n    public void getNodeDataAsync(){\n        String result = null;\n        try {\n            zooKeeper.getData(\"/zk01\", null, new AsyncCallback.DataCallback() {\n                @Override\n                public void processResult(int i, String s, Object o, byte[] bytes, Stat stat) {\n                    LOGGER.info(\"getNodeDataAsync={}\",new String(bytes));\n\n                }\n            },null);\n            Thread.sleep(3000);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n            Assert.fail();\n        }\n    }\n```\n\n\n\n**列出所有的子节点**\n\n```java\n    //获取所有的子节点\n    @Test\n    public void getChilds(){\n        try {\n            List<String> children = zooKeeper.getChildren(\"/zk01\", true);\n            for(String node:children){\n                LOGGER.info(\"================{}\",node);\n            }\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n        }\n    }\n```\n\n获取所有子节点，并打印其信息\n\n```java\n    @Test\n    public void getChilds2(){\n        try {\n            List<String> children = zooKeeper.getChildren(\"/zk01\", true);\n            Stat stat=null;\n            for(String node:children){\n                stat=new Stat();//封装结点的信息\n                LOGGER.info(\"================{}\",node);\n                byte[] data=zooKeeper.getData(\"/zk01/\"+node,null,stat);\n                System.out.println(new String(data)+\", stat:\"+stat);\n            }\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n        }\n    }\n```\n\n### 4.6 修改节点\n\n```java\n// 同步方式\nsetData(String path, byte[] data, int version)\n// 异步方式\nsetData(String path, byte[] data, int version， AsyncCallback.StatCallback\ncallBack， Object ctx)\n```\n\n- `path`：znode路径\n- `data`：要存储在指定znode路径中的数据\n- `version`：这里的version指的是znode节点的dataVersion的值，每次数据的修改都会更新这个值，主要是为了保证一致性。通俗来讲就是如果你指定的version比保持的version值小，则表示已经有其他线程所更新了，你也就不能更新成功了，否则则可以更新成功。如果你不管别的线程有没有更新成功都要更新这个节点的值，则version可以指定为-1。\n- `callBack`：异步回调接口\n- `ctx`：传递上下文参数\n\n下面是删除节点的例子\n\n```java\n    @Test\n    public void deleteNode(){\n        try {\n            //-1指的是无论你的结点是什么版本，都将删除\n            zooKeeper.delete(\"/zk06/test-0000000008\",-1);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n            Assert.fail();\n        }\n    }\n```\n\n### 4.7 watcher监听\n\n客户端注册 Watcher，注册 watcher 有 3 种方式，getData、exists、getChildren，可以触发观察的操作有：create、delete、setData，下面分别进行测试：\n\n（1）**对于getData**\n\n```java\n    @Test\n    public void testGetDataWather(){\n        String result = \"\";\n        try {\n            //在读取数据时添加一个监听事件\n            byte[] data = zooKeeper.getData(\"/zk01\", new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {\n                    LOGGER.info(\"testGetDataWather watch type:{}\", event.getType());\n                    //只能监听一次，想要持续监听可以通过循环或递归的方式\n                    //testGetDataWather();\n                }\n            }, null);\n            result = new String(data);\n            Thread.sleep(30000);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n            Assert.fail();\n        }\n        LOGGER.info(\"result = {}\",result);\n    }\n```\n\n主要`/zk01`节点中的数据改变，就会输出`testGetDataWather watch type:NodeDataChanged`，但只会出发一次，想要持续监听可以通过循环或递归的方式。\n\n（2）**对于exists**\n\n使用系统默认的Watcher是在zookeeper的构造函数中传递的Watcher\n\n```java\n    /**\n     * watch:true:表示使用系统默认的Watcher是在zookeeper的构造函数中传递的Watcher\n     * watch:false:不使用Watcher\n     */\n    @Test\n    public void isExistWatcher1(){\n        Stat stat = null;\n        try {\n            stat = zooKeeper.exists(\"/zk01/node1\", true);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n        }\n        //如果stat不为null，继续往后执行\n        Assert.assertNotNull(stat);\n        try {\n            zooKeeper.delete(\"/zk01/node1\",-1);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n        }\n    }\n```\n\n出发事件时，将输出`process info,eventType:NodeDeleted,eventState:SyncConnected,eventPath:/zk01/node1`，因为zookeeper的构造函数中传递的Watcher的内容是：\n\n```java\nString msg=String.format(\"process info,eventType:%s,eventState:%s,eventPath:%s\",event.getType(),event.getState(),event.getPath());\nLOGGER.info(msg);\n```\n\n然后，使用自定义的监听对象\n\n```java\n    /**\n     * 注册了自定义的监听对象，走自定义的。\n     */\n    @Test\n    public void isExistWatcher2(){\n        Stat stat = null;\n        try {\n            stat = zooKeeper.exists(\"/zk01/node2\", new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {\n                    LOGGER.info(\"isExistWatcher2  wather type:{}\",event.getType());\n                }\n            });\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n        }\n        //如果stat不为null，继续往后执行\n        Assert.assertNotNull(stat);\n        try {\n            zooKeeper.setData(\"/zk01/node2\",\"isExistWatcher2_edited\".getBytes(),-1);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n        }\n        try {\n            zooKeeper.delete(\"/zk01/node2\",-1);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n        }\n    }\n```\n\n输出的结果：`isExistWatcher2  wather type:NodeDataChanged` 或者 `isExistWatcher2  wather type:NodeDeleted`，不过一般是第一个操作触发。\n\n（3）**对于getChildren**\n\n对于getChildren只有子节点创建和删除时，才能触发watcher事件，**子节点数据改变不会触发该事件**，只有在子节点创建和删除时，才能触发watcher事件。\n\n```java\n    @Test\n    public void getChildsWatcher(){\n        try {\n            List<String> children = zooKeeper.getChildren(\"/zk01\", new Watcher() {\n                @Override\n                public void process(WatchedEvent watchedEvent) {\n                    LOGGER.info(\"getChildsWatcher  wather type:{}\",watchedEvent.getType());\n                }\n            });\n            for(String node:children){\n                LOGGER.info(\"================{}\",node);\n                //  /zk06/test-0000000001\n            }\n            Thread.sleep(30000);\n        } catch (Exception e) {\n            LOGGER.error(e.getMessage());\n        }\n    }\n```\n\n当子节点创建和删除时，会输出：`getChildsWatcher  wather type:NodeChildrenChanged`\n\n\n\n本文所有的demo的github地址为：[https://github.com/wxler/zookeeperAPI.git](https://github.com/wxler/zookeeperAPI.git \"zookeeper demo\")\n\n另外，我也通过zookeeper使用RMI远程调用，通过三个IP实现简单的负载均衡，也在上述github地址中。\n\n\n\n\n\n\n\n\n\n【参考资料】\n\n1. https://www.cnblogs.com/leesf456/p/6022357.html\n2. https://blog.csdn.net/wx_it/article/details/105862972\n3. https://www.runoob.com/w3cnote/zookeeper-acl.html","tags":["zookeeper"],"categories":["zookeeper"]},{"title":"详解磁盘IO、网络IO、零拷贝IO、BIO、NIO、AIO、IO多路复用(select、poll、epoll)","url":"/2021/02/19/134758/","content":"\n文章很长，但是很用心！\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. 什么是I/O\n\n在计算机操作系统中，所谓的I/O就是输入（Input）和输出（Output），也可以理解为读（Read）和写（Write)，针对不同的对象，I/O模式可以划分为磁盘IO模型和网络IO模型。\n\nIO操作会涉及到用户空间和内核空间的转换，先来理解以下规则：\n\n- 内存空间分为用户空间和内核空间，也称为用户缓冲区和用户缓冲区\n- 用户的应用程序不能直接操作内核空间，需要将数据从内核空间拷贝到用户空间才能使用\n- 无论是read操作，还是write操作，都只能在内核空间里执行\n- 磁盘IO和网络IO请求加载到内存的数据都是先放在内核空间的\n\n再来看看所谓的读（Read）和写（Write)操作：\n\n- 读操作：操作系统检查内核缓冲区有没有需要的数据，如果内核缓冲区已经有需要的数据了，那么就直接把内核空间的数据copy到用户空间，供用户的应用程序使用。如果内核缓冲区没有需要的数据，对于磁盘IO，直接从磁盘中读取到内核缓冲区（这个过程可以不需要cpu参与）。而对于网络IO，应用程序需要等待客户端发送数据，如果客户端还没有发送数据，对应的应用程序将会被阻塞，直到客户端发送了数据，该应用程序才会被唤醒，从Socket协议找中读取客户端发送的数据到内核空间，然后把内核空间的数据copy到用户空间，供应用程序使用。\n- 写操作：用户的应用程序将数据从用户空间copy到内核空间的缓冲区中（如果用户空间没有相应的数据，则需要从磁盘—>内核缓冲区—>用户缓冲区依次读取），这时对用户程序来说写操作就已经完成，至于什么时候再写到磁盘或通过网络发送出去，由操作系统决定。除非应用程序显示地调用了sync命令，立即把数据写入磁盘，或执行flush()方法，通过网络把数据发送出去。\n- 绝大多数磁盘IO和网络IO的读写操作都是上述过程，除了后面要讲到的零拷贝IO。\n\n\n\n## 2. 磁盘IO\n\n磁盘IO的流程如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210219155217.png)\n\n（1）读操作\n\n当应用程序调用read()方法时，操作系统检查内核缓冲区中是否存在需要的数据，如果存在，那么就直接把内核空间的数据copy到用户空间，供用户的应用程序使用。如果内核缓冲区没有需要的数据，通过通过**DMA方式**（一种IO设备控制方式，下面会讲解）从磁盘中读取数据到内核缓冲区，然后由CPU控制，把内核空间的数据copy到用户空间。\n\n这个过程会涉及到两次缓冲区copy，第一次是从磁盘的缓冲区到内核缓冲区，第二次是从内核缓冲区到用户缓冲区（或应用缓冲区），第一次是cpu的copy，第二次是DMA的copy。\n\n（2）写操作\n\n当应用程序调用write()方法时，应用程序将数据从用户空间copy到内核空间的缓冲区中（如果用户空间没有相应的数据，则需要从磁盘—>内核缓冲区—>用户缓冲区依次读取），这时对用户程序来说写操作就已经完成，至于什么时候把数据再写到磁盘（从内核缓冲区到磁盘的写操作也由DMA控制，不需要cpu参与），由操作系统决定。除非应用程序显示地调用了sync命令，立即把数据写入磁盘。\n\n如果应用程序没准备好写的数据，则必须先从磁盘读取数据才能执行写操作，这时会涉及到四次缓冲区的copy，第一次是从磁盘的缓冲区到内核缓冲区，第二次是从内核缓冲区到用户缓冲区，第三次是从用户缓冲区到内核缓冲区，第四次是从内核缓冲区写回到磁盘。前两次是为了读，后两次是为了写。这其中有两次cpu拷贝，两次DMA copy。\n\n（3）磁盘IO的延时\n\n为了读或写，磁头必须能移动到所指定的磁道上，并等待所指定的扇区的开始位置旋转到磁头下，然后再开始读或写数据。磁盘IO的延时分成以下三部分：\n\n- **寻道时间**：把磁头移动到指定磁道上所经历的时间\n- **旋转延迟时间** ：指定扇区移动到磁头下面所经历的时间\n- **传输时间** ：数据的传输时间（数据读出或写入的时间）\n\n![img](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127225002.png)\n\n\n\n\n\n## 3. 网络IO\n\n网络IO的流程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210219161442.png)\n\n（1）读操作\n\n网络IO的既可以从物理磁盘中读数据，也可以从socket中读数据（从网卡中获取）。当从物理磁盘中读数据的时候，其流程和磁盘IO的读操作一样。当从socket中读数据，应用程序需要等待客户端发送数据，如果客户端还没有发送数据，对应的应用程序将会被阻塞，直到客户端发送了数据，该应用程序才会被唤醒，从Socket协议找中读取客户端发送的数据到内核空间（这个过程也由DMA控制），然后把内核空间的数据copy到用户空间，供应用程序使用。\n\n（2）写操作\n\n为了简化描述，我们假设网络IO的数据从磁盘中获取，读写操作的流程如下：\n\n- 当应用程序调用read()方法时，通过DMA方式将数据从磁盘拷贝到内核缓冲区\n- 由cpu控制，将内核缓冲区的数据拷贝到用户空间的缓冲区中，供应用程序使用\n- 当应用程序调用write()方法时，cpu会把用户缓冲区中的数据copy到内核缓冲区的Socket Buffer中\n- 最后通过DMA方式将内核空间中的Socket Buffer拷贝到Socket协议栈（即网卡设备）中传输。\n\n\n\n网络IO的写操作也有四次缓冲区的copy，第一次是从磁盘缓冲区到内核缓冲区（由cpu控制），第二次是内核缓冲区到用户缓冲区（DMA控制），第三次是用户缓冲区到内核缓冲区的Socket Buffer（由cpu控制），第四次是从内核缓冲区的Socket Buffer到网卡设备（由DMA控制）。四次缓冲区的copy工作两次由cpu控制，两次由DMA控制。\n\n\n\n（3）网络IO的延时\n\n网络IO主要延时是由：服务器响应延时+带宽限制+网络延时+跳转路由延时+本地接收延时 决定。一般为几十到几千毫秒，受环境影响较大。所以，一般来说，网络IO延时要大于磁盘IO延时。\n\n\n\n## 4. IO中断与DMA\n\n以前传统的IO读写是通过中断由cpu控制的，为了减少CPU对I/O的干预，引入了直接存储器访问方式（DMA）方式。在DMA方式下，数据的传送是在DMA的控制下完成的，不需要cpu干预，所以CPU和I/O设备可以并行工作，提高了效率。现在来看看它们各自的原理：\n\n（1）IO中断原理\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210219171705.png)\n\n1. 用户进程通过read等系统调用接口向操作系统（即CPU）发出IO请求，请求读取数据到自己的用户内存缓冲区中，然后**该进程进入阻塞状态**。\n2. 操作系统收到用户进程的请求后，进一步将IO请求发送给磁盘。\n3. 磁盘驱动器收到内核的IO请求后，把数据读取到自己的缓冲区中，此时不占用CPU。当磁盘的缓冲区被读满之后，向内核发起**中断信号**告知自己缓冲区已满。\n4. 内核收到磁盘发来的中断信号，**使用CPU将磁盘缓冲区中的数据copy到内核缓冲区中**。\n5. 如果内核缓冲区的数据少于用户申请读的数据，则重复步骤2、3、4，直到内核缓冲区的数据符合用户的要求为止。\n6. 内核缓冲区的数据已经符合用户的要求，CPU停止向磁盘IO请求。\n7. CPU将数据从内核缓冲区拷贝到用户缓冲区，同时从系统调用中返回。\n8. 用户进程读取到数据后继续执行原来的任务。\n\n**中断IO缺点：每次IO请求都需要CPU多次参与。**\n\n\n\n（2）DMA原理\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210219175029.png)\n\n1. 用户进程通过read等系统调用接口向操作系统（即CPU）发出IO请求，请求读取数据到自己的用户内存缓冲区中，然后**该进程进入阻塞状态**。\n2. 操作系统收到用户进程的请求后，**进一步将IO请求发送给DMA，然后CPU就可以去干别的事了**。\n3. DMA将IO请求转发给磁盘。\n4. 磁盘驱动器收到内核的IO请求后，把数据读取到自己的缓冲区中，当磁盘的缓冲区被读满后，向DMA发起中断信号告知自己缓冲区已满。\n5. DMA收到磁盘驱动器的信号，将磁盘缓存中的数据copy到内核缓冲区中，**此时不占用CPU（IO中断这里是占用CPU的）**。\n6. 如果内核缓冲区的数据少于用户申请读的数据，则重复步骤3、4、5，直到内核缓冲区的数据符合用户的要求为止。\n7. 内核缓冲区的数据已经符合用户的要求，**DMA停止向磁盘IO请求**。\n8. DMA发送中断信号给CPU。\n9. **CPU收到DMA的信号，知道数据已经准备好，于是将数据从内核空间copy到用户空间**，系统调用返回。\n10. 用户进程读取到数据后继续执行原来的任务。\n\n**跟IO中断模式相比，DMA模式下，DMA就是CPU的一个代理，它负责了一部分的拷贝工作，从而减轻了CPU的负担**。\n\n需要注意的是，DMA承担的工作是从磁盘的缓冲区到内核缓冲区或网卡设备到内核的soket buffer的拷贝工作，以及内核缓冲区到磁盘缓冲区或内核的soket buffer到网卡设备的拷贝工作，而内核缓冲区到用户缓冲区之间的拷贝工作仍然由CPU负责。\n\n\n\n## 5. 零拷贝IO\n\n在上述IO中，读写操作要经过四次缓冲区的拷贝，并经历了四次内核态和用户态的切换。 零拷贝（zero copy）IO技术减少不必要的内核缓冲区跟用户缓冲区之间的拷贝，从而减少CPU的开销和状态切换带来的开销，达到性能的提升。\n\n在zero copy下，如果从磁盘中读取文件然后通过网络发送出去，**只需要拷贝三次，只发生两次内核态和用户态的切换**。\n\n下图是不使用zero copy的网络IO传输过程：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210219161442.png)\n\n**零拷贝的传输过程**：硬盘 >> kernel buffer (快速拷贝到kernel socket buffer) >>Socket协议栈（网卡设备中）\n\n- 当应用程序调用read()方法时，通过DMA方式将数据从磁盘拷贝到内核缓冲区\n- 由cpu控制，将内核缓冲区的数据直接拷贝到另外一个与 socket相关的内核缓冲区，即kernel socket buffer\n- 然后由DMA 把数据从kernel socket buffer直接拷贝给Socket协议栈（网卡设备中）。\n\n这里，**只经历了三次缓冲区的拷贝**，第一次是从磁盘缓冲区到内核缓冲区，第二次是从内核缓冲区到kernel socket buffer，第三次是从kernel socket buffe到Socket协议栈（网卡设备中）。**只发生两次内核态和用户态的切换**，第一次是当应用程序调用read()方法时，用户态切换到内核到执行read系统调用，第二次是将数据从网络中发送出去后系统调用返回，从内核态切换到用户态。\n\n 零拷贝（zero copy）的应用：\n\n- Linux下提供了zero copy的接口：sendfile和splice，用户可通过这两个接口实现零拷贝传输\n- Nginx可以通过sendfile配置开启零拷贝\n- 在linux系统中，Java NIO中FileChannel.transferTo的实现依赖于 sendfile()调用。\n- Apache使用了sendfile64()来传送文件，sendfile64()是sendfile()的扩展实现\n- kafka也用到了零拷贝的功能，具体我没有深究\n\n注意：零拷贝要求输入的fd必须是文件句柄，不能是socket，输出的fd必须是socket，也就是说，数据的来源必须是从本地的磁盘，而不能是从网络中，如果数据来源于socket，就不能使用零拷贝功能了。我们看一下sendfile接口就知道了：\n\n```c\n#include <sys/sendfile.h>\nssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count)\n```\n\n- out_fd：待写入文件描述符\n- in_fd： 待读出文件描述符\n- offset：从读入文件流的哪个位置开始读，如果为空，则默认从起始位置开始\n- count：指定在文件描述符in_fd 和out_fd之间传输的字节数\n- 返回值：成功时，返回出传输的字节数，失败返回-1\n\n **in_fd必须指向真实的文件，不能是socket和管道；而out_fd则必须是一个socket。由此可见，sendfile几乎是专门为在网络上传输文件而设计的。**\n\n> 在Linxu系统中，一切皆文件，因此socket也是一个文件，也有文件句柄（或文件描述符）。\n\n\n\n\n\n\n\n\n\n## 6. BIO\n\n现在，我们就来讲解BIO、NIO、IO多路复用、AIO，在这之前，**我必须强调，这些IO大多用于网络IO，并且这里主要介绍用户程序从网络中获取数据那一部分**。一方面是为了方便描述，另一方式，更能体现出这些IO的区别。\n\n网络IO从Socket获取数据的步骤：\n\n1. 用户进程执行系统调用转入内核态\n2. 操作系统等待远处客户端发送数据（前提是客户端和服务器通过TCP三次握手成功），客户端发送数据后，操作系统通过从网卡设备获取数据，并把数据从Socket协议栈拷贝到内核缓冲区\n3. 把内核缓冲区的数据拷贝到用户缓冲区\n4. 用户进程获取到数据，继续执行\n\nBIO、NIO、AIO的主要区别在于：\n\n- 步骤1里用户进程执行系统调用后的状态如何，是阻塞（或挂起），还是非阻塞。\n- 步骤3里把内核缓冲区的数据拷贝到用户缓冲区，在拷贝过程中，用户进程的状态又如何，是阻塞，还是非阻塞。\n\n如果用户进程在步骤1执行后的状态是阻塞的，且步骤3过程中，进程也是阻塞的，那么是BIO（同步阻塞IO）。\n\n如果用户进程在步骤1执行后的状态是非阻塞的，且步骤3过程中，进程是阻塞的，那么是NIO（同步非阻塞IO）。\n\n如果用户进程在步骤1执行后的状态是非阻塞的，且步骤3过程中，进程也是非阻塞的，**也就是说真正读（或写）时，进程的状态是非阻塞的**，那么是AIO（异步IO）。\n\n至于多路复用IO和BIO、NIO、AIO的区别，后面会细细讲解。\n\n\n\n那么，我们就开始吧！\n\nBIO （Blocking I/O），称之为同步阻塞I/O，其IO模型传输如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210219202400.png)\n\n*上图红色表示进程处理阻塞状态，绿色表示进程处于非阻塞状态*\n\n我相信BIO模型的传输过程上图已经描述很清楚了，可以看到，BIO模型的用户进程在执行系统调用后，一直处于阻塞状态，等待内核数据到位后，进程继续阻塞，直到内核数据拷贝到用户空间。\n\n该模式下，一个线程只能处理一个Socket IO连接，高并发时，服务端会启用大量的线程来处理多个Socket，由于是阻塞IO，会导致大量线程处于阻塞状态，导致cpu资源浪费，且大量线程会导致大量的上下文切换，造成过多的开销。\n\n> 当前绝大操作系统都支持多线程，当操作系统引入多线程之后，进程的执行实际就是进程中的多个线程在执行，同一时刻，cpu只能执行一个线程，多个线程通过轮询的方式交替执行。\n\n这时你可能会有疑问，用户进程都被阻塞（或挂起）了，在内核态还怎么操作呢？事实上，read和write都是内核级的操作，只要用户进程调用相应的系统调用接口后，内核进程（或线程）在真正执行读和写操作硬件时，与用户进程就没什么关系了。\n\n\n\n\n\n## 7. NIO\n\nNIO （Non-blocking IO），称之为非阻塞IO，其传输过程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210219204649.png)\n\n在NIO模式下，当用户进程执行系统调用后，如果当前数据还没有准备好，则会立即返回（NIO的非阻塞就提现在这里），然后再次进行系统调用，不断测试数据是否准备好。如果数据准备好了，当前进程会进入阻塞转态，直到数据从内核空间拷贝到用户空间，进程才会被唤醒，就可以处理数据了。\n\nNIO模式下，一个线程就可以处理多个Socket连接，没必要开启多线程进行处理（如果多个NIO，会有多个线程一起执行多次系统调用，结果会很可怕）。但是，当有1000个Socket连接时，用户进程会以轮询的方式执行1000次系统调用判断数据有没有准备好，即会发生1000次用户态到内核态的切换，成本几何上升。即使当前只有一个Socket连接，也会重复进行系统调用，因为此时的用户进程不仅要接收新的Socket连接并把它拷贝到内核，还要判断已有的Socket连接是否准备好数据，这都会有系统调用，极大的浪费cpu资源。\n\n## 8. IO多路复用\n\nIO多路复用的传输过程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210219221516.png)\n\n由于NIO会多次执行系统调用进行测试，大大浪费系统的资源，而**多路复用IO把轮询多个Socket文件句柄的事情放在内核空间里执行，即让内核负责轮询所有socket（这样就不会有用户态和系统态的切换）**，当某个或几个socket有数据到达了，返回所有就绪的Socket文件句柄给用户进程，然后用户进程执行read系统调用接口，并进入阻塞状态。内核进程（或线程）把数据从内核空间拷贝到用户空间，用户进程读取到数据就可以进行处理了。\n\n多路复用IO在执行系统调用后，进程就处于阻塞状态，所以多路复用IO本质上也是同步阻塞IO，只不过它是在内核态轮询所有socket，大大提高了IO的处理速度，也减少了系统状态切换的开销。此外，它与同步阻塞的BIO不同，多路复用IO可以使用一个线程同时处理多个Socket的IO请求，这是BIO做不到的。而在BIO中，必须通过多线程的方式才能达到这个目的。\n\n另外，大家可以思考一下，**为什么用户进程从网络中获取数据的第一步就要执行系统调用**，我举一个例子来说明。\n\n假如一个服务端上的用户进程要读取客户端发来的数据，此时用户进程在用户态，当进程执行了accept()方法获取客户端的链接，此时就得到了客户端Socket的文件句柄（或文件描述符），但是该用户进程并不知道该Socket的文件句柄是否就绪（即是否可读），这就要执行系统调用进入内核态，并把当前网络连接的Socket文件句柄（或文件描述符）复制到内核态。为什么要进入内核态呢？因为数据是从Socket协议栈（或网卡设备）发过来的，要操作硬件设备才能读取数据，所以必须在内核态下判断客户端的Socket是否发来消息。进入内核态以后，内核进程会判断该Socket是否可读（即是否准备好数据），如果准备好了数据，就把数据从Socket协议栈（或网卡设备）拷贝到内核缓冲区，再把内核缓冲区的数据拷贝到用户缓冲区。所以只要有一个客户端的Socket连接到来，就会进入一次系统调用判断Socket的文件句柄是否就绪。这里可能不好理解，但对下面多路复用模式的理解很有用处。\n\n\n\n多路复用模式包含三种，即select、poll和epoll，**这几种模式主要区别在于获取可读Socket文件句柄的方式**。\n\n\n\n### 8.1 select\n\n为了更清楚的进行说明，这里给出selet函数的定义：\n\n```bash\nint select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);\n```\n\nselect 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。**调用后select函数后用户进程会阻塞，直到有文件描述符就绪（有数据可读、可写、或者有exception）**或者超时（timeout指定等待时间），函数返回。当select函数返回后，可以通过遍历fd_set（文件描述符的集合），来找到就绪的描述符。\n\nselect目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024（可以修改）。\n\nselect机制的详细过程如下：\n\n1. select创建3个文件描述符的集合（即writefds、readfds、和exceptfds），当有Socket连接时，将该Sokcet的文件句柄放入这3个文件描述符的集合中一个或多个，并将这些文件描述符集全部拷贝到内核中，这里限制了文件句柄的最大的数量为1024，此时用户进程会进入阻塞状态。\n2. **内核分别遍历（或轮询）这3个文件描述符集**，判断是否有Socket连接可读、可写、或者有exception，这个动作和select无关。\n3. 内核检测文件描述符集中有某个或某几个Socket文件句柄就绪（有数据可读、可写、或者有exception），修改相应Socket文件句柄的状态位，表示该Socket已经就绪，以区别其它没有就绪的文件句柄。然后，就产生中断通知用户进程，并返回就绪的文件句柄的总数，只需遍历一遍文件描述符集合，就可以得到就绪的 Socket（可使用FD_ISSET宏函数来检测哪些文件句柄就绪）。\n4. 用户进程调用系统调用接口（read或write），转入内核态执行read或write操作，此时进程又进入阻塞状态，在内核态执行完系统调用以后，再次通知用户进程，用户进程获取到数据就可以进行处理了。\n5. 内核对就绪文件句柄的监控是建立在Socket状态位之上的，也就是说经过一次监控之后，相应Socket的状态位会被修改，因此再次监控时需要再次从用户态向内核态进行拷贝（即复位内核里面所有的fd）。\n6. 如果有新的Socket连接进来，用户进程（处于非阻塞时）将新Socket的文件句柄放入3个文件描述符集后，再次将这些文件描述符集全部拷贝到内核中（文件句柄的最大的数量为1024）。\n\n使用select以后最大的优势是用户可以在一个线程内同时处理多个socket的IO请求。当有多个Socket连接时，把它们放入select创建的3个文件描述符集中的一个即可，然后由内核分别轮询这3个文件描述符集，即可达到在同一个线程内同时处理多个IO请求的目的。\n\n\n\n在select机制中，为了减少数据拷贝带来的性能损坏，内核对被监控的`fd_set`集合大小做了限制（默认最大为1024），且想要做到多次监控或者有未就绪的文件句柄，select机制会不断地将文件描述符集从用户态向内核态进行拷贝，会大大浪费系统的资源。此外，select机制只能知道有socket就绪（有数据可读或可写、或者有exception），无法知道具体是哪一个socket接收到了数据，所以需要用户进程进行遍历，才能知道具体是哪个socket接收到了数据。\n\n\n\n### 8.2 poll\n\n不同于select使用3个文件描述符集合，poll使用一个pollfd结构体的指针实现。\n\n```c\n# poll函数定义\nint poll (struct pollfd *fds, unsigned int nfds, int timeout);\n# pollfd指针的结构体\nstruct pollfd {\nint fd; /* file descriptor */  \nshort events; /* requested events to watch */\nshort revents; /* returned events witnessed */\n};\n```\n\npoll函数参数说明：\n\n- `struct pollfd *fds` ：`fds`是一个`struct pollfd`类型的数组，用于存放需要检测其状态的socket文件描述符，并且调用poll函数之后`fds`数组不会被清空，这一点与select()函数不同，调用select()函数之后，select() 函数会清空它所检测的socket描述符集合，导致每次调用select()之前都必须把socket描述符重新加入到待检测的集合中。一个`pollfd`结构体表示一个被监视的Socket文件描述符，通过给poll函数传递`pollfd`数组来监视多个socket文件描述符。\n- `nfds`：是监控的socket文件句柄数量。\n- `timeout`是等待的毫秒数，这段时间内无论IO是否准备好，poll都会返回。timeout为负数表示无线等待，timeout为0表示调用后立即返回。\n- poll函数执行结果：为0表示超时前没有任何事件发生，-1表示失败，成功则返回结构体`pollfd`的`revents`不为0的文件描述符个数。\n\n接下来，我们就介绍一下，结构体`pollfd`的events域和revents域。**`events`域是监视该文件描述符的事件，由用户来设置这个域，`revents`域是文件描述符的操作结果事件，由内核在调用返回时设置这个域。events域中请求的任何事件都可能在revents域中返回**，events域合法的事件如下：\n\n- `POLLIN`：有数据可读。\n- `POLLRDNORM`：有普通数据可读。\n- `POLLRDBAND`：有优先数据可读。\n- `POLLPRI`：有紧迫数据可读。\n- `POLLOUT`：写数据不会导致阻塞。\n- `POLLWRNORM`：写普通数据不会导致阻塞。\n- `POLLWRBAND`：写优先数据不会导致阻塞。\n- `POLLMSG、SIGPOLL`：消息可用。\n\n此外，revents域中还可能返回下列事件，这些事件在events域中无意义，因为它们在合适的时候总是会从revents中返回。\n\n- `POLLER`：指定的文件描述符发生错误。\n- `POLLHUP`：指定的文件描述符挂起事件。\n- `POLLNVAL`：指定的文件描述符非法。\n\n举一个例子，`events=POLLIN | POLLPRI`等价于select()的读事件，`events=POLLOUT | POLLWRBAND`等价于select()的写事件。此外，`POLLIN`等价于`POLLRDNORM |POLLRDBAND`，而`POLLOUT`则等价于`POLLWRNORM`。\n\n当我们要同时监视一个文件描述符是否可读和可写，可以设置 `events=POLLIN |POLLOUT`（由用户设置）。在poll返回时，可以检查revents中的标志（由内核在调用返回时设置）。如果`revents=POLLIN`，表示Socket文件描述符可以被读取而不阻塞。如果`revents=POLLOUT`，表示Socket文件描述符可以写入而不导致阻塞。注意，`POLLIN`和`POLLOUT`并不是互斥的：它们可能被同时设置，表示这个文件描述符的读取和写入操作都会正常返回而不阻塞。\n\n现在，我们来总结一下poll机制的过程：\n\n1. 调用poll函数，进入系统调用，将pollfd数组拷贝至内核，此时用户进程会进入阻塞状态。\n2. 此时已经从用户空间复制了pollfd数组来存放所有的Socket文件描述符，此时把数组pollfd组织成poll_list链表。\n3. 对poll_list链表进行遍历，判断每个Socket文件描述符的状态，如果某个Socket就绪，就在Socket的就绪队列上加入一项，然后继续遍历。若遍历完所有的文件描述符后，都没发现任何Socket就绪，则继续阻塞当前进程，直到有Socket就绪或者等待超时，就唤醒用户进程，返回就绪队列。\n4. 如果用户进程被唤醒后，Socket就绪队列有就绪的Socket，用户进程就获取所有就绪的Socket，调用系统调用接口（read或write），转入内核态执行read或write操作，此时进程又进入阻塞状态，在内核态执行完系统调用以后，再次通知用户进程，用户进程获取到数据就可以进行处理了。\n5. 如果用户进程因等待超时被唤醒，Socket就绪队列为空，进程会再次执行系统调用，重复步骤1、2、3。\n\n可以看到，poll的实现和select非常相似，只是文件描述符集合不同，poll使用pollfd结构体，select使用的是fd_set结构，其他的都差不多，管理多个文件描述符也都是采用轮询的方式，然后根据文件描述符的状态进行处理。由于poll使用的链表，故没有最大文件描述符数量的限制，而select监控文件描述符的最大默认数量为1024。\n\npoll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的内存空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的文件描述符数量的增长，其效率也会线性下降。\n\npoll还有一个特点是“水平触发”，如果报告了某个就绪的Socket文件描述符后，没有被处理，那么下次poll时会再次报告该Socket fd。\n\n\n\n### 8.3 epoll\n\nepoll是在Linux2.6内核中提出的，是之前的select和poll的增强版本，先来看下epoll系统调用的三个函数\n\n```c\nint epoll_create(int size);  \nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);  \nint epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);\n```\n\n解释：\n\n1. `int epoll_create(int size);`\n   创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，这里的size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议，也就是说，size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。**当创建好epoll句柄后，它就会占用一个fd值**，所以在使用完epoll后，必须调用close()关闭，否则可能导致该进程的fd被耗尽。\n\n2. `int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);  `\n   该函数是对上面建立的epoll文件句柄执行op操作。例如，将刚建立的socket加入到epoll中让其监控，或者把 epoll正在监控的某个socket句柄移出epoll不再监控它等等。\n\n   - `epfd`：是epoll的文件句柄。\n\n   - `op`：表示op操作，用三个宏来表示：添加`EPOLL_CTL_ADD`，删除`EPOLL_CTL_DEL`，修改`EPOLL_CTL_MOD`，分别表示添加、删除和修改对fd的监听事件。\n\n   - `fd`：是需要监听的fd（文件描述符）\n\n   - `epoll_event`：告诉内核需要监听什么事件，`struct epoll_event`结构如下：\n\n     ```bash\n     struct epoll_event {\n     __uint32_t events; /* Epoll events */\n     epoll_data_t data; /* User data variable */\n     };\n     \n     //events可以是以下几个宏的集合：\n     EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；\n     EPOLLOUT：表示对应的文件描述符可以写；\n     EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；\n     EPOLLERR：表示对应的文件描述符发生错误；\n     EPOLLHUP：表示对应的文件描述符被挂断；\n     EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。\n     EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里\n     ```\n   \n3. `int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);`\n   epoll_wait在调用时，在给定的timeout时间内，当在监控的所有句柄中有事件发生时（即有Socket就绪时），就返回用户态的进程。\n\n   - `epfd`：是epoll的文件句柄。\n   - `events`：从内核得到事件的集合\n   - `maxevents`：events的大小\n   - `timeout`：超时时间，timeout为负数表示无线等待，timeout为0表示调用后立即返回。\n\nselect机制的详细过程如下：\n\n1. 执行epoll_create在内核建立专属于epoll的高速cache区，并在该缓冲区建立**红黑树和就绪链表**，用户态传入的文件句柄将被放到红黑树中（第一次拷贝）。\n2. **epoll_ctl执行add动作时除了将Socket文件句柄放到红黑树上之外，还向内核注册了该文件句柄的回调函数**，内核在检测到某Socket文件句柄就绪，则调用该回调函数，回调函数将文件句柄放到就绪链表。\n3. epoll_wait只监控就绪链表就可以，如果就绪链表有文件句柄，则表示该文件句柄可读（或可写），并返回到用户态（少量的拷贝）；\n4. 由于内核不修改文件句柄的状态位，因此只需要在第一次传入就可以重复监控，直到使用epoll_ctl删除，否则不需要重新传入，因此无多次拷贝。\n\n**从上面的调用方式就可以看到epoll比select/poll的优越之处：**\n\n- epoll同poll一样，也没有文件句柄的数量限制。\n- select/poll每次系统调用时，都要传递所有监控的socket给内核缓冲区，如果有数以万计的Socket文件句柄，意味着每次都要copy几十几百KB的内存到内核态，非常低效。epoll不需要每次都将Socket文件句柄从用户态拷贝到内核态，在执行epoll_create时已经在内核建立了epoll句柄，每次调用epoll_ctl只是在往内核的数据结构里加入新的socket句柄，所以不需要每次都重新复制一次。\n- select 和 poll 都是主动轮询，select在内核态轮询所有的fd_set来判断有没有就绪的文件句柄，poll 轮询链表判断有没有就绪的文件句柄，而epoll是被动触发方式。epoll_ctl执行add动作时除了将Socket文件句柄放到红黑树上之外，还向内核注册了该文件句柄的回调函数，当Socket就绪时，则调用该回调函数将文件句柄放到就绪链表，epoll_wait只监控就绪链表就可以判断有没有事件发生了。\n\n\n\n**epoll的工作模式**\n\nepoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：\n\n- 水平触发（LT模式）：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。\n- 边缘触发（ET模式）：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。\n- LT模式：LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket，在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不做任何操作，内核还是会继续通知你的。\n- ET模式：ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作（错误的操作）导致那个文件描述符不再为就绪状态了。但是请注意，如果一直不对这个fd做IO操作（从而导致它再次变成未就绪），内核不会发送更多的通知(only once)。ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用no-block socket，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。\n\n\n\n另外，还需要补充一点，epoll机制下，用户进程在执行epoll_create进入系统调用之后，并没有进入阻塞状态，因为它还要执行后面的epoll_ctl和epoll_wait方法来监控多个Socket连接，这点与select和poll不同，用户进程在第一次执行系统调用后就进入阻塞状态，等待就绪的Socket来唤醒它。但是，epoll机制下，用户进程在真正执行read或write系统调用接口时，还是会进入阻塞状态。所以这方面来讲，epoll是同步非阻塞IO，select和poll是同步阻塞IO（个人见解）。\n\n\n\n## 9. AIO\n\nAIO （ Asynchronous I/O）：异步非阻塞I/O模型。传输过程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210220151020.png)\n\n可以看到，异步非阻塞I/O在判断数据有没有准备好（即Socket是否就绪）和真正读数据两个阶段都是非阻塞的。AIO在第一次执行系统调用后，会注册一个回调函数，内核在检测到某Socket文件句柄就绪，调用该回调函数执行真正的读操作，将数据从内核空间拷贝到用户空间，然后返回给用户使用。在整个过程，用户进程都是非阻塞状态，可以做其它的事情。\n\n没有Linux系统采用AIO模型，只有windows的IOCP是此模型。\n\n\n\n## 10. 总结\n\nIO可以分为两个阶段，第一阶段，判断有没有事件发生（或判断数据有没有准备好，或判断Socket是否就绪），第二阶段，在数据准备好以后，执行真正的读（或写）操作，将数据从内核空间拷贝到用户空间。\n\n这几个阶段：\n\n- 同步阻塞IO（BIO）：两个阶段的用户进程都阻塞。\n- 同步非阻塞IO（NIO）：第一阶段没有阻塞，但是用户进程（或线程）必须不断的轮询，判断有没有Socket就绪，这时cpu疯狂被占用。第二阶段，数据拷贝的过程是阻塞的。所以，所有的同步过程，在第二阶段都是阻塞的，尽管这是非阻塞的调用。\n- 多路复用：NIO的第一阶段没有阻塞，但是由用户线程不断轮询多个Socket有没有就绪。而多路复用把这件事情交给一个内核线程去处理，速度非常快。select和poll机制下，第一阶段是也是阻塞的，而epoll机制，用户线程除了要执行epoll_create，还要执行epoll_ctl和epoll_wait，所以是非阻塞的。在第二阶段，所有的多路复用IO都是阻塞的。所以，多路复用IO也是同步IO。\n- 异步IO（AIO）：两个阶段都是非阻塞的。\n\n\n\n另外，不得不提的是，上述的阻塞和非阻塞指的是IO模型，用户进程获取数据后执行业务逻辑的时候，也分异步和同步。比如，进程执行一段很复杂的业务逻辑，需要很长的时间才能返回，也可以注册一个回调函数，等待此段代码执行完毕后，就通知用户进程。例如，nginx在Linux2.6以后的内核中用的IO模型是epoll，即同步IO，而Nginx的worker进程的处理请求的时候是异步的。\n\n业务逻辑的同步和异步概念如下：\n\n- 同步：所谓同步，就是在发出一个功能调用时，在没有得到结果之前，该调用就不返回。\n- 异步：异步的概念和同步相对。当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者。\n- 阻塞：阻塞调用是指调用结果返回之前，当前线程会被挂起，函数只有在得到结果之后才会返回。有人也许会把阻塞调用和同步调用等同起来，实际上它们是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是当前函数没有返回而已。\n- 非阻塞：非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。\n\n\n\n\n\n\n【参考文档】\n\n1. https://www.cnblogs.com/Mr-shen/p/12832501.html\n2. https://www.zhihu.com/question/20122137\n3. https://www.cnblogs.com/pluto-yang/p/12546942.html\n4. https://www.cnblogs.com/sunsky303/p/8962628.html\n5. https://blog.csdn.net/qq_22121229/article/details/103101191\n6. https://www.cnblogs.com/pugang/p/12823108.html\n7. https://blog.csdn.net/qq_33330687/article/details/81558198\n8. https://www.cnblogs.com/ljbkyBlog/p/10190576.html\n9. https://zhuanlan.zhihu.com/p/121651179\n10. https://zhuanlan.zhihu.com/p/260450151\n11. https://zhuanlan.zhihu.com/p/272891398\n12. https://zhuanlan.zhihu.com/p/159357381\n\n\n\n\n\n\n","tags":["IO"],"categories":["IO"]},{"title":"session的实现原理","url":"/2021/02/18/143128/","content":"\n**Session：记录一系列状态**\n\n**Session与cookie功能效果相同**。Session与Cookie的区别在于Session是记录在服务端的，而Cookie是记录在客户端的。\n\n<!-- more -->\n\n\n\n\n\n**解释session**：当访问服务器否个网页的时候，会在服务器端的内存里开辟一块内存，这块内存就叫做session，而这个内存是跟浏览器关联在一起的。这个浏览器指的是浏览器窗口，或者是浏览器的子窗口，意思就是，只允许当前这个session对应的浏览器访问，就算是在同一个机器上新启的浏览器也是无法访问的。而另外一个浏览器也需要记录session的话，就会再启一个属于自己的session。\n\n**原理：**HTTP协议是非连接性的，取完当前浏览器的内容，然后关闭浏览器后，链接就断开了，而没有任何机制去记录取出后的信息。而当需要访问同一个网站的另外一个页面时(就好比如在第一个页面选择购买的商品后，跳转到第二个页面去进行付款)这个时候取出来的信息，就读不出来了。所以必须要有一种机制让页面知道原理页面的session内容。\n\n**问题**：如何知道浏览器和这个服务器中的session是一一对应的呢？又如何保证不会去访问其它的session呢？\n\n**原理解答：**就是当访问一个页面的时候给浏览器创建一个独一无二的号码，也给同时创建的session赋予同样的号码。这样就可以在打开同一个网站的第二个页面时获取到第一个页面中session保留下来的对应信息（理解：当访问第二个页面时将号码同时传递到第二个页面。找到对应的session。）。这个号码也叫sessionID，session的ID号码，session的独一无二号码。\n\n**session的两种实现方式（也就是传递方式）：第一种通过cookies实现。第二种通过URL重写来实现**\n\n**第一种方式的理解**：就是把session的id 放在cookie里面（为什么是使用cookies存放呢，因为cookie有临时的，也有定时的，临时的就是当前浏览器什么时候关掉即消失，也就是说session本来就是当浏览器关闭即消失的，所以可以用临时的cookie存放。保存再cookie里的sessionID一定不会重复，因为是独一无二的。），当允许浏览器使用cookie的时候，session就会依赖于cookies，当浏览器不支持cookie后（或者浏览器的cookie被禁止），就可以通过第二种方式获取session内存中的数据资源。\n\n更详细一点就是说：session的常见实现形式是会话cookie（session cookie），即未设置过期时间的cookie，这个cookie的默认生命周期为浏览器会话期间，只要关闭浏览器窗口，cookie就消失了。实现机制是当用户发起一个请求的时候，服务器会检查该请求中是否包含sessionid，**如果未包含，则系统会创造一个名为JSESSIONID的输出 cookie返回给浏览器(只放入内存，并不存在硬盘中)，并将其以HashTable的形式写到服务器的内存里面**；当已经包含sessionid是，服务端会检查找到与该session相匹配的信息，如果存在则直接使用该sessionid，若不存在则重新生成新的 session。这里需要注意的是session始终是有服务端创建的，并非浏览器自己生成的。当浏览器不支持cookie后（或者浏览器的cookie被禁止），就可以通过第二种方式获取session内存中的数据资源。\n\n\n\n**第二种方式的理解：在客户端不支持cookie的情况下使用。为了以防万一，也可以同时使用。**\n\n当浏览器不支持cookie后（或者浏览器的cookie被禁止），session就需要用get方法的URL重写的机制或使用POST方法提交隐藏表单的形式来实现，这个必须自己编程实现。\n\n 如何重写URL：通过response.encodeURL()方法。\n\n**encodeURL()的两个作用**\n\n第一个作用：转码（说明：转中文的编码，或者一些其他特殊的编码。就好比如网页的链接中存在中文字符，就会转换成为一些百分号或者其他的符号代替。）\n\n第二个作用：URL后面加入sessionID，当不支持cookie的时候，可以使用encodeURL()方法，encodeUTL()后面跟上sessionID，这样的话，在禁用cookie的浏览器中同时也可以使用session了。但是需要自己编程，只要链接支持，想用session就必须加上encodeURL()。\n\n**提示**：若想程序中永远支持session，那就必须加上encodeURL()，当别人禁用了cookie，一样可以使用session。\n\n**简单的代码例子**：在没有使用encodeURL()方法的代码\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218144416.png)\n\n使用encodeURL()方法的代码\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218144440.png)\n\n看下图，当重写URL 的时候，每一次访问的时候都会将sessionID传过来，传过来了，就没有必要再在cookie里读了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218144505.png)\n\n**规则：**\n\n1. 如果浏览器支持cookie，创建session的时候，会被sessionID保存再cookie里。只要允许cookie，session就不会改变。**如果不允许使用cookie，每刷新一次浏览器就会换一个session（因为浏览器以为这是一个新的链接）**。\n2. **所以如果不支持cookie，必须自己编程使用URL重写的方式实现session**。\n3. session不像cookie一样拥有路径访问的问题，同一个application下的servlet/jsp都可以共享同一个session，前提下是同一个客户端窗口。\n\n**session能干什么**：\n\nsession就是服务器里面的一块内存，内存里面能放任何东西，只要是名值对就可以了。\n\n\n\n> 本文转载自[https://blog.csdn.net/weixin_42217767/article/details/92760353](https://blog.csdn.net/weixin_42217767/article/details/92760353)\n\n\n\n\n\n\n\n","tags":["session"],"categories":["工具"]},{"title":"CPU多核多线程有什么用？","url":"/2021/02/15/200435/","content":"\n何为多核CPU？所谓的[多核CPU](https://mini.eastday.com/a/171209211712571.html?qid=02263&vqid=qid02650 \"多核CPU\")就是CPU有多个核心，CPU运作时，每个核心各自处理各自任务，互不干扰。\n<!-- more -->\n\n\n\n\n\n线程又是什么？线程是指一个CPU**分离**出来的一个任务，本质就是一个核心通过CPU不断的切换同时进行的任务工作，因为CPU速度非常快，让你感觉不到有切换，但本身是只有一核心在工作。\n\n再来看看多核多线程，多核数决定了你运作程序时最多能有多少程序独占一个核心工作互不干扰，多线程决定了CPU一个核心下同时处理多少任务互不干扰，当然带来的就是性能上的折扣。\n\n多核心多线程有什么用？\n\n多线程就代表了可以开更多的应用，同时当你一个程序卡住的时候，其他程序也还是可以正常工作的。\n\ncpu的几个常用架构：\n\n（1）多个物理CPU，各个CPU通过总线进行通信，效率比较低\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210215202327.png)\n\n（2）多核CPU，不同的核通过L2 cache进行通信，存储和外设通过总线与CPU通信\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210215202502.png)\n\n（3）多核多线程，每个核有两个逻辑的处理单元，两个线程共同分享一个核的资源\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210215202528.png)\n\n\n\n总核数 = 物理CPU个数 X 每颗物理CPU的核数\n\n总逻辑CPU数（也即线程数） = 物理CPU个数 X 每颗物理CPU的核数 X 线程数\n\n如何查看电脑CPU核心数量与线程数量？\n\n打开命令提示符（即cmd），输入wmic回车，再输入`cpu get`回车即可获取到CPU详情，往后拖动滑块找到两个值，一个是NumberOfCores表示是核心数，另一个是NumberOfLogicalProcessors表示线程数。\n\n\n\n【参考文档】\n\n[查看服务器CPU的个数、CPU的核数、多核超线程数](https://blog.csdn.net/l1394049664/article/details/81811642?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.baidujs)\n\n","tags":["cpu"],"categories":["工具"]},{"title":"从0开始，在Linux中配置Nginx反向代理、负载均衡、session共享、动静分离","url":"/2021/02/14/140316/","content":"\n\n\n写这篇文章花费了我近一周的时间，参考网上许多优秀的博客文章，我不敢说写的很好，至少很全很详细。本文先介绍原理部分，然后再进行实战操作，我认为这样才会有更深的理解，不过这也导致了文章篇幅很长。但是，如果你耐心看下去，会有很多收获。\n\n<!-- more -->\n\n\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n## 1. 问题引入\n\n### 1.1 为什么需要Nginx\n\ntomcat作为javaweb服务器非常流行，为什么还需要Nginx呢？我们先来看看**单个tomcat的并发测试数据**：\n\n| 并发人数 | 响应时间 |               说明               |\n| :------: | :------: | :------------------------------: |\n|  100人   |   0.8s   |               完美               |\n|  150人   |    1s    |               完美               |\n|  200人   |   1.5s   |        响应时间有微小波动        |\n|  250人   |   1.8s   | **理想情况下最大的并发用户数量** |\n|  280人   |  约2.5s  |       开始出现连接丢失问题       |\n|  300人   |    3s    |        响应时间有较大波动        |\n|  350人   |    3s    |  连出现接丢失问题，连接很不稳地  |\n|  400人   |   3.8s   |    连接丢失数量达到3000次以上    |\n|  450人   |    4s    |    连接丢失数量达到6000次以上    |\n|  500人   |    4s    |   连接丢失数量达到11000次以上    |\n|  550人   |    6s    |    连接丢失数量达21000次以上     |\n|  600人   |    -     |      系统出现异常，停止测试      |\n\n> 上述的并发人数是指同一时刻请求连接的人数，如果并发达到600，基本上能支持几万人的请求。\n\n可以看到，单个tomcat在600并发请求下，系统出现异常，  **理想情况下最大的并发用户数量为250**，那么怎么解决高并发问题呢？\n\n这就需要用到Nginx，Nginx作为一个轻量级Web服务器，可以支持近50000个并发，并且消耗的资源却非常少，是现代Web平台的首选服务器。\n\n\n\n### 1.2 Nginx和Apache比较\n\nNginx是俄罗斯人Igor Sysoev编写的轻量级Web服务器，它不仅是一个高性能的静态HTTP和反向代理服务器，也是一个 IMAP/POP3/SMTP 代理服务器。Apache httpd也是一个Web服务器，但这两者的适应场景不同，专注于解决不同的问题。总的来说，apache httpd：**稳定、对动态请求处理强，但同时高并发时性能较弱，耗费资源多**。nginx：**高并发处理能力强、擅长处理静态请求、反向代理、均衡负载。**下面详细说明[Nginx和Apache区别](https://blog.51cto.com/11837799/1826624 \"Nginx和Apache区别\")：\n\n1. nginx是轻量级的web服务，比apache占用更少的内存及资源，且并发能力更强，在高并发下nginx能保持低资源、低消耗、高性能。另外，nginx具有高度模块化的设计，编写模块相对简单，社区活跃，高性能模块出品迅速。\n2. apche的**rewrite能力**比nginx强大，且模块超多，基本想到的都可以找到。此外，apache的bug相比于nginx较少，比较稳定。一般来说，在性能方面，nginx更有优势，如果不需要性能只求稳定，那就用apache。\n   \n   > Rewrite是一种服务器的重写脉冲技术，它可以使得服务器可以支持 URL 重写，还可以实现限制特定IP访问网站的功能。 \n3. nginx处理请求是异步非阻塞模式，而apache则是同步阻塞模式。\n   - 同步：所谓同步，就是在发出一个功能调用时，在没有得到结果之前，该调用就不返回。\n   - 异步：异步的概念和同步相对。当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者。\n   - 阻塞：阻塞调用是指调用结果返回之前，当前线程会被挂起，函数只有在得到结果之后才会返回。有人也许会把阻塞调用和同步调用等同起来，实际上它们是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是当前函数没有返回而已。\n   - 非阻塞：非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。\n4. nginx配置简单，apache配置复杂，nginx静态处理性能比apache高3倍以上，但对动态处理请求弱。apache对PHP支持比较简单，nginx需要配合其他后端使用。\n5. 最核心的区别在于apache是同步多进程模式，一个连接对应一个进程，而nginx是异步的，多个连接（万级别）可以对应一个进程。两者处理请求模式的不同，导致了nginx的抗并发能力强，对资源需求更少。\n6. nginx配置文件简洁，正则配置让很多事情变得简单，运行效率高，占用资源少，代理功能强大，很适合做前端响应服务器。\n7. **nginx处理动态请求是鸡肋**，一般动态请求要apache去做，nginx只适合静态HTTP请求和反向代理。\n\n\n### 1.3 什么是Tengine\n\nTengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝、天猫等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。\n\nTengine和nginx官方性能测试网址为`http://tengine.taobao.org/document_cn/benchmark_cn.html`，测试结果：\n\n- Tengine相比Nginx默认配置，提升200%的处理能力。\n- Tengine相比Nginx优化配置，提升60%的处理能力。\n\nTengine详细参数解释详见：`http://tengine.taobao.org/nginx_docs/cn/docs/`\n\n\n\n## 2. Nginx下载与安装\n\nNginx官方网址`http://nginx.org/en/download.html`提供了三个类型的版本：\n\n- Mainline Version：主线版，是最新版，但未经过过多的生产测试。\n- Stable Version：稳定版，生产环境使用版本。\n- Legacy Version：老版本。\n\n这里下载Stable Version，稳定版又分为Linux 版与 Windows 版，本文下载Linxu版的nginx-1.16.1\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210215153359.png)\n\n\n\n在安装Nginx前，最好先配置好Linxu系统，并克隆两台以上虚拟机，因为后面的反向代理和负载均衡、Session共享配置，要用到多台虚拟机。可以参考我之前的文章[Linux切换运行级别、关闭防火墙、禁用selinux、关闭sshd、时间同步、修改时区、拍摄快照、克隆操作](https://mp.weixin.qq.com/s/kk1sY9EYajaEt_mevwmfIg)。\n\n接下来，采用[源码编译的方式安装Nginx](https://wxler.github.io/2021/02/10/112210/#src-build%E5%AE%89%E8%A3%85nginx \"安装Nginx\")！\n\n（1）安装Nginx依赖的库\n\n```bash\nyum install gcc gcc-c++ pcre  pcre-devel openssl openssl-devel zlib  zlib-devel vim -y\n```\n\n> 推荐使用yum的清华源和东北大学源，可参考[把Linux系统中的yum替换为清华源](https://mp.weixin.qq.com/s/bTMqYLEzJsi3BsT_leLAVA)进行配置。\n\n（2）创建存放源文件的文件夹\n\n```bash\ncd /opt # 进入opt目录下\nmkdir apps # 创建apps目录，用于存放源文件以及解压后的文件\n```\n\n（3）通过Xftp上传`nginx-1.16.1.tar.gz`文件到Linux虚拟机的`opt/apps`目录下，可参考[使用Xftp和lrzsz在Linxu与Windows之间互传文件](https://mp.weixin.qq.com/s/RMw9b7AT6U2t0PdfM8hspw)上传文件。\n\n（4）解压 Nginx压缩包\n\n```bash\ncd /opt/apps\ntar -zxvf nginx-1.16.1.tar.gz # 解压\n```\n\n（5）配置configure\n\n配置configure可理解为预编译，主要用于检测系统基准环境库是否满足gcc环境，生成makefile文件，可以使用 configure 命令可以生成该文件。那么，configure 命令需要配置些什么参数呢？使用`./configure  --help`可以查看到可以使用的参数说明。\n\n```bash\n[root@nginx1 apps]# cd /opt/apps/nginx-1.16.1\n[root@nginx1 nginx-1.16.1]# ./configure  --help\n\n  --help                             print this message\n\n  --prefix=PATH                      set installation prefix\n  --sbin-path=PATH                   set nginx binary pathname\n  --modules-path=PATH                set modules path\n  --conf-path=PATH                   set nginx.conf pathname\n  --error-log-path=PATH              set error log pathname\n  --pid-path=PATH                    set nginx.pid pathname\n  --lock-path=PATH                   set nginx.lock pathname\n\n  --user=USER                        set non-privileged user for\n                                     worker processes\n  --group=GROUP                      set non-privileged group for\n                                     worker processes\n##. . .(内容太长，这里只展示部分内容). . . ##\n```\n\n这些参数可以分为三类：\n\n- 第一类：基本信息的配置。\n- 第二类：默认没有安装，可以指定安装的模块，使用`--with`开头。Nginx 的高扩展性就体现在这里。\n- 第三类：默认已经安装，可以指定卸载的模块，使用`--without` 开头。\n\n这里简单介绍基本信息配置的含义：\n\n- prefix：Nginx 安装目录\n- sbin-path：sbin即binary file，为Nginx 命令文件存放目录\n- modules-path：Nginx 模块存放路径\n- conf-path：Nginx 配置文件存放路径\n- error-log-path：错误日志文件存放路径\n- pid-path：Nginx 的进程 id 存放路径\n- user：为Nginx的工作进程创建所属用户，如果不创建，则默认为nobody用户\n- group：为Nginx的工作进程创建所属用户组，如果不创建，则默认为nobody用户组\n\n下面，我给出两种配置方案，一种简单版，一种详细版，这里使用简单版进行配置：\n\n```bash\n### 1. 首先创建/var/tmp/nginx/client目录\n[root@nginx1 nginx-1.16.1]# mkdir –p /var/tmp/nginx/client\n### 2. 进入/opt/apps/nginx-1.16.1目录下\n[root@nginx1 nginx-1.16.1]# pwd\n/opt/apps/nginx-1.16.1\n### 3. 执行配置命令，简单版（当前使用）：\n./configure --prefix=/opt/nginx --with-http_ssl_module --with-http_gzip_static_module --error-log-path=/var/log/nginx/nginx.log --pid-path=/var/log/nginx/pid\n\n### 详细版（留着以后备用）：\n### 下面的 \\ 意思是这个命令还没完，可以继续往下写\n[root@nginx1 nginx-1.16.1]# ./configure \\\n  --prefix=/opt/nginx \\\n  --sbin-path=/usr/sbin/nginx\\\n  --conf-path=/etc/nginx/nginx.conf \\\n  --error-log-path=/var/log/nginx/error.log \\\n  --http-log-path=/var/log/nginx/access.log \\\n  --pid-path=/var/run/nginx/nginx.pid \\\n  --lock-path=/var/lock/nginx.lock \\\n  --user=nginx \\\n  --group=nginx \\\n  --with-http_ssl_module \\\n  --with-http_flv_module \\\n  --with-http_stub_status_module \\\n  --with-http_gzip_static_module \\\n  --http-client-body-temp-path=/var/tmp/nginx/client/ \\\n  --http-proxy-temp-path=/var/tmp/nginx/proxy/ \\\n  --http-fastcgi-temp-path=/var/tmp/nginx/fcgi/ \\\n  --http-uwsgi-temp-path=/var/tmp/nginx/uwsgi \\\n  --http-scgi-temp-path=/var/tmp/nginx/scgi \\\n  --with-pcre  \n```\n\n*注意：详细版还需要执行`useradd nginx` 添加nginx用户。*\n\n在执行过 configure 命令后并不会立即生成`/opt/nginx`目录，也不会马上开始安装指定的模块，而仅仅是将命令中指定的参数及默认配置写入到即将要生成的 Makefile文件中。\n\n配置成功后，再次查看 Nginx 解压目录，发现其中多出了一个文件 Makefile，后面的编译就是依靠该文件进行的。\n\n```bash\n[root@nginx1 nginx-1.16.1]# ls\nauto  CHANGES  CHANGES.ru  conf  configure  contrib  html  LICENSE  Makefile  man  objs  README  src\n```\n\n\n\n（6）编译\n\n`make clean`命令用来清除上一次编译生成的目标文件（**清除上次的make命令所产生的object文件**，即后缀为`.o`的文件及可执行文件）。这个步骤可有可无，但为了确保编译的成功，还是加上为好。防止由于软件中含有残留的目标文件导致编译失败。\n\n生成脚本及配置文件：`make` （根据Makefile文件编译源代码、连接、生成目标文件、可执行文件）\n\n所以一般先输入`make clean`清除编译再输入`make`进行编译。\n\n（7）安装\n\n输入`make install`，将编译成功的可执行文件安装到系统目录中。\n\n（8）添加nginx进程到服务列表\n\n安装之后进入nginx的安装目录/opt/nginx，查看包含哪些文件：\n\n```bash\n[root@nginx1 nginx]# cd /opt/nginx\n[root@nginx1 nginx]# ls\nclient_body_temp  conf  fastcgi_temp  html  logs  proxy_temp  sbin  scgi_temp  uwsgi_temp\n```\n\n- **conf**：保存nginx所有的配置文件，其中nginx.conf是nginx服务器的最核心最主要的配置文件，其他的.conf则是用来配置nginx相关的功能的，例如fastcgi功能使用的是fastcgi.conf和fastcgi_params两个文件，配置文件一般都有个样板配置文件，以文件名.default结尾，使用的时候将其复制并将default去掉即可。\n- **html**：目录中保存了nginx服务器的web文件，但是可以更改为其他目录保存web文件，另外还有一个50x的web文件是默认的错误提示页面。\n- **logs**：用来保存nginx服务器访问的错误日志等，logs目录可以放在其他路径，比如/var/logs/nginx里面。\n- **sbin**：保存nginx二进制启动脚本，可以接受不同的参数以实现不同的功能。\n\n默认情况下，想要启动nginx ，必须在`/opt/nginx/sbin` 目录中输入`./nginx`启动nginx进程，使用起来很不方便。为了能够在任意目录下均可直接执行 nginx 命令，可通过以下三种方式完成：\n\n**方式一（推荐采用）**\n\n添加安装的nginx到服务列表，步骤如下：\n\n1. 将如下内容添加到`/etc/init.d/nginx`脚本中\n```bash\n[root@nginx1 nginx]# vim /etc/init.d/nginx  # 所有开机启动的进程都在/etc/init.d下\n\n#!/bin/sh\n#\n# nginx - this script starts and stops the nginx daemon\n#\n# chkconfig:   - 85 15 \n# description:  Nginx is an HTTP(S) server, HTTP(S) reverse \\\n#               proxy and IMAP/POP3 proxy server\n# processname: nginx\n \n# Source function library.\n. /etc/rc.d/init.d/functions\n \n# Source networking configuration.\n. /etc/sysconfig/network\n \n# Check that networking is up.\n[ \"$NETWORKING\" = \"no\" ] && exit 0\n \nnginx=\"/opt/nginx/sbin/nginx\"\nprog=$(basename $nginx)\n \nNGINX_CONF_FILE=\"/opt/nginx/conf/nginx.conf\"\n \n[ -f /etc/sysconfig/nginx ] && . /etc/sysconfig/nginx\n \nlockfile=/var/lock/subsys/nginx\n \nmake_dirs() {\n   # make required directories\n   user=`nginx -V 2>&1 | grep \"configure arguments:\" | sed 's/[^*]*--user=\\([^ ]*\\).*/\\1/g' -`\n   options=`$nginx -V 2>&1 | grep 'configure arguments:'`\n   for opt in $options; do\n       if [ `echo $opt | grep '.*-temp-path'` ]; then\n           value=`echo $opt | cut -d \"=\" -f 2`\n           if [ ! -d \"$value\" ]; then\n               # echo \"creating\" $value\n               mkdir -p $value && chown -R $user $value\n           fi\n       fi\n   done\n}\n \nstart() {\n    [ -x $nginx ] || exit 5\n    [ -f $NGINX_CONF_FILE ] || exit 6\n    make_dirs\n    echo -n $\"Starting $prog: \"\n    daemon $nginx -c $NGINX_CONF_FILE\n    retval=$?\n    echo\n    [ $retval -eq 0 ] && touch $lockfile\n    return $retval\n}\n \nstop() {\n    echo -n $\"Stopping $prog: \"\n    killproc $prog -QUIT\n    retval=$?\n    echo\n    [ $retval -eq 0 ] && rm -f $lockfile\n    return $retval\n}\n \nrestart() {\n    configtest || return $?\n    stop\n    sleep 1\n    start\n}\n \nreload() {\n    configtest || return $?\n    echo -n $\"Reloading $prog: \"\n    killproc $nginx -HUP\n    RETVAL=$?\n    echo\n}\n \nforce_reload() {\n    restart\n}\n \nconfigtest() {\n  $nginx -t -c $NGINX_CONF_FILE\n}\n \nrh_status() {\n    status $prog\n}\n \nrh_status_q() {\n    rh_status >/dev/null 2>&1\n}\n \ncase \"$1\" in\n    start)\n        rh_status_q && exit 0\n        $1\n        ;;\n    stop)\n        rh_status_q || exit 0\n        $1\n        ;;\n    restart|configtest)\n        $1\n        ;;\n    reload)\n        rh_status_q || exit 7\n        $1\n        ;;\n    force-reload)\n        force_reload\n        ;;\n    status)\n        rh_status\n        ;;\n    condrestart|try-restart)\n        rh_status_q || exit 0\n            ;;\n    *)\n        echo $\"Usage: $0 {start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest}\"\n        exit 2\nesac\n\n```\n2. 然后，修改nginx文件的执行权限。\n```bash\nchmod +x /etc/init.d/nginx\n```\n3. 最后，添加该文件到系统服务中去。\n```bash\nchkconfig --add /etc/init.d/nginx\n```\n查看服务列表中的nginx进程：\n```bash\n[root@nginx1 nginx]# chkconfig --list nginx\nnginx          \t0:off\t1:off\t2:off\t3:off\t4:off\t5:off\t6:off\n```\n\n这样，就完成了添加nginx进程到服务列表，可以使用service命令来管理nginx进程：\n\n- `service nginx start|stop|reload`：设置nginx进程启动|停止|重新加载\n- `service nginx restart` ：重启nginx进程\n- `service nginx on` ：设置nginx进程开机启动\n\n现在，我们设置nginx进程当前启动和开机启动：\n\n```bash\n[root@nginx1 nginx]# chkconfig nginx on #设置nginx进程开机启动\n[root@nginx1 nginx]# chkconfig --list nginx\nnginx          \t0:off\t1:off\t2:on\t3:on\t4:on\t5:on\t6:off\n[root@nginx1 nginx]# service nginx start #启动nginx进程\n[root@nginx1 nginx]# ps aux | grep nginx\nroot       1216  0.0  0.1  49400  1292 ?        Ss   Feb14   0:00 nginx: master process /opt/nginx/sbin/nginx -c /opt/nginx/conf/nginx.conf\nnobody     1218  0.0  0.2  49920  2348 ?        S    Feb14   0:00 nginx: worker process                              \nroot       1509  0.0  0.0 103256   864 pts/0    S+   07:16   0:00 grep nginx\n```\n\n\n\n**方式二**\n\n在`/etc/profile` 文件最后添加以下内容，将安装目录下的 sbin 目录添加到 PATH 系统环境变量中，然后再重新加载该文件即可。\n\n```bash\n#1. 进入/etc/profile\nvim /etc/profile\n\n#2. 在文件末尾添加环境变量配置\nexport PATH=$PATH:/opt/nginx/sbin  #注意可执行文件的位置，即--prefix的配置位置\n\n#3. 使配置立即生效\nsource /etc/profile\n\n#4. 启动nginx\nnginx  #启动nginx\n```\n\n方式二在系统中的任何位置都可以使用下面方式三的命令。\n\n**方式三**\n\n直接在`/opt/nginx/sbin`目录下管理nginx进程\n\n```bash\n[root@nginx1 sbin]# pwd\n/opt/nginx/sbin\n[root@nginx1 sbin]# ./nginx # 启动nginx进程\n[root@nginx1 sbin]# ./nginx -c /opt/nginx/conf/nginx.conf # 以特定目录下的配置文件启动nginx进程\n[root@nginx1 sbin]# ./nginx -s stop # 立即停止服务\n[root@nginx1 sbin]# ./nginx -s reload # 执行这个命令后，master进程会等待worker进程处理完当前请求，然后根据最新配置重新创建新的worker进程，完成Nginx配置的热更新。\n[root@nginx1 sbin]# ./nginx -s quit # 执行该命令后，Nginx在完成当前工作任务后再停止。\n[root@nginx1 sbin]# ./nginx -t  # 检查配置文件是否正确\nnginx: the configuration file /opt/nginx/conf/nginx.conf syntax is ok\nnginx: configuration file /opt/nginx/conf/nginx.conf test is successful\n[root@nginx1 sbin]# ./nginx -t -c /opt/nginx/conf/nginx.conf # 检查特定目录的配置文件是否正确\nnginx: the configuration file /opt/nginx/conf/nginx.conf syntax is ok\nnginx: configuration file /opt/nginx/conf/nginx.conf test is successful\n\n[root@nginx1 sbin]# ./nginx -v # 查看版本信息\nnginx version: nginx/1.16.1\n```\n\n\n\n\n\n这里，我使用**方式一**给nginx添加到服务列表，启动nginx进程后，在浏览器中输入虚拟机的ip地址，如`http://192.168.50.55/`，即可出现如下页面：\n\n![img](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208230509.png)\n\n\n\n\n\n\n\n## 3. Nginx工作模型\n\n### 3.1 Master-Worker模式\n\n当我们启动nginx以后，会有两个nginx进程，一个master进程，一个worker进程，可以输入`ps aux | grep nginx`查看：\n\n```bash\n[root@nginx1 nginx]# ps aux | grep nginx\nroot       1641  0.0  0.1  49400  1292 ?        Ss   09:18   0:00 nginx: master process /opt/nginx/sbin/nginx -c /opt/nginx/conf/nginx.conf\nnobody     1643  0.0  0.1  49920  1876 ?        S    09:18   0:00 nginx: worker process                              \nroot       1645  0.0  0.0 103256   860 pts/0    S+   09:18   0:00 grep nginx\n```\n\n\n\n在预编译配置nginx时，我们没有为nginx的wrok进程创建用户，所以默认情况下worker进程是以\"nobody\"用户的身份运行的，如果想要指定worker进程的运行用户，则可以创建一个用户，并修改`/opt/nginx/conf/nginx.conf`的配置即可。\n\n现在，我们指定worker进程以nginx用户的身份运行，操作步骤如下：\n\n```bash\n[root@nginx1 nginx]# useradd nginx  # 添加nginx用户(默认添加一个同名的用户组)\n[root@nginx1 nginx]# id nginx  # 查看nginx用户的信息\nuid=500(nginx) gid=500(nginx) groups=500(nginx)\n[root@nginx1 nginx]# vim /opt/nginx/conf/nginx.conf # 将user nobody注释去掉，并改为user nginx;\n[root@nginx1 nginx]# service nginx restart # 重启nginx进程\nnginx: the configuration file /opt/nginx/conf/nginx.conf syntax is ok\nnginx: configuration file /opt/nginx/conf/nginx.conf test is successful\nStopping nginx:                                            [  OK  ]\nStarting nginx:                                            [  OK  ]\n[root@nginx1 nginx]# ps aux | grep nginx # 再次查看nginx的work进程所属的用户\nroot       1698  0.0  0.1  49404  1300 ?        Ss   09:27   0:00 nginx: master process /opt/nginx/sbin/nginx -c /opt/nginx/conf/nginx.conf\nnginx      1700  0.0  0.1  49920  1884 ?        S    09:27   0:00 nginx: worker process                              \nroot       1702  0.0  0.0 103256   860 pts/0    S+   09:27   0:00 grep nginx\n\n```\n\n[master进程和nginx进程各自有什么用呢](https://www.cnblogs.com/jianmingyuan/p/13960906.html \"work和master进程的作用\")？见名知意，worker进程天生就是来\"干活\"的，真正负责处理请求的进程就是你看到的worker进程。master进程其实是负责管理worker进程的，除此之外，master进程还负责读取配置文件、判断配置文件语法的工作。master进程也叫主进程，在nginx中，主进程只能有一个，而worker进程可以有多个，可以由管理员自己进行定义。在`nginx.conf`下的配置文件中有这样一条配置：\n\n```bash\nworker_ processes 1;\n```\n\n默认情况下，启动nginx后只有1个worker进程，你想要多少个worker进程，将worker_ processe指令的值设置成多少就好了。\n\nworker_ processes的值通常不会大于服务器中cpu的核心数量，换句话说，worker进程的数通常与服务器有多少\ncpu核心有关，比如，nginx所在主机拥有4核cpu，那么worker_ processes的值通常不会大于4，这样做的原因是为了尽力让每个worker进程都有一个cpu可以使用，尽量避免了多个worker进程抢占同一个cpu的情况。我们也可以将worker_ processes的值设置为\"auto\"，这时nginx会自动检测当前主机的cpu核心数，并启动对应数量的worker进程。\n\n### 3.2 worker_cpu_affinity\n\n为了避免cpu在切换进程时产生性能损耗，我们也可以将worker进程与cpu核心进行\"绑定\"，当worker进程与cpu核心绑定以后，worker进程可以更好的专注的使用某个cpu核心上的缓存，从而减少因为cpu切换不同worker进程而带来的缓存失效。此功能，可以通过nginx.conf的`worker_ cpu_ affinity`配置来完成。不过，若指定 worker_processes 的值为auto，则无法设置 worker_cpu_affinity。\n\nworker_cpu_affinity的设置是通过二进制进行的。每个内核使用一个二进制位表示，0 代表内核关闭，1 代\n\n表内核开启。也就是说，有几个内核，就需要使用几个二进制位。下图是worker_processes的数量与绑定的cpu取值的设定关系：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210215195127.png)\n\n比如，4核cpu可以设置如下的配置：\n\n```bash\nuser  nginx;\nworker_processes  4;\nworker_cpu_affinity 1000 0100 0010 0001;\n```\n\n### 3.3 accept_mutex\n\naccept_mutex机制通过accept锁来避免所有休眠的work子进程被唤醒的情形，从而避免了惊群问题。\n\n[所谓的惊群问题](https://blog.csdn.net/weixin_33759269/article/details/93019512 \"Nginx惊群问题\")，当一个新的连接到达时，所有处于休眠状态的work进程都会被唤醒，但是只有一个wrok进程能获取连接，其它的work进程会重新进入休眠状态。这就导致出现了很多不必要的调度及上下文切换，浪费了系统的资源。\n\n> 在Linux内核的较新版本中，accept调用本身所引起的惊群问题已经得到了解决，但是在Nginx中，accept是交给epoll机制来处理的，epoll的accept带来的惊群问题并没有得到解决，执行epoll_wait后，所有监听这个事件的进程会被这个epoll_wait唤醒，所以Nginx的accept惊群问题仍然需要定制一个自己的解决方案。\n\nNginx通过**accept锁**来解决惊群问题，accept锁本质上这是一个跨进程的互斥锁（共享锁）。在实现上，accept锁是一个全局变量，放在一块进程间共享的内存中，以保证所有进程都能访问这一个实例，加锁、解锁是借由linux的原子变量来做CAS算法，如果加锁失败则立即返回，是一种非阻塞的锁。\n\n**Nginx默认激活了accept_mutex**，也就是说不会有惊群问题，即每个worker进程在执行accept之前都需要先获取锁（共享锁），获取不到就放弃执行accept()。有了这把锁之后，同一时刻，就只会有一个进程去accpet()，这样就不会有惊群问题了。\n\n\n\n实际上Nginx作者Igor Sysoev曾经给过相关的解释：\n\n> OS may wake all processes waiting on accept() and select(), this is called thundering herd problem. This is a problem if you have a lot of workers as in Apache (hundreds and more), but this insensible if you have just several workers as nginx usually has. Therefore turning accept_mutex off is as scheduling incoming connection by OS via select/kqueue/epoll/etc (but not accept()).\n\n意思就是说，OS可能会唤醒所有等待的进程，也就是惊群问题。如果像Apache那样有很多work者（几百个甚至更多），会出现这样的问题，但如果像nginx那样只有几个work进程，这就不太可能了。因此，关闭accept_mutex（默认开启）就像操作系统通过select/kqueue/epoll/等来调度传入的请求连接一样（但不是accept）。\n\nNginx默认激活了accept_mutex，是一种保守的选择。如果关闭accept_mutex，可能会引起一定程度的惊群问题，主要表现为浪费了很多不必要的调度及上下文切换。但是开启accept_mutex，即同时唤醒所有的work进程，让他们主动去抢占式获取连接，整体的效率无疑大大增强了。\n\n如果想要关闭accept_mutex，只要在nginx.conf中加入以下配置即可：\n\n```bash\nevents { \naccept_mutex off; \n} \n```\n\nselect、epoll等都是常用的IO模型，准确的说是IO多路复用，想要更深入的了解，可以参考我的文章[详解磁盘IO、网络IO、零拷贝IO、BIO、NIO、AIO、IO多路复用(select、poll、epoll)](https://mp.weixin.qq.com/s/6aDzR8EIYWX8wP_LFBF5WA)\n\n\n\n### 3.4 使用进程而不用线程\n\nNginx使用的进程（即worker process）而不是线程进行工作，为什么使用进程不使用线程呢？\n\n使用进程能够节省锁机制带来的开销。每个worker进程都是独立的进程，独立拥有资源（且拥有独立的内存空间），各进程之间不共享资源。而一个进程的多个线程共享所属进程的资源，线程本身不拥有资源，仅有一点不可缺少的、能保证独立运行的资源。这就导致了一个进程中的某个线程出现问题后（如地址越界），会导致整个进程垮掉。当采用多进程来实现时，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断。在Nginx启动后，会启动相应数量的worker进程，在worker进程在异常情况下退出后，Master进程会自动启动新的 worker 进程，保证worker进程的个数和配置文件里设置的个数相对应。\n\n当一个worker进程在accept()这个连接之后，就开始读取请求、解析请求、处理请求，然后把处理的结果再返回客户端，最后才断开连接，完成一个完整的请求。所以，一个请求，完全由worker进程来处理，而且只能在一个worker进程中处理。每个worker进程之间不会相互影响，就算一个worker进程因为异常挂掉了，也不会影响其他worker进程。\n\n\n\n### 3.5 处理高并发请求\n\n我们知道，worker进程负责处理每个请求，当有多个并发的请求到来时，如何进行处理呢？\n\n当有多个并发的请求到来时，每进来一个请求，会有一个worker进程去处理，但不是全程的处理，处理到什么程度呢？一般处理到可能发生阻塞的地方，比如worker进程转发来自客户端的请求到相应的服务器（如tomcat或apache)后，它不会这么傻等着，而是在发送完请求后，注册一个**事件**（Nginx以事件驱动的特征就体现在这里）：“如果upstream返回了，告诉我一声，我再接着干”，于是他就休息去了（或者为其他的请求进行处理）。此时，如果再有新的请求进程，worker进程就可以很快再按这种方式处理。一旦服务器返回了，就会触发这个事件，worker才会来接手这个请求，把请求的结果返回给客户端。\n\n一般来说，Nginx服务器只有几个worker进程，但会处理数十万个请求，也就是说每个worker进程会处理多个请求，这个可以在Nginx.conf中进程设置：\n\n```bash\nevents {\n    worker_connections  1024;\n}\n```\n\n由于web server的工作性质决定了每个请求的大部份时间都是在网络传输中，实际上花费在server机器上的时间片不多，并且Nginx主要用来转发请求的，一般不真正处理请求，所以几个worker进程就能解决高并发的请求。\n\n\n\n\n\n## 4. Nginx参数详解\n\n### 4.1 nginx.conf全览\n\nnginx.conf有三部分组成，分别是全局块、events块、http块：\n\n```bash\n#---全局块开始----\n#user  nobody;\nworker_processes  1;\n\n#error_log  logs/error.log;\n#error_log  logs/error.log  notice;\n#error_log  logs/error.log  info;\n\n#pid        logs/nginx.pid;\n#----全局块结束----\n\n#====events块开始====\nevents {\n    worker_connections  1024;\n}\n#====events块结束====\n#****http块开始****\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n    #log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n    #\t\t\t'$status $body_bytes_sent \"$http_referer\" '\n    ......\n}\n#****http块结束****\n```\n\nNginx 配置详解可以从参考这里：`https://www.runoob.com/w3cnote/nginx-setup-intro.html`\n\n#### 4.1.1 全局块\n\n从Nginx.conf的开始到events之间的内容是全局块，主要设置一些nginx服务器整体运行的配置命令。下面我们分别进程理解和配置：\n\n**（1）worker进程的用户**\n\n在nginx.conf中，worker进程的默认用户为nobody，查看这个用户的信息\n\n```bash\n[root@nginx1 conf]# id nobody\nuid=99(nobody) gid=99(nobody) groups=99(nobody)\n[root@nginx1 conf]# cat /etc/passwd\n......\nnobody:x:99:99:Nobody:/:/sbin/nologin\n......\n```\n\n可以看到，该用户对应的程序是`/sbin/nologin`，所以就算有人通过worker进程黑到了你的服务器，也就是不能做任何事情！\n\n输入`ps -ef|grep nginx`查看当前nginx进程信息：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217161048.png)\n\n可以看到，用之前的配置方法，没有创建nginx用户和用户组，这时maste进程所属的用户是root，worker进程所属的用户是nobody，一般正式开发需要创建一个用户，毕竟root权限太大了。\n\n可以添加一个nginx用户，并修改nginx.conf的配置：\n\n```bash\n#1.添加用户\nuseradd nginx  # 添加nginx用户(默认添加一个同名的用户组)\n#2.修改nginx.conf配置\nuser nginx;\n#3.默认创建完之后，nginx用户登入后执行的是`/bin/bash`，可以修改为`/opt/nginx/sbin/nginx`\nusermod -s /opt/nginx/sbin/nginx nginx\n```\n\n**（2）worker进程数**\n\nworker_processes用于配置nginx启动后的woker进程数，也可以将worker_ processes的值设置为\"auto\"，这时nginx会自动检测当前主机的cpu核心数，并启动对应数量的worker进程。\n\n**（3）错误配置**\n\nerror_log配置nginx日志文件的全路径名\n\n**（4）进程pid**\n\npid配置进程PID存放路径\n\n\n\n如果在nginx中不进行上述配置，默认的配置时nginx.conf注释的配置。\n\n\n\n#### 4.1.2 events块\n\nevents块主要用于配置**每个worker进程所支持的最大连接数**。\n\n```bash\nevents {\n    worker_connections  1024;\n}\n```\n\nNginx服务器总的并发数即最大连接数`max_clients = worker_processes * worker_connections`。一般。不会达到这么大的连接数，因为要受到物理内存和系统可以打开的最大文件数的限制，这个我们在4.2节介绍。\n\nevents除了配置最大连接数，也用于许多其他的配置：\n\n```bash\nevents {\n    accept_mutex on; #设置网路连接序列化，防止惊群现象发生，默认为on\n    multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off\n    use epoll; #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport\n    worker_connections 1024; # 最大连接数\n}\n```\n\n这里，重点介绍一下`use epoll`，Nginx服务器提供了多个事件驱动模型来处理网络消息。\n\n与apache类似，nginx针对不同的操作系统，有不同的事件模型。如果你不知道Nginx该使用哪种轮询方法的话，它会选择一个最适合你操作系统的。[常用的I/O模型有](https://blog.csdn.net/u010832551/article/details/85160336 \"Nginx的I/O模型\")：\n\n- 标准事件模型：select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll\n- 高效事件模型：\n  - kqueue：使用于FreeBSD 4.1+、OpenBSD 2.9+、NetBSD 2.0 和 MacOS X，但双处理器的MacOS X系统使用kqueue可能会造成内核崩溃\n  - epoll：使用于Linux内核2.6版本及以后的系统\n  - /dev/poll：使用于Solaris 7 11/99+、HP/UX 11.22+ (eventport)、IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+\n  - eventport：使用于Solaris 10，为了防止出现内核崩溃的问题， 有必要安装安全补丁\n\n可以使用`uname -a`命令查看服务器系统相关的信息。\n\n```bash\n[root@nginx1 conf]# uname -a\nLinux nginx1 2.6.32-431.el6.x86_64 #1 SMP Fri Nov 22 03:15:09 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n可以看到，我的Linux内核版本是2.6.32，大于2.6版本的内核，故可以选用epoll作为事件驱动模型。\n\n\n\n#### 4.1.3 http块\n\nhttp块是Nginx服务器配置中最频繁的部分，代理、缓存和日志定义等绝大多数功能和第三方模块的配置都在这里。\n\n```bash\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n\n    #log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n    #                  '$status $body_bytes_sent \"$http_referer\" '\n    #                  '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    #access_log  logs/access.log  main;\n\n    sendfile        on; # 开启零拷贝\n    #tcp_nopush     on;\n    #test\n    keepalive_timeout  0;\n    #keepalive_timeout  65; # 会话的保持时间，默认是65\n    gzip  on; #是否启动压缩\n    server {\n        listen       80;#监听的端口号\n        server_name  localhost;#监听的域名\n        #charset koi8-r;\n        #access_log  logs/host.access.log  main;\n        location / { #路径中包含 /\n            root   html;\n            index  index.html index.htm;\n        }\n        ......\n    }\n    ......\n}\n```\n\nhttp中几个常用配置：\n\n- `keepalive_timeout` 是会话的保持时间，在部署时需要设置合适的数值来提高效率\n- `log_format`和`access_log`是http块的日志配置，后面会详细讲解。\n- `gzip  on`指启用压缩，压缩可以有效减少文件的大小，**有利于网络传输**。\n\nhttp块中最重要的就属server块，server块和虚拟主机有密切关系，虚拟主机从用户角度看，和一台独立的硬件主机是完全一样的，该技术的产生是为了节省互联网服务器硬件成本。每个http块可以包括多个server块，而每个server块就相当于一个虚拟主机。每个server块可以同时包含多个location块，**location主要配置nginx转发的出口**。\n\n这里先说个大概，后面会详细介绍这些模块的配置。\n\n\n\n### 4.2 连接数上限\n\n```bash\n#user  nobody;\nworker_processes  1;\nevents {\n\tuse epoll;\n\tworker_connections 1024;\n}\n```\n\n前面也提到了，Nginx服务器总的并发数即最大连接数`max_clients = worker_processes * worker_connections`。一般，不会达到这么大的连接数，因为：\n\n- `worker_connections`值的设置跟物理内存大小有关\n- 因为并发受IO约束，**max_clients的值须小于系统可以打开的最大文件数**。\n\n系统可以打开的最大文件数和内存大小成正比，一般1GB内存的机器上可以打开的文件数大约是10万左右。假如服务器内存是16G，则可以打开的最大文件数约为16万。\n\n[Linux可以打开的最大文件数](https://blog.csdn.net/sunny05296/article/details/54952009 \"Linux可以打开的最大文件数\")可以由`/proc/sys/fs/file-max`和`ulimit -n`来配置。\n\n- `file-max`表示系统级别的能够打开的文件句柄的数量，是对整个系统的限制，并不是针对用户的。\n- `ulimit -n`是进程级别能够打开的文件句柄的数量，提供shell及其启动的进程的可用文件句柄的控制，是进程级别的。\n\n对于服务器来说，file-max和ulimit都需要设置，**否则会出现文件描述符耗尽的问题**。\n\n一般如果遇到文件句柄达到上限时，会碰到`Too many open files`或者`Socket/File: Can’t open so many files`等错误。\n\n当前，我的Linux虚拟机为1G，来看下可以打开的文件句柄数是多少：\n\n```bash\n[root@nginx1 conf]# cat /proc/sys/fs/file-max #整个系统能够打开的文件句柄的数量\n97319\n[root@nginx1 conf]# ulimit -n  #单个用户进程打开的文件句柄的数量\n1024\n```\n\n可以看到，虚拟机内存为1G的Linxu系统可以打开的最大文件数是9.7万多个。\n\n可以修改`file-max`和`ulimit`，下面我们就来尝试一下！\n\n（1）修改`file-max`\n\n一般我们不需要主动修改`file-max`值，Linux系统在安装时会根据物理内存自动生成合适的大小，当然如果你认为它的调小了，也可以进行修改，方法如下：\n\n1. 第一步：`vim /etc/sysctl.conf`\n2. 第二步：在文件末尾加入配置内容：`fs.file-max = 2000000`\n3. 第三步：执行`sysctl -p` ，使修改配置立即生效\n\n再次查看`/proc/sys/fs/file-max`的内容，发现已经改变：\n\n```bash\n[root@nginx1 conf]# cat /proc/sys/fs/file-max\n2000000\n```\n\n上述修改永久生效，即重启系统后也不会失效。\n\n（2）修改`ulimit`\n\n执行`vim /etc/security/limits.conf`在文件中添加如下两行记录：\n\n```bash\n* soft nofile 65535\n* hard nofile 65535\n```\n\n重启系统后，输入`ulimit -n`查看单个用户进程打开的文件句柄的数量：\n\n```bash\n[root@nginx1 ~]# ulimit -n\n65535\n```\n\n可以查看，修改已经生效。\n\n如果需要设置当前用户session立即生效，则执行`ulimit -n 65535 `命令即可。\n\n对于服务器，**一般修改进程级的最大打开文件句柄数即可（系统默认1024，有点小），一般不需要调整系统级的最大数。**如果真的出现了达到系统级别最大限制时，也可以调整系统级的最大数。\n\n只要让并发连接总数小于系统可以打开的文件句柄总数，操作系统才可以在承受的范围之内。所以，**worker_connections 的值需根据 worker_processes 进程数目和用户进程打开的文件句柄的数量适当地进行设置，使得并发总数小于操作系统可以打开的最大文件数目**，实质上也就是根据主机的物理CPU和内存进行配置。\n\n当然，理论上的并发总数可能会和实际有所偏差，因为主机还有其他的工作进程需要消耗系统资源。\n\n\n\n### 4.3 开启零拷贝\n\nsendfile实际上是 Linux2.0+以后的推出的一个系统调用，web服务器可以通过调整自身的配置来决定是否利用 sendfile这个系统调用。\n\n先来看一下不用sendfile的传统网络传输过程，从逻辑上看只需要两行代码即可：\n\n```bash\nread(file,tmp_buf, len);\nwrite(socket,tmp_buf, len);\n```\n\n一般，一个基于socket的服务，首先读硬盘数据，然后写数据到socket 来完成网络传输的。上面2行用代码解释了这一点，不过上面2行简单的代码掩盖了底层的很多操作。来看看底层是怎么执行上面2行代码的：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217191408.png)\n\n执行过程：硬盘 >> kernel buffer >> user buffer>> kernel socket buffer >>协议栈\n\n- 当应用程序调用read()方法时，通过DMA方式将数据从磁盘拷贝到kernel buffer\n- 由cpu控制，将kernel buffer的数据拷贝到用户空间的User buffer中，供应用程序使用\n- 当应用程序调用write()方法时，cpu会把User buffer的数据copy到内核缓冲区的Socket Buffer中\n- 最后通过DMA方式将内核空间中的Socket Buffer拷贝到Socket协议栈（即网卡设备）中传输。\n\n这里经历了4次上下文切换和4次缓冲区的copy，第一次是从磁盘缓冲区到内核缓冲区（由cpu控制），第二次是内核缓冲区到用户缓冲区（DMA控制），第三次是用户缓冲区到内核缓冲区的Socket Buffer（由cpu控制），第四次是从内核缓冲区的Socket Buffer到网卡设备（由DMA控制）。四次缓冲区的copy工作两次由cpu控制，两次由DMA控制。\n\n我们发现如果能减少切换次数和拷贝次数将会有效提升性能。在kernel2.0+ 版本中，系统调用 sendfile() 就是用来简化上面步骤提升性能的，sendfile() 不但能减少切换次数而且还能减少拷贝次数。\n\n再来看一下用 sendfile()来进行网络传输的过程，逻辑上也就一行代码：\n\n```bash\nsendfile(socket,file, len);\n```\n\n底层执行过程如下：\n\n硬盘 >> kernel buffer (快速拷贝到kernelsocket buffer) >>协议栈\n\n- 当应用程序调用read()方法时，通过DMA方式将数据从磁盘拷贝到kernel buffer\n- 由cpu控制，将kernel buffer的数据直接拷贝到另外一个与 socket相关的内核缓冲区，即kernel socket buffer\n- 然后由DMA 把数据从kernel socket buffer直接拷贝给Socket协议栈（网卡设备中）。\n\n\n\n这里，**只经历了三次缓冲区的拷贝**，第一次是从磁盘缓冲区到内核缓冲区，第二次是从内核缓冲区到kernel socket buffer，第三次是从kernel socket buffe到Socket协议栈（网卡设备中）。**只发生两次内核态和用户态的切换**，第一次是当应用程序调用read()方法时，用户态切换到内核到执行read系统调用，第二次是将数据从网络中发送出去后系统调用返回，从内核态切换到用户态。\n\n需要注意的是，sendfile要求输入的fd必须是文件句柄，不能是socket，输出的fd必须是socket，也就是说，数据的来源必须是从本地的磁盘，而不能是从网络中，如果数据来源于socket，就不能使用零拷贝功能了。我们看一下Linux内核的sendfile接口就知道了：\n\n```c\n#include <sys/sendfile.h>\nssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count)\n```\n\n- out_fd：待写入文件描述符\n- in_fd： 待读出文件描述符\n- offset：从读入文件流的哪个位置开始读，如果为空，则默认从起始位置开始\n- count：指定在文件描述符in_fd 和out_fd之间传输的字节数\n- 返回值：成功时，返回出传输的字节数，失败返回-1\n\n **in_fd必须指向真实的文件，不能是socket和管道；而out_fd则必须是一个socket。由此可见，sendfile几乎是专门为在网络上传输文件而设计的。所以，当 Nginx 是作为一个反向代理来使用的时候，SENDFILE 则没什么用了，因为 Nginx 是反向代理的时候，in_fd 就不是文件句柄而是 socket，此时就不符合 sendfile 函数的参数要求了。**\n\n\n\n> fd全称file descriptor，即文件描述符，是内核为了高效管理已被打开的文件所创建的索引，其是一个非负整数（通常是小整数），用于指代被打开的文件，所有执行I/O操作的系统调用都通过文件描述符。in_fd是输入的文件描述符，out_fd是输出的文件描述符。\n\n### 4.4 nginx虚拟主机\n\n上面也提到了，http块中的server块是用来配置虚拟主机的，所谓的虚拟主机，就是将一台物理服务器虚拟为多个服务器来使用，从而实现在一台服务器上配置多个站点，即可以在一台物理主机上配置多个域名。在Nginx 中，一个 server 标签就是一台虚拟主机，配置多个 server 标签就虚拟出了多台主机。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217193542.png)\n\nNginx 虚拟主机的实现方式有两种：域名虚拟方式与端口虚拟方式。域名虚拟方式是指不同的虚拟机使用不同的域名，通过不同的域名虚拟出不同的主机；端口虚拟方式是指不同的虚拟机使用相同的域名不同的端口号，通过不同的端口号虚拟出不同的主机。但是，基于端口的虚拟方式不常用。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217201143.png)\n\n现在，就来配置一下吧！\n\n先来看一个重要参数：\n\n```bash\nautoindex on; #开启目录列表访问，合适下载服务器，默认关闭\n```\n\n在nginx.conf的http块中添加两个server块，配置如下：\n\n```bash\nserver {\n\tlisten       80;\n\tserver_name  www.layne.com;\n\tlocation / {\n\t\troot   html;\n\t\tindex  index.html index.htm;\n\t}\n}\nserver {\n\tlisten       80;\n\tserver_name  www.123.com;\n\tlocation / {\n\t\troot   /mnt;\n\t\tautoindex  on;\n\t}\n}\n```\n\n然后，重启Nginx：\n\n```bash\n[root@nginx1 conf]# service nginx restart\nnginx: the configuration file /opt/nginx/conf/nginx.conf syntax is ok\nnginx: configuration file /opt/nginx/conf/nginx.conf test is successful\nStopping nginx:                                            [  OK  ]\nStarting nginx:                                            [  OK  ]\n```\n\n最后，在windows平台的hosts文件（在`C:\\Windows\\System32\\drivers\\etc`路径下）添加一条域名解析：\n\n```bash\n192.168.218.55 nginx1 www.123.com  www.layne.com\n```\n\n上述ip就是nginx所在Linux系统的ip，后面是两条刚刚在server块中配置的域名。这样在浏览器中输入网址访问的时候，会先解析本地hosts文件中的记录，如果找不到，才会到相应的域名解析服务器中去找。\n\n在浏览器中输入`http://www.layne.com/`，可以看到如下界面：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217202635.png)\n\n在浏览器中输入`http://www.123.com/`，会发现里面没有任何东西，这是因为`/mnt`目录下没有文件。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217202959.png)\n\n如果我们复制一个文件到`/mnt`目下，再刷新一下`http://www.123.com/`，会页面中出现一条记录，点击也可以进行下载：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217203350.png)\n\n现在，我们为/mnt挂载，并重新测试，可以看到我们挂载的CD或U盘：\n\n```bash\n[root@nginx1 nginx]# mount /dev/cdrom /mnt #将cdrom挂载到/mnt目录下\nmount: block device /dev/sr0 is write-protected, mounting read-only\n```\n\n再次刷新http://www.123.com/\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217203607.png)\n\n这样我们就完成了一个下载服务器的搭建工作，是不是感觉自己很强！别急，后面还有更好玩的。\n\n\n\n### 4.5 nginx日志配置\n\nnginx的默认日志在安装目录的logs文件夹下，现在我们输入`tail -f access.log`持续监控这个文件：\n\n```bash\n[root@nginx1 logs]# pwd\n/opt/nginx/logs\n[root@nginx1 logs]# tail -f access.log\n```\n\n刷新`http://www.123.com/`，会发现日志中多了一条记录：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217204807.png)\n\n上图的信息是：\n\n- 第一列：ip地址为192.168.218.1，这是因为我们采用的NAT连接方式，所以主机访问虚拟机的时候，采用的IP地址是虚拟机同样网段下的192.168.218.1。\n- 第二列：是东八区的时间，即CST时间（北京时间）\n- 第三列：get请求方式，使用的协议是http1.1\n- 第四列：浏览器的相关信息\n\n上述的access.log是http块的日志文件，也就是不管我们请求哪个server下的域名，该日志文件都可以进行记录。如果想要记录某一个server下的日志，我们在该server下进行配置即可。\n\n另外，Nginx也支持日志格式的自定义，现在我们定义一个myfmt日志格式，并在一个server下进行配置，如下所示：\n\n```bash\nlog_format  myfmt  '$remote_addr - $remote_user [$time_local] \"$request\" ';\nserver {\n\tlisten       80;\n\tserver_name  www.layne.com;\n\t#access_log  logs/host.access.log  main;\n\taccess_log logs/layne.log myfmt;\n\n\tlocation / {\n\t\troot   html;\n\t\tindex  index.html index.htm;\n\t}\n}\n```\n\n然后执行`service nginx restart`重启nginx，并执行`tail -f layne.log`检测文件，在浏览器中分别输入`http://www.layne.com/`和`http://www.layne.com/assfgf`，会发现layne.log中多了两条记录，并且日志的格式就是我们刚刚自定义的格式。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217210327.png)\n\n查看access.log，会发现当我们请求`http://www.layne.com/`时，access.log并不会进行检测。所以，当我们在某个server下配置日志文件时，http块下的日志文件就不会对该server下域名的请求进行检测了，**http块下的日志记录的是所有没有配置日志文件的server**。\n\n\n\n### 4.6 Location详解\n\n一个server可以有很多location，请求进来之后，server根据location配置将请求转发到哪里，所以location相当于nginx的出口。其[语法规则](http://tengine.taobao.org/nginx_docs/cn/docs/http/ngx_http_core_module.html \"tengine配置文档\")如下：\n\n```tex\n语法:\tlocation [ = | ~ | ~* | ^~ ] uri { ... }\nlocation @name { ... }\n默认值:\t—\n上下文:\tserver, location\n```\n\n- `location URI {}`:\n  - 对当前路径及子路径下的所有对象都生效\n- `location = URI {}`: 注意，URI最好为具体路径\n  - 精确匹配指定的路径，不包括子路径，故只对当前资源生效\n- `location ~ URI {}`：\n  - 模糊匹配URI，此处的URI可使用正则表达式，`~`表示区分字符大小写\n- `location ~* URI {}`：\n  - 模糊匹配URI，此处的URI可使用正则表达式，`~*`表示不区分字符大小写\n- `location ^~ URI {}`:\n  - 不使用正则表达式\n- **优先级**：`= > ^~ > ~|~* > /|/dir/`\n\n让我们用一个例子解释上面的说法：\n\n```bash\nlocation = / {\n    [ configuration A ]\n}\n\nlocation / {\n    [ configuration B ]\n}\n\nlocation /documents/ {\n    [ configuration C ]\n}\n\nlocation ^~ /images/ {\n    [ configuration D ]\n}\n\nlocation ~* \\.(gif|jpg|jpeg)$ { \n    [ configuration E ] \n} \n```\n\n- `= /` 是等值匹配，故只有域名后面没跟任何东西的时候，才匹配到这个location下面。\n- `/` 或 `/documents/` 是**最大前缀匹配**，即跟谁相同的地方最多（即匹配的地方最多），就匹配哪个。\n- `^~ /images/` 不使用正则表达式，即如果这里匹配上了，不管后面有没正则，都不找后面的匹配了。\n- `~* \\.(gif|jpg|jpeg)$` 是正则表达式的写法，即请求包含.gif、.jpg、.jpeg，都满足这个匹配。\n\n所以，\n\n- 请求`/`匹配配置A\n- 请求`/index.html`匹配配置B\n- 请求`/documents/document.html`匹配配置C\n- 请求`/images/1.gif`匹配配置D\n- 请求`/documents/1.jpg`匹配配置E，这个即和最大前缀匹配，又和正则匹配，当时正则匹配优先级高于最大前缀匹配，所以走正则匹配。\n\n一般来说，location的执行逻辑与location的编辑顺序无关，但需要注意的是：\n\n- uri前加了`^~`，即不使用正则表达式，表示本条uri一旦匹配上，则不需要继续正则匹配。\n- uri前加了`=`，即等值匹配，表示该uri恰好精确匹配上，则不需要继续往下匹配。\n\n而**带有正则的匹配是顺序匹配**，即多个带正则的location只要第一个满足，就停止后面的正则匹配了。\n\n现在，让我来测试一下吧！\n\n1. 启动一个新的Linux虚拟机，比如我启动一个ip为`192.168.218.52`的虚拟机，执行如下命令安装htttp应用：\n\n   ```bash\n   yum install httpd -y\n   ```\n2. 然后进入`/var/www/html`目下，执行`vim index.html`加入以下内容：\n\n   ```bash\n   from 192.168.218.52\n   ```\n3. 启动httpd服务：`service httpd start`。\n\n4. 在浏览器中执行`http://192.168.218.52/`，可出现如下页面：\n    ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217222817.png)\n\n5. 配置一下nginx服务器的server块，如下：\n\n   ```bash\n   server {\n   \tlisten       80;\n   \tserver_name  www.layne.com;\n   \t#access_log  logs/host.access.log  main;\n   \taccess_log logs/layne.log myfmt;\n   \n   \tlocation / {\n   \t\troot   html;\n   \t\tindex  index.html index.htm;\n   \t}\n   \n   \tlocation /abc {\n   \t\tproxy_pass http://192.168.218.52/; \n   \t\t#在url末尾带上/表示访问该url的首页，不带/则表示把访问的地址与url拼接起来，即访问http://192.168.218.52/abc，可以自己尝试。\n   \t}\n   \n   }\n   ```\n6. 执行`service nginx restart`重启nginx，在浏览器中分别执行`http://www.layne.com/`和`http://www.layne.com/abc`，会出现不同的页面。\n\n**现在，我们在nginx中模拟一个百度**\n\n1. 在`www.layne.com`所在的server块中加入以下location，并重启nginx。\n\n   ```bash\n   location /aaa {\n      proxy_pass http://www.baidu.com/;\n   }\n   ```\n   \n2. 在浏览器中输入`http://www.layne.com/aaa`进入百度首页，但是会发现浏览器地址栏中的域名也变成百度的域名了。\n   ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217224235.png)\n   如果我们即想用百度的功能，又不想跳转到其域名下，可以修改配置，让url的重定向尽量在服务器端跳转，不要在客户端跳转。\n   把之前的配置改为：\n\n   ```bash\n   location /aaa {\n      proxy_pass https://www.baidu.com/; #把http变为了https，其它的不变\n   }\n   ```\n   \n3. 重启nginx，再次输入`http://www.layne.com/aaa`，会发现地址栏地址栏没有重定向：\n    ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217224658.png)\n    但是当我们在百度搜索框输入我们要查询的内容时，会出现404：\n    ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217224909.png)\n\n4. 再次修改配置，在之前的基础上加一个location，如下：\n\n  ```bash\n  location ~* /s.* {\n            proxy_pass https://www.baidu.com;\n  }\n  ```\n5. 在百度搜索框再次输入我们要查询的内容时，能够出现搜索结果，并且浏览器地址栏也是我们的域名。\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210217225106.png)\n\n\n\n\n\n进行了上述配置之后，当我们请求`http://www.layne.com/aaa`时，nginx服务器把请求转发到百度。这是因为location配置的 proxy_pass是`https://www.baidu.com/`，浏览器地址栏不会跳转到百度域名下，在百度搜索框输入一些内容后，就匹配了`location ~* /s.*`，这就相当于再次向百度转发了一个`https://www.baidu.com/ s.*` 的请求，需要注意的是，此时配置的`https://www.baidu.com`并没有在结尾处加`/`，所以会把请求的路径拼接到配置的url后面。\n\n这里，就相当于在我们的网站了嵌套了一个百度，在里面搜索的任何内容都将记录到当前server的日志下面，是不是感觉非常的棒。但这个不太稳定，因为百度为预防这个行为，进行了一些处理，导致搜索某些词可能会跳转到其它地方。\n\n\n\n\n\n### 4.7 重新安装nginx\n\n当我们在安装nginx的时候，由于忘记某些配置而导致有些功能无法使用，比如当编译的时候没有启用SSL支持，在配置反向代理到 https的网站时，编辑配置文件报错，无法启动nginx，报错内容如下：\n\n```bash\n[root@nginx1 conf]# service nginx reload\nnginx: [emerg] https protocol requires SSL support in /opt/nginx/conf/nginx.conf:45\nnginx: configuration file /opt/nginx/conf/nginx.conf test failed\n```\n\n解决办法：先将nginx.conf备份`/root/`目录下，删除`/opt/nginx`和`/opt/apps/nginx-1.16.1`，然后再解压一份，最后编译安装。\n\n```bash\n[root@nginx1 nginx-1.16.1]# ./configure --prefix=/opt/nginx --with-http_ssl_module\n[root@nginx1 nginx-1.16.1]# make && make install\n[root@nginx1 nginx-1.16.1]# cd /opt/nginx/conf/\n[root@nginx1 conf]# cp /root/nginx.conf  ./\ncp: overwrite `./nginx.conf'? yes\n[root@nginx1 conf]# service nginx reload\nnginx: the configuration file /opt/nginx/conf/nginx.conf syntax is ok\nnginx: configuration file /opt/nginx/conf/nginx.conf test is successful\nReloading nginx:\n```\n\n\n\n## 5. 反向代理\n\n所谓的**代理**其实就是一个中介，比如两台主机A和B，因为某种原因不能直连，在中间插入一个主机C，通过C联通A和B。\n\n早些时候，代理多数是帮助内网client访问外网server用的，这就是正向代理。\n\n后来出现了[反向代理](https://www.cnblogs.com/taostaryu/p/10547132.html \"正向代理与反向代理\")，\"反向\"这个词在这儿的意思其实是指方向相反，即代理将来自外网客户端的请求转发到内网服务器，是从外到内的。\n\n所以，代理服务器根据其代理对象的不同，可以分为正向代理服务器与反向代理服务器。\n\n### 5.1 正向代理服务器\n\n正向代理是对客户端的代理。客户端 C 想要从服务端 S 获取资源，但由于某些原因不能，直接访问服务端，而是通过另外一台主机 P 向服务端发送请求。当服务端处理完毕请求后，将响应发送给主机 P，主机 P 在接收到来自服务端的响应后，将响应又转给了客户端 C。**此时的主机 P，就称为客户端 C 的正向代理服务器**。\n\n客户端在使用正向代理服务器时是知道其要访问的目标服务器的地址等信息的。**正向代理服务器是为服务用户（客户端）而架设的主机，与服务端无关，对服务器端透明。**正向代理服务器类似一个跳板机，代理访问外部资源。\n\n比如我们国内访问谷歌，直接访问访问不到，我们可以通过一个正向代理服务器，请求发到代理服，代理服务器能够访问谷歌，这样由代理去谷歌取到返回数据，再返回给我们，这样我们就能访问谷歌了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218121426.png)\n\n\n\nMaven的Nexus私服也是典型的用于“缓存”的正向代理服务器：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218120516.png)\n\n正向代理的用途：\n\n1. 访问原来无法访问的资源，如google\n2. 可以做缓存，加速访问资源\n3. 对客户端访问授权，上网进行认证\n4. 代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息。\n\n\n\n### 5.2 反向代理服务器\n\n反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。这个时候，对于客户端来说，感觉不到内部目标服务器的存在。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218122001.png)\n\n反向代理的作用：\n\n1. 保证内网的安全，阻止web攻击，大型网站，通常将反向代理作为公网访问地址，Web服务器是内网\n2. 负载均衡，通过反向代理服务器来优化网站的负载\n\n总的来说：\n\n- 正向代理即是客户端代理, 代理客户端, 服务端不知道实际发起请求的客户端\n- 反向代理即是服务端代理, 代理服务端, 客户端不知道实际提供服务的服务端\n\n\n\n### 5.3 反向代理之负载均衡\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218122240.png)\n\n由于反向代理服务器处在最终处理请求访问的服务器之前，因此可以在代理服务器上做负载均衡。\n\n所谓的负载均衡（Load Balancing），就是将对请求的处理分摊到多个操作单元上进行。这个均衡是指在大批量访问前提下的一种基本均衡，并非是绝对的平均。**对于 Web 工程中的负载均衡，就是将相同的 Web 应用部署到多个不同的 Web 服务器上，形成多个 Web 应用服务器。当请求到来时，由负载均衡服务器负责将请求按照事先设定好的比例向 Web 应用服务器进行分发，从而增加系统的整体吞吐量。**\n\n负载均衡可以通过负载均衡软件实现，也可通过硬件负载均衡器实现。\n\n（1）硬件负载均衡\n\n硬件负载均衡器的性能稳定，且有生产厂商作为专业的服务团队。但其成本很高，一台硬件负载均衡器的价格一般都在十几万到几十万，甚至上百万。知名的负载均衡器有 F5、Array、深信服、梭子鱼等。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218122945.png)\n\n\n\n（2）软件负载均衡\n\n软件负载均衡成本几乎为零，基本都是开源软件。例如：LVS、HAProxy、Nginx 等。\n\n下面我们Nginx反向代理服务器配置负载均衡。\n\n我准备三台虚拟机，一台是作为Nginx服务器，主机命名为nginx1，ip地址为`192.168.218.55`，另外两台作为web服务器，主机名分别为layne2和layne3，ip地址分别为`192.168.218.52`和`192.168.218.53`。\n\nNginx配置负载均衡有两种方式，我将分别进行介绍。\n\n**方式一**\n\n1. 修改nginx.conf的http块下的内容：\n\n   ```bash\n   # 默认负载权重是一样的\n   upstream rss{\n   \tserver 192.168.218.52;\n   \tserver 192.168.218.53;\n   }\n   server {\n   \tlisten       80;\n   \tserver_name  www.layne.com;\n   \t#access_log  logs/host.access.log  main;\n   \taccess_log logs/layne.log myfmt;\n   \n   \tlocation / {\n   \t\troot   html;\n   \t\tindex  index.html index.htm;\n   \t}\n   \n   \tlocation /dogs {\n   \t\tproxy_pass http://rss/;\n   \t}\n   }\n   ```\n\n2. 这里最好也把http块下的`keepalive_timeout`设置为0，即让会话保持时间为0，便于看出**负载切换**的效果，但部署时需要设置合适的数值来提高效率。\n\n   ```bash\n   keepalive_timeout  0;\n   ```\n\n3. 执行`service nginx restart`重启nginx\n\n4. 分别在layne2和layne3虚拟机安装执行如下命令安装httpd：\n\n   ```bash\n   yum install httpd -y\n   ```\n\n5. 进入`/var/www/html`目下，执行`vim index.html`分别在layne2和layne3里加入`from 192.168.218.52`、`from 192.168.218.53`。\n\n6. 启动两台虚拟机上的httpd服务：`service httpd start`。\n\n7. 在浏览中分别输入`http://192.168.218.52/`和`http://192.168.218.53/`进行测试，出现`from 192.168.218.5x`即代表httpd配置成功。\n\n8. 最后，输入`http://www.layne.com/dogs`，不断进行刷新，可看到页面内容发生变化，说明负载均衡配置成功。\n\n一般来说，中小企业一般使用方式一。\n\n\n\n**方式二**\n\n在Nginx服务器上配置hosts本地域名解析：\n\n```bash\n[root@nginx1 logs]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.218.55 nginx1\n192.168.218.50 layne\n192.168.218.51 layne1\n192.168.218.52 layne2 xxx\n192.168.218.53 layne3 xxx\n192.168.218.54 layne4\n\n[root@nginx1 logs]# \n```\n\n修改nginx.conf配置文件，在server块中添加如下配置\n\n```bash\n       location /cats {\n           proxy_pass http://xxx/;\n       }\n```\n\n重启nginx：`service nginx restart`\n\n浏览器中输入`http://www.layne.com/cats`不端刷新进行测试。\n\n这种方式一样能够实现负载均衡的目的，该方式下，Nginx服务器会用到域名解析，如果一个域名解析出多个ip地址，会在这些ip地址之间做负载均衡。\n\n一般来说，大型企业使用方式二，如果需要解析的ip地址过多，还会搭建一个DNS域名解析服务器，只需要修改DNS域名解析服务器（中的IP列表）即可。\n\n\n\n\n\n### 5.4 负载均衡进阶\n\n在5.3节我们采用两种方式实现了负载均衡，对于方式一还有许多的扩充配置，我们一起来看看吧。\n\n[Nginx提供的负载均衡策略](https://www.runoob.com/w3cnote/nginx-proxy-balancing.html \"Nginx负载均衡策略\")有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash（ip绑定）。扩展策略，就天马行空，只有你想不到的没有他做不到的啦，最常用的扩展策略为fair、url_hash。扩展策略一般不能直接使用，需要安装安三方扩展包。\n\n轮询：nginx默认就是轮询其权重都默认为1，服务器处理请求的顺序：ABABABABAB....\n\n```bash\n# 默认负载权重是一样的\nupstream rss{\n\tserver 192.168.218.52;\n\tserver 192.168.218.53;\n}\n```\n\n加权轮询：跟据配置的权重的大小而分发给不同服务器不同数量的请求。如果不设置，则默认为1。下面服务器的请求顺序为：ABBABBABBABBABB....\n\n```bash\n# 加权轮训\nupstream rss{\n\tserver 192.168.218.52 weight=1;\n\tserver 192.168.218.53 weight=2;\n}\n```\n\nip_hash：nginx会让相同的客户端ip请求相同的服务器。\n\n```bash\nupstream rss{\n\tserver 192.168.218.52;\n\tserver 192.168.218.53;\n\tip_hash;\n}\n```\n\n热备：如果你有2台服务器，当一台服务器发生事故时，才启用第二台服务器给提供服务。服务器处理请求的顺序：AAAAAA突然A挂啦，BBBBBBBBBBBBBB.....\n\n```bash\nupstream rss{\n\tserver 192.168.218.52;\n\tserver 192.168.218.53 backup;  #热备 \n}\n```\n\nfair（第三方）：按后端服务器的响应时间来分配请求，响应时间短的优先分配。\n\nurl_hash(第三方)：按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。\n\n关于第三方的扩展策略，可参考`https://www.cnblogs.com/ztlsir/p/8945043.html`。\n\nnginx负载均衡配置的几个状态参数：\n\n- down：表示当前的server暂时不参与负载均衡。\n- backup：预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的压力最轻。\n- max_fails：允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。\n- fail_timeout：在经历了max_fails次失败后，暂停服务的时间。max_fails可以和fail_timeout一起使用。\n\n```bash\nupstream rss { \n    server 192.168.218.52 weight=2 max_fails=2 fail_timeout=2;\n    server 192.168.218.53 weight=1 max_fails=2 fail_timeout=1;\n    #server  服务器的ip   权重   最大失败次数   暂停服务的时间\n}\n```\n\n\n\n## 6. session共享\n\n### 6.1 session共享的解决方案\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218133405.png)\n\n当nginx做了负载均衡之后，同一个ip的url请求服务器的时候，负载均衡会根据每台服务器的权重等一些设置将请求转发到不同的服务器上去进行处理，这样会带来一个很大的问题，**即不能保证带有状态的请求的完整性**。假如A用户登陆系统，负载均衡机制把A用户的登陆请求分发给了S1服务器，这个时候S1服务器上就会记录A用户登陆的session信息。登陆成功后，当A用户进行相应的操作，比如进入个人中心，这时候这个请求经过反向代理服务器的时候，负载均衡机制根据当前集群中的各个服务器的压力性能等情况可能把请求分发给了S2服务器处理，那么这个时候会去验证用户的状态是否登录，也就是验证session。可是A用户的session保存在了S1服务器上，造成再S2服务器上请求验证状态找不到对应的session，就会认为用户未登录而做的异常操作，提醒用户去登录，从而跳转到登录页面。这就是负载均衡针对带有状态的请求的一个弊端。\n\n> 本小节引用自[负载均衡的session共享](https://www.cnblogs.com/zengguowang/p/8261695.html \"负载均衡的session共享\")，里面加了少部分自己的理解\n\n常用的解决方案有4种。\n\n（1）不使用session，使用cookie\n\nsession是存放在服务器端的，cookie是存放在客户端的，我们可以把用户访问页面产生的session放到cookie里面，就是以cookie为中转站。你访问web服务器A，产生了session然后把它放到cookie里面，当你的请求被分配到B服务器时，服务器B先判断服务器有没有这个session，如果没有，再去看看客户端的cookie里面有没有这个session，如果也没有，说明session真的不存，如果cookie里面有，就把cookie里面的sessoin同步到服务器B，这样就可以实现session的同步了。其实session底层原理也是通过cookie实现的，但它是自动实现的，不需要人为进行操作，但是这里我们是通过编程cookie的方式实现session共享。\n\n说明：这种方法实现起来简单，方便，也不会加大数据库的负担，但是如果客户端把cookie禁掉了的话，那么session就无从同步了，这样会给网站带来损失；cookie的安全性不高，虽然它已经加了密，但是还是可以伪造的，所以这种方式也是不推荐的。\n\n（2）session存在数据库mysql\n\nsession保存在数据库中，是把session表和其他的数据表存放在一起，那么当用户只要登录后随便操作了些什么就要去数据库验证一下session的状态，这样无疑加重了mysql数据库的压力；如果数据库也做了集群的话，那么也就是说每个数据库集群的节点都得保存这个session表，而且要保证每个集群的节点中数据库的session表的数据保持一致，实时同步。\n\n说明：session保持在数据库，加重了数据库的IO，增大数据库的压力和负担，从而影响数据库的读写性能，而且mysql集群的话也不利于session的实时同步。\n\n（3）session存在缓存memcache或者redis中\n\n这种方式来同步session，不会加大数据库的负担，而且安全性比用cookie保存session大大的提高，把session放到内存里面，比从文件中读取要快很多。但是memcache把内存分成很多种规格的存储块，有块就有大小，这种方式也就决定了，memcache不能完全利用内存，会产生内存碎片，如果存储块不足，还会产生内存溢出。\n\n> memcache可以做分布式，php配置文件中设置存储方式就为memcache，这样php自己会建立一个session集群，将session数据存储在memcache中。\n\n\n\n（4）ip_hash技术\n\n ip_hash技术，nginx中可以配置，当某个ip下的客户端请求指定（固定，因为根据IP地址计算出一个hash值，根据hash值来判断分配给那台服务器，从而每次该ip请求都分配到指定的服务器）的服务器，这样就可以保证有状态请求的状态的完整性，不至于出现状态丢失的情况。下面是nginx的配置，可以参考一下：\n\n```bash\nupstream rss{\n\tserver 192.168.218.52;\n\tserver 192.168.218.53;\n\tip_hash;\n```\n\n说明：ip_hash**确实可以保证带有状态的请求的完整性**，但是它有一个很大的缺陷，那就是ip_hash方案必须保证Nginx是最前端的服务器（接受真实的ip），如果nginx不是最前端的服务器，还存在中间件（中间服务器什么的），那么nginx获取的ip地址就不是真实的ip地址，那么这个ip_hash就没有任何意义。\n\n（5）tomcat 本身带有复制session的功能，不过由于tomcat的性能有限，这种方案基本不用。\n\n\n\n下面我将通过memcache实现session共享。\n\n\n\n### 6.2 memcache实现session共享\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218150400.png)\n\n**（1）安装jdk、tomcat**\n\n**在layne2和layne3虚拟机上安装jdk和tomcat。**\n\n1. 首先把相应的软件`apache-tomcat-7.0.69.tar.gz`  和`jdk-7u80-linux-x64.rpm`拷贝到两台虚拟机的`/opt/apps`目录下（可联系博主获取本文所有软件），然后执行下面命令进行安装：\n\n   ```bash\n   [root@layne2 ~]# cd /opt/apps\n   [root@layne2 apps]# ls\n   apache-tomcat-7.0.69  apache-tomcat-7.0.69.tar.gz  jdk-7u80-linux-x64.rpm\n   [root@layne apps]# rpm -ivh jdk-7u80-linux-x64.rpm\n   Preparing...                ########################################### [100%]\n      1:jdk                    ########################################### [100%]\n   Unpacking JAR files...\n   \trt.jar...\n   \tjsse.jar...\n   \tcharsets.jar...\n   \ttools.jar...\n   \tlocaledata.jar...\n   \tjfxrt.jar...\n   [root@layne2 apps]# find / -name java\n   /etc/pki/ca-trust/extracted/java\n   /etc/pki/java\n   /usr/bin/java\n   /usr/java\n   /usr/java/jdk1.7.0_80/bin/java\n   /usr/java/jdk1.7.0_80/jre/bin/java\n   [root@layne2 apps]# ll /usr/java\n   total 4\n   lrwxrwxrwx 1 root root   16 Feb 11 18:58 default -> /usr/java/latest\n   drwxr-xr-x 8 root root 4096 Feb 11 18:57 jdk1.7.0_80\n   lrwxrwxrwx 1 root root   21 Feb 11 18:58 latest -> /usr/java/jdk1.7.0_80\n   ```\n\n2. 配置环境变量，进入`vim /etc/profile` ，在文件最后加入以下两行代码：\n\n   ```bash\n   export JAVA_HOME=/usr/java/default\n   export PATH=$PATH:$JAVA_HOME/bin\n   ```\n\n3. 让配置生效，使用`source /etc/profile`\n\n4. 测试安装配置是否成功\n\n   ```bash\n   [root@layne ~]# java -version\n   java version \"1.7.0_80\"\n   Java(TM) SE Runtime Environment (build 1.7.0_80-b15)\n   Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n   [root@layne ~]# jps\n   1423 Jps\n   ```\n\n5. 解压`apache-tomcat-7.0.69.tar.gz`\n\n   ```bash\n   tar -zxvf apache-tomcat-7.0.69.tar.gz\n   ```\n\n6. 编辑`apache-tomcat-7.0.69/webapps/ROOT/index.jsp`\n\n   ```bash\n   [root@layne2 apps]# cd apache-tomcat-7.0.69/webapps/ROOT/\n   [root@layne2 ROOT]# vim index.jsp\n   ```\n\n   使用`:%d`删除index.jsp里面的全部内容，并添加：\n\n   ```bash\n   from 192.168.218.52 session=<%=session.getId()%>\n   ```\n\n7. 在layne3上进行同样的安装配置操作。\n\n8. 启动tomcat\n\n   ```bash\n   [root@layne2 bin]# pwd\n   /opt/apps/apache-tomcat-7.0.69/bin\n   [root@layne2 bin]# ./startup.sh \n   Using CATALINA_BASE:   /opt/apps/apache-tomcat-7.0.69\n   Using CATALINA_HOME:   /opt/apps/apache-tomcat-7.0.69\n   Using CATALINA_TMPDIR: /opt/apps/apache-tomcat-7.0.69/temp\n   Using JRE_HOME:        /usr/java/default\n   Using CLASSPATH:       /opt/apps/apache-tomcat-7.0.69/bin/bootstrap.jar:/opt/apps/apache-tomcat-7.0.69/bin/tomcat-juli.jar\n   ```\n\n9. 浏览器中输入 `http://192.168.218.52:8080/`，就可以出现页面了，并显示当前的sessionid，刷新页面，session也不会变。\n\n\n\n\n\n**（2）配置nginx.conf**\n\n修改nginx.conf为如下配置\n\n```bash\nupstream rss{\n\tserver 192.168.218.52:8080;\n\tserver 192.168.218.53:8080;\n}\n# 如果默认不写8080，则默认为80端口，如果ip后面写上8080端口，则请求的是tomcat的服务。\n# 当然，也可以通过修改tomcat配置，让tomcat服务端口变为80，这样我们的Ip地址后面就不用加80端口了\n# 但是，要把httpd服务关了，因为httpd服务的端口也是80，会占用端口\n```\n\n重启nginx：`service nginx restart `\n\n访问`http://www.layne.com/dogs`，不断刷新，会发现session一直在改变，接下来我们就配置session共享了。\n\n**（3）nginx服务器安装memcached**\n\n1. 安装libevent\n\n   ```bash\n   yum install libevent -y\n   ```\n\n2. 安装memcached\n\n   ```bash\n   yum install memcached  -y\n   ```\n\n3. 启动memcached\n\n   ```bash\n   [root@nginx1 conf]# memcached -d -m 128m -p 11211 -l 192.168.218.55 -u root -P /opt/mempid\n   [root@nginx1 conf]# ps aux |grep  memcached\n   root       1870  0.0  0.0 330844   884 ?        Ssl  19:45   0:00 memcached -d -m 128m -p 11211 -l 192.168.218.55 -u root -P /opt/mempid\n   root       1877  0.0  0.0 103256   864 pts/2    S+   19:45   0:00 grep memcached\n   [root@nginx1 conf]# cat /opt/mempid\n   1870\n   ```\n\n   可以看到，/opt/mempid存储的就是memcached进程的pid，对上面memcached的启动命令进行解释：\n\n- `-d`：后台启动服务\n- `-m`：缓存大小\n- `-p`：启动的端口\n- `-l`：所在服务器的ip地址\n- `-P`：服务器启动后的系统进程ID，存储的文件\n- `-u`：服务器启动是以哪个用户名作为管理用户\n\n**（4）配置session共享**\n\n1. 拷贝session共享所需的jar包到两个虚拟机的`apache-tomcat-7.0.69/lib`目录下，如下所示：\n   ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218154311.png)\n   以上jar包，有需要的可以留言或者直接私信我。\n\n2. **配置两个的tomcat**，在每个`apache-tomcat-7.0.69/conf/context.xml`的`<Context>...</Context>`结点里面加入如下内容：\n\n   ```bash\n   <Manager className=\"de.javakaffee.web.msm.MemcachedBackupSessionManager\" \n   \tmemcachedNodes=\"n1:192.168.218.55:11211\" \n       sticky=\"false\" \n       lockingMode=\"auto\"\n       sessionBackupAsync=\"false\"\n   \trequestUriIgnorePattern=\".*\\.(ico|png|gif|jpg|css|js)$\"\n       sessionBackupTimeout=\"1000\" transcoderFactoryClass=\"de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory\" \n   />\n   ```\n\n   注意：**里面的ip地址为nginx服务器的ip地址，不是当前虚拟机的ip地址**\n\n3. 执行以下命令重启两个tomcat\n\n   ```bash\n   [root@layne2 bin]# pwd\n   /opt/apps/apache-tomcat-7.0.69/bin\n   [root@layne2 bin]# ./shutdown.sh && ./startup.sh \n   ```\n\n4. 在浏览器中输入`http://www.layne.com/dogs`，不断刷新页面，发现session保持不变。显示类似如下的内容：\n\n   ```tex\n   from 192.168.218.53 session=4493A2D98EC0934FA892B16BA0D3AAFE-n1\n   ```\n\n   \n\n至此，session共享就配置成功了。接下来，我们来配置nginx动静分离。\n\n\n\n\n\n\n\n## 7. 动静分离\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218155520.png)\n\nNginx动静分离简单来说就是把动态和静态请求分开，不能理解成知识单纯的把动态页面和静态页面物理分离。严格意义上说应该是动态请求和静态请求分开，可以理解成使用Nginx处理静态请求，Tomcat处理动态请求。\n\n动静分离从目前实现方式大致分为两种：\n\n- 方式一：纯粹的把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案。\n- 方式二：动态和静态文件混合在一起发布，通过nginx分开。通过配置location指定不同的后缀名实现不同的请求转发。\n\n接下来，我通过方式一实现动静分离。下面是我们即将配置动静分离的服务器拓扑图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218161147.png)\n\n- nginx1作为反向代理服务器，负责session共享、负载均衡、反向代理，区分动态请求和静态请求并转发给不同的服务器\n- layne2和layne3作为web服务器，部署了tomcat，负责解析动态请求\n- nginx2中存放的是静态资源，负责解析静态请求。\n\n让我们实现nginx的动静分离吧！\n\n1. 先在两个tomcat中执行`./shutdown.sh`关闭\n\n2. 编辑layne2中tomcat的`apache-tomcat-7.0.69/webapps/ROOT/index.jsp`文件，修改为以下内容：\n\n   ```bash\n   <link rel=\"stylesheet\" type=\"text/css\" href=\"/css/index.css\">\n   <img src=\"/image/logo.jpg\" ><br/>\n   <font class=\"myfont\">\n   from 192.168.218.52 <br/>\n    session=<%=session.getId()%></font>\n   ```\n\n3. 在`apache-tomcat-7.0.69/webapps/ROOT/`目下执行如下命令把layne2中的index.jsp拷贝到layne3的相同目录下：\n\n   ```bash\n   scp index.jsp 192.168.218.53:`pwd`\n   ```\n\n4. 修改layne3中的tomcat，把index.jsp中的ip改为`192.168.218.53`\n\n5. 从nginx1克隆nginx2，修改nginx2的`/etc/udev/ rules.d/70-persistent-net.rules`\n\n   ```bash\n   [root@nginx2 rules.d]# pwd\n   /etc/udev/rules.d\n   [root@nginx2 rules.d]# vim 70-persistent-net.rules\n   ```\n\n   将原来的eth0一行删掉，并将eth1改为eth0 。这里参考[Linux切换运行级别、关闭防火墙、禁用selinux、关闭sshd、时间同步、修改时区、拍摄快照、克隆操作](https://mp.weixin.qq.com/s/kk1sY9EYajaEt_mevwmfIg)。\n\n6. 在nginx2中输入 `vim /etc/sysconfig/network`修改计算机名称（hostname）为nginx2。然后输入 `vim /etc/sysconfig/network-scripts/ifcfg-eth0` 配置网络，将ip地址改为`192.168.218.56`即可。\n\n7. 在nginx2服务器上创建目录 /data/image和/data/css，然后将logo.jpg和index.css上传到对应的目录。\n\n   ```bash\n   [root@nginx2 ~]# mkdir -p /data/image  /data/css\n   ```\n\n   logo.jpg为：\n\n   ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218163142.jpg)\n\n   index.css内容为：\n\n   ```css\n   .myfont{color:#F00;font-size:45px}\n   ```\n\n8. 修改nginx2服务器上的nginx.conf配置文件，执行`vim /opt/nginx/conf/nginx.conf`，加入以下内容：\n\n   ```bash\n   server {\n   \tlisten       80;\n   \tserver_name  192.168.218.56;\n   \n   \tlocation / {\n   \t\troot   /mnt;\n   \t\tautoindex  on;\n   \t}\n   \n   \tlocation /image {\n   \t\troot /data;\n   \t}\n   \n   \tlocation /css {\n   \t\troot /data;\n   \t}\n   \t#不用再路径最后加/，不加/意味着可以拼接路径，加了/意味着直接访问填写的路径\n   }\n   ```\n\n9. 修改nginx1服务器上的nginx.conf配置文件，内容如下：\n\n   ```bash\n   upstream rss{\n   \tserver 192.168.218.52:8080;\n   \tserver 192.168.218.53:8080;\n   }\n   \n   server {\n   \tlisten       80;\n   \tserver_name  www.layne.com;\n   \t#access_log logs/layne.log myfmt;\n   \tlocation / {\n   \t\tproxy_pass http://rss/;\n   \t}\n   \t\n   \tlocation /image {\n   \t\tproxy_pass http://192.168.218.56;\n   \t}\n   \t\n   \tlocation /css {\n   \t\tproxy_pass http://192.168.218.56;\n   \t}\n   ```\n\n10. 重启nginx1、nginx2，重启两个tomcat。\n\n11. 在nginx1服务器中输入以下命名开启memcached\n\n    ```bash\n    memcached -d -m 128m -p 11211 -l 192.168.218.55 -u root -P /opt/mempid\n    ```\n\n12. 在浏览器中输入`http://www.layne.com/`测试，出现如下页面，不断刷新页面，session也不会变。\n    ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210218170753.png)\n\n\n\n\n\n\n\n","tags":["nginx"],"categories":["nginx"]},{"title":"正则表达式grep详解","url":"/2021/02/10/121527/","content":"\n我总结了多种grep常用正则表达式及其组合，能够满足大多数使用场景。\n\n<!-- more -->\n\n我们先来看看**正则表达式的匹配符：**\n\n1. `\\`  转义字符 `\\+ \\< \\>`\n2. `.`  **匹配任意单个字符** \n\n3. `[1234abc]`，`[^1234]`，`[1-5]`，`[a-d]` 字符序列单字符占位\n\n4. `^`  行首  `^.k`\n\n5. `$`  行尾  `.k$`\n\n6. `\\<,\\>`，`\\<abc,abc\\>`,`\\<are\\>`  单词首尾边界  okhelloworld ok hello world\n\n7. `|`  连接操作符，并集 `（\\<are\\>)| (\\<you\\>)`\n\n8. `(,)`  选择操作符\n\n9. `\\n`  反向引用\n\n**重复操作符：**\n\n1. `*` 匹配0到多次\n2. `?`  **匹配0到1次**\n3. `+` **匹配1到多次**\n4. `{n}`  **匹配n次**\n5. `{n,}` **匹配n到多次**\n6. `{m,n}` **匹配m到n次**\n\n\n\n先给出本文中的全部命令及其解释：\n\n- `grep \"after\" profile` 查找文件内的包含\"after\"的行\n- `grep -n \"after\" profile` 查找文件内的包含\"after\"的行，并查看匹配行所在文档的行号。\n- `grep -n \"after\" profile | grep \"then\"` 在包含\"after\"的行中查找含有\"then\"的行\n- `grep -v -n \"after\" profile` 查找不包含after的行，并显示行号\n- `grep \"a*re\" hello.txt`  这里的`*` 表示前面a出现0到多次，表示匹配如`are aare xre`d的行\n- `grep \"a.re\" hello.txt` 这里的`.` 表示匹配任意单个字符\n- `grep -E \"a+re\" hello.txt` 这里的`+`表示a有一到多个\n- `grep \"a\\+re\" hello.txt` 这里有`\\+`就不需要`-E`选项了\n- `grep \"[b-d]\" hello.txt` 匹配包含bcd中任意一个字符的行\n- `grep -v \"[b-d]\" hello.txt` 匹配不包含bcd中任意一个字符的行\n- `grep \"?\" hello.txt` 查找带问号的行\n- `grep  \"a.re\"  hello.txt` 这里的`.`占一个位置，匹配任意字符\n- `grep  \"..re\"  hello.txt` 匹配re前面有两个任意字符的行\n- `grep  \"[xz]k\"  hello.txt` 匹配带有zk和xk的行\n- `grep -v '[xz]k' hello.txt` 匹配不带有zk和xk的行\n- `grep  \"\\<[zx]k\"  hello.txt` 匹配含有以zk和xk为开头的单词的行\n- `grep  \"^.k\"  hello.txt`  匹配每一行的第二个字符一定得是k的行（行头）\n- `grep  \".k$\"  hello.txt` 匹配以k结尾的行（行尾），该行最少两个字符，最后一个是k\n- `grep  \"\\<are\\>\"  hello.txt` 匹配包含单词are的行\n- `grep  \"\\<are\"  hello.txt` 匹配以are为开头的单词所在的行\n- `grep  \"re\\>\"  hello.txt` 匹配（一个单词的）单词尾\n- `grep -E \"are|you\" hello.txt` 匹配包含are或者you的行，满足其中任意条件（are或you）就会匹配。\n- `grep are hello.txt | grep you | grep ok`  必须同时满足三个条件（are、you和ok）才匹配。\n- `grep  \"\\<a*re\\>\" hello.txt` 命令中的`*`表示（a）0到多次，即匹配单词以a字符开头，或者不以a字符开头，且以re结尾的行。\n- `grep  \"\\<a*re\\>\" hello.txt`  等同于 `grep -E \"(\\<a*re\\>)\" hello.txt`\n- `grep -E \"a{3}\" hello.txt` 匹配有3个重复a的行\n- `grep -E \"a+\" hello.txt` 匹配含有一个或多个a的行，等同于`grep \"a\\+\" hello.txt`\n- `grep -E \"a?\" hello.txt` 匹配出现0或1次a的行，等同于 `grep \"a\\?\" hello.txt`\n- `grep -E \"*\" hello.txt` 匹配任意字符，即匹配全部内容\n\n\n\n后面是上面命令的执行过程。\n\n现在来试试吧！\n\n```bash\n[root@layne tdir]# cp /etc/profile  .\n[root@layne tdir]# grep \"after\" profile #查找文件内的包含\"after\"的行\n            if [ \"$2\" = \"after\" ] ; then\n    pathmunge /usr/local/sbin after\n    pathmunge /usr/sbin after\n    pathmunge /sbin after\n[root@layne tdir]# grep -n \"after\" profile #添加查找的行在文档的行号。\n16:            if [ \"$2\" = \"after\" ] ; then\n42:    pathmunge /usr/local/sbin after\n43:    pathmunge /usr/sbin after\n44:    pathmunge /sbin after\n[root@layne tdir]# grep -n \"after\" profile | grep \"then\"  #在包含\"after\"的行中查找含有\"then\"的行\n16:            if [ \"$2\" = \"after\" ] ; then\n```\n\n**-v表示不包含的行**\n\n```bash\n[root@layne tdir]# grep -v -n \"after\" profile  # 不包含after的行，并显示行号\n1:# /etc/profile\n2:\n3:# System wide environment and startup programs, for login setup\n4:# Functions and aliases go in /etc/bashrc\n5:\n6:# It's NOT a good idea to change this file unless you know what you\n7:# are doing. It's much better to create a custom.sh shell script in\n8:# /etc/profile.d/ to make custom changes to your environment, as this\n9:# will prevent the need for merging in future updates.\n10:\n11:pathmunge () {\n12:    case \":${PATH}:\" in\n13:        *:\"$1\":*)\n14:            ;;\n15:        *)\n17:                PATH=$PATH:$1\n18:            else\n...\n```\n\n再尝试更多的例子！\n\n创建hello.txt，内容为：\n\n```tex\nhello world\nare you ok?\nareyou ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nabcre you ok?\nxxre you ok\nare yyyou ok?\nxk\nzk\nok\nyk\nzzk\nzxzxk\nbxx\ncxx\ndxx\nareyou are youok?\nzk kz 1\nkz zk 2\nokk koo 3\nzkkz\nkzzk\n```\n\n匹配`are aare xre`，0到多个a字符 \n\n```bash\n[root@layne tdir]# grep \"a*re\" hello.txt   # `*` 表示前面a出现0到多次\nare you ok?\nareyou ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nabcre you ok?\nxxre you ok\nare yyyou ok?\nareyou are youok?\n```\n\n匹配“a任意单个字符re”\n\n```bash\n[root@layne tdir]# grep \"a.re\" hello.txt  #`.` 表示匹配任意单个字符\naaare you ok?\naare you ok\naaaare you ok\n```\n\n**匹配a一个到多个任意字符re**\n\n```bash\n[root@layne tdir]# grep \"a+re\" hello.txt\n[root@layne tdir]# \n```\n\n但是，**发现查询不出来，这是为什么？**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209141540.png)\n\n上图第1个是基本匹配，第2~6个是扩展匹配\n\ngrep命令默认处于基本工作模式下，加上`-E` 选项**让grep工作于扩展模式**\n\n这样就可以解决上述问题来匹配a一个到多个任意字符re了：\n\n```bash\n[root@layne tdir]# grep -E \"a+re\" hello.txt\nare you ok?\nareyou ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nare yyyou ok?\nareyou are youok?\n[root@layne tdir]# grep \"a\\+re\" hello.txt #同样，\\也可以实现上述命令\nare you ok?\nareyou ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nare yyyou ok?\nareyou are youok?\n```\n\n所以，在基本工作模式下，`?，+，{，|，(,)`这些符号就丢失了意义，需要加`-E` 选项让grep工作于扩展模式才能生效，或者前面加上`\\`也能生效。\n\n`[a-d]`匹配abcd中的一个 \n\n匹配包含bcd中任意一个字符的行**（两边都是闭区间）**和 不包含bcd中任意一个字符的行\n\n```bash\n[root@layne tdir]# grep \"[b-d]\" hello.txt #匹配包含bcd中任意一个字符的行\nhello world\nabcre you ok?\nbxx\ncxx\ndxx\n[root@layne tdir]# grep -v \"[b-d]\" hello.txt #匹配不包含bcd中任意一个字符的行\nare you ok?\nareyou ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nxxre you ok\nare yyyou ok?\nxk\nzk\nok\nyk\nzzk\nzxzxk\nareyou are youok?\nzk kz 1\nkz zk 2\nokk koo 3\nzkkz\nkzzk\n```\n\n查找带问号的行：\n\n```bash\n[root@layne tdir]# grep \"?\" hello.txt\nare you ok?\nareyou ok?\nareyou are youok?\naaare you ok?\nabcre you ok?\nare yyyou ok?\nareyou are youok?\n```\n\n`.`占一个位置，匹配任意字符\n\n```bash\n[root@layne tdir]# grep  \"a.re\"  hello.txt\naaare you ok?\naare you ok\naaaare you ok\n[root@layne tdir]# grep  \"a..re\"  hello.txt\naaare you ok?\naaaare you ok\nabcre you ok?\n[root@layne tdir]# grep  \"..re\"  hello.txt\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nabcre you ok?\nxxre you ok\nareyou are youok?\n[root@layne tdir]# grep  \"...re\"  hello.txt\nareyou are youok?\naaare you ok?\naaaare you ok\nabcre you ok?\nareyou are youok?\n```\n\n**匹配带有zk和xk的行** 和没不带有zk和xk的行\n\n```bash\n[root@layne tdir]# grep  \"[xz]k\"  hello.txt # 匹配带有zk和xk的行\nxk\nzk\nzzk\nzxzxk\nzk kz 1\nkz zk 2\nzkkz\nkzzk\n[root@layne tdir]# grep -v '[xz]k' hello.txt  # 匹配不带有zk和xk的行\nhello world\nare you ok?\nareyou ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nabcre you ok?\nxxre you ok\nare yyyou ok?\nok\nyk\nbxx\ncxx\ndxx\nareyou are youok?\nokk koo 3\n```\n\n匹配含有**以zk和xk为开头的单词**的行\n\n```bash\n[root@layne tdir]# grep  \"\\<[zx]k\"  hello.txt\nxk\nzk\nzk kz 1\nkz zk 2\nzkkz\n```\n\n匹配第二个字符一定得是k的行（行头）\n\n```bash\n[root@layne tdir]# grep  \"^.k\"  hello.txt\nxk\nzk\nok\nyk\nzk kz 1\nokk koo 3\nzkkz\n```\n\n匹配以k结尾的行（行尾）\n\n```bash\n[root@layne tdir]# grep  \".k$\"  hello.txt  #该行最少两个字符，最后一个是k\naare you ok\naaaare you ok\nxxre you ok\nxk\nzk\nok\nyk\nzzk\nzxzxk\nkzzk\n```\n\n匹配单词边界（匹配一个单词）\n\n```bash\n[root@layne tdir]# grep  \"\\<are\\>\"  hello.txt #匹配包含单词are的行\nare you ok?\nareyou are youok?\nare yyyou ok?\nareyou are youok?\n[root@layne tdir]# grep  \"\\<are\"  hello.txt # 只匹配单词开头\nare you ok?\nareyou ok?\nareyou are youok?\nare yyyou ok?\nareyou are youok?\n[root@layne tdir]# grep  \"re\\>\"  hello.txt # 匹配（一个单词的）单词尾\nare you ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nabcre you ok?\nxxre you ok\nare yyyou ok?\nareyou are youok?\n```\n\n**同时匹配多个关键字–或关系**\n\n```bash\n[root@layne tdir]# grep -E \"are|you\" hello.txt #匹配包含are或者you的行，满足其中任意条件（are或you）就会匹配。\nare you ok?\nareyou ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nabcre you ok?\nxxre you ok\nare yyyou ok?\nareyou are youok?\n```\n\n**同时匹配多个关键字–与关系**\n\n使用管道符连接多个 grep ，间接实现多个关键字的与关系匹配：\n\n```bash\n[root@layne tdir]# grep are hello.txt | grep you | grep ok #必须同时满足三个条件（are、you和ok）才匹配。\nare you ok?\nareyou ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nare yyyou ok?\nareyou are youok?\n```\n\n\n\n`grep  \"\\<a*re\\>\" hello.txt`  命令中的`*`表示（a）0到多次，即匹配单词以a字符开头，或者不以a字符开头，且以re结尾的行。\n\n```bash\n[root@layne tdir]# grep  \"\\<a*re\\>\" hello.txt\nare you ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nare yyyou ok?\nareyou are youok?\n```\n\n另外，`grep  \"\\<a*re\\>\" hello.txt`  等同于 `grep -E \"(\\<a*re\\>)\" hello.txt`\n\n```bash\n[root@layne tdir]# grep -E \"(\\<a*re\\>)\" hello.txt\nare you ok?\nareyou are youok?\naaare you ok?\naare you ok\naaaare you ok\nare yyyou ok?\nareyou are youok?\n```\n\n`grep -E \"a{3}\" hello.txt` 匹配该行中3个a重复的\n\n```bash\n[root@layne tdir]# grep -E \"a{3}\" hello.txt\naaare you ok?\naaaare you ok\n```\n\n匹配一个到多个a ：`grep -E \"a+\" hello.txt`  或`grep \"a\\+\" hello.txt`\n\n匹配0到1次a：`grep -E \"a?\" hello.txt`  或 `grep \"a\\?\" hello.txt`\n\n匹配任意字符：`grep -E \"*\" hello.txt` （即匹配全部内容）\n\n","tags":["grep"],"categories":["Linux"]},{"title":"为CentOS6.5离线安装man-pages-zh-CN（中文man手册）","url":"/2021/02/10/121000/","content":"\n我们可以使用man命令查看帮助信息，可以使用`yum install man man-pages -y` 安装该软件包。\n\n现在使用`man ls`查看ls的帮助信息：\n\n<!-- more -->\n\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208154707.png)\n\n可以看到，看到的是全英文信息。那么，有没有中文的帮助提示信息呢？答案是肯定的！\n\n要想使用汉化man命令，**系统中需要安装man-pages-zh-CN软件包**。默认情况下，**系统自带的yum源不包含man-pages-zh-CN**，比如我执行`yum search man-pages-zh-CN`，会提示`No Matches found`：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207105110.png)\n\n当然，你也可以为yum更换其它源，我经过测试后发现：\n\n- 大多数yum不支持CentOS-6，一般从CentOS-7开始支持\n- 找到了centOS-6的源（如清华源），里面也没有man-pages-zh-CN软件包\n\n既然，在线的不行，可以离线安装啊！\n\n经过一番摸索，**离线安装man-pages-zh-CN（中文man手册）的步骤如下：**\n\n（1）下载man-pages-zh-CN软件包\n\n输入以下命令：\n\n```bash\nwget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/manpages-zh/manpages-zh-1.5.1.tar.gz\n```\n\n如果下载失败，直接在浏览器中输入上述网址下载即可（或者可以联系博主获取）\n\n（2）将man-pages-zh-CN拷贝到当前用户目录下\n\n可通过Xftp或cp命令将man-pages-zh-CN拷贝到`/root`目录下（如果是root用户的话）\n\n（3）解压manpages-zh-1.5.1.tar.gz\n\n```bash\ntar -zxvf manpages-zh-1.5.1.tar.gz\n```\n\n（4）`cd manpages-zh-1.5.1`进入manpages-zh目录\n\n（5）配置manpages-zh\n\n```bash\n./configure --prefix=/usr/local/zhman --disable-zhtw  \n```\n\n（6）编译并安装\n\n```ba\nmake\nmake install\n```\n\n（7）设置别名\n\n```bash\ncd ~ #进入当前用户目录下\nvim .bashrc\n```\n\n使用`vim`命令在.bashrc增加 `alias cman='man -M /usr/local/zhman/share/man/zh_CN` ，如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207110803.png)\n\n（8）编译并执行.bashrc\n\n输入`source.bashrc ` 使刚刚添加的alias生效\n\n现在就可以使用中文版的man了，如果想要查看`ls`的帮助信息，只要输入`cman ls`即可。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207111653.png)\n\n\n\n【参考文档】\n\n[CentOS6 中文man手册使用说明](https://blog.csdn.net/weixin_34411563/article/details/92106911)\n\n\n\n","tags":["Linux"],"categories":["Linux"]},{"title":"Vim编辑器的基本用法","url":"/2021/02/10/120754/","content":"\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208154827.png)\n<!-- more -->\n\n\n\n## 一般模式切换到插入模式的命令\n\n一般模式切换到插入模式的命令：`iao|IAO`\n\n- i      进入编辑模式（默认在选定字符前插入）\n- a     在选定字符后插入字符\n- o     在当前行下添加新行\n- I      在当前行首进入编辑模式\n- A     在当前行末进入编辑模式\n- O     在当前行上添加新行\n\n关于一般模式和末行模式的切换：\n\n- ESC  退出编辑模式\n- : 末行模式\n\n## 末行模式下的退出命令\n\n末行模式下的**退出命令**：\n\n- :wq 保存并退出\n- :q! 不保存退出，在修改过文件的情况下使用该命令不会保存\n- :q 在没有动过文件情况下退出\n\n\n\n在一般模式下**输入`ZZ`（大写）保存并退出编辑器**\n\n\n\n## 移动光标\n\nh左 j下 k上 l右\n\n5j向下移动5行，5K 向上移动5行\n\ngg：光标移动到文档开头\n\nG ：光标移动到文件末尾\n\n3G：光标移动到第三行\n\n56G：光标移动到第56行\n\nEnd光标移到**行尾**\n\nHome键光标**行首**\n\n\n\n## 翻页\n\nctrl-f 向下翻页    forward\n\nctrl-b 向上翻页    backward\n\n\n\n## 删除内容\n\ndd   删除整行\n\nD    删除光标所在位置到行尾\n\ndG 删除光标到**文件的最后一行**\n\n3dd 删除光标所在行至后面的2行，共删除3行\n\nx   删除光标位置字符\n\n3x  删除光标开始3个字符\n\n\n\n## 复制粘贴\n\nyy 复制1行\n\nyw 复制单词\n\nnyy 复制n行，n是数字  复制从光标所在行开始的N行，一般在5行以内非常直观的数字时使用的比较多\n\np 粘贴 paste\n\n小写p 粘贴到光标所在行的下一行\n\n大写P 粘贴到光标所在行的上一行\n\n\n\n## 撤销\n\nu：撤销  undo\n\nctrl+r：重做     操作结束后使用u退回到上次操作，则ctrl+r重做，抵消一次u的撤销\n\n\n\n## 设置\n\nset：设置\n\n:set nu  number 显示行号\n\n:set nonu  nunumber 取消行号的显示\n\n:set readonly 设置只读\n\n\n\n## 查找替换\n\n:/after  搜索\n\nn向下搜索，N 向上搜索\n\n在查找模式中加入\\c表示大小写不敏感查找，\\C表示大小写敏感查找\n\n例如，`/foo\\c` 将会查找所有的\"foo\"、\"FOO\"、\"Foo\"等字符串。\n\n`:s`（substitute）**命令用来查找和替换字符串。**语法如下：\n\n`:{作用范围}s/{目标}/{替换}/{替换标志}`\n\n- 替换当前行第一个 vivian为sky：`:s/vivian/sky/`\n- 替换当前行所有 vivian为sky ：`:s/vivian/sky/g`\n- 当前行.与接下来两行+2：`:.,+2s/vivian/sky/g`\n- 替换第 n 行开始到最后一行中，每一行的第一个vivian为sky：`:n,$s/vivian/sky/`\n- 替换第 n 行开始到最后一行中，每一行所有vivian为sky，这里n为数字，**若n为`.`**，表示从当前行开始到最后一行：`:n,$s``/vivian/sky/g`\n- 替换每一行的第一个vivian为sky：`:%s/vivian/sky/`\n- 替换每一行中所有 vivian为sky：`:%s/vivian/sky/g`\n- 替换每一行中所有 vivian为sky，且忽略大小写：`:%s/vivian/sky/gi`\n\n\n\n**g：一行内全部替换**\n\n**i：忽略大小写**\n\n\n\n## 通用符号\n\n`n`：行号\n`.`：当前光标行\n`+n`：偏移n行\n`$`：末尾行，-3\n`%`：全文\n\n\n\n比如，d代表删除，那么：\n\n- `:%d`   删除全文 ，或`:.,$d`也是删除全文\n- `:.,$-1d` 从当前行删除到倒数第二行\n- `:.,$0d` **从当前行删除到文章末尾**\n- `:.,+3d` 从当前行再往下数三行删除\n- `:.,13d`   从当前行到第13行删除\n\n\n\n比如，y代表复制，那么：\n\n- `:.,$-1y` 赋值从当前行到倒数第二行\n- `:.,$0y` 赋值从当前行到文章末尾\n- `:.,+3y` 复制从当前行再往下数三行\n\n\n\n## 其它\n\n`:!ls /etc/…`  可以在vim下执行Shell命令，可以执行任何命令。\n\n\n\n\n\n\n\n\n\n\n","tags":["Vim"],"categories":["Linux"]},{"title":"Linux进程管理命令nohup、&、jobs、fg、bg、ps、kill","url":"/2021/02/10/115802/","content":"\n对Linux进程的管理是我们经常遇到的，如何查看一个进程的状态？如何把一个后台的进程调至进程执行？如何杀死一个进程......看了本文后，你将会全部掌握！\n<!-- more -->\n\n\n\n## 1. nohup\n\nnohup的用法：\n\n- 用途：不挂断地运行命令。\n- 语法：`nohup Command [ Arg … ] [　& ]`\n  - 在默认情况下（非重定向时），会输出一个名叫 nohup.out 的文件到当前目录下。\n  - 如果当前目录的 nohup.out 文件不可写，输出重定向到 `$HOME/nohup.out` （`$HOME`为用户主目录）文件中。\n  - 如果没有文件能创建或打开以用于追加，那么 Command 参数指定的命令不可调用。\n- 参数说明：\n  - Command：要执行的命令。\n  - Arg：一些参数，可以指定输出文件。\n  - &：让命令在后台执行，终端退出后命令仍旧执行。\n\n\n\n现在，来尝试一下！\n\n创建my.sh文件，文件内容如下：\n\n```bash\n#!/bin/bash\necho  \"hello\"\necho  \"----------\"\nsleep  20  #休眠20s\necho  \"world\"\n```\n\n执行：\n\n```bash\n[root@layne bashdir]# chmod +x my.sh  # 给my.sh增加执行权限\n[root@layne bashdir]# nohup ./my.sh &\n[1] 2064  #这个2064就是my.sh进程的id\n[root@layne bashdir]# nohup: ignoring input and appending output to `nohup.out' #看到这个信息说明运行成功，再按一下回车即可回到当前shell命令行\n\n[root@layne bashdir]# cat nohup.out\nhello\n----------\n[root@layne bashdir]# cat nohup.out #等待20s再次查看\nhello\n----------\nworld\n[1]+  Done                    nohup ./my.sh\n```\n\n以下命令在后台执行 my.sh 脚本，并重定向输入到 my.log 文件：\n\n```bash\nnohup ./my.sh > my.log 2>&1 &\n```\n\n解释**`2>&1`** ：将标准错误 2 重定向到标准输出 &1 ，标准输出 &1 再被重定向输入到 my.log 文件中。这样无论正确的输出，还是错误的输出都将重定向到my.log文件中。\n\n## 2. &\n\n`&`用在一个命令的最后，可以把这个命令放到**后台执行**，可以输入jobs 查看后台执行的命令。如下所示：\n\n```bash\n[root@layne bashdir]# sleep 30 & #休眠30s，并放在后台执行\n[1] 2156\n[root@layne bashdir]# jobs  # 查看后台的进程\n[1]+  Running                 sleep 30 &\n```\n\n\n\n## 3. jobs\n\njobs命令用于查看正在执行的后台进程，但只能看当前终端生效的进程，如果关闭当前终端后，在另一个终端下，`jobs`已经无法看到后台跑得程序了，此时利用ps（进程查看命令）。\n\njobs的选项如下：\n\n```tex\n-l：显示进程号；\n-p：仅任务对应的显示进程号；\n-n：显示任务状态的变化；\n-r：仅输出运行状态（running）的任务；\n-s：仅输出停止状态（stoped）的任务。\n```\n\njobs命令一般和`-l`搭配使用，可以显示后台执行进程的进程号。\n\n这里介绍一些常见的快捷键和进程命令：\n\n- `ctrl+c` 停止当前正在执行的进程，相当于直接kill掉。\n- `ctrl+z` 将当前正在执行的进程放到后台，并且暂停执行，此时进程处于stop状态。\n- `fg` **将后台中的进程调至前台继续运行**。如果后台中有多个命令，可以用`fg %jobnumbe`将选中的命令调出，`%jobnumber`是通过jobs命令查到的后台任务的编号，不是进程的pid号。\n- `bg`  **将一个在后台暂停的命令**，变成继续执行。如果后台中有多个命令，可以用`bg %jobnumber`将选中的命令调出。\n\n我们首先看看`jobs -l`输出的信息：\n\n```bash\n[root@layne bashdir]# sleep 30 &\n[2] 2157\n[1]   Done                    sleep 30\n[root@layne bashdir]# jobs -l\n[2]+  2157 Running                 sleep 30 &\n```\n\n上面`jobs -l`输出4列信息，第一列表示任务编号（jobnumber），第二列表示任务所对应的进程号（pid)，第三列表示任务的运行状态，第四列表示启动任务的命令。\n\n现在，我们多启动几个后台进程，让它们处于不同的状态，并用`fg`和`bg`命令调用它们到前台执行：\n\n```bash\n[root@layne bashdir]# sleep 60  #执行后，按下ctrl+z将该进程放置后台，并暂定执行\n^Z\n[1]+  Stopped                 sleep 60\n[root@layne bashdir]# sleep 40 &   # 让该进程放到后台执行\n[2] 2159\n[root@layne bashdir]# nohup ./my.sh &\n[3] 2160\n[root@layne bashdir]# nohup: ignoring input and appending output to `nohup.out'\n\n[root@layne bashdir]# jobs -l\n[1]+  2158 Stopped                 sleep 60\n[2]   2159 Running                 sleep 40 &\n[3]-  2160 Running                 nohup ./my.sh &\n[root@layne bashdir]# fg  # 等待60s，可以看到另外两个进程也执行完了，如果这里使用fg 2，则将任务号为2的进程调至前台执行\nsleep 60\n[2]   Done                    sleep 40\n[3]-  Done                    nohup ./my.sh\n```\n\n从上述执行过程会发现，输入`jobs -l`后，任务号（jobnumber）后面有 `+` 和 `-` 两个标志，其中，`+` 代表我们输入fg或bg的时候，将该进程调至前台执行。当我们把带有`+`的进程调至前台执行后，`-`标志的进程就自动变成`+`了，下次我们再执行fg或bg，就会调用`-`变为`+`的那个进程了。\n\n这里不用纠结 `fg` 和 `bg` 的区别，fg是将后台中的进程调至前台继续运行，bg将一个在后台暂停的命令变成继续执行。我在使用过程中，并没有很在意，不过用的最多的还是`fg`命令。\n\n\n\n## 4. ps\n\nps命令用于查看当前系统运行的进程信息。\n\n常用选项：\n\n- a ： 显示所有程序\n- x ：显示所有程序，不区分终端机\n- u ：以用户为主的格式来显示\n- `-f` 显示程序间的关系\n- `-e` 显示所有程序\n\n**常用组合 :**\n\n`ps aux` 观察系统所有的进程数据\n\n`ps -ef` 显示所有进程基本信息（比`aux`较简略一些）\n\n示例：\n\n```bash\n[root@layne bashdir]# nohup ./my.sh &\n[1] 2179\n[root@layne bashdir]# nohup: ignoring input and appending output to `nohup.out'\n\n[root@layne bashdir]# ps aux | grep my.sh  # 查看包含my.sh进程的信息\nroot       2179  0.0  0.1 106072  1332 pts/0    S    21:06   0:00 /bin/bash ./my.sh\nroot       2184  0.0  0.0 103256   872 pts/0    S+   21:07   0:00 grep my.sh\n[root@layne bashdir]# ps aux | head -5  # 查看当前系统所有正在执行进程的前5条\nUSER        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot          1  0.0  0.1  19232  1424 ?        Ss   10:08   0:01 /sbin/init\nroot          2  0.0  0.0      0     0 ?        S    10:08   0:00 [kthreadd]\nroot          3  0.0  0.0      0     0 ?        S    10:08   0:00 [migration/0]\nroot          4  0.0  0.0      0     0 ?        S    10:08   0:00 [ksoftirqd/0]\n```\n\n上述输出的含义：\n\n- USER：该 process 所属的使用者。\n- PID ：该 process 的进程标识符。  \n- %CPU：该 process 使用掉的 CPU 资源百分比。\n- %MEM：该 process 所占用的物理内存百分比。\n- VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) 。\n- RSS ：该 process 占用的物理的内存量 (Kbytes) 。\n- TTY ：该 process 是在哪个终端机上面运作，若与终端机无关则显示 `?`。另外，tty1-tty6 是本机上面的登入者程序，若为 pts/0 等，**则表示为由网络连接进主机的程序**。 \n- STAT：该进程目前的状态，状态显示与`ps -l`的 S 旗标相同 (R/S/D/T/Z) \n- START：该 process 被触发启动的时间\n- TIME ：该 process 实际使用 CPU 运作的时间。 \n- COMMAND：该程序的实际命令\n\n\n\n## 5. kill\n\nkill命令用于杀死进程，主要有两个选项：\n\n- `kill -9 pid` （见人就杀，不做善后工作）\n- `kill -15 pid` （调用destory等方法善后）\n\n**优先使用 `-15`选项，因为`-15`温柔一些，会做一些善后的处理（比如释放所占用的资源），如果使用`-15`无法杀死进程，再用`-9` 选项**\n\n一般情况下，先用ps命令查找要杀死进程的pid，再用kill命令杀死进程，例如：\n\n```bash\n[root@layne bashdir]# sleep 30 &\n[1] 2194\n[root@layne bashdir]# ps -aux | grep sleep\nWarning: bad syntax, perhaps a bogus '-'? See /usr/share/doc/procps-3.2.8/FAQ\nroot       2194  0.0  0.0 100916   620 pts/0    S    21:16   0:00 sleep 30\nroot       2196  0.0  0.0 103256   864 pts/0    S+   21:16   0:00 grep sleep\n[root@layne bashdir]# kill -15 2194\n```\n\n\n\n\n\n【参考文档】\nhttps://ipcmen.com/jobs\nhttps://www.runoob.com/linux/linux-comm-nohup.html\nhttps://www.linuxprobe.com/linux-nohup.html\nhttps://blog.csdn.net/u011630575/article/details/48288663\n\n\n\n\n\n\n","tags":["Linux"],"categories":["Linux"]},{"title":"Linux中软件的三种安装方式","url":"/2021/02/10/112210/","content":"\nLinux系统中，最常用的有三种安装方式\n\n- 源码编译安装（根据当前系统编译）\n- rpm安装（需要自己安装相应的依赖）\n- yum安装（自动安装依赖）\n\n当然，解压、配置（相当于windwos下绿色版的安装），也算是一种安装方式。\n\n解压来，我将介绍这三种安装方式的使用方法！\n\n<!-- more -->\n\n**文章目录**\n\n@[TOC](文章目录)\n\n\n\n\n\n## 源码编译安装\n\n### src-build概述\n\n源码编译安装即src-build式安装。\n\n源码的安装一般由3个步骤组成：配置(configure)、编译(make)、安装(make install)。\n\nConfigure是一个可执行脚本，它有很多选项，在待安装的源码路径下使用命令`./configure -help`输出详细的选项列表。\n\n其中`-prefix`选项是配置安装的路径，如果不配置该选项，安装后可执行文件默认放在`/usr/local/bin`，库文件默认放在`/usr/local/lib`，配置文件默认放在`/usr/local/etc`，其它的资源文件放在`/usr/local/share`，比较凌乱。\n\n如果配置`-prefix`，如下：\n\n```bash\n./configure --prefix=/usr/local/test\n```\n\n可以把所有资源文件放在/usr/local/test的路径中，不会杂乱。\n\n用了`-prefix`选项的另一个好处是卸载软件或移植软件。当某个安装的软件不再需要时，只须简单的删除该安装目录，就可以把软件卸载得干干净净；移植软件只需拷贝整个目录到另外一个机器即可（相同的操作系统）。\n\n当然要卸载程序，也可以在原来的make目录下用一次make uninstall，**但前提是make文件指定过uninstall**。\n\n### src-build安装Nginx\n\n接下来，我以Nginx为例子，介绍源码编译安装方式：\n\n（1）安装 gcc\n\nNginx 具有很强的可扩展性，需要使用哪些功能，只需通过 with 配置后重新编译即可添加此功能，所以对于 Nginx 的源码安装方式是我们最常接触到的。\n\nNginx 是由 C/C++语言编写的，所以对其进行编译就必须要使用相关编译器。对于C/C++语言的编译器，使用最多的是 gcc 与 gcc-c++，而这两款编译器在 CentOS6.5 中是没有安装的，所以首先要安装这两款编译器。\n\n> gcc，GNU Compiler Collection，GNU 编译器集合，其可以编译多种语言。\n\n\n\n（2）安装依赖库\n\n基本的 Nginx 功能依赖于一些基本的库，在安装 Nginx 之前需要提前安装这些库。\n\nA. pcre-devel：pcre，Perl Compatible Regular Expressions，Perl 脚本语言兼容正则表达式，为 Nginx 提供正则表达式库。（即安装正则表达式的库）\n\nB. openssl-devel：为 Nginx 提供 SSL（安全套接字层）密码库，包含主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。在安装之前需要注意，很多库具有 devel 库与非 devel 库两种。devel 库表示 development开发库，比非 devel 库会多出一些头文件、静态库、源码包等。而这些包在运行时不可能用到，但在开发时有可能用到。所以对于程序员来说，一般都是需要安装 devel 库的。不过，在 yum 安装 devel 库时，由于其依赖于非 devel 库，所以其会先自动安装非 devel 库，而后再安装 devel 库。所以真正在安装时，只需显示的安装 devel 库即可。通过以下命令可以查看到，非 devel 库也被安装了。\n\n```bash\n# 1.为了编译nginx源码\nyum  install  gcc  gcc-c++ -y\n# 2. 用于支持https协议\nyum  install  openssl  openssl-devel -y\n# 3. 解析正则表达式\nyum install  pcre  pcre-devel -y\n# 4. 压缩 gzip  deflate（下面是和压缩有关的包）\nyum  install  zlib  zlib-devel -y\n```\n\n然后，下载nginx，下载地址为：http://nginx.org/en/download.html\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208223050.png)\n\n这里选择nginx-1.8.1版本进行下载。\n\n5. 将nginx-1.8.1.tar.gz上传到linux的/opt/apps/目录下（需要创建apps文件夹）\n6. 解压nginx-1.8.1.tar.gz，输入命令`tar -zxvf nginx-1.8.1.tar.gz`\n7. 配置nginx（configure）\n\n```bash\n[root@layne testdir]# cd /opt/apps/nginx-1.8.1\n# ./configure  --help查看帮助信息\n[root@layne nginx-1.8.1]# ./configure  --help\n\n  --help                             print this message\n\n  --prefix=PATH                      set installation prefix\n  --sbin-path=PATH                   set nginx binary pathname\n  --conf-path=PATH                   set nginx.conf pathname\n  --error-log-path=PATH              set error log pathname\n  --pid-path=PATH                    set nginx.pid pathname\n  --lock-path=PATH                   set nginx.lock pathname\n...(太多了，这里把后面的删除了）\n```\n\n输入以下命令进行配置\n\n```bash\n./configure --prefix=/opt/nginx --with-http_ssl_module --with-http_gzip_static_module --error-log-path=/var/log/nginx/nginx.log --pid-path=/var/log/nginx/pid\n```\n\n- `/opt/nginx`指的是安装目录\n- `http_ssl_module` 支持安全协议\n- `--with-http_gzip_static_module` 支持压缩\n- `--error-log-path`  错误日志目录\n- `--pid-path=/var/log/nginx/pid`  进程id放置的目录信息\n\n执行后输出以下结果\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208224110.png)\n\n上面表示：\n\n```tex\nnginx path prefix: \"/opt/nginx\"  将来安装的时候放的目录\n  nginx binary file: \"/opt/nginx/sbin/nginx\" 执行文件目录\n  nginx configuration prefix: \"/opt/nginx/conf\" 配置文件目录前缀\n  nginx configuration file: \"/opt/nginx/conf/nginx.conf\"   配置文件\n  nginx pid file: \"/var/log/nginx/pid\"  进程ID目录\n  nginx error log file: \"/var/log/nginx/nginx.log\" 错误日志文件\n  nginx http access log file: \"/opt/nginx/logs/access.log\"  里面的access目录\n  nginx http client request body temporary files: \"client_body_temp\"\n  nginx http proxy temporary files: \"proxy_temp\"\n  nginx http fastcgi temporary files: \"fastcgi_temp\"\n  nginx http uwsgi temporary files: \"uwsgi_temp\"\n  nginx http scgi temporary files: \"scgi_temp\"\n```\n\n这个步骤可以理解为预编译，主要用于检测系统基准环境库是否满足gcc环境，生成makefile文件。\n\n8. 编译(make)\n\n`make clean`命令用来清除上一次编译生成的目标文件（**清除上次的make命令所产生的object文件**（后缀为“.o”的文件）及可执行文件）。这个步骤可有可无，但为了确保编译的成功，还是加上为好。防止由于软件中含有残留的目标文件导致编译失败。\n\n生成脚本及配置文件：`make`  （根据Makefile文件编译源代码、连接、生成目标文件、可执行文件）\n\n编译步骤：根据Makefile文件生成相应的模块\n\n所以一般先输入`make clean` 再输入`make` \n\n\n\n9. 安装(make install)\n\n输入`make install`（将编译成功的可执行文件安装到系统目录中）\n\n```bash\n# 安装之后进入nginx的安装目录/opt/nginx\n[root@layne nginx-1.8.1]# cd /opt/nginx\n[root@layne nginx]# ls\nclient_body_temp  conf  fastcgi_temp  html  logs  proxy_temp  sbin  scgi_temp  uwsgi_temp\n[root@layne nginx]# cd conf\n[root@layne conf]# ll\ntotal 60\n-rw-r--r-- 1 root root 1034 Feb  5 17:14 fastcgi.conf\n-rw-r--r-- 1 root root 1034 Feb  5 17:14 fastcgi.conf.default\n-rw-r--r-- 1 root root  964 Feb  5 17:14 fastcgi_params\n-rw-r--r-- 1 root root  964 Feb  5 17:14 fastcgi_params.default\n-rw-r--r-- 1 root root 2837 Feb  5 17:14 koi-utf\n-rw-r--r-- 1 root root 2223 Feb  5 17:14 koi-win\n-rw-r--r-- 1 root root 3957 Feb  5 17:14 mime.types\n-rw-r--r-- 1 root root 3957 Feb  5 17:14 mime.types.default\n-rw-r--r-- 1 root root 2656 Feb  5 17:14 nginx.conf\n-rw-r--r-- 1 root root 2656 Feb  5 17:14 nginx.conf.default\n-rw-r--r-- 1 root root  596 Feb  5 17:14 scgi_params\n-rw-r--r-- 1 root root  596 Feb  5 17:14 scgi_params.default\n-rw-r--r-- 1 root root  623 Feb  5 17:14 uwsgi_params\n-rw-r--r-- 1 root root  623 Feb  5 17:14 uwsgi_params.default\n-rw-r--r-- 1 root root 3610 Feb  5 17:14 win-utf\n```\n\n**conf**：保存nginx所有的配置文件，其中nginx.conf是nginx服务器的最核心最主要的配置文件，其他的.conf则是用来配置nginx相关的功能的，例如fastcgi功能使用的是fastcgi.conf和fastcgi_params两个文件，配置文件一般都有个样板配置文件，以文件名.default结尾，使用的使用将其复制并将default去掉即可。\n\n**html**：目录中保存了nginx服务器的web文件，但是可以更改为其他目录保存web文件，另外还有一个50x的web文件是默认的错误页面提示页面。\n\n**logs**：用来保存nginx服务器的访问日志错误日志等日志，logs目录可以放在其他路径，比如/var/logs/nginx里面。\n\n**sbin**：保存nginx二进制启动脚本，可以接受不同的参数以实现不同的功能。\n\n现在，我们启动nginx\n\n```bash\n[root@layne conf]# cd ../sbin/\n[root@layne sbin]# ls\nnginx\n[root@layne sbin]# ./nginx\n```\n\n在浏览器中输入你的ip地址，如`http://192.168.50.33/`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208230509.png)\n\n出现该界面说明nginx安装成功。\n\n\n\n**关于卸载**\n\n要卸载程序，也可以在原来的`make`目录下用一次`make uninstall`，但前提是`Makefile`文件有`uninstall`命令。\n\n也可以将有关目录删除，这个方法会比较麻烦，建议网上查找详细步骤。\n\n\n\n## rpm安装软件\n\n### rpm概述\n\nRPM（RedHat Package Manager）安装管理，这个机制最早是由Red Hat开发出来，后来实在很好用，因此很多 distributions（发行版）就使用这个机制来作为软件安装的管理方式 ，包括Fedora,CentOS,SuSE等等知名的开发商。 \n\nRPM的优点 \n1. RPM内含已经编译过的程序与配置文件等数据,可以让用户免除重新编译的困扰 \n2. RPM在被安装之前,会先检查系统的硬盘容量、操作系统版本等,可避免文件被错误安装 \n3. RPM文件本身提供软件版本信息、相依属性软件名称、软件用途说明、软件所含文件等信息，便于了解软件 \n4. RPM管理的方式使用数据库记录 RPM 文件的相关参数,便于升级 、移除、查询与验证 \n\n\n\nrpm默认安装的路径如下：\n1.\t/etc    一些配置文件放置的目录,例如/etc/crontab \n2.\t/usr/bin 一些可执行文件 \n3.\t/usr/lib   一些程序使用的动态链接库 \n4.\t/usr/share/doc 一些基本的软件使用手册与说明文件 \n5.\t/usr/share/man 一些man page（Linux命令的随机【跟随虚拟机】帮助说明）文件 \n\n\n\n**rpm安装命令**\n\n```bash\nrpm -ivh package_name\n```\n\n选项与参数: \n\n- `-i` :install的意思\n- `-v `:察看更细部的安装信息画面\n- `-h` :以安装信息列显示安装进度\n\n\n\n**安装单个rpm包** \n\n```bash\nrpm -ivh package_name \n```\n\n**安装多个rpm包** \n\n```bash\n rpm -ivh a.i386.rpm b.i386.rpm *.rpm \n```\n\n**安装网上某个位置rpm包**\n\n```bash\nrpm -ivh http://website.name/path/pkgname.rpm\n```\n\n\n\n### rpm安装jdk\n\n这里，我通过rpm安装jdk为例子，进行说明：\n\n1. 将jdk上传到/opt/apps目录下\n\n```bash\n[root@layne ~]# cd /opt/apps\n[root@layne apps]# ls\njdk-7u80-linux-x64.rpm  nginx-1.8.1  nginx-1.8.1.tar.gz  rpm_mysql\n```\n\n2. 安装当前目录下的jdk-7u80-linux-x64.rpm\n\n```bash\n[root@layne apps]# rpm -ivh jdk-7u80-linux-x64.rpm\nPreparing...                ########################################### [100%]\n   1:jdk                    ########################################### [100%]\nUnpacking JAR files...\n\trt.jar...\n\tjsse.jar...\n\tcharsets.jar...\n\ttools.jar...\n\tlocaledata.jar...\n\tjfxrt.jar...\n```\n\n3. 查找java安装目录的位置：\n\n```bash\n[root@layne apps]# whereis java\njava: /usr/bin/java\n[root@layne apps]# ll /usr/bin/java\nlrwxrwxrwx 1 root root 26 Feb  5 19:29 /usr/bin/java -> /usr/java/default/bin/java\n[root@layne apps]# cd /usr/java\n[root@layne java]# ls\ndefault  jdk1.7.0_80  latest\n[root@layne java]# cd jdk1.7.0_80/\n[root@layne jdk1.7.0_80]# pwd\n/usr/java/jdk1.7.0_80\n```\n\n4. 配置环境变量\n\n进入`vim /etc/profile` ，在文件最后加入以下两行代码：\n\n```bash\nexport JAVA_HOME=/usr/java/default\nexport PATH=$PATH:$JAVA_HOME/bin\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209110929.png)\n\n5. 让配置生效，使用`source /etc/profile`\n6. 测试安装配置是否成功\n\n```bash\n[root@layne ~]# java -version\njava version \"1.7.0_80\"\nJava(TM) SE Runtime Environment (build 1.7.0_80-b15)\nJava HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n[root@layne ~]# jps\n1423 Jps\n```\n\n出现以上信息，说明配置成功\n\nrpm安装：\n\n- redhat提供了rpm管理系统\n- 已经编译的软件包：针对不同的平台系统编译目标软件包\n- 操作系统维护安装信息\n- 软件包包含依赖检查，这是人需要参与处理的。**你得下载它需要的所有依赖包并安装，才能安装一个rpm软件**。\n\n\n\n### rpm查询\n\n简单原理：rpm在查询的时候,其实查询的地方是在/var/lib/rpm/ 这个目录下的数据库文件 \n\n```bash\n[root@layne ~]# ll /var/lib/rpm/\ntotal 28752\n-rw-r--r--. 1 root root  1523712 Feb  9 10:40 Basenames\n-rw-r--r--. 1 root root    12288 Feb  5 16:46 Conflictname\n-rw-r--r--  1 root root    24576 Feb  9 10:40 __db.001\n-rw-r--r--  1 root root   229376 Feb  9 10:40 __db.002\n-rw-r--r--  1 root root  1318912 Feb  9 10:40 __db.003\n-rw-r--r--  1 root root   753664 Feb  9 10:40 __db.004\n-rw-r--r--. 1 root root   331776 Feb  9 10:40 Dirnames\n...\n```\n\n**rpm查询已安装软件，选项与参数：**\n\n-q :仅查询,后面接的软件名称是否有安装 \n\n-qa :列出所有的,已经安装在本机Linux系统上面的所有软件名称 ！！！\n\n-qi :列出该软件的详细信息,包含开发商、版本和说明等 !!\n\n-ql :列出该软件所有的文件与目录所在完整文件名 !!\n\n-qc :列出该软件的所有配置文件 !\n\n-qd :列出该软件的所有说明文件 \n\n-qR :列出和该软件有关的相依软件所含的文件 \n\n-qf :由后面接的文件名,找出该文件属于哪一个已安装的软件 、\n\n查询某个 RPM 文件内含有的信息:  `-qp[icdlR]`\n\n注意 -qp 后面接的所有参数以上面的说明一致。但用途仅在于找出 某个 RPM 文件内的信息,而非已安装的软件信息 \n\n案例1：查找是否安装jdk\n\n```bash\n[root@layne ~]# rpm -qa |grep jdk\njdk-1.7.0_80-fcs.x86_64\n```\n\n注意：`grep jdk` 本来就是模糊查询，不用加`*`。\n\n\n\n案例2：查询jdk所包含的文件及目录\n\n```bash\n[root@layne ~]# rpm -ql jdk\n```\n\n\n\n案例3：查看jdk包的相关说明\n\n```bash\n[root@layne ~]# rpm -qi jdk\nName        : jdk                          Relocations: /usr/java \nVersion     : 1.7.0_80                          Vendor: Oracle Corporation\nRelease     : fcs                           Build Date: Sat 11 Apr 2015 11:15:36 AM CST\nInstall Date: Fri 05 Feb 2021 07:29:53 PM CST      Build Host: sca00efd.us.oracle.com\nGroup       : Development/Tools             Source RPM: jdk-1.7.0_80-fcs.src.rpm\nSize        : 219404663                        License: http://java.com/license\nSignature   : (none)\nPackager    : Java Software <jre-comments@java.sun.com>\nURL         : URL_REF\nSummary     : Java Platform Standard Edition Development Kit\nDescription :\nThe Java Platform Standard Edition Development Kit (JDK) includes both\nthe runtime environment (Java virtual machine, the Java platform classes\nand supporting files) and development tools (compilers, debuggers,\ntool libraries and other tools).\n\nThe JDK is a development environment for building applications, applets\nand components that can be deployed with the Java Platform Standard\nEdition Runtime Environment.\n```\n\n\n\n案例4：列出iptables的配置文件\n\n```bash\n[root@layne ~]# rpm -qc iptables\n/etc/sysconfig/iptables-config\n```\n\n\n\n案例5：查看apr需要的依赖\n\n```bash\n[root@layne ~]# yum install apr\n[root@layne ~]# rpm -qR apr\n```\n\n\n\n温馨提示：**大改动之前一定要拍快照**\n\n\n\n\n\n### rpm安装MySQL\n\n1.  查找当前系统中安装的MySQL:\n\n```bash\n[root@layne ~]# rpm -qa | grep mysql\nmysql-libs-5.1.71-1.el6.x86_64\n```\n\n上面是CentOS6.5自带的mysql，但是版本有点低\n\n2. 将上一步找到的mysql相关的包都删除（`-e`是erase的意思）\n\n```bash\n[root@layne ~]# rpm -e mysql-libs-5.1.71-1.el6.x86_64\nerror: Failed dependencies:\n\tlibmysqlclient.so.16()(64bit) is needed by (installed) postfix-2:2.6.6-2.2.el6_1.x86_64\n\tlibmysqlclient.so.16(libmysqlclient_16)(64bit) is needed by (installed) postfix-2:2.6.6-2.2.el6_1.x86_64\n\tmysql-libs is needed by (installed) postfix-2:2.6.6-2.2.el6_1.x86_64\n```\n\n会发现mysql5.1.7被postfix-2:2.6.6-2.2.el6_1.x86_64依赖：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209121603.png)\n\n加上`--nodeps` 不检查依赖直接删除\n\n```bash\n[root@layne ~]# rpm -e --nodeps mysql-libs-5.1.71-1.el6.x86_64\n[root@layne ~]# rpm -qa | grep mysql\n```\n\n\n\n3. 检查并删除老版本mysql的开发头文件和库(如果有的话)\n\n```bash\n[root@layne ~]# find / -name \"mysql*\"\nrm -fr /usr/lib/mysql\nrm -fr /usr/include/mysql\nrm -f /etc/my.cnf\nrm -fr /var/lib/mysql\nrm -fr /usr/share/mysql\n```\n\n注意：卸载后/var/lib/mysql中的数据及`/etc/my.cnf`不会删除（可以先备份，后删除），如果确定没用后就手工删除。\n\n4. 安装perl\n\n```bash\n[root@layne ~]# yum install perl -y\n```\n\n\n\n5. 下载numactl-2.0.9-2.el6.x86_64.rpm并安装\n\n```bash\n[root@layne apps]# cd /opt/apps/rpm_mysql\n[root@layne rpm_mysql]# ls\nmysql-community-client-5.7.19-1.el6.x86_64.rpm  mysql-community-server-5.7.19-1.el6.x86_64.rpm\nmysql-community-common-5.7.19-1.el6.x86_64.rpm  numactl-2.0.9-2.el6.x86_64.rpm\nmysql-community-libs-5.7.19-1.el6.x86_64.rpm    rpm??װmysql.txt\n[root@layne rpm_mysql]# rpm -ivh numactl-2.0.9-2.el6.x86_64.rpm\n```\n\nnumactl-2.0.9-2.el6.x86_64.rpm为Mysql依赖的包\n\n\n\n6. 安装mysql（有顺序要求）\n\n方法1：依次安装如下软件包\n\n```bash\nrpm -ivh mysql-community-common-5.7.19-1.el6.x86_64.rpm\nrpm -ivh mysql-community-libs-5.7.19-1.el6.x86_64.rpm\nrpm -ivh mysql-community-client-5.7.19-1.el6.x86_64.rpm\nrpm -ivh mysql-community-server-5.7.19-1.el6.x86_64.rpm\n```\n\n方法2：执行`rpm -ivh mysql-community-*`，**让系统自己判断rpm安装顺序**\n\n\n\n7. 修改/etc/my.cnf文件，设置数据库的编码方式\n\n```bash\n[client]\ndefault-character-set=utf8\n[mysql]\ndefault-character-set=utf8\n[mysqld]\ncharacter_set_server=utf8\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209122724.png)\n\n\n\n8. 如果出现错误，请查看/etc/my.cnf文件中指定的错误log日志的文件\n\n错误log的存放位置为`log-error=/var/log/mysqld.log`\n\n这里可能会出现找不到文件的情况，如果找不到这个文件，就先执行第9步再尝试，如果再不行就重启系统。\n\n\n\n9. 启动MySQL：`service mysqld start`\n\n\n\n10. 找到随机密码\n\n在`/var/log/mysqld.log`中有一行：A temporary password is generated for root@localhost，后面就是随机密码\n\n然后使用随机密码登陆：\n\n```bash\nmysql -uroot -p\"yAe7QGVJ;HlR\"\n```\n\n\n\n11. 修改模式密码\n\n```bash\nmysql> set global validate_password_policy=0;\nmysql> set global validate_password_length=6;\nmysql> set password for 'root'@'localhost'=password('123456');\n```\n\n\n\n12. 登陆mysql查看编码方式\n\n```bash\nmysql> show variables like '%character%';\n+--------------------------+----------------------------+\n| Variable_name            | Value                      |\n+--------------------------+----------------------------+\n| character_set_client     | utf8                       |\n| character_set_connection | utf8                       |\n| character_set_database   | utf8                       |\n| character_set_filesystem | binary                     |\n| character_set_results    | utf8                       |\n| character_set_server     | utf8                       |\n| character_set_system     | utf8                       |\n| character_sets_dir       | /usr/share/mysql/charsets/ |\n+--------------------------+----------------------------+\n8 rows in set (0.09 sec)\n```\n\n\n\n13. 给root设置远程登录权限\n\n```bash\nmysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;\nmysql> FLUSH PRIVILEGES;\n```\n\n>  ALL PRIVILEGES指所有的权限\n>\n> 'root'@'%' 指root用户除了localhost，也支持其他的远程登录\n\n\n\n14. 设置msyql开机启动\n\n```bash\nmysql> exit\nBye\n[root@layne rpm_mysql]# chkconfig mysqld on\n```\n\n\n\n到这里就安装完毕，如果安装失败，就把第6步安装的4个包删了重新来。\n\n\n\n### rpm卸载\n\n找出与apr有关的软件名称，并尝试移除apr这个软件 \n\n```bash\n[root@layne ~]# rpm -qa|grep apr\napr-1.3.9-5.el6_9.1.x86_64\n[root@layne ~]# rpm -e apr-1.3.9-5.el6_9.1.x86_64\n[root@layne ~]# rpm -qa|grep apr\n[root@layne ~]# \n```\n\n\n\n## yum安装软件\n\n### 了解epl\n\n全称：Extra Packages for Enterprise Linux，企业版Linux的**额外软件包**。它是Fedora小组维护的一个软件仓库项目，为RHEL/CentOS提供他们默认不提供的软件包。这个源兼容RHEL及像CentOS和Scientific Linux这样的衍生版本。\n\n我们可以很容易地通过yum命令从EPEL源上获取上万个在CentOS自带源上没有的软件。EPEL提供的软件包大多基于其对应的Fedora软件包，不会与企业版Linux发行版本的软件发生冲突或替换其文件。更多关于EPEL 项目的细节可以到以下网站获取：https://fedoraproject.org/wiki/EPEL。\n\nRHEL/CentOS系统有许多第三方源，比较流行的比如RpmForge，RpmFusion，EPEL，Remi等等。然而需要引起注意的是，如果系统添加了多个第三方源，可能会因此产生冲突——一个软件包可以从多个源获取，一些源会替换系统的基础软件包，从而可能会产生意想不到的错误。已知的就有Rpmforge与EPEL会产生冲突。**对于这些问题我们建议，调整源的优先权或者有选择性的安装源，但是这需要复杂的操作，如果你不确定如何操作，我们推荐你只安装一个第三方源。**\n\n### yum命令\n\nyum的源在`/etc/yum.repos.d/` 目录下，**以.repo结尾的**的文件都是yum源配置文件，yum**可以同时有多个镜像**，名字无所谓，只要文件以.repo结尾就可以使用。\n\n```bash\n[root@layne ~]# ls /etc/yum.repos.d/\nCentOS-Base-Neu.repo          CentOS-Base.repo      CentOS-Base-Sohu.repo.bak     CentOS-Debuginfo.repo  CentOS-Vault.repo\nCentOS-Base-QingHua.repo.bak  CentOS-Base.repo.bak  CentOS-Base-YuanShi.repo.bak  CentOS-Media.repo\n```\n\n可以看到，我启用了三个源，分别是CentOS-Base-Neu.repo、CentOS-Base.repo、 CentOS-Media.repo\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209125236.png)\n\n**yum命令：**\n\n```bash\nyum repolist  #查看yum源\nyum clean all  # 清除缓存\nyum makecache   # 重新生成缓存（生成软件花名册）\nyum update  #更新系统使用该命令（更新系统已经存在的软件）\n```\n\n**查询：**\n\n`yum list` 列出系统中已经安装的和可以安装的包\n\n```bash\nyum list | grep mysql\n```\n\n`yum search` 在yum源搜索指定的包\n\n```bash\nyum search mysql\n```\n\n`yum info` 打印指定包的描述信息\n\n```bash\nyum info mysql\n```\n\n**安装和卸载：**\n\n```bash\nyum install xxx\nyum remove|erase xxx\n```\n\n**yum分组命令：**\n\n查询yum源中rpm包的组信息（一个group组包含多个软件包）\n\n```bash\nyum grouplist\n```\n\n查看指定组的信息\n\n```bash\nyum groupinfo \"Chinese Support\"\n```\n\n安装指定的组（即安装该组下的所有软件包）\n\n```bash\nyum groupinstall \"Chinese Support\"\n```\n\n删除指定软件\n\n```bash\nyum groupremove \"Chinese Support\"\n```\n\n更新指定软件组\n\n```bash\nyum groupupdate \"Chinese Support\"\n```\n\n其它命令（了解）\n\n```tex\nUsage: yum [options] COMMAND\n\nList of Commands:\n\ncheck          Check for problems in the rpmdb\ncheck-update   Check for available package updates\nclean          移除缓存的数据\ndeplist        列出包的依赖\ndistribution-synchronization Synchronize installed packages to the latest available versions\ndowngrade      downgrade a package\nerase          从系统中移除一个或几个包\ngroupinfo      打印软件组的细节信息\ngroupinstall   在系统中安装一组软件包\ngrouplist      列出可用的软件包\ngroupremove   从系统中移除一组软件包\nhelp           Display a helpful usage message\nhistory        Display, or use, the transaction history\ninfo           Display details about a package or group of packages\ninstall        在系统中安装一个或几个软件包\nlist           列出包信息或一组包的信息\nload-transaction load a saved transaction from filename\nmakecache      生成元数据缓存\nprovides       哪个包提供了指定的值。\nreinstall      重新安装一个软件包\nrepolist       打印配置的软件源\nresolvedep     Determine which package provides the given dependency\nsearch         搜索指定的包\nshell          Run an interactive yum shell\nupdate         Update a package or packages on your system\nupgrade        Update packages taking obsoletes into account\nversion        Display a version for the machine and/or available repos.\n```\n\n\n\n### yum的三类源\n\n（1）本地自定义源\n\n```bash\ncd  /etc/yum.repos.d/\ncp  CentOS-Base.repo   localRepo.repo  #先备份文件\nmkdir  old\nmv  CentOS-*  old     #将原来的文件备份到一个位置\n\nvim   localRepo.repo\n```\n\nlocalRepo.repo中base那一部分改为\n\n```tex\n...\n[base]\n  name=CentOS-$releasever - Base\n  #mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os\n  baseurl=file:///mnt\n  gpgcheck=0\n  gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n...\n```\n\n插入DVD的ISO镜像文件(CentOS-6.8-x86_64-bin-DVD1.iso)，挂载到/mnt目录\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209130451.png)\n\n挂载设备\n\n```bash\nmount  /dev/cdrom  /mnt\ncd  /mnt    # 查看其中的内容\ncd  /mnt/Packages\nls  -l  |  wc  -l\n```\n\n统计出该目录有多少个rpm包\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209130649.png)\n\n然后清缓存，再生成数据缓存\n\n```bash\nyum  clean  all    #清空本地元数据缓存\nyum  makecache    #创建本地元数据缓存\nyum  repolist  #查看元数据的具体信息\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209130756.png)\n\n至此，本地yum源配置完成。\n\n\n\n（2）发布局域网yum源\n\ntomcat发布自定义的yum源\n\n解压tomcat的tar包：\n\n```bash\ntar -zxf apache-tomcat-7.0.68.tar.gz\n```\n\n首先将dvd1和dvd2的内容从node2主机的/mnt目录拷贝到tomcat的webapps/ROOT/目录\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209130927.png)\n\n输入`./startup.sh`启动tomcat\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209130958.png)\n\n在node1的tomcat.repo中进行配置：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209131043.png)\n\n配置tomcat.repo:\n\n```bash\n[base]\nname=CentOS-$releasever - Base - tomcat\nbaseurl=http://node2:8080\ngpgcheck=0\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6\n```\n\n然后清缓存，再生成数据缓存\n\n```bash\nyum  clean  all    #清空本地元数据缓存\nyum  makecache    #创建本地元数据缓存\n```\n\n这样就可以了\n\n**（3）CentOS使用其他源**\n\n这个可以参考我的博客[把Linux系统中的yum替换为清华源，速度瞬间从5KB/s变到3.5MB/s](https://wxler.github.io/2021/02/04/211126/)\n\n\n","tags":["Linux"],"categories":["Linux"]},{"title":"Linux中的用户、用户组的管理","url":"/2021/02/10/111843/","content":"\n接下来，我将介绍以下四个内容：\n\n1. 用户\n2. 用户组\n3. 其他人的概念\n4. Linux用户身份与用户组记录的文件\n\n<!-- more -->\n\n**文章目录如下：**\n\n@[TOC](文章目录)\n\n\n\n## 1. 用户\n\n案例：假如当你把给你心意的女神写了封Email情书转存成了文件之后，放在你自己的主文件夹中，你总不希望被其他人看见自己的情书吧？这个时候你就可以把该文件设置成只有所有者才能查看和修改该文件的内容，那么即使其他人知道你这个相当有趣的文件，不过由于你设置适当权限，所以其他人自然不知道该文件的具体内容。\n\n由于Linux是多用户,多任务的操作系统，为此，会经常有多个用户同时使用某一台主机。为了考虑每个用户的隐私安全以及每个用户特殊的工作环境，设计了文件所有者这个概念。\n\n**而文件所有者就是文件所属的用户。**\n\nLinux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。\n\n用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。\n\n每个用户账号都拥有一个惟一的用户名和各自的口令。用户在登录时键入正确的用户名和口令后，就能够进入系统和自己的主目录。\n\n\n\n## 2. 用户组\n\n那么用户组呢？为何要配置文件所属的用户组呢？\n\n用户组是为了团队开发资源而设计的。举例来说：某一台主机上的资源有两个团队共同使用，team A（曹操、许褚、夏侯渊）和teamB（刘备、关羽、诸葛亮）。\n\n```tex\n-rwxrwx--- caocao teamA wei.doc        \n-rwxrwx--- liubei teamB   shu.doc\n-rwx------  zhugeliang  teamB qingshu.txt\n```\n\n这两个团队之间是竞争关系，但却要提交同一份报告《如何获得天下》，每个团队的成员需要有权修改该团队其他成员的数据，而另一个团队的成员无权查看本组自己的文件内容，此时用户组就起到了关键作用。\n\n在Linux下我们可以进行简单的文件权限设置，就能限制非自己团队的用户权限。同时，我们还可以设置个人私密文件，使团队内其他成员无法获取私人的文件内容。除此之外，若是有项目主管想监督这两个团队的开发进度，需要查看两个团队文件的权限，你便可以设置主管账号，同时支持这两个用户组。换句话说：每个账号都可以有多个用户组的支持。\n\n用户组和用户关系分析案例：\n\n可以使用“家庭”和家庭成员的关系进行分析，比如王大毛家的三兄弟：大毛、二毛、三毛，而家庭登记的王大毛的名下，他们分别有自己的房间。所以“王大毛家”（用户组）有3个人（用户）：大毛、二毛、三毛；而且这三个人分别有自己的房间，并且共同拥有一个客厅。\n\n**用户的意义**：由于王家3个人拥有自己的房间，所以二毛虽然可以进入三毛的房间，但是二毛不能随便翻三毛的抽屉，因为抽屉里可能有三毛的私有物品，比如情书、日记等；这是私人空间，所以不能让二毛随便动。\n\n**用户组的意义：**由于共同拥有客厅，所以王家三兄弟可以在客厅看电视、看杂志、或望着天花板发呆等，总之只要是客厅的东西，三兄弟都可以使用，大家一家人嘛。\n\n王大毛家->用户组；大毛、二毛、三毛->分别对应一个用户。\n\n他们三个人在同一个用户组中，可以通过设置他们文件的权限，将一些文件设置为“私有的”（只能他自己访问和使用），同用户组的其他用户无法访问和使用。而通过设置用户组共享的，则可让大家共同分享。\n\n\n\n## 3. 其他人(others)\n\nLinux系统中任何一个文件都具有“User、Group、Other”三种身份的个别权限。\n\n我们仍然以上述案例分析，文件所有者、用户组与其他人的示意图如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208170430.png)\n\n有个人叫张小猪，他是张小猪家的人，和王大毛家没有关系，此时无法进入王大毛家；如果张小猪和王三毛成了好朋友，他就可以通过王三毛进入王家，那么这个张小猪就是王家所谓的其他人（Others）\n\n 若某一用户对于一个与他毫无关系的用户组而言，就可以被视作其他用户。\n\n在这有一个特殊用户需要提及一下。Linux中的root用户（上图中的天神）,在整个Linux系统中拥有最大的权限，理论上能干任何事情。\n\n\n\n## 4. Linux用户身份与用户组记录的文件\n\n在Linux系统当中，默认情况下所有的系统上的账号信息都记录在`/etc/passwd`这个文件内(包括root用户)，而个人密码记录在`/etc/shadow`这个文件内。所有Linux的组名都记录在`/etc/group`内。这三个文件非常重要，不要轻易做变动。\n\n综上，用户身份与用户组的概念，能够帮助我们的Linux多任务环境变得更为容易管理。\n\n实现用户账号的管理，要完成的工作主要有如下几个方面：\n\n- 用户账号的添加、删除与修改。 \n- 用户口令的管理。\n- 用户组的管理。 \n\n\n\n## 5. Linux系统用户账号的管理\n\n### 5.1 新建用户\n\n用户账号的管理工作主要涉及到用户账号的添加、修改和删除。 \n\n添加用户账号就是在系统中创建一个新账号，然后为新账号分配用户号、用户组、主目录和登录Shell等资源。刚添加的账号是被锁定的，无法使用。\n\n（1）**添加新的用户账号使用useradd命令**，其语法如下：\n\n参数说明\n\n- 选项：\n  - -c comment 指定一段注释性描述。\n  - -d 目录 指定用户主目录，如果此目录不存在，**则同时使用-m选项，可以创建主目录**。\n  - -g 用户组 指定用户所属的用户组。\n  - -G 用户组，用户组 指定用户所属的附加组。\n  - -s Shell文件 指定用户的登录Shell。\n  - -u 用户号 指定用户的用户号，如果同时有-o选项，则可以重复使用其他用户的标识号。\n\n- 用户名：\n  - 指定新账号的登录名。\n\n**补充：**\n\n①`-d` 为用户指定一个主目录，home下的用户目录默认为用户的家目录，如果在添加用户时不指定`-d` ,则用户的家目录放在/home下，**所以创建文件时，一般不指定`-d`选项。**\n\n②`- g` 当你创建用户的时候，**没有指定用户组的话，默认给你创建一个同名的用户组**。\n\n一个用户可以有多个用户组，即一个主用户组（主组），多个附加组（用-G选项创建）\n\n一个用户处于其主用户组下，可以使用用户组的权限，但处于附加组下，不能直接使用附加组的权限，只有切换到附加组，才可以使用。\n\n③`-s` 是该用户登录之后，执行的应用程序是谁，如果不指定，默认是/bin/bash（root的shell也是/bin/bash），可以输入vim /etc/passwd 查看\n\n**实例1：添加用户lucy，并设置他的个人主目录练习**\n\n`useradd -d /usr/lucy -m lucy`\n\n此命令创建了一个用户lucy，其中-d和-m选项用来为登录名lucy产生一个主目录/usr/lucy。如果不指定`-d`设置主目录，则该用户的主目录为`/home/lucy`。\n\n此时可进入`vim /etc/passwd` 查看用户的相关信息\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208180903.png)\n\n所谓的shell就是，你登录完之后，会马上启动一个/bin/bash进程（root也是/bin/bash）。\n\n上图的`/sbin/nologin` 是伪用户进程，即什么也做不了\n\n输入`id lucy`查看lucy的信息：\n\n```bash\n[root@layne ~]# id lucy\nuid=502(lucy) gid=502(lucy) groups=502(lucy)\n```\n\n可以看到，用户lucy的id位502，其用户组为lucy，用户组id为502\n\n\n\n**实例2：创建用户gem，指定它属于主用户组“sys”，附加组“adm、root”，已经登录的Shell是/bin/sh**\n\n`useradd -s /bin/sh -g sys -G adm,root gem`\n\n此命令新建了一个用户gem，该用户的登录Shell是 `/bin/sh`，它属于sys用户组，同时又属于adm和root用户组，其中sys用户组是其**主组**。\n\n此时，输入`id gem`\n\n```bash\n[root@layne ~]# id gem\nuid=503(gem) gid=3(sys) groups=3(sys),0(root),4(adm)\n```\n\n查看/etc/passwd中gem的那一条记录：\n\n```tex\n[root@layne ~]# cat /etc/passwd | grep gem\ngem:x:503:3::/home/gem:/bin/sh\n```\n\n\n\n增加用户账号就是在/etc/passwd文件中为新用户增加一条记录，同时更新其他系统文件如/etc/shadow, /etc/group等。 \n\n### 5.2 修改帐号\n\n修改用户账号就是根据实际情况更改用户的有关属性，如用户号、主目录、用户组、登录Shell等。 \n\n修改已有用户的信息使用`usermod`命令，其格式：`usermod 选项用户名`\n\n常用的选项包括`-c, -d, -m, -g, -G, -s, -u以及-o等`，这些选项的意义与`useradd`命令中的选项一样，可以为用户指定新的资源值。\n\n另外，有些系统可以使用选项：`-l` 新用户名，这个选项指定一个新的账号，即将原来的用户名改为新的用户名。 格式为：`usermod -l 新用户  旧用户 `。 这只会更改用户名，而其他的东西，比如用户组、家目录、ID 等都保持不变。\n\n**将用户gem的用户名修改为gemda**\n\n`usermod -l gemda gem`\n\n```bash\n[root@layne ~]# usermod -l gemda gem\n[root@layne ~]# id gemda\nuid=503(gemda) gid=3(sys) groups=3(sys),0(root),4(adm)\n[root@layne ~]# id gem\nid: gem: No such user\n```\n\n**修改家目录**\n\n`usermod -d 目录 -m 用户名` \n\n将用户gemda的家目录修改为`/usr/gemda` ，命令为：`usermod -d /usr/gemda/ -m gemda`\n\n**修改UID**\n\n`usermod -u newuid 用户名`\n\n比如，将用户gemda的uid改为550，命令为：`usermod -u 550 gemda`\n\n```bash\n[root@layne ~]# usermod -u 550 gemda\n[root@layne ~]# id gemda\nuid=550(gemda) gid=3(sys) groups=3(sys),0(root),4(adm)\n```\n\n### 5.3 删除账号\n\n如果一个用户的账号不再使用，可以从系统中删除。删除用户账号就是要将/etc/passwd等系统文件中的该用户记录删除，必要时还删除用户的主目录。\n\n删除一个已有的用户账号使用`userdel`命令，其格式如下： \n\n```bash\nuserdel 选项 用户名\n```\n\n常用的选项是`-r`，它的作用是把用户的主目录一起删除。 \n\n例如：`userdel -r liubei`\n\n此命令删除用户liubei在系统文件中（主要是/etc/passwd, /etc/shadow, /etc/group等）的记录，同时删除用户的主目录。 \n\n### 5.4 用户口令的管理\n\n用户管理的一项重要内容是用户口令的管理。**用户账号刚创建时没有口令，并且被系统锁定，因此利用这个用户登录系统，是登录不了的，必须为其指定口令后才可以使用，即使是指定空口令**。 \n\n指定和修改用户口令的Shell命令是`passwd`。超级用户可以为自己和其他用户指定口令，普通用户只能用它修改自己的口令。\n\n命令的格式为：\n\n```bash\npasswd 选项 用户名\n```\n\n可使用的选项： \n\n- -l（lock）     锁定口令，即禁用账号。\n- -u（unlock）     口令解锁。\n- -d（HOME_DIR） 使账号无口令。\n- -f 强迫用户下次登录时修改口令。\n- -S 查看用户口令状态\n\n如果是超级用户，可以用下列形式指定任何用户的口令：\n\n比如，为gemda设置密码（或修改密码）\n\n```bash\n[root@layne ~]# passwd gemda\nChanging password for user gemda.\nNew password: \nBAD PASSWORD: it is too simplistic/systematic\nBAD PASSWORD: is too simple\nRetype new password: \npasswd: all authentication tokens updated successfully.\n```\n\n查看用户gemda口令状态\n\n```bash\n[root@layne ~]# passwd -S gemda\ngemda PS 2021-02-08 0 99999 7 -1 (Password set, SHA512 crypt.)\n```\n\n用户锁定\n\n```bash\n[root@layne ~]# passwd -l gemda\nLocking password for user gemda.\npasswd: Success\n[root@layne ~]# passwd -S gemda\ngemda LK 2021-02-08 0 99999 7 -1 (Password locked.)\n```\n\n用户解锁\n\n```bash\n[root@layne ~]# passwd -u gemda\nUnlocking password for user gemda.\npasswd: Success\n[root@layne ~]# passwd -S gemda\ngemda PS 2021-02-08 0 99999 7 -1 (Password set, SHA512 crypt.)\n```\n\n假设当前用户是gemda（当前的登陆用户），则下面的命令修改该用户自己的口令：\n\n```bash\n$ passwd \nOld password:******\nNew password:*******\nRe-enter new password:*******\n```\n\n普通用户修改自己的口令时，passwd命令会先询问原口令，验证后再要求用户输入两遍新口令，如果两次输入的口令一致，则将这个口令指定给用户；而超级用户为用户指定口令时，就不需要知道原口令。\n\n为了系统安全起见，用户应该选择比较复杂的口令，例如最好使用8位长的口令，口令中包含有大写、小写字母和数字，并且应该与姓名、生日等不相同。\n\n为用户指定空口令时，执行下列形式的命令：\n\n```bash\npasswd -d gemda\n[root@layne ~]# passwd -S gemda\ngemda NP 2021-02-08 0 99999 7 -1 (Empty password.)\n```\n\n此命令将用户gemda的口令删除，这样用户gemda下一次登录时，系统就不再询问口令。\n\n\n\n## 6. 系统用户组的管理\n\n 每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，比如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。\n\n用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。\n\n### 6.1 增加用户组\n\n增加一个新的用户组使用groupadd命令。\n\n其格式如下：\n\n```bash\ngroupadd 选项 用户组\n```\n\n可以使用的选项有：\n\n- -g GID 指定新用户组的组标识号（GID）。**（不指定也可以自动生成）**\n- -o 一般与-g选项同时使用，表示新用户组的GID可以与系统已有用户组的GID相同。\n\ncat /etc/group 查看用户组。\n\n**实例1：添加用户组group1**\n\n```\ngroupadd group1\n```\n\n此命令向系统中增加了一个新组group1，新组的组标识号是在当前已有的最大组标识号的基础上加1。 \n\n**实例2：向系统中增加了一个新组group2，同时指定新组的组标识号是101**\n\n```bash\ngroupadd -g 101 group2\n```\n\n此命令向系统中增加了一个新组group2，同时指定新组的组标识号是101。\n\n### 6.2 修改用户组\n\n修改用户组语法如下：\n\n```bash\ngroupmod 选项 用户组\n```\n\n常用的选项有： \n\n- -g GID 为用户组指定新的组标识号。\n- -o 与-g选项同时使用，用户组的新GID可以与系统已有用户组的GID相同。\n- -n新用户组 将用户组的名字改为新名字\n\n**实例1：将组group2的组标识号修改为102**\n\n```bash\ngroupmod -g 102 group2\n```\n\n实例2：将组group2的标识号改为10000，组名修改为group3\n\n```bash\ngroupmod -g 10000 -n group3 group2\n```\n\n将一个用户组改名后，**该用户组下所有的用户都将使用改名后的组名**\n\n\n\n### 6.3 删除用户组\n\n删除用户组用`groupdel 用户组`\n\n从系统中删除组group1，命令为\n\n```bash\ngroupdel group1\n```\n\n**当一个用户组下有用户的时候不能删除该用户组**\n\n\n\n### 6.4 用户组的切换\n\n**用户可以在登录后**，使用命令`newgrp`切换到其他用户组，这个命令的参数就是目的用户组。例如：\n\n```bash\nnewgrp root\n```\n\n这条命令将当前用户切换到root用户组，**前提条件是root用户组确实是该用户的主组或附加组**。类似于用户账号的管理，用户组的管理也可以通过集成的系统管理工具来完成。\n\n\n\n## 7. 与用户账号有关的系统文件\n\n完成用户管理的工作有许多种方法，但是每一种方法实际上都是对有关的系统文件进行修改。\n\n与用户和用户组相关的信息都存放在一些系统文件中，这些文件包括/etc/passwd, /etc/shadow, /etc/group等。\n\n下面分别介绍这些文件的内容。 \n\n### 7.1 /etc/passwd文件\n\nLinux系统中的每个用户都在/etc/passwd文件中有一个对应的记录行，它记录了这个用户的一些基本属性。\n\n这个文件对所有用户都是可读的。它的内容类似下面的例子：\n\n```bash\n[root@layne ~]# cat /etc/passwd\nroot:x:0:0:root:/root:/bin/bash\nbin:x:1:1:bin:/bin:/sbin/nologin\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\nadm:x:3:4:adm:/var/adm:/sbin/nologin\nlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\nsync:x:5:0:sync:/sbin:/bin/sync\nshutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\nhalt:x:7:0:halt:/sbin:/sbin/halt\nmail:x:8:12:mail:/var/spool/mail:/sbin/nologin\nuucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin\noperator:x:11:0:operator:/root:/sbin/nologin\ngames:x:12:100:games:/usr/games:/sbin/nologin\ngopher:x:13:30:gopher:/var/gopher:/sbin/nologin\nftp:x:14:50:FTP User:/var/ftp:/sbin/nologin\nnobody:x:99:99:Nobody:/:/sbin/nologin\nvcsa:x:69:69:virtual console memory owner:/dev:/sbin/nologin\nsaslauth:x:499:76:\"Saslauthd user\":/var/empty/saslauth:/sbin/nologin\npostfix:x:89:89::/var/spool/postfix:/sbin/nologin\nsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin\nntp:x:38:38::/etc/ntp:/sbin/nologin\nmysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/false\nlucy:x:502:502::/usr/lucy:/bin/bash\ngemda:x:550:3::/usr/gemda/:/bin/sh\n```\n\n从上面的例子我们可以看到，/etc/passwd中一行记录对应着一个用户，每行记录又被冒号(:)分隔为7个字段，其格式和具体含义如下：\n\n**用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell**\n\n**1）\"用户名\"是代表用户账号的字符串。**\n\n通常长度不超过8个字符，并且由大小写字母和/或数字组成。登录名中不能有冒号(:)，因为冒号在这里是分隔符。\n\n为了兼容起见，登录名中最好不要包含点字符(.)，并且不使用连字符(-)和加号(+)打头。\n\n **2）“口令”一些系统中，存放着加密后的用户口令字。**\n\n虽然这个字段存放的只是用户口令的加密串，不是明文，但是由于/etc/passwd文件对所有用户都可读，所以这仍是一个安全隐患。因此，现在许多Linux 系统（如SVR4）都使用了shadow技术，**把真正的加密后的用户口令字存放到/etc/shadow文件中**，而在/etc/passwd文件的口令字段中只存放一个特殊的字符，例如“x”或者“*”。\n\n**3）“用户标识号”是一个整数，系统内部用它来标识用户。**\n\n一般情况下它与用户名是一一对应的。如果几个用户名对应的用户标识号是一样的，系统内部将把它们视为同一个用户，但是它们可以有不同的口令、不同的主目录以及不同的登录Shell等。\n\n通常用户标识号的取值范围是0～65535。0是超级用户root的标识号，1～99由系统保留，作为管理账号，普通用户的标识号从100开始。在Linux系统中，这个界限是500。\n\n**4）“组标识号”字段记录的是用户所属的用户组。**\n\n它对应着/etc/group文件中的一条记录。（**记录的是用户主用户组的ID，不是附加组的ID**）\n\n**5)“注释性描述”字段记录着用户的一些个人情况。**\n\n例如用户的真实姓名、电话、地址等，这个字段并没有什么实际的用途。在不同的Linux 系统中，这个字段的格式并没有统一。在许多Linux系统中，这个字段存放的是一段任意的注释性描述文字，用做finger命令的输出。\n\n**6)“主目录”，也就是用户的起始工作目录。**\n\n它是用户在登录到系统之后所处的目录。在大多数系统中，各用户的主目录都被组织在同一个特定的目录下，而用户主目录的名称就是该用户的登录名。各用户对自己的主目录有读、写、执行（搜索）权限，其他用户对此目录的访问权限则根据具体情况设置。\n\n**7)用户登录后，要启动一个进程，负责将用户的操作传给内核，这个进程是用户登录到系统后运行的命令解释器或某个特定的程序，即Shell。**\n\n如果指定某一个进程（程序），那么该用户登录后只能操作这个进程（程序），其他的操作不了。\n\nShell是用户与Linux系统之间的接口。Linux的Shell有许多种，每种都有不同的特点。常用的有sh(Bourne Shell), csh(C Shell), ksh(Korn Shell), tcsh(TENEX/TOPS-20 type C Shell), bash(Bourne Again Shell)等。 \n\n系统管理员可以根据系统情况和用户习惯为用户指定某个Shell。如果不指定Shell，那么系统使用sh(或bash)为默认的登录Shell，即这个字段的值为/bin/sh（或/bin/bash）。\n\n用户的登录Shell也可以指定为某个特定的程序（**此程序不是一个命令解释器**）。\n\n利用这一特点，我们可以限制用户只能运行指定的应用程序，在该应用程序运行结束后，用户就自动退出了系统。有些Linux 系统要求只有那些在系统中登记了的程序才能出现在这个字段中。\n\n**8)系统中有一类用户称为伪用户（psuedo users）。**\n\n这些用户在/etc/passwd文件中也占有一条记录，**但是不能登录，因为它们的登录Shell为空**。它们的存在主要是方便系统管理，满足相应的系统进程对文件属主的要求。\n\n常见的伪用户如下所示：\n\n```tex\n伪用户含义\nbin 拥有可执行的用户命令文件\nsys 拥有系统文件\nadm 拥有帐户文件\nuucp UUCP使用\nlp lp或lpd子系统使用\nnobody NFS使用\n```\n\n\n\n由于/etc/passwd文件是所有用户都可读的，如果用户的密码太简单或规律比较明显的话，一台普通的计算机就能够很容易地将它破解，因此对安全性要求较高的Linux系统都把加密后的口令字分离出来，单独存放在一个文件中，这个文件是/etc/shadow文件。 有超级用户才拥有该文件读权限，这就保证了用户密码的安全性。\n\n### 7.2 /etc/shadow文件\n\n/etc/shadow中的记录行与/etc/passwd中的一一对应，它由pwconv命令根据/etc/passwd中的数据自动产生。\n\n它的文件格式与/etc/passwd类似，由若干个字段组成，字段之间用\":\"隔开。\n\n```bash\n[root@layne ~]# cat /etc/shadow\nroot:$6$6f.vksV8ywQJhXse$L7xDLlGQ3VZDw3YphDxw36XqsJC6/HzYQvudRT5JP5aO7zGXOgHz2LPLr6UZbWmRbqMcnEz26h.UpS36jy/nu1:18660:0:99999:7:::\nbin:*:15980:0:99999:7:::\ndaemon:*:15980:0:99999:7:::\nadm:*:15980:0:99999:7:::\nlp:*:15980:0:99999:7:::\nsync:*:15980:0:99999:7:::\nshutdown:*:15980:0:99999:7:::\nhalt:*:15980:0:99999:7:::\nmail:*:15980:0:99999:7:::\nuucp:*:15980:0:99999:7:::\noperator:*:15980:0:99999:7:::\ngames:*:15980:0:99999:7:::\ngopher:*:15980:0:99999:7:::\nftp:*:15980:0:99999:7:::\nnobody:*:15980:0:99999:7:::\nvcsa:!!:18660::::::\nsaslauth:!!:18660::::::\npostfix:!!:18660::::::\nsshd:!!:18660::::::\nntp:!!:18661::::::\nmysql:!!:18663::::::\nlucy:!!:18666:0:99999:7:::\ngemda::18666:0:99999:7:::\n```\n\n这些字段是：**登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志**\n\n1. \"登录名\"是与/etc/passwd文件中的登录名相一致的用户账号\n2. \"口令\"字段存放的是加密后的用户口令字，长度为13个字符。如果为空，则对应用户没有口令，登录时不需要口令；如果含有不属于集合     { ./0-9A-Za-z }中的字符，则对应的用户不能登录。\n3. \"最后一次修改时间\"表示的是从某个时刻起，到用户最后一次修改口令时的天数。时间起点对不同的系统可能不一样。例如在SCO Linux 中，这个时间起点是1970年1月1日。\n4. \"最小时间间隔\"指的是两次修改口令之间所需的最小天数。\n5. \"最大时间间隔\"指的是口令保持有效的最大天数。\n6. \"警告时间\"字段表示的是从系统开始警告用户到用户密码正式失效之间的天数。\n7. \"不活动时间\"表示的是用户没有登录活动但账号仍能保持有效的最大天数。\n8. \"失效时间\"字段给出的是一个绝对的天数，如果使用了这个字段，那么就给出相应账号的生存期。期满后，该账号就不再是一个合法的账号，也就不能再用来登录了。\n\n### 7.3 /etc/group文件\n\n用户组的所有信息都存放在/etc/group文件中。\n\n将用户分组是Linux 系统中对用户进行管理及控制访问权限的一种手段。\n\n每个用户都属于某个用户组；一个组中可以有多个用户，一个用户也可以属于不同的组。**当一个用户同时是多个组中的成员时，在/etc/passwd文件中记录的是用户所属的主组，也就是登录时所属的默认组，而其他组称为附加组。**\n\n**用户要访问属于附加组的文件时，必须首先使用newgrp命令使自己成为所要访问的组中的成员。**\n\n用户组的所有信息都存放在/etc/group文件中。此文件的格式也类似于/etc/passwd文件，由冒号(:)隔开若干个字段\n\n```bash\n[root@layne ~]# cat /etc/group\nroot:x:0:gemda\nbin:x:1:bin,daemon\ndaemon:x:2:bin,daemon\nsys:x:3:bin,adm\nadm:x:4:adm,daemon,gemda\ntty:x:5:\ndisk:x:6:\nlp:x:7:daemon\nmem:x:8:\nkmem:x:9:\nwheel:x:10:\nmail:x:12:mail,postfix\nuucp:x:14:\nman:x:15:\ngames:x:20:\ngopher:x:30:\nvideo:x:39:\ndip:x:40:\nftp:x:50:\nlock:x:54:\naudio:x:63:\nnobody:x:99:\nusers:x:100:\nfloppy:x:19:\nvcsa:x:69:\nutmp:x:22:\nutempter:x:35:\ncdrom:x:11:\ntape:x:33:\ndialout:x:18:\nsaslauth:x:76:\npostdrop:x:90:\npostfix:x:89:\nfuse:x:499:\nsshd:x:74:\nntp:x:38:\nlucy_new:x:500:\nmysql:x:27:\nlucy:x:502:\ngroup3:x:10000:\n```\n\n这些字段有：**组名:口令:组标识号:组内用户列表**\n\n1. \"组名\"是用户组的名称，由字母或数字构成。与/etc/passwd中的登录名一样，组名不应重复。\n2. \"口令\"字段存放的是用户组加密后的口令字。一般Linux 系统的用户组都没有口令，即这个字段一般为空，或者是。\n3. \"组标识号\"与用户标识号类似，也是一个整数，被系统内部用来标识组。\n4. \"组内用户列表\"是属于这个组的所有用户的列表/b]，不同用户之间用逗号(,)分隔。这个用户组可能是用户的主组，也可能是附加组。**（有多个用户，都可以列出来）**","tags":["Linux"],"categories":["Linux"]},{"title":"Linux中的bash语法详解","url":"/2021/02/10/111447/","content":"\nLinux中的bash语法，主要介绍bash的io、变量、分支、循环等基本操作。\n<!-- more -->\n\n@[TOC](文章目录)\n\n\n\n\nshell就是一个bash程序（进程），就是一个解释器，启动器\n\n脚本本质：\n\n- #!/bin/bash\n- #!/usr/bin/python\n\n上面的不是普通的注释，是你用`./`执行的时候，默认走的哪一个解释器\n\n用`bash test.sh`执行的时候，是不看第一行的`#!/bin/bash`或`#!/usr/bin/python` ，直接用bash解释器。\n\n同样的，用`sh test.sh`执行的时候也一样，其直接用sh解释器。\n\n但是，用`./ test.sh`执行的时候，要看第一行，然后选择走哪一个解释器。\n\n脚本读取（或解释执行）方式常用的有五种：\n\n- `bash filename`\n- `sh filename`\n- `./filename`\n- `source filename `\n- `. filename`\n\n说明： `./*.sh`的执行方式等价于`sh ./*.sh`或者`bash ./*.sh`，**此三种执行脚本的方式都是重新启动一个子shell，在子shell中执行此脚本。**\n\n`source ./*.sh`和 `. ./*.sh`的执行方式是等价的，即两种执行方式都是在当前shell进程中执行此脚本，而不是重新启动一个子shell执行。\n\n一个重要的点：**父shell进程中没有被export导出的变量（即非环境变量）是不能被子shell进程继承的，但是在父进程中被export的变量可以被子shll进程使用。子shell进程中的所有变量（无论是否被export）都不能被父进程使用**\n\n\n\n## 1. bash基本使用\n\n```bash\n[root@layne ~]# bash  # 启动一个新的bash进程，该bash进程为之前bash的子进程\n[root@layne ~]# echo $$  # 打印当前bash的PID\n1852\n[root@layne ~]# pstree -p # 打印进程树，并显示进程id\ninit(1)─┬─auditd(899)───{auditd}(900)\n        ├─crond(1312)\n        ├─mingetty(1338)\n        ├─mingetty(1340)\n        ├─mingetty(1342)\n        ├─mingetty(1344)\n        ├─mingetty(1346)\n        ├─mingetty(1348)\n        ├─ntpd(973)\n        ├─rsyslogd(915)─┬─{rsyslogd}(917)\n        │               ├─{rsyslogd}(919)\n        │               └─{rsyslogd}(920)\n        ├─sshd(965)───sshd(1030)───bash(1219)───bash(1852)───pstree(1862)\n        └─udevd(354)───udevd(640)\n[root@layne ~]# exit # （第一次输入exit，退出当前bash，再输入exit会退出ssh连接的bash，即就断掉连接了）\nexit\n[root@layne ~]# echo $$\n1219\n[root@layne ~]# pstree -p\ninit(1)─┬─auditd(899)───{auditd}(900)\n        ├─crond(1312)\n        ├─mingetty(1338)\n        ├─mingetty(1340)\n        ├─mingetty(1342)\n        ├─mingetty(1344)\n        ├─mingetty(1346)\n        ├─mingetty(1348)\n        ├─ntpd(973)\n        ├─rsyslogd(915)─┬─{rsyslogd}(917)\n        │               ├─{rsyslogd}(919)\n        │               └─{rsyslogd}(920)\n        ├─sshd(965)───sshd(1030)───bash(1219)───pstree(1863)\n        └─udevd(354)───udevd(640)\n```\n\n可以看到，当我们用ssh连接虚拟机时，会启动`sshd`进程，进入bash命令行界面后，会启动`bash`进程，这个bash进程是sshd进程的子进程，当我们在命令行中输入`bash`会启动一个新的bash进程，这个新的bash进程是之前bash进程的子进程。\n\n现在我们来验证：**父shell进程中没有被export导出的变量（即非环境变量）是不能被子shell进程继承的，但是在父进程中被export的变量可以被子shll进程使用。**\n\n```bash\n[root@layne ~]# name=layne\n[root@layne ~]# age=24\n[root@layne ~]# echo \"$name,$age\"\nlayne,24\n[root@layne ~]# export name\n[root@layne ~]# bash\n[root@layne ~]# echo \"$name,$age\"\nlayne,\n```\n\n可以看到，在子进程的bash，只能输出被export导出的变量（即环境变量），不能输出普通变量。\n\n还有一个重要点：**子shell进程中的所有变量（无论是否被export）都不能被父进程使用**。验证如下：\n\n```bash\n[root@layne bashdir]# GH=lazz\n[root@layne bashdir]# export GH\n[root@layne bashdir]# echo $GH\nlazz\n[root@layne bashdir]# exit\nexit\n[root@layne bashdir]# echo $GH #输出空\n\n```\n\n\n\n另外，在`.sh`文件的开头一般输入下列中的一个：\n\n```bash\n#!/bin/bash\n#!/usr/bin/python\n#!/bin/awk -f\n```\n\n用于指定该脚本由哪个程序负责解释执行。\n\n现在我们创建两个文件mysh.sh和mysh1.sh，这两个文件内容分别为：\n\nmysh.sh内容\n\n```bash\n#!/bin/bash\necho \"hello mysh\"\necho $$\n```\n\nmysh1.sh内容\n\n```bash\n#!/bin/bash\necho \"hello mysh1\"\necho $$\npstree -p\n```\n\n下面是执行过程：\n\n```bash\n[root@layne bashdir]# ll\ntotal 8\n-rwxr-xr-x 1 root root 49 Feb  9 16:27 mysh1.sh\n-rwxr-xr-x 1 root root 39 Feb  9 16:27 mysh.sh\n[root@layne bashdir]# chmod -x mysh.sh  # 清除执行权限\n[root@layne bashdir]# chmod -x mysh1.sh\n[root@layne bashdir]# ll\ntotal 8\n-rw-r--r-- 1 root root 49 Feb  9 16:27 mysh1.sh\n-rw-r--r-- 1 root root 39 Feb  9 16:27 mysh.sh\n[root@layne bashdir]# source mysh.sh\nhello mysh\n1219\n[root@layne bashdir]# source mysh1.sh\nhello mysh1\n1219\ninit(1)─┬─auditd(899)───{auditd}(900)\n        ├─crond(1312)\n        ├─mingetty(1338)\n        ├─mingetty(1340)\n        ├─mingetty(1342)\n        ├─mingetty(1344)\n        ├─mingetty(1346)\n        ├─mingetty(1348)\n        ├─ntpd(973)\n        ├─rsyslogd(915)─┬─{rsyslogd}(917)\n        │               ├─{rsyslogd}(919)\n        │               └─{rsyslogd}(920)\n        ├─sshd(965)───sshd(1030)───bash(1219)───pstree(1890)\n        └─udevd(354)───udevd(640)\n[root@layne bashdir]# pstree\ninit─┬─auditd───{auditd}\n     ├─crond\n     ├─6*[mingetty]\n     ├─mysqld_safe───mysqld───27*[{mysqld}]\n     ├─ntpd\n     ├─rsyslogd───3*[{rsyslogd}]\n     ├─sshd───sshd───bash───pstree\n     └─udevd───udevd\n[root@layne bashdir]# ./mysh.sh\n-bash: ./mysh.sh: Permission denied\n[root@layne bashdir]# chmod +x mysh.sh\n[root@layne bashdir]# ./mysh.sh\nhello mysh\n1894\n[root@layne bashdir]# chmod +x mysh1.sh\n[root@layne bashdir]# ./mysh1.sh\nhello mysh1\n1898\ninit(1)─┬─auditd(899)───{auditd}(900)\n        ├─crond(1312)\n        ├─mingetty(1338)\n        ├─mingetty(1340)\n        ├─mingetty(1342)\n        ├─mingetty(1344)\n        ├─mingetty(1346)\n        ├─mingetty(1348)\n        ├─ntpd(973)\n        ├─rsyslogd(915)─┬─{rsyslogd}(917)\n        │               ├─{rsyslogd}(919)\n        │               └─{rsyslogd}(920)\n        ├─sshd(965)───sshd(1030)───bash(1219)───mysh1.sh(1898)───pstree(1899)\n        └─udevd(354)───udevd(640)\n\n```\n\n可以发现：\n\n- 当前shell执行脚本：`source  mysh.sh` ，**不需要执行权限就可以执行**\n- 子进程执行：`bash mysh.sh`或者`./mysh.sh`，需要该文件具有可执行权限才能执行\n\n\n\n `.sh`文件中的函数格式为：\n\n```bash\nfunName() {\n  各种命令\n}\n```\n\n在下面的例子中会用到。\n\n\n\n## 2. bash的io\n\n程序自身都有I/O\n\n- 0：标准输入\n- 1：标准输出\n- 2：错误输出\n\n例如：\n\n```bash\nls / /hello 1> log.out 2> log.err\n```\n\n上面命令把正确的输出放在log.out里面，把错误的输出放在log.err里面。\n\n下面，我们看几个常用场景\n\n（1）输出重定向： 重定向从左到右绑定\n\n```bash\n# 命令1： 将错误输出（2）绑定到标准输出（1），此时标准输出先输出到控制台,然后才是标准输出重定向到文件。两个重定向的绑定没有关系，所以第二个标准输出1> mylog.log的数据仍然是正确的输出\nls  /  /hello   2>&1  1> mylog.log\n## 命令2：先让标准输出重定向到文件，然后将错误输出绑定到标准输出，也就是左边绑定的文件。\nls  /  /hello   1> mylog1.log  2>&1\n## 命令3：标准输出和错误输出都重定向到文件\nls  /  /hello   >& mylog2.log\n## 命令4：作用同命令3\nls  /  /hello  &> mylog3.log\n```\n\n\n\n（2）现在，我们创建rd.sh文件，内容如下：\n\n```bash\n#!/bin/bash\nread -p \"请输入一个整数：\" num\necho $num\nread -p \"再输入一个数：\"\necho $REPLY\n```\n\n执行结果：\n\n```bash\n[root@layne bashdir]# chmod +x rd.sh\n[root@layne bashdir]# ./rd.sh\n请输入一个整数：123\n123\n再输入一个数：45\n45\n```\n\n`sh -x rd.sh` 检查你写的脚本，执行到哪一步，把执行的过程打出来\n\n```bash\n[root@layne bashdir]# sh -x rd.sh\n+ read -p $'\\350\\257\\267\\350\\276\\223\\345\\205\\245\\344\\270\\200\\344\\270\\252\\346\\225\\264\\346\\225\\260\\357\\274\\232' num\n请输入一个整数：123\n+ echo 123\n123\n+ read -p $'\\345\\206\\215\\350\\276\\223\\345\\205\\245\\344\\270\\200\\344\\270\\252\\346\\225\\260\\357\\274\\232'\n再输入一个数：45\n+ echo 45\n45\n```\n\n（3）**将标准输入重定向到字符串，read读取后赋值给指定的变量：**\n\n```bash\n[root@layne bashdir]# read  aaa  0<<<\"hello\"  # 赋给变量aaa\n[root@layne bashdir]# echo $aaa\nhello\n```\n\n上面 `<<<`将标准输入重定向到字符串\n\n（4）`cat 0<<CATEOF`用在脚本中用于向控制台打印n行，比如：\n\n```bash\n[root@layne bashdir]# cat 0<<CATEOF\n> aaaaa\n> bbbbb\n> ccccc\n> CATEOF\naaaaa\nbbbbb\nccccc\n```\n\n创建catof.sh文件，内容为：\n\n```bash\n#!/bin/bash\ncat 0<<CATEOF\n这里有一个bug\n这个bug是index out of bounds exception\nCATEOF\n```\n\n执行：\n\n```bash\n[root@layne bashdir]# chmod +x catof.sh \n[root@layne bashdir]# ./catof.sh\n这里有一个bug\n这个bug是index out of bounds exception\n```\n\n（5）exec：使用指定的命令替换当前shell命令。\n\n以读写方式打开到`www.baidu.com`的80端口的tcp连接\n\n```bash\n[root@layne bashdir]# exec 8<> /dev/tcp/www.baidu.com/80 # 创建文件描述符8，并赋值\n[root@layne bashdir]# echo -e \"GET / HTTP/1.0\\n\" >&8  # 表示重定向到8文件描述符\n[root@layne bashdir]# cat <&8 # 从文件描述符8读取信息\nHTTP/1.0 200 OK\nAccept-Ranges: bytes\nCache-Control: no-cache\nContent-Length: 14615\nContent-Type: text/html\n...\n```\n\necho的`-e`选项含义为：激活转义字符。使用-e选项时，若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出：\n\n- `\\a` 发出警告声；\n- `\\b` 删除前一个字符；\n- `\\c` 最后不加上换行符号；\n- `\\f` 换行但光标仍旧停留在原来的位置；\n- `\\n` 换行且光标移至行首；\n- `\\r` 光标移至行首，但不换行；\n- `\\t` 插入tab；\n- `\\v` 与`\\f`相同；\n- `\\\\` 插入`\\`字符；\n- `\\nnn` 插入nnn（八进制）所代表的ASCII字符；\n\n\n\n`echo -e \"GET / HTTP/1.0\\n\" >&8`  表示重定向到8文件描述符\n\n`cat <&8`   从文件描述符8读取信息\n\n\n\n## 3. bash的变量\n\n### 3.1 bash中变量的类型\n\n-\t本地变量\n-\t局部变量\n-\t位置变量\n-\t特殊变量\n-\t环境变量\n\n\n\n### 3.2 本地变量\n\n-\t当前shell所有\n-\t生命周期跟当前shell一样\n\n看下面的例子：\n\n```bash\n[root@layne bashdir]# a=99 # 定义一个变量\n[root@layne bashdir]# echo $a # 输出变量a\n99\n[root@layne bashdir]# myfunc() {\n>   myvar=99\n>   echo  $myvar\n> }\n[root@layne bashdir]# echo $myvar  #访问不到\n\n[root@layne bashdir]# myfunc  #调用函数\n99\n[root@layne bashdir]# echo $myvar    #可以访问到\n99\n[root@layne bashdir]# abc=sxt\n[root@layne bashdir]# echo $abc\nsxt\n[root@layne bashdir]# echo \"$abcisnothere\"  #访问不到\n\n[root@layne bashdir]# echo \"${abc}isnothere\" #可以访问\nsxtisnothere\n[root@layne bashdir]# echo \"$abc isnotthere\"\nsxt isnotthere\n[root@layne bashdir]# echo \"{$abc}isnotthere\"\n{sxt}isnotthere\n```\n\n\n\n### 3.3 局部变量\n\n-\t只能用于函数\n-\t`local var=100` 定义一个局部变量\n\n看例子：\n\n```bash\n[root@layne bashdir]# unset myvar # 取消变量\n[root@layne bashdir]# echo $myvar\n\n[root@layne bashdir]#  myfunc(){\n> local myvar=101\n> echo $myvar\n> }\n[root@layne bashdir]# echo $myvar #访问不到局部变量\n\n[root@layne bashdir]# \n\n```\n\n继续往下看：\n\n```bash\n[root@layne bashdir]# funa(){\n> a=1\n> local b=2\n> echo \"a = $a\"\n> echo \"b = $b\"\n> }\n[root@layne bashdir]# funa\na = 1\nb = 2\n[root@layne bashdir]# echo \"$a,$b\" #b访问不到\n1,\n```\n\n上面，a是本地变量，b是局部变量，本地变量生命周期跟当前shell一样，局部变量的声明周期在一个函数内，函数执行完，局部变量就消失了。\n\n### 3.4 位置变量\n\n直接看例子：\n\n```bash\n[root@layne bashdir]# myfun1(){\n> echo $1\n> }\n[root@layne bashdir]# myfun1  \n\n[root@layne bashdir]# myfun1 hello # hello为函数myfun1的参数\nhello\n[root@layne bashdir]# myfun2(){\n> echo $4\n> }\n[root@layne bashdir]# myfun2 a b c\n\n[root@layne bashdir]# myfun2 a b c d\nd\n[root@layne bashdir]# myfunc3(){\n> echo $13\n> }\n[root@layne bashdir]# myfunc3 1 2 3 4 5 6 7 8 9 a b c d e f\n13\n[root@layne bashdir]# myfunc3 a b c d 1 2 3 4 5 6 7 8 9\na3\n[root@layne bashdir]# myfunc3(){\n> echo ${13}\n> }\n[root@layne bashdir]# myfunc3 1 2 3 4 5 6 7 8 9 a b c d\nd\n```\n\n可以看到，`$n`为函数的第n个位置变量（第n个参数），`${13}`是函数的第13个参数，不能写为`$13`\n\n再来看一个例子，创建文件mysh1.h，内容如下：\n\n```bash\n#!/bin/bash\necho $1\necho $2\necho ${10}                     \n```\n\n执行结果：\n\n```bash\n[root@layne bashdir]# source mysh1.sh a b 1 2 3 4 5 6 7 8 9\na\nb\n8\n```\n\n### 3.5 特殊变量\n\n-\t`$#`：位置参数个数\n-\t`$*`：参数列表，所有的参数作为一个字符串，以空格隔开\n-\t`$@`：参数列表，双引号引用为单独的字符串，所有的参数作为单个的字符串，以空格隔开\n-\t`$$`：当前shell的PID\n-\t`$?`：上一个命令的退出状态\n- 0：成功\n- 其他：失败\n\n\n\n例子：创建mysh2.sh，内容如下：\n\n```bash\n#!/bin/bash\necho \"number of args:$#\"\necho \"string of args:$*\"\necho \"args:$@\"\necho \"current pid:$$\"\npstree -p\n```\n\n\n\n执行结果：\n\n```bash\n[root@layne bashdir]# chmod +x mysh2.sh\n[root@layne bashdir]# ./mysh2.sh a b 1 2 3 4 5  #创建子shell进程执行\nnumber of args:7\nstring of args:a b 1 2 3 4 5\nargs:a b 1 2 3 4 5\ncurrent pid:2033\ninit(1)─┬─auditd(899)───{auditd}(900)\n        ├─crond(1312)\n        ├─mingetty(1338)\n        ├─mingetty(1340)\n        ├─mingetty(1342)\n        ├─mingetty(1344)\n        ├─mingetty(1346)\n        ├─mingetty(1348)\n        ├─ntpd(973)\n        ├─rsyslogd(915)─┬─{rsyslogd}(917)\n        │               ├─{rsyslogd}(919)\n        │               └─{rsyslogd}(920)\n        ├─sshd(965)───sshd(1945)───bash(1947)───mysh2.sh(2033)───pstree(2034)\n        └─udevd(354)───udevd(640)\n[root@layne bashdir]# source mysh2.sh a b 1 2 3 4 5 #在当前shell执行\nnumber of args:7\nstring of args:a b 1 2 3 4 5\nargs:a b 1 2 3 4 5\ncurrent pid:1947\ninit(1)─┬─auditd(899)───{auditd}(900)\n        ├─crond(1312)\n        ├─mingetty(1338)\n        ├─mingetty(1340)\n        ├─mingetty(1342)\n        ├─mingetty(1344)\n        ├─mingetty(1346)\n        ├─mingetty(1348)\n        ├─ntpd(973)\n        ├─rsyslogd(915)─┬─{rsyslogd}(917)\n        │               ├─{rsyslogd}(919)\n        │               └─{rsyslogd}(920)\n        ├─sshd(965)───sshd(1945)───bash(1947)───pstree(2036)\n        └─udevd(354)───udevd(640)\n\n```\n\n\n\n\n\n**`$?`用于获取上一个命令的退出状态**\n\n```bash\n[root@layne bashdir]# mydir=/root\n[root@layne bashdir]# myfile=mysh2.sh\n[root@layne bashdir]# [ -d $mydir ] && echo \"$mydir is dir\"  # [ -d $mydir ]用于测试该变量是否是一个目录\n/root is dir\n[root@layne bashdir]# [ -d $myfile ] && echo \"$mydir is dir\"\n[root@layne bashdir]# [ -d $mydir ]\n[root@layne bashdir]# echo $?\n0  # 0相当于成立\n[root@layne bashdir]# [ -d $myfile ]\n[root@layne bashdir]# echo $?\n1\n```\n\n\n\n### 3.6 数组\n\nBash提供了一维数组变量。任何变量都可以作为一个数组；内建命令 declare 可以显式地定义数组。数组的**大小没有上限（初始创建多大的数组，可以超过这个大小继续往后赋值）**，也没有限制在连续对成员引用和赋值时有什么要求。数组以整数为下标，从0开始。\n\n如果变量赋值时使用语法 `name[subscript]=value`，那么就会自动创建数组。 subscript 被当作一个算术表达式，结果必须是大于等于 0 的值。数组赋值可以使用复合赋值的方式，形式是 `name=(value1 value2 ... valuen)`，这里每个 value 的形式都是[subscript]=string。string 必须出现。如果出现了可选的括号和下标，将为这个下标赋值，否则被 赋值的元素的下标是语句中上一次赋值的下标加一。下标从 0 开始。这个语法也被内建命令 declare 所接受。单独的数组元素可以用上面介绍的语法 `name[subscript]=value`  来赋值 \n\n数组的任何元素都可以用 `${name[subscript]}` 来引用。花括号是必须的，以避免和路径扩展冲突。如果subscript 是 `@` 或是 `*`，它扩展为 name 的所有成员。这两种下标只有在双引号中才不同。在双引号中，`${name[*]}` 扩展为一个词，**由所有数组成员的值组成**，用特殊变量 IFS 的 第  一 个 字 符 分 隔；`${name[@]}`将 name 的**每个成员**扩展为一个词。如果数组没有成员，`${name[@]}` 扩展为空串。这种不同类似于特殊参数 `* `和` @` 的扩展 (参见上面的 Special  Parameters 段落)。`${`&#35;name[subscript]} 扩展为 `${name[subscript]}` 的长度。如果 subscript 是  `*` 或者是 `@`，扩展结果是数组中元素的个数。**引用没有下标数组变量等价于引用元素 0。**       \n\n**内建命令 unset 用于销毁数组**。`unset name[subscript]` 将销毁下标是 subscript 的 元 素 。 `unset name`, 这里 name 是一个数组，或者 `unset name[subscript]`, 这里 subscript 是  `*` 或者是 `@`，将销毁整个数组。         内建命令 declare,  local, 和 readonly 都能接受` -a` 选项，从而指定一个数组。内建命令 read 可 以接受 `-a` 选项，从标准输入读入一列词来为数组赋值。内建命令 set 和  declare 使用一种可以重用为输入的格式来显示数组元素。  \n\n```bash\n[root@layne bashdir]# sxt=(a  b  c)  #数组\n[root@layne bashdir]# echo  $sxt\na\n[root@layne bashdir]# echo  ${sxt[1]}\nb\n[root@layne bashdir]# echo  ${sxt[*]}\na b c\n[root@layne bashdir]# echo  ${sxt[@]}\na b c\n```\n\n注意：`echo $sxt[1] ` 是错误的写法\n\n\n\n### 3.7 管道\n\n直接看例子：\n\n```bash\n[root@layne bashdir]# a=9\n[root@layne bashdir]# echo $a\n9\n[root@layne bashdir]# b=22 | echo ok #启动子进程给b赋值22\nok\n[root@layne bashdir]# echo $b #访问不到子进程的数据，父进程访问不了子进程的结果\n```\n\n**管道两边的命令在当前shell的两个子进程中执行。**\n\n\n\n### 3.8 $$和$PASHPID\n\n**`$$`和`$BASHPID`的区别**\n\n- `$$`是在哪个进程中执行命令，该值就是哪个shell进程的PID\n- `echo \"hello\"| echo $BASHPID` 这里的id是前面执行`echo \"hello\"`的shell的id，而`echo $$` 是当前shell的id\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209185227.png)\n\n另外，我们创建一个test.sh再来测试一下，test.sh内容如下：\n\n```bash\n#!/bin/bash\necho \"\\$\\$ value:$$\"\necho \"BASHPID value:$BASHPID\"\npstree -p\n```\n\n执行：\n\n```bash\n[root@layne bashdir]# chmod +x test.sh\n[root@layne bashdir]# ./test.sh\n$$ value:2208\nBASHPID value:2208\ninit(1)─┬─auditd(899)───{auditd}(900)\n        ├─crond(1312)\n        ├─mingetty(1338)\n        ├─mingetty(1340)\n        ├─mingetty(1342)\n        ├─mingetty(1344)\n        ├─mingetty(1346)\n        ├─mingetty(1348)\n        ├─ntpd(973)\n        ├─rsyslogd(915)─┬─{rsyslogd}(917)\n        │               ├─{rsyslogd}(919)\n        │               └─{rsyslogd}(920)\n        ├─sshd(965)───sshd(1945)───bash(1947)───test.sh(2208)───pstree(2209)\n        └─udevd(354)───udevd(640)\n\n```\n\n可以看到，在子进程执行的时候，`$$`和`$BASHPID` 输出的都是当前进程的ID\n\n另外，再创建test1.sh，内容如下：\n\n```bash\n#!/bin/bash\n\necho \"\\$\\$ outside of subshell = $$\"                              # 9602\necho \"\\$BASH_SUBSHELL  outside of subshell = $BASH_SUBSHELL\"      # 0\necho \"\\$BASHPID outside of subshell = $BASHPID\"                   # 9602\n\necho\n\n( echo \"\\$\\$ inside of subshell = $$\"                             # 9602\n  echo \"\\$BASH_SUBSHELL inside of subshell = $BASH_SUBSHELL\"      # 1\n  echo \"\\$BASHPID inside of subshell = $BASHPID\" )                # 9603\n  # Note that $$ returns PID of parent process.\n```\n\n执行：\n\n```bash\n[root@layne bashdir]# chmod +x test1.sh\n[root@layne bashdir]# ./test1.sh\n$$ outside of subshell = 2214\n$BASH_SUBSHELL  outside of subshell = 0\n$BASHPID outside of subshell = 2214\n\n$$ inside of subshell = 2214\n$BASH_SUBSHELL inside of subshell = 1\n$BASHPID inside of subshell = 2215\n```\n\n我的理解： `$$`表示脚本文件在其下运行的进程ID。对于任何给定的脚本，当它运行时，**它将只有一个“主”进程标识**。不管您调用了多少个子shell，`$$`将始终返回与脚本关联的第一个进程标识。 `$BASHPID`将显示当前bash实例的进程ID，因此在子shell中它将与可能调用它的**“顶级”**bash不同。`$BASH_SUBSHELL` 表示你所在的“subshell级别”。如果你不在任何子级别，则级别为零。如果你在主程序中启动子shell，则该子shell级别为1。如果在该子shell内启动子shell，则级别为2，依此类推。\n\n\n\n## 4. bash的基本语法\n\n### 4.1 单引号和双引号\n\n单引号将其中的内容都作为了字符串来，忽略所有的命令和特殊字符，类似于一个字符串的用法\n\n双引号与单引号的区别在于其可以包含特殊字符（单引号直接输出内部字符串，不解析特殊字符；双引号内则会解析特殊字符），包括`', \", $, \\`，如果要忽略特殊字符，就可以利用`\\`来转义，忽略特殊字符，作为普通字符输出：\n\n```bash\n[root@layne bashdir]# var=3\n[root@layne bashdir]# echo '$var'\n$var\n[root@layne bashdir]# echo \"$var\"\n3\n[root@layne bashdir]# echo \"Here \\\"this is a string\\\" is a string\"\nHere \"this is a string\" is a string\n```\n\n### 4.2 命令替换\n\n命令替换允许我们**将shell命令的输出赋值给变量**。它是脚本编程中的一个主要部分。\n\n命令替换会创建子shell进程来运行相应的命令。子shell是由运行该脚本的shell所创建出来的一个独立的子进程，由该子进程执行的命令无法使用（父）脚本中所创建的变量（除非是export的环境变量）。\n\n反引号提升扩展优先级，**先执行反引号的内容，再执行其他的**。\n\n```bash\n[root@layne bashdir]# myvar=echo \"hello\"\n-bash: hello: command not found\n[root@layne bashdir]# myvar=`echo \"hello\"` #使用反引号，先创建子shell执行echo \"hello\"，再将执行的结果赋给myvar\n[root@layne bashdir]# echo $myvar\nhello\n```\n\n再来看一个例子：\n\n```bash\n[root@layne tdir]# scp /root/tdir/test.txt  layne2:/root/tdir #把当前主机的test.txt文件copy到layne2主机的同一个目录下，前提是layne2的/root/tdir目录存在\nroot@layne2's password: \ntest.txt                                                                                   100%   20     0.0KB/s   00:00    \n[root@layne tdir]# scp /root/tdir/log.txt layne2:`pwd` # 作用同上，相当于先输入pwd，把pwd的返回结果赋值给`pwd`\nroot@layne2's password: \nlog.txt                                                                                    100%   12     0.0KB/s   00:00    \n[root@layne tdir]# \n```\n\n\n\n### 4.3 逻辑判断\n\n- command1 && command2\n  - n 如果command1退出状态是0（0代表正确），则执行command2\n- command1  ||  command2\n  - n 如果command1的退出状态不是0，则执行command2\n\n```bash\n[root@layne2 ~]# test -d /hello || echo \"文件夹/hello不存在\"\n文件夹/hello不存在\n[root@layne2 ~]# test -d /bin && echo \"文件夹/bin存在\"\n文件夹/bin存在\n[root@layne2 ~]# test -f a.txt && rm -f a.txt && touch a.txt #-f 判断是不是文件\n[root@layne2 ~]# ls / && echo ok\nbin  boot  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  sbin  selinux  srv  sys  tmp  usr  var\nok\n[root@layne2 ~]# ls / || echo ok\nbin  boot  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  sbin  selinux  srv  sys  tmp  usr  var\n```\n\n\n\n### 4.4 表达式\n\n**（1）算术表达式**\n\n- let  算数运算表达式\n  - `let  C=$A+$B`    （很重要）\n- $[算术表达式]\n  - `C=$[$A+$B]`\n- $((算术表达式))\n  - `C=$((A+B))`\n  - `C=$((A+B+1))`   #在原来结果的基础上再加1\n- expr算术表达式\n  - 表达式中各操作数及运算符之间要有空格，同时要使用命令引用\n  - ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210209223824.png)\n\n```bash\n[root@layne2 ~]# a=1\n[root@layne2 ~]# b=2\n[root@layne2 ~]# let c=$a+$b\n[root@layne2 ~]# echo $c\n3\n[root@layne2 ~]# d=$((a+b+1))\n[root@layne2 ~]# echo $d\n4\n[root@layne2 ~]# ((a++))\n[root@layne2 ~]# echo $a\n2\n[root@layne2 ~]# e=$((--d+a))\n[root@layne2 ~]# echo $e\n5\n```\n\n\n\n**（2）条件表达式**\n\n-\t[  表达式  ]\n-\ttest  表达式\n-\t[[  表达式  ]]\n\n```bash\ntest  3  -gt  2  &&  echo  ok\n等价于\n[  3  -gt  2  ]  &&  echo  ok\n\ntest  3  -gt  8  &&  echo  ok\n等价于\n[  3  -gt  8  ]  &&  echo  ok\n```\n\n\n\n## 5. bash的分支\n\n-\tif\n-\tcase\n\n\n\n### 5.1 if分支\n\n**单分支结构**\n\n```tex\nif [ 条件判断 ] \n then\n    //命令\nfi\n\n或者\n\nif [ 条件判断 ]; then \n 条件成立执行，命令;\nfi # 将if反过来写,就成为fi，结束if语句 \n```\n\n **双分支结构**\n\n```bash\nif [ 条件1 ];then \n 条件1成立执行，指令集1\nelse \n 条件1不成执行指令集2; \n fi\n```\n\n **多分支结构**\n\n```bash\nif [ 条件1 ];then\n 条件1成立，执行指令集1\nelif [ 条件2 ];then\n 条件2成立，执行指令集2\nelse\n 条件都不成立，执行指令集3 \n fi\n```\n\n**使用[命令判断**\n\n```bash\nif [ 3 -gt 2 ]; then\n  echo  ok \nfi\n\nif [ 3 -gt 2 ]\nthen\n  echo  ok \nfi\n```\n\n示例：\n\n创建sh01.sh，内容为：\n\n```bash\n#!/bin/bash\na=20\nif [ $a -gt $1 ];\nthen\n  echo \"你输入的数字太小\"\nelif [ $a -eq $1 ];\nthen\n  echo \"恭喜哈，数字相等\"\nelse\n  echo \"你输入的数字太大\"\nfi \n```\n\n执行：\n\n```bash\n[root@layne bashdir]# chmod +x sh01.sh\n你输入的数字太大\n[root@layne bashdir]# ./sh01.sh 20\n恭喜哈，数字相等\n[root@layne bashdir]# ./sh01.sh 15\n你输入的数字太小\n[root@layne bashdir]# ./sh01.sh 25\n你输入的数字太大\n[root@layne bashdir]# ./sh01.sh\n./sh01.sh: line 3: [: 20: unary operator expected\n./sh01.sh: line 6: [: 20: unary operator expected\n你输入的数字太大\n```\n\n注意：`if`和中括号之间要有空格，中括号和条件表达式之间要有空格\n\n`;`和then之前有空格没空格都可以\n\n\n\n### 5.2 case分支\n\n```bash\ncase $变量名称 in “值1\") \n  程序段1\n  ;;\n“值2\") \n  程序段2\n  ;;\n*)\nexit 1\n  ;;\nesac # esac是case的逆置，代表case的结尾\n\n```\n\n上面`*`类似java里面的default\n\n`exit 1` 是`echo $?`获取到的返回值，`exit 0`代表正确的返回，`exit 1`代表错误的返回结果\n\n\n\n案例：判断用户输入的是哪个数，1-7显示输入的数字，1显示 Mon,2 :Tue,3:Wed,4:Thu,5:Fir,6-7:weekend,其它值的时候，提示：please input [1,7]，该如何实现？\n\n创建sh02.sh，内容为：\n\n```bash\n#!/bin/bash\nread -p \"please input a number[1,7]:\" num\ncase $num in\n1)\n    echo \"Mon\"\n;;\n2)\n    echo \"Tue\"\n;;\n3)\n    echo \"Wed\"\n;;\n4)\n    echo \"Thu\"\n;;\n5)\n    echo \"Fir\"\n;;\n[6-7])\n    echo \"weekend\"\n;;\n*)\n    echo \"please input [1,7]\"\n;;\n\nesac\n\n或者\n#!/bin/bash\n# Mon,2 :Tue,3:Wed,4:Thu,5:Fir,6-7:weekend\ncase $1 in 1)\n   echo \"Mon\"\n   exit 0\n;;\n2)\n   echo \"Tue\"\n   exit 0\n;;\n3)\n   echo \"Wed\"\n   exit 0\n;;\n4)\n   echo \"Thu\"\n   exit 0\n;;\n5)\n   echo \"Fir\"\n   exit 0\n;;\n[6-7])\n   echo \"weekend\"\n   exit 0\n;;\n*)\n   echo \"please input [1,7]\"\n   exit 1\n;;\n\nesac\n```\n\n\n\n执行：\n\n```bash\n[root@layne bashdir]# chmod +x sh02.sh\n[root@layne bashdir]# ./sh02.sh\nplease input a number[1,7]:5\nFir\n[root@layne bashdir]# ./sh02.sh\nplease input a number[1,7]:8\nplease input [1,7]\n```\n\n\n\n## 6. bash的循环\n\n### 6.1 while循环\n\n```bash\nwhile [ condition ] ; do # 如果do换一行就不要;了\n  命令\ndone\n或者\nwhile [ condition ] \n do\n  命令\ndone\n```\n\n注意：while也与中括号之间有空格\n\n案例一：每隔两秒打印系统负载情况，如何实现？\n\n创建while1.sh，内容如下：\n\n```bash\n#!/bin/bash\nwhile true\n do\n   uptime   #显示系统负载情况\n   sleep 2 #休眠2秒\n done\n```\n\n执行：\n\n```bash\n[root@layne bashdir]# chmod +x while1.sh\n[root@layne bashdir]# ./while1.sh\n 23:05:07 up 12:56,  1 user,  load average: 0.00, 0.00, 0.00\n 23:05:09 up 12:56,  1 user,  load average: 0.00, 0.00, 0.00\n 23:05:11 up 12:56,  1 user,  load average: 0.00, 0.00, 0.00\n # 按ctrl +c 终止进程\n```\n\n案例二：使用while循环，编写shell脚本，计算1+2+3+…+100的和并输出，如何实现？\n\n创建while2.sh，内容如下：\n\n```bash\n#!/bin/bash\nsum=0\ni=1\nwhile [ $i -le 100 ]  #while和[之间要加一个空格  true则执行\ndo\n  sum=$((sum+i))\n  i=$((i+1)) #运算结果为变量赋值可以使用$(( … ))\ndone\necho \"the result of '1+2+3+...+100' is $sum\"\n\n或者：\n\n#!/bin/bash\nsum=0\ni=1\nwhile [ $i -le 100 ]  #while和[之间要加一个空格  true则执行\ndo\n  let sum=$sum+$i\n  let i=$i+1 #运算结果为变量赋值可以使用$(( … )) 或let i++ 或 let i+=1\ndone\necho \"the result of '1+2+3+...+100' is $sum\"\n```\n\n\n\n### 6.2 for循环\n\n```bash\nfor 变量名 in 变量取值列表\ndo\n  命令\ndone\n```\n\n案例1：创建for1.sh，输入以下内容\n\n```bash\n#!/bin/sh\nfor num in 1 2 3 4 \ndo \n  echo $num\ndone\n```\n\n执行：\n\n```bash\n[root@layne bashdir]# chmod +x for1.sh\n[root@layne bashdir]# ./for1.sh\n1\n2\n3\n4\n```\n\n**使用大括号的方法：**\n\n```bash\n[root@layne bashdir]# echo {1..8}\n1 2 3 4 5 6 7 8\n[root@layne bashdir]# echo {a..z}\na b c d e f g h i j k l m n o p q r s t u v w x y z\n[root@layne bashdir]# echo 10.13.20.{1..3}\n10.13.20.1 10.13.20.2 10.13.20.3\n```\n\n编辑for1.sh，输入以下内容：\n\n```bash\n#!/bin/sh\nfor num in {1..4} \ndo \n  echo $num\ndone\n```\n\n\n\n案例2：使用`seq –s 分隔符 起始 步长 终点 `\n\n```bash\n[root@layne bashdir]# seq -s \" \" 2 2 100\n2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100\n[root@layne bashdir]# seq -s \" \" 1 1 5\n1 2 3 4 5\n```\n\n创建for2.sh ，内容为：\n\n```bash\n#!/bin/sh\nfor num in `seq -s \" \" 1 1 5`\ndo\n  echo $num\ndone\n```\n\n**注意：for后面\\`\\`里面是执行的命令**\n\n执行for2.sh：\n\n```bash\n[root@layne bashdir]# chmod +x for2.sh\n[root@layne bashdir]# ./for2.sh\n1\n2\n3\n4\n5\n```\n\n\n\n## 7. bash练习\n\n### 7.1 案例1\n\n-\t用户给定路径\n-\t输出文件大小最大的文件\n-\t递归子目录\n\n`$IFS`默认空格、制表符和换行符 都可以识别\n\nfor循环的时候 空格、制表符和换行符都可以识别（在不改`$IFS`情况下），如果要改变`$IFS`，让它识别这三个中的一个，比如让它只识别换行符，把`IFS=$'\\n'`，这样一行一个循环，即一次一行来循环操作，不用管一行里面的空格或制表符。\n\n`du -a` 列出所有的文件与目录\n\n```bash\n[root@layne bashdir]# du #不用-a，不会列出所有的\n32\t./rdir\n120\t.\n[root@layne bashdir]# du -a\n4\t./rdir/a.sh\n4\t./rdir/test.txt\n32\t./rdir\n4\t./mylog2.log\n4\t./log.err\n4\t./mylog.log\n4\t./mylog3.log\n4\t./nohup.out\n4\t./test.sh\n4\t./my.log\n4\t./sh01.sh\n...\n```\n\n步骤一：\n\n思路：使用du命令加-a遍历用户指定目录（`$1`获取）的所有文件，使用管道将结果传递给sort，让sort使用数值序倒序排序，依次输出各个条目。\n\n```bash\n#!/bin/bash\noldIFS=$IFS\nIFS=$'\\n'  # 将for循环获取不同元素的标记修改为换行符\n# for循环获取元素的时候使用空格区分各个不同的元素\nfor  item  in  `du  -a  $1 |  sort   -nr`; do\n  echo  $item\ndone\nIFS=$oldIFS    #用完后重置IFS变量的值。\n```\n\n- `$1`是第一个参数\n- `sort  -nr` ：因为du输出的两列，第一列是大小，第二列是目录或文件，中间用制表符（\\t）隔开，sort可以默认识别这种写法，并默把两列分开后用第一列排序，这里是缩写，全称为`sort -t '\\t' -k 1 -nr sort.txt`\n\n**因为要获取最大的文件，需要改进**\n\n```bash\n#!/bin/bash\noldIFS=$IFS\nIFS=$'\\n'  # 将for循环获取不同元素的标记修改为换行符\n# for循环获取元素的时候使用空格区分各个不同的元素\nfor  item  in  `du -a  $1 |  sort -nr`; do\n  fileName=`echo  $item | awk '{print  $2}'`\n  if  [  -f  $fileName  ]; then\n      echo  $fileName\n      break\n  fi\ndone\nIFS=$oldIFS    #用完后重置IFS变量的值。\n```\n\n- awk默认也是识别制表符，换行符，空格等\n- **`awk '{print $2}'`** **这是取第二列，不是文件的第二个参数**\n\n\n\n### 7.2 案例2\n\n问题：\n\n​    循环遍历文件每一行：流程控制语句 IFS\n\n- 定义一个计数器\n- 打印num正好是文件行数\n\n思路：\n-\t管道\n-\t重定向\n-\t命令替换\n\n\n\n**方案一**：\n\n思路：定义一个计数器，使用for循环从文件内容按行获取元素，每个元素是一行，在for循环中递增计数器用于计行数\n\n```bash\n#!/bin/bash\nnum=0\noldIFS=$IFS\nIFS=$'\\n'\n# hello world are you ok\nfor  item  in  `cat  $1`; do\n  echo $item\n  ((num++))  # 或$((num++)) 或 let num++\ndone\necho \"line number is :$num\"\nIFS=$oldIFS\n```\n\n解释：\n\n```bash\nfor item in `cat $1`; do\n```\n\n这里for循环可以识别换行符，所以可以遍历（即一行循环一次）\n\n**方案二：**\n\n思路：先使用**wc数出总行数**，然后使用带下标的for循环分页遍历该文件，打印出每行，最后打印出总行数。\n\n注意：\n\n1、`cat $1 | wc -l `：返回行数\n\n2、如果for循环想要java的写法，则可以在for后面用两个`((`包起来 ，如`for ((i=1;i<=lines;i++))`\n\n```bash\n#!/bin/bash\nnum=0\nlines=`cat $1 | wc  -l`\nhello=$1\nfor  ((i=1;i<=lines;i++)); do\n  line=`head  -$i  $hello |  tail  -1`\n  echo $line\n  ((num++))\ndone\necho \"line number is :$num\"\n```\n\n\n\n**方案三：**\n\n思路：将while的read标准输入重定向到文件file.txt，按行读取，每读一行，就打读到的行记录，同时计数器+1，最后得出总行数。\n\n```bash\n#!/bin/bash\nnum=0\nwhile  read  line  ;do\n   echo  $line\n   ((num++))\ndone  <  $1\necho  \"line number is : $num\"\n```\n\n解释：`>>`和`>`都属于输出重定向，`<`属于输入重定向\n\n\n\n**while read line [linux] shell 学习**可参考：https://blog.csdn.net/qq_22083251/article/details/80484176\n\n写的不错。\n\n\n\n**方案四：**\n\n使用管道命令，**但是通过管道无法向父进程传递数据**（**管道两边的命令在当前shell的两个子进程中执行**），需要将结果数据写到文件中，运行结束后，将数据读出来即可\n\n```bash\n#!/bin/bash\nnum=0    # 该num和管道后面的num不是一回事，因为num没有export\ncat   $1  |  while  read  line; do\n  echo  $line\n  ((num++))    #即使当前环境没有num这个变量，((num++))一样可以使用\n  done\necho  \"line number is: $num\"  > $2 #将结果重定向到一个文件\n```\n\n\n","tags":["Linux"],"categories":["Linux"]},{"title":"Linux系统中文件的解压缩操作","url":"/2021/02/10/111324/","content":"\n**压缩**：指通过某些算法，将文件尺寸进行相应的缩小，同时不损失文件的内容。\n\n**打包**：指将多个文件（或目录）合并成一个文件，方便传递或部署。\n<!-- more -->\n\n\n\n\n\n压缩文件或打包文件常见的扩展名： \\*.tar.gz, \\*.tar.bz2；linux系统一般文件的扩展名用途不大，但是压缩或打包文件的扩展名是必须的，因为linux支持的压缩命令较多，不同的压缩技术使用的压缩算法区别较大，根据扩展名能够使用对应的解压算法。\n\n常见文件扩展名：\n\n- \\*.tar.gz tar程序打包的文件，并且经过 gzip 的压缩 \n- \\*.tar.bz2 tar程序打包的文件，并且经过 bzip2 的压缩\n\n**tar**命令，选项与参数：\n\n-c :建立打包文件,\n\n-t :查看打包文件的内容含有哪些文件 \n\n-x :解打包或解压缩的功能,可以搭配-C(大写)在特定到特定目录解开 \n\n-j :通过bzip2的支持进行压缩/解压缩:此时文件最好为 *.tar.bz2 \n\n-z :通过gzip的支持进行压缩/解压缩:此时文件最好为 *.tar.gz \n\n-v :在压缩/解压缩的过程中,将正在处理的文件名显示出来\n\n-f filename:-f 后面跟处理后文件的全名称（路径+文件名+后缀名） \n\n-C 目录:这个选项用在解压缩,若要在特定目录解压缩,可以使用这个 选项\n\n-p :保留备份数据的原本权限与属性,常用于备份(-c)重要的配置文件\n\n**注意** -c, -t, -x 不可同时出现在一串指令列中 \n\n\n\n**tar常用的指令组合**\n\n第一步确定压缩算法即z还是j，第二步确定c（压缩）还是x（解压），第三步v查看过程，第四步跟f。\n\n（1）**打包与压缩：**\n\n```bash\ntar -zcv -f [/路径/]filename.tar.gz 被压缩的文件或目录 \n即 tar -zcvf [/路径/]filename.tar.gz 被压缩的文件或目录\ntar -jcv -f [/路径/]filename.tar.bz2 被压缩的文件或目录\n即 tar -jcvf [/路径/]filename.tar.bz2 被压缩的文件或目录\n```\n\n练习：①将/etc目录下的所有文件打包并压缩至当前目录下\n\n```bash\ntar -zcvf ./a.tar.gz /etc\n```\n\n②将/etc目录下的所有文件打包并压缩至/root/etc01.tar.gz\n\n注意：压缩时目录一定要存在，才能压缩成功，否则要先创建目录。\n\n```bash\ntar -zcvf /root/etc01.tar.gz /etc\n```\n\n③ 将/etc目录下的所有文件打包并压缩至/root/etc02tar.bz2\n\n```bash\ntar -jcvf /root/etc02tar.bz2 /etc\n```\n\n**bz2压缩方式文件更小，但是压缩速度较慢**\n\n```bash\n[root@layne testdir]# ll /root | grep etc0\n-rw-r--r--  1 root root 8493768 Feb  8 21:43 etc01.tar.gz\n-rw-r--r--  1 root root 7467102 Feb  8 21:43 etc02tar.bz2\n```\n\n（2）**备份**：(保留之前的权限)\n\n```bash\ntar -zpcv -f  [/路径/]filename.tar.gz  被备份文件或目录\ntar -jpcv -f  [/路径/]filename.tar.bz2  被备份文件或目录\n```\n\n**（3）解压到当前目录**\n\n```bash\ntar -zxv -f [/路径/]filename.tar.gz\n即 tar -zxvf [/路径/]filename.tar.gz\ntar -jxvf [/路径/]filename.tar.bz2\n即 tar -jxvf [/路径/]filename.tar.bz2\n```\n\n案例：将/root/etc01.tar.gz解压到当前目录下：\n\n`tar -zxvf /root/etc01.tar.gz`\n\n**（4）解压到指定目录**\n\n```bash\ntar -zxv -f [/路径/]filename.tar.gz  -C 指定目录\n即 tar -zxvf [/路径/]filename.tar.gz  -C 指定目录\ntar -jxv  -f [/路径/]filename.tar.bz2 -C  指定目录\n即 tar -jxvf [/路径/]filename.tar.bz2 -C  指定目录\n```\n\n案例：解压/root/etc01.tar.gz解压到当/root/my/目录下：\n\n`tar -zxvf /root/etc01.tar.gz  -C /root/my/`","tags":["Linux"],"categories":["Linux"]},{"title":"Linux系统Shell基本命令","url":"/2021/02/10/111016/","content":"\n下面是经常使用到的Linux系统Shell基本命令，这里汇总一下，便于以后查阅。\n<!-- more -->\n\n\n\n@[TOC](文章目录)\n\n\n\n## 1. 命令入门\n\n **命令提示符详解**\n\n进入Linux系统，通过root用户登录，会出现`[root@localhost ~]# `，含义为：\n\n- 用户名root\n- 主机名localhost\n- 当前用户目录`~`\n- `#`为系统权限\n\n而普通用户`[lucy@localhost ~]$`中的`$`为普通权限。\n\n`~`为当前用户目录，在别的目录下只会显示目录名。\n\nroot为超级管理员，拥有一切系统权限，可通过`cd ~`进入root用户主目录下，普通用户lucy，可通过`~lucy`进入其主目录下。 \n\n\n\n**命令格式**：\n\n`命令 选项 参数 (三者之间要有空格，区分大小写)`   即 `command [-options] [args]`\n\n`[args]`为参，多个参数之间用空格分隔。\n\n\n\n## 2. help命令\n\n### 2.1 help\n\n使用help查看内建命令的帮助(enable查看内建命令)\n\n```bash\n[root@layne ~]# help -s cd\ncd: cd [-L|-P] [dir]\n[root@layne ~]# help -d cd\ncd - Change the shell working directory.\n[root@layne ~]# help -m cd\nNAME\n    cd - Change the shell working directory.\n\nSYNOPSIS\n    cd [-L|-P] [dir]\n\nDESCRIPTION\n    Change the shell working directory.\n    \n    Change the current directory to DIR.  The default DIR is the value of the\n    HOME shell variable.\n    \n    The variable CDPATH defines the search path for the directory containing\n    DIR.  Alternative directory names in CDPATH are separated by a colon (:).\n    A null directory name is the same as the current directory.  If DIR begins\n    with a slash (/), then CDPATH is not used.\n    ...\n```\n\n但是help只能查看内建命令，比如我想查看mv的帮助信息就不行：\n\n```bash\n[root@layne ~]# help mv\n-bash: help: no help topics match `mv'.  Try `help help' or `man -k mv' or `info mv'.\n```\n\n那么我们怎么知道哪些是**内建命令**呢？\n\n```bash\n[root@layne ~]# enable\nenable .\nenable :\nenable [\nenable alias\nenable bg\nenable bind\nenable break\nenable builtin\nenable caller\nenable cd\nenable command\nenable compgen\nenable complete\nenable compopt\nenable continue\nenable declare\nenable dirs\nenable disown\nenable echo\nenable enable\nenable eval\nenable exec\nenable exit\nenable export\nenable false\nenable fc\nenable fg\nenable getopts\nenable hash\nenable help\nenable history\nenable jobs\nenable kill\nenable let\nenable local\nenable logout\nenable mapfile\nenable popd\nenable printf\nenable pushd\nenable pwd\nenable read\nenable readarray\nenable readonly\nenable return\nenable set\nenable shift\nenable shopt\nenable source\nenable suspend\nenable test\nenable times\nenable trap\nenable true\nenable type\nenable typeset\nenable ulimit\nenable umask\nenable unalias\nenable unset\nenable wait\n```\n\nenable也不用背，该方式比较费时间，help无法使用时，根据提示换其他帮助方式就ok了。\n\n### 2.2 --help\n\n如输入`mv --help`可查看mv的命令介绍。\n\n\n\n## 3. man命令\n\n操作快捷键：\n\n- enter向下一行\n- 空格按页向下翻，b向上翻页 或 **`ctrl +f`向下翻页，`ctrl+b`向上翻页**\n- p直接翻到首页\n- 查找按 /要查找的内容，查找 下一个/上一个：**按n/N**（即Shift +n）；\n- 退出按q\n\n\n\nman：\n\n-\t1用户命令（/bin，/usr/bin，/usr/local/bin）\n-\t2系统调用\n-\t3库函数\n-\t4特殊文件（设备文件）\n-\t5文件格式（配置文件的语法）\n-\t6游戏\n-\t7杂项（Miscellaneous）\n-\t8管理命令（/sbin，/usr/sbin，/usr/local/sbin）\n\n自己尝试以下 `man 2 read`和`man read`的区别。\n\n\n\n## 4. echo\n\n- `echo $x`输出变量x\n- `echo \"hello\"`输出字符串\n- `echo \"hello layne\" > log.txt` 将输出的内容写入到log.txt，如果文件不存在，则会直接创建文件并写入内容。\n- `echo \"hello layne123456\" >> log.txt`将输出的内容**追加到**log.txt。\n- `echo  -n`表示不要另起新行，如输入`echo -n \"hello world\"`\n  ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207115454.png)\n- `-e`表示解释逃逸字符，如输入`echo -e \"hello \\nworld\"`\n  ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207115627.png)\n\n\n\n## 5. test\n\n几个算术运算符：\n\n- gt表示greater than，即大于\n- lt表示less than，即小于\n- eq表示equal，即等于\n- ge表示大于等于\n- le表示小于等于\n\n命令：\n\n- `test 3 -gt 2` 测试3大于2\n- `echo $?` 返回上一条命令执行的结果（linux中0表示正确，1表示错误）\n  ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207120014.png)\n- `test 3 -ge 3 && echo 'yes'`表示3大于等于3成立，然后执行逻辑与，输出yes\n  ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207120053.png)\n- `test 3 -ge 5 || echo 'no'` 表示3大于等于2不成立，然后执行逻辑或，输出no\n  ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207120241.png)\n\n**除了test，还可以用[ ]**\n\n```bash\n[root@node1 ~]# [ 3 -gt 2 ]\n[root@node1 ~]# echo $?\n0\n[root@node1 ~]# [ 5 -lt 2 ]\n[root@node1 ~]# echo $?\n1\n[root@node1 ~]# [ 5 -lt 2]\n-bash: [: missing `]'\n[root@node1 ~]# [5 -lt 2 ]\n-bash: [5: command not found\n[root@layne ~]# [ 3 -gt 2 ] && echo 'yes'\nyes\n[root@layne ~]# mydir=/root \n[root@layne ~]# [ -d $mydir ] && echo \"$mydir is dir\"\n/root is dir\n```\n\n- **[]和内容之间一定要有空格，否则抛错。**\n- `[ -d $mydir ]` 测试mydir变量是否是一个目录\n\n\n\n\n\n**当在shell中执行命令的时候，默认到PATH指定的路径中查找可执行文件。**\n\n**如果在PATH中的多个目录都包含该可执行文件，则执行最先找到的。**\n\n输入`echo $PATH`可输出环境变量：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207120549.png)\n\n如果执行命令的时候，识别不到任何一个path路径下的命令，会提示命令不识别。\n\n\n\n## 6. type\n\ntype打印一个字符串的类型，可能的值为：别名，内置命令，函数，关键字，可执行文件或者什么都不输出（不是前面的类型）。\n\ntype命令可以识别外部命令和内部命令（内建命令）\n\n查看命令的type：\n\n```bash\n[root@layne ~]# type cd\ncd is a shell builtin\n[root@layne ~]# type cp\ncp is aliased to `cp -i'\n[root@layne ~]# type ls\nls is aliased to `ls --color=auto'\n[root@layne ~]# type ll\nll is aliased to `ls -l --color=auto'\n```\n\n别名也可以用alias命令查看，如：\n\n```bash\n[root@layne ~]# alias ll\nalias ll='ls -l --color=auto'\n```\n\n查看yum命令的类型：\n\n```bash\n[root@layne ~]# type yum\nyum is hashed (/usr/bin/yum)\n```\n\n**yum是外部命令**\n\n\n\n## 7. file\n\n**file命令：检查文件的类型。**\n\n```bash\n[root@layne ~]# file /usr/bin/yum\n/usr/bin/yum: a /usr/bin/python script text executable\n```\n\nyum命令是外部命令，它的文件是python脚本类型文件。\n\n```bash\n[root@layne ~]# file /bin/ls\n/bin/ls: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped\n```\n\nls是二进制可执行文件（ELF）。\n\n\n\n## 8. ls\n\nls命令用于列出目录内容\n\n- `ls -a /` 显示根目录下所有文件\n- `ls -l /` 以格式化的形式显示文件\n- `ls -a -l` 以格式化形式显示根目录下所有文件\n- `ls -ali /` 列出文件和目录的**inode信息**（相当于c语言的指针）\n- `ls / /usr`可以后面跟多个目录，列出多个目录的内容\n- `ll`相当于`ls -l --color=auto`\n- ls 等于ls ./\n\nls也支持通配符\n\n- `ls *log`\n- `ls install*`\n- `ls \\*tall\\*`\n\n\n\n## 9. pwd和history\n\n`pwd`打印当前工作目录路径\n\n`history`打印历史命令\n\n\n\n## 10. cd\n\ncd用于更改shell工作目录，也就是切换目录，cd全称是**change directory**\n\n**如果cd后什么都不写，表示直接回当前用户家目录**，在root下相当于`cd ~`。\n\n**cd后也可以跟减号（-）用以表示回到最后一次切换之前的目录**，多次使用减号在最近两个目录之间切换\n\nroot用户可以直接通过绝对路径进到普通用户的家目录，也可以直接跟波浪线用户名表示直接进入到某个用户的家目录\n\n```bash\ncd /home/god #切换到god家目录\ncd ~bjsxt #到用户bjsxt家中去\ncd ~ #到root家目录\n```\n\n超级管理员root的家目录在根目录下，即/root，普通用户的家目录在home下。\n\n\n\n## 11. ps\n\n**可以通过命令查看系统内进程信息**（查看当前运行的进程信息）\n\n进程查看命令`ps`(process status) ：将某个时间点的程序运作情况截取下来 \n\n**选项和参数 ：**\n\n- a ： 显示所有程序\n- x ：显示所有程序，不区分终端机\n- u ：以用户为主的格式来显示\n\n**常用组合 :**\n\n`ps aux` 观察系统所有的进程数据\n\n`ps -ef` 显示所有进程基本信息（比`aux`较简略一些）\n\n`ps -ef|grep mysql` 查看有关mysql的进程\n\n\n\n例如：\n\n```bash\n[root@layne ~]# ps aux\nUSER        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot          1  0.0  0.1  19232  1496 ?        Ss   10:15   0:03 /sbin/init\nroot          2  0.0  0.0      0     0 ?        S    10:15   0:00 [kthreadd]\nroot          3  0.0  0.0      0     0 ?        S    10:15   0:00 [migration/0]\nroot          4  0.0  0.0      0     0 ?        S    10:15   0:00 [ksoftirqd/0]\nroot          5  0.0  0.0      0     0 ?        S    10:15   0:00 [migration/0]\nroot          6  0.0  0.0      0     0 ?        S    10:15   0:00 [watchdog/0]\nroot       1333  0.0  0.0   4064   592 tty1     Ss+  10:15   0:00 /sbin/mingetty /dev/tty1\nroot       1335  0.0  0.0   4064   588 tty2     Ss+  10:15   0:00 /sbin/mingetty /dev/tty2\nroot       1337  0.0  0.0   4064   592 tty3     Ss+  10:15   0:00 /sbin/mingetty /dev/tty3\nroot       1339  0.0  0.0   4064   588 tty4     Ss+  10:15   0:00 /sbin/mingetty /dev/tty4\nroot       1341  0.0  0.0   4064   584 tty5     Ss+  10:15   0:00 /sbin/mingetty /dev/tty5\nroot       1343  0.0  0.0   4064   588 tty6     Ss+  10:15   0:00 /sbin/mingetty /dev/tty6\nroot       1346  0.0  0.1  10944  1032 ?        S<   10:15   0:00 /sbin/udevd -d\nroot       1347  0.0  0.1  10944  1032 ?        S<   10:15   0:00 /sbin/udevd -d\nroot       1417  0.0  0.5 144452  5072 pts/1    T    11:07   0:00 vim .bashrc\nroot       1751  0.0  0.1 110216  1092 pts/1    R+   13:35   0:00 ps aux\n```\n\n下为各选项的含义：\n\n- USER：该 process 属于那个使用者\n- PID ：该 process 的程序标识符。  \n- %CPU：该 process 使用掉的 CPU 资源百分比; \n- %MEM：该 process 所占用的物理内存百分比; \n- VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) \n- RSS ：该 process 占用的物理的内存量 (Kbytes) \n- TTY ：该 process 是在那个终端机上面运作,若与终端机无关则显示 `?`。另外，tty1-tty6 是本机上面的登入者程序，若为 pts/0 等，**则表示为由网络连接进主机的程序**。 \n- STAT：该进程目前的状态，状态显示与`ps -l`的 S 旗标相同 (R/S/D/T/Z) \n- START：该 process 被触发启动的时间\n- TIME :该 process 实际使用 CPU 运作的时间。 \n- COMMAND:该程序的实际命令\n\n另外，输入`ps -ef`如下：\n\n```bash\n[root@layne ~]# ps -ef\nUID         PID   PPID  C STIME TTY          TIME CMD\nroot          1      0  0 10:15 ?        00:00:03 /sbin/init\nroot          2      0  0 10:15 ?        00:00:00 [kthreadd]\nroot          3      2  0 10:15 ?        00:00:00 [migration/0]\nroot          4      2  0 10:15 ?        00:00:00 [ksoftirqd/0]\nroot          5      2  0 10:15 ?        00:00:00 [migration/0]\nroot          6      2  0 10:15 ?        00:00:00 [watchdog/0]\nroot          7      2  0 10:15 ?        00:00:11 [events/0]\n...\n```\n\n- STIME 该进程什么时候启动的\n- **PPID 为进程的父进程ID**\n\n\n\n## 12. pstree\n\nLinux pstree命令将所有行程以树状图显示\n\n- `ps`查看进程数\n\n\n\n## 13. mkdir\n\nmkdir用于创建目录\n\n- `mkdir a1 a2 a3` 创建目录a1、a2、a3\n- `mkdir -p a/b/c` 可以添加**-p**选项，用以创建多层目录，因为系统发现某一级目录不存在的时候创建父目录\n- `mkdir  ./abc/1dir ./abc/2dir  ./abc/3dir` 用于一次性创建多个目录\n\n可以使用大括号高效创建相似的目录\n\n```bash\nmkdir ./abc/{x,y,z}dir  （不连续的） #创建xdir、ydir、zdir目录\nmkdir ./abc/{x..z}dir    （连续的）  # 创建xdir、ydir、zdir目录\nmkdir b{1..3}  #创建b1、b2、b3目录\n```\n\n\n\ntouch用于创建文件，如`torch a.txt`创建文件a.txt\n\n`touch a.txt b.txt` 创建两个文件\n\n## 14、rm\n\n**删除之前一定要备份，打包加日期。除非确定以后真的不用了**\n\n\n\nrm用于**删除文件**\n\n- `rm a.txt` 删除文件a.txt，需要按y确认删除\n- `rm -f file`，删除文件，不需要确认强制删除\n\n**如果rm的参数是目录，则会提示需要迭代删除而不能成功**\n\n- `rm -r dira` 用于删除目录\n- `rm -rf dira` 删除目录，不要确认，**此命令慎用**\n\n\n\n尤其是 `rm -rf /`，这个命令直接删除整个Linux操作系统\n\n\n\n## 15. cp\n\ncp用于拷贝文件\n\n- `cp a.txt b.txt abc/` 拷贝两个文件到abc目录下\n- `cp /root/install.log /root/abc/xdir/` 也可以是绝对路径\n- `cp ~/install.log .` 拷贝用户目录下的文件到当前文件夹下\n- `cp a.txt zz.txt` **将文件a.txt复制为zz.txt**\n\n **`cp -r 目录  目标目录`** 用于拷贝整个目录\n\n- `cp -r efg/ abc/` 拷贝efg文件夹到abc文件夹下\n\n加上`-f` 表示覆盖已经存在的目标文件而不给出提示。**但是，我试了还是提示我覆盖，哈哈，以后有时间再试试**\n\n\n\n## 16. mv\n\nmv用于移动或重命名文件\n\n**既可以移动文件，也可以移动目录，即可为文件改变，也可以为目录改名。**\n\n- `mv a.txt b.txt x/y` 复制两个文件到x/y目录下\n- `mv abc/ x/y` **复制目录abc到x/y目录下**\n\nLinux中没有专门改名的命令，mv兼职改名工作：\n\n- `mv zdir zzdir` 将zdir目录改名为zzdir\n- `mv zz.txt zk.txt` 将zz.txt文件改名为zk.txt\n\n\n\n## 17. ln\n\nln 用于创建硬链接或软链接\n\n**创建硬链接**\n\n```bash\n[root@layne laydir]# cp /etc/profile ./\n[root@layne laydir]# ln profile  ln1\n```\n\n**创建软链接**\n\n让ln2指向profile， 给profile创建软链接\n\n```bash\n[root@layne laydir]# ln -s profile ln2\n```\n\n\n\n创建完两个链接后，修改ln1、ln2、profile任何一个文件的内容，都会导致这三个文件改变，说明确实实现了链接的功能。\n\n输入`ll -i`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207165958.png)\n\n\n\n> **在 Linux 中，元数据中的 inode 号（inode 是文件元数据的一部分但其并不包含文件名，inode 号即索引节点号）才是文件的唯一标识而非文件名。文件名仅是为了方便人们的记忆和使用，系统或程序通过 inode 号寻找正确的文件数据块。**\n\n**文件的 inode 值，可以简单把它想成 C 语言中的指针，它指向了物理硬盘的一个区块。只要一个文件的inode存在，它就不会从硬盘上消失。**\n\n看上图，可以发现：\n\n- **硬链接和原文件的inode信息一样，软链接和原文件的inode信息不一样。**\n- 软链接ln2的类型是`l`，说明是链接类型，而硬链接ln1和原文件的类型都是`-`，说明这两个都是文件类型。\n- 图片上也显示了l`ln2->profile`，说明ln2这个链接指向profile文件\n\n下面我试着删除 profile 文件，然后通过`cat`命令分别输出软硬链接的文件内容：\n\n**删除文件后，硬链接ln1输出了profile文件里面的内容，软链接ln2报错`cat: ln2: No such file or directory`**\n\n**这说明，硬链接ln1没有丝毫地影响，因为它的 inode 所指向的区块由于有一个硬链接在指向它，所以这个区块仍然有效，并且可以访问到。然而软链接的 inode 所指向的内容实际上是保存了一个绝对路径，当用户访问这个文件时，系统会自动将其替换成其所指的文件路径，由于这个文件已经被删除了，所以自然就会显示无法找到该文件了。**\n\n接下来，我向这个软连接ln2写点东西，**会发现刚才删除的profile文件竟然又出现了**！**这就说明，当我们写入访问软链接时，系统自动将其路径替换为其所代表的绝对路径，并直接访问那个路径了。**\n\n参考：[“软链接”和“硬链接”的区别](https://www.linuxprobe.com/soft-and-hard-links.html)\n\n## 18. du\n\n du （英文全拼：disk usage）命令用于显示目录或文件的大小。\n\ndu 会显示指定的目录或文件所占用的磁盘空间。\n\n- a ：列出所有的文件与目录容量 \n- h ：**以人们较易读的容量格式(G/M/K)显示**\n- s ：summarize 仅显示总计。\n- k ：以 KBytes 列出容量显示 \n- m :以 MBytes 列出容量显示 \n\n\n\n`du /root `统计/root下的目录或文件所占用的磁盘空间。（如果只输入`du`表示统计当前目录下的）\n\n```bash\n[root@layne laydir]# du /root\n4\t/root/b7/b3\n4\t/root/b7/etc/cron.monthly\n4\t/root/b7/etc/blkid\n160\t/root/b7/etc/ssh\n8\t/root/b7/etc/xdg/autostart\n12\t/root/b7/etc/xdg\n316\t/root/b7/etc/pki/ca-trust/extracted/openssl\n180\t/root/b7/etc/pki/ca-trust/extracted/java\n604\t/root/b7/etc/pki/ca-trust/extracted/pem\n1108\t/root/b7/etc/pki/ca-trust/extracted\n4\t/root/b7/etc/pki/ca-trust/source/blacklist\n4\t/root/b7/etc/pki/ca-trust/source/anchors\n...\n```\n\n添加-s参数可以生成指定目录的汇总信息，也就是共占用多大的磁盘空间\n\n```bash\n[root@layne laydir]# du -s /root\n58384\t/root\n```\n\n添加-h参数可以显示为人类可以读懂的格式\n\n```bash\n[root@layne laydir]# du -sh /root\n58M\t/root\n```\n\n将路径写成`./*`统计当前目录下每项内容占用的磁盘空间信息\n\n```bash\n[root@layne laydir]# du -sh /root/*\n40K\t/root/1611819376845.png\n4.0K\t/root/anaconda-ks.cfg\n40M\t/root/b7\n16K\t/root/day5\n12K\t/root/install.log\n4.0K\t/root/install.log.syslog\n28K\t/root/layne\n4.0K\t/root/log.txt\n15M\t/root/manpages-zh-1.5.1\n1.9M\t/root/manpages-zh-1.5.1.tar.gz\n52K\t/root/my\n4.0K\t/root/profile\n4.0K\t/root/test1.txt\n4.0K\t/root/test.txt\n4.0K\t/root/TRANS.TBL\n```\n\n查看/usr/local目录下共占用多大磁盘空间\n\n```bash\n[root@layne laydir]# du -sh /usr/local\n7.0M\t/usr/local\n```\n\n## 19. stat\n\nstat命令用于显示inode内容。\n\nstat以文字的格式来显示inode的内容。\n\n例如，查看 etc下的profile文件的inode内容内容，可以用以下命令：\n\n```bash\n[root@layne laydir]# stat /etc/profile\n  File: `/etc/profile'\n  Size: 1896      \tBlocks: 8          IO Block: 4096   regular file\nDevice: 803h/2051d\tInode: 131623      Links: 1\nAccess: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)\nAccess: 2021-02-07 10:15:36.277006386 +0800\nModify: 2021-02-05 19:43:06.556412802 +0800\nChange: 2021-02-05 19:43:06.589413340 +0800\n```\n\n\n\n## 20. touch\n\n用于创建文件\n\n`touch a.txt b.txt` 创建a.txt和b.txt\n\ntouch 对于已存在的文件，抹平各个时间（**不会改变里面的内容**）\n\ntouch 对于不存在的文件，则创建文件\n\n\n\n## 21. kill\n\n杀死进程\n\nkill -9 pid （见人就杀，不做善后工作）\n\nkill -15 pid （调用destory方法善后，）\n\n**优先使用 -15，因为-15温柔一些，会做一些善后的处理，不行再用-9**\n\n\n\n## 22. whereis\n\n 作用：locate the binary, source, and manual page files for a command.即：定位/返回与指定名字匹配的二进制文件（参数-b）、源文件（参数-s）和man帮助手册（参数-m）文件所在的路径。如果省略参数，则返回所有信息。\n\n和find相比，whereis查找的速度非常快，这是因为linux系统会将系统内的所有文件都记录在一个数据库文件中，当使用whereis和下面即将介绍的locate时，会从数据库中查找数据，而不是像find命令那样，通过遍历硬盘来查找，效率自然会很高。 \n\n但是该数据库文件并不是实时更新，默认情况下时每天自动更新一次，因此，我们在用whereis和locate 查找文件时，有时会找到已经被删除的数据，或者刚刚建立文件，却无法查找到，原因就是因为数据库文件没有被更新。 可以通过`updatedb`去读取 /etc/updatedb.conf 这个配置文件的设定，更新整个数据库！因为 updatedb 会去查找硬盘，所以执行 updatedb ，可能会等待数分钟。\n\n原理：whereis命令首先会去掉filename中的前缀空格和以.开头的任何字符，然后再**在数据库（var/lib/slocate/slocate.db）中**查找与上述处理后的filename相匹配的二进制文件、源文件和帮助手册文件,使用之前可以使用updatedb命令手动更新数据库。\n\n**命令参数：**\n\n-b  定位可执行文件。\n\n-m  定位帮助文件。\n\n-s  定位源代码文件。\n\n-u  搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。\n\n-B  指定搜索可执行文件的路径。\n\n-M  指定搜索帮助文件的路径。\n\n-S  指定搜索源代码文件的路径。\n\n适用场合：二进制文件、源文件和帮助手册文件路径的查找。\n\n```bash\n[root@layne apps]# whereis java\njava: /usr/bin/java\n[root@layne apps]# \n```\n\n\n\n## 23. locate\n\nlocate而且使用范围比whereis大的多，只需要部分文件名就可以进行模糊查询，同时locte还可以通过`-r`选项使用正则表达式，功能十分强大。与whereis一样 locate使用的索引数据库里的信息并不一定是实时有效的，可以使用updatedb命令更新索引数据库。\n\n原理：默认情况下(当filename中不包含通配符)，locate会给出所有与filename相匹配的文件的路径。\n\n适用场合：没有文件类型性质的模糊查找（你只记得某个文件的部分名称）。\n\n```bash\n[root@layne apps]# locate java|head -n 5\n/etc/.java\n/etc/.java/.systemPrefs\n/etc/.java/.systemPrefs/.system.lock\n/etc/.java/.systemPrefs/.systemRootModFile\n/etc/pki/java\n```\n\n\n\n\n\n## 24. find\n\n参数说明：\n\n**时间查找参数：**\n- -atime  n: 在过去n天内被读取过的文件\n- -cmin n : **在过去 n 分钟内被创建的文件** （按文件创建时间来查找文件）\n- -mmin n：在过去 n 分钟内被修改过的文件（按文件更改时间来查找文件）\n- -ctime  n: 在过去n天内被创建的文件（按文件创建时间来查找文件）\n- -mtime  n: 在过去n天内被修改过的文件（按文件更改时间来查找文件）\n- -newer file: 把比file还要新的文件列出来\n\n**名称查找参数：**\n\n- -gid  n:  寻找群组ID为n的文件\n- -group name: 寻找群组名称为name的文件\n- -uid  n:  寻找拥有者ID为n的文件\n- -user name:  寻找拥有者名称为name的文件\n- -name file:   寻找文件名为file的文件（可以使用通配符）\n\n**其它参数：**\n\n- -type c : 文件类型是 c 的文件。\n  - d: 目录\n  - c: 字型装置文件\n  - b: 区块装置文件\n  - p: 具名贮列\n  - f: 一般文件\n  - l: 符号连结\n  - s: socket\n- -size   n[c]        #查长度为n块[或n字节]的文件\n- -user  username       #按文件属主来查找\n- -group groupname      #按组来查找\n\n作用：search for files in a directory hierarchy. 从当前目录递归的搜索文件（通过遍历硬盘来查找）。\n\n这里介绍一下**时间查找参数的含义，以-mtime参数为例**\n\n```tex\n-mtime n ：n 为数字，意义为在 n 天之前被更改过的文件； \n+n ：列出在 n 天（不含你、）之前被更改过的文\n-n ：列出在 n 天之内(包含 n 天本身)被更改过内容的文件。\n```\n\n比如\n\n```tex\n+4代表>=5天前的文件名：ex> find /var -mtime +4 #查看/var目录下>=5天前的文件名\n-4代表<=4天内的文件名：ex> find /var -mtime -4\n 4则是代表4-5那一天的文件名：ex> find /var -mtime 4\n```\n\n\n\nfind的使用实例：\n\n- `find . -name \"*.c\"` 将当前目录及其子目录下所有文件后缀为 **.c** 的文件列出来\n- `find . -type f`  将目前目录其其下子目录中所有一般文件列出\n\n- `find . -ctime -20`  将当前目录及其子目录下所有最近 20 天内更新过的文件列出\n- `find /var/log -type f -mtime +7 -ok rm {} \\;` 查找 /var/log 目录中**更改时间**在 7 日以前的普通文件，并在删除之前询问它们\n- `find . -type f -mmin -10` 搜索当前目录中，所有过去10分钟中**更新过**的普通文件。如果不加-type f参数，则搜索普通文件+特殊文件+目录。\n- `find . -name \"my*\" -ls` 搜索当前目录中，所有文件名以my开头的文件，并显示它们的详细信息。\n- `find  /home  -size  +512k`         查大于512k的文件\n- `find  /home  -size  -512k`        查小于512k的文件\n- `find /home -user noas`  找到目下下所有该用户的文件\n- `find / -nouser` #找到不数据任何用户的文件\n\n\n\n## 25. pstree\n\n`pstree` 打印进程树（有的子进程折叠）\n\n`pstree -p`打印并展开所有的进程树，并显示每个进程的id\n\n\n\n## 26. read\n\nread命令用于从标准输入读取数值。\n\nread 内部命令被用来从标准输入**读取单行数据**。这个命令可以用来读取键盘输入，当使用重定向的时候，可以读取文件中的一行数据。\n\n语法：`read [-ers] [-a aname] [-d delim] [-i text] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name ...]`\n\n**参数说明:**\n\n- -a 后跟一个变量，该变量会被认为是个数组，然后给其赋值，默认是以空格为分割符。\n- -d 后面跟一个标志符，其实只有其后的第一个字符有用，作为结束的标志。\n- -p **后面跟提示信息**，即在输入前打印提示信息。\n- -e 在输入的时候可以使用命令补全功能。\n- -n 后跟一个数字，定义输入文本的长度，很实用。\n- -r 屏蔽\\，如果没有该选项，则\\作为一个转义字符，有的话 \\就是个正常的字符了。\n- -s 安静模式，**在输入字符时不再屏幕上显示**，例如login时输入密码。\n- -t 后面跟秒数，定义输入字符的等待时间。\n- -u 后面跟fd，从文件描述符中读入，该文件描述符可以是exec新开启的。\n\n现在来试试吧。\n\n**（1）简单读取**\n\n创建文件a.sh，内容为：\n\n```bash\n#!/bin/bash\n\n#这里默认会换行  \necho \"输入网站名: \"  \n#读取从键盘的输入  \nread website  \necho \"你输入的网站名是 $website\"  \nexit 0  #退出\n```\n\n执行：\n\n```bash\n[root@layne rdir]# chmod +x a.sh\n[root@layne rdir]# ./a.sh \n输入网站名: \nlayne\n你输入的网站名是 layne\n[root@layne rdir]# \n```\n\n**（2）`-p` 参数，允许在 read 命令行中直接指定一个提示。**\n\n创建文件b.sh，内容为：\n\n```bash\n#!/bin/bash\n\nread -p \"输入网站名:\" website\necho \"你输入的网站名是 $website\" \nexit 0\n```\n\n执行：\n\n```bash\n[root@layne rdir]# chmod +x b.sh\n[root@layne rdir]# ./b.sh\n输入网站名:layne\n你输入的网站名是 layne\n```\n\n**（3）`-t` 参数指定 read 命令等待输入的秒数，当计时满时，read命令返回一个非零退出状态。**\n\n创建c.sh，内容：\n\n```bash\n#!/bin/bash\n\nif read -t 5 -p \"输入网站名:\" website\nthen\n    echo \"你输入的网站名是 $website\"\nelse\n    echo \"\\n抱歉，你输入超时了。\"\nfi\nexit 0\n```\n\n执行（不输入，等待 5 秒后）：\n\n```bash\n[root@layne rdir]# chmod +x c.sh\n[root@layne rdir]# ./c.sh  \n输入网站名:\\n抱歉，你输入超时了。\n```\n\n（4）除了输入时间计时，还可以使用 **-n** 参数设置 **read** 命令计数输入的字符。当输入的字符数目达到预定数目时，自动退出，并将输入的数据赋值给变量。\n\n创建d.sh，内容：\n\n```bash\n#!/bin/bash\n\nread -n1 -p \"Do you want to continue [Y/N]?\" answer\ncase $answer in\nY | y)\n      echo \"fine ,continue\";;\nN | n)\n      echo \"ok,good bye\";;\n*)\n     echo \"error choice\";;\n\nesac\nexit 0\n```\n\n执行：\n\n```bash\n[root@layne rdir]# chmod +x d.sh\n[root@layne rdir]# ./d.sh\nDo you want to continue [Y/N]?yfine ,continue\n```\n\n该例子使用了-n 选项，后接数值 1，指示 read 命令只要接受到一个字符就退出。只要按下一个字符进行回答，read 命令立即接受输入并将其传给变量，**无需按回车键。**\n\n（5）**-s** 选项能够使 **read** 命令中输入的数据不显示在命令终端上（实际上，数据是显示的，只是 **read** 命令将文本颜色设置成与背景相同的颜色）。输入密码常用这个选项。\n\n创建e.sh，内容：\n\n```bash\n#!/bin/bash\n\nread  -s  -p \"请输入您的密码:\" pass\necho \"\\n您输入的密码是 $pass\"\nexit 0\n```\n\n执行：\n\n```bash\n[root@layne rdir]# chmod +x e.sh\n[root@layne rdir]# ./e.sh \n请输入您的密码:\\n您输入的密码是 123\n```\n\n（6） **读取文件**\n\n**每次调用 read 命令都会读取文件中的 \"一行\" 文本**。当文件没有可读的行时，read 命令将以非零状态退出。\n\n通过什么样的方法将文件中的数据传给 read 呢？使用 cat 命令并通过管道将结果直接传送给包含 read 命令的 while 命令。\n\n创建test.txt，内容如下：\n\n```bash\n123\n456\nlayne\n```\n\n创建f.sh，内容为：\n\n```bash\n#!/bin/bash\n  \ncount=1    # 赋值语句，不加空格\ncat test.txt | while read line      # cat 命令的输出作为read命令的输入,read读到>的值放在line中\ndo\n   echo \"Line $count:$line\"\n   count=$[ $count + 1 ]          # 注意中括号中的空格。\ndone\necho \"finish\"\nexit 0\n```\n\n执行：\n\n```bash\n[root@layne rdir]# chmod +x f.sh\n[root@layne rdir]# ./f.sh\nLine 1:123\nLine 2:456\nLine 3:layne\nfinish\n```\n\n（7）**read在不提供参数的时候，会将用户的输入存储在REPLY变量中**\n\n```bash\n[root@layne rdir]# read\nabc\n[root@layne rdir]# echo $REPLY\nabc\n```\n\n\n","tags":["Linux"],"categories":["Linux"]},{"title":"Linux文件系统相关操作","url":"/2021/02/10/105922/","content":"\n本文带来Linux文件系统相关操作，主要介绍虚拟目录树、文件元数据以及df、mount、umount相关命令。\n<!-- more -->\n\n\n\n## 虚拟目录树的各个目录用途\n\n| **目录** | **用途**                                       |\n| -------- | ---------------------------------------------- |\n| /        | 虚拟目录的根目录。通常不会在这里存储文件       |\n| /bin     | 二进制目录，存放许多用户级的GNU工具            |\n| /boot    | 启动目录，存放启动文件                         |\n| /dev     | 设备目录，Linux在这里创建设备节点              |\n| /etc     | 系统配置文件目录                               |\n| /home    | 主目录，Linux在这里创建用户目录                |\n| /lib     | 库目录，存放系统和应用程序的库文件             |\n| /media   | **媒体目录，可移动媒体设备的常用挂载点**       |\n| /mnt     | **挂载目录，另一个可移动媒体设备的常用挂载点** |\n| /opt     | **可选目录，常用于存放第三方软件包和数据文件** |\n| /proc    | **进程目录**，存放现有硬件及当前继承的相关信息 |\n| /root    | root用户的主目录                               |\n| /sbin    | 系统二进制目录，存放许多GNU管理员级工具        |\n| /srv     | 服务目录，存放本地服务的相关文件               |\n| /sys     | 系统目录，存放系统硬件信息的相关文件           |\n| /tmp     | 临时目录，可以在该目录中创建和删除临时工作文件 |\n| /usr     | **大量用户级的GNU工具和数据文件都存储在这里**  |\n| /var     | 可变目录，用以存放经常变化的文件，比如日志文件 |\n\n\n\n## 文件元数据\n\n输入`ll -i`会发现：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207173337.png)\n\n第一列表示inode\n\n第二列，有10个字符\n\n- 第一个字符表示文件类型，有`-`表示文件，`l`表示链接，`d`表示目录\n- 后面9个字符，3个为一组，分别表示拥有者、用户组、others的权限，`r`表示读，`w`表示写，`x`表示执行，`b`开始块设备文件，`c`表示字符设备文件\n\n第三列现在先忽略\n\n第四列表示所属者\n\n第五列表示用户组\n\n第六列：\n\n- 文件所在行的数字表示文件的大小，目录所在的行数字那列一般显示4096（并不是所里面的文件有4096byte大小）\n\n第七列：文件创建时间或文件最后一次的修改时间\n\n第八列：文件或目录名\n\n\n\n## 相关命令\n\n### df\n\n显示文件系统的**磁盘**使用情况统计\n\n输入`df`显示以下信息：\n\n```bash\n[root@layne laydir]# df\nFilesystem     1K-blocks    Used Available Use% Mounted on\n/dev/sda3       18375548 3570988  13871136  21% /\ntmpfs             502204       0    502204   0% /dev/shm\n/dev/sda1         198337   27804    160293  15% /boot\n```\n\n- 第一列指定文件系统的名称\n- 第二列指定一个特定的文件系统1K-块，1K是1024字节为单位的总内存，这里1837554K表示总共的磁盘容量（sda相当于windows下的卷）\n- 第三列表示已经使用的磁盘容量\n- 第四列表示空闲的磁盘容量\n- 第五列是以百分比的形式显示已经使用的磁盘容量\n- 第六列是磁盘挂载的路径\n\n\n\ndf也可以显示磁盘使用的文件系统信息，输入`df 文件名（或目录名）` 即可显示该文件或目录所使用的磁盘信息：\n\n```bash\n[root@layne laydir]# df ln1\nFilesystem     1K-blocks    Used Available Use% Mounted on\n/dev/sda3       18375548 3570988  13871136  21% /\n```\n\n上面就显示ln1使用磁盘情况，可以看到，ln1使用的是sda3的磁盘。\n\n加上-h（--human-readable ）表示使用人类可读的格式(预设值是不加这个选项的...)\n\n```bash\n[root@layne laydir]# df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda3        18G  3.5G   14G  21% /\ntmpfs           491M     0  491M   0% /dev/shm\n/dev/sda1       194M   28M  157M  15% /boot\n```\n\n\n\n### mount\n\n用于挂载Linux系统外的文件。\n\n现在，我们试着将光驱挂载到/mnt目录\n\n首先，查看光驱（cdrom)的文件：\n\n```bash\n[root@layne laydir]# ll /dev | grep cd\nlrwxrwxrwx 1 root root           3 Feb  7 10:15 cdrom -> sr0\nlrwxrwxrwx 1 root root           3 Feb  7 10:15 cdrw -> sr0\nlrwxrwxrwx 1 root root           3 Feb  7 10:15 scd0 -> sr0\ncrw-rw---- 1 root cdrom    21,   1 Feb  7 10:15 sg1\nbrw-rw---- 1 root cdrom    11,   0 Feb  7 10:15 sr0\n[root@layne laydir]# ll /mnt\ntotal 0\n```\n\n可以看到，cdrom是sr0的软链接文件，而sr0是块设备文件。\n\n然后，我们通过`mount /dev/cdrom /mnt`挂载光驱：\n\n```bash\n[root@layne laydir]# mount /dev/cdrom /mnt\nmount: block device /dev/sr0 is write-protected, mounting read-only\n[root@layne laydir]# ll -i /mnt\ntotal 82\n1863 -r--r--r-- 1 root root    14 Nov 29  2013 CentOS_BuildTag\n3264 dr-xr-xr-x 3 root root  2048 Nov 29  2013 EFI\n1871 -r--r--r-- 1 root root   212 Nov 28  2013 EULA\n1874 -r--r--r-- 1 root root 18009 Nov 28  2013 GPL\n3648 dr-xr-xr-x 3 root root  2048 Nov 29  2013 images\n3584 dr-xr-xr-x 2 root root  2048 Nov 29  2013 isolinux\n1984 dr-xr-xr-x 2 root root 40960 Nov 29  2013 Packages\n1889 -r--r--r-- 1 root root  1354 Nov 28  2013 RELEASE-NOTES-en-US.html\n3392 dr-xr-xr-x 2 root root  4096 Nov 29  2013 repodata\n1907 -r--r--r-- 1 root root  1706 Nov 28  2013 RPM-GPG-KEY-CentOS-6\n1911 -r--r--r-- 1 root root  1730 Nov 28  2013 RPM-GPG-KEY-CentOS-Debug-6\n1902 -r--r--r-- 1 root root  1730 Nov 28  2013 RPM-GPG-KEY-CentOS-Security-6\n1897 -r--r--r-- 1 root root  1734 Nov 28  2013 RPM-GPG-KEY-CentOS-Testing-6\n1915 -r--r--r-- 1 root root  3380 Nov 29  2013 TRANS.TBL\n```\n\n可看到，光驱已经挂载到/mnt目录下，这是/mnt下的文件就是光驱里面的文件。\n\n注意，如果/mnt没有显示任何文件，可能是文件还没有刷新出来，尝试cd到别的目录，然后再次查看/mnt下的文件。\n\n通过`df -h`查看文件系统的磁盘使用情况：\n\n```bash\n[root@layne laydir]# df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda3        18G  3.5G   14G  21% /\ntmpfs           491M     0  491M   0% /dev/shm\n/dev/sda1       194M   28M  157M  15% /boot\n/dev/sr0        398M  398M     0 100% /mnt\n```\n\n### umount\n\n卸载掉挂载的分区\n\n注意卸载/mnt时，当前目录不能在/mnt下面，否则报出如下图所示提示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207185628.png)\n\n\n\n现在我们卸载挂载到/mnt下的光驱：\n\n```bash\n[root@layne laydir]# umount /mnt\n[root@layne laydir]# ll -i /mnt\ntotal 0\n[root@layne laydir]# df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda3        18G  3.5G   14G  21% /\ntmpfs           491M     0  491M   0% /dev/shm\n/dev/sda1       194M   28M  157M  15% /boot\n```\n\n\n\n\n\n\n\n","tags":["Linux"],"categories":["Linux"]},{"title":"Linux文件属性与权限的管理","url":"/2021/02/10/105911/","content":"\n## 1. Linux文件属性与权限的含义\n\n首先我们以root用户的身份登陆Linux,执行`ls -al`查看文件：\n<!-- more -->\n\n\n\n```bash\n[root@layne ~]# ls -al\ntotal 2104\ndr-xr-x---.  8 root root    4096 Feb  8 18:17 .\ndr-xr-xr-x. 22 root root    4096 Feb  8 11:13 ..\n-rw-r--r--   1 root root   39974 Jan 28 15:36 1611819376845.png\n-rw-------.  1 root root     903 Feb  3 03:44 anaconda-ks.cfg\ndrwxr-xr-x   4 root root    4096 Feb  6 18:25 b7\n-rw-------.  1 root root   13726 Feb  7 21:39 .bash_history\n-rw-r--r--.  1 root root      18 May 20  2009 .bash_logout\n-rw-r--r--.  1 root root     176 May 20  2009 .bash_profile\n-rw-r--r--   1 root root     229 Feb  3 23:59 .bashrc\n-rw-r--r--.  1 root root     100 Sep 23  2004 .cshrc\n...\n```\n\nls是list的缩写，能显示文件的文件名和相关属性。而`-al`表示列出所有的文件详细的权限与属性(包含隐藏文件，诸如文件名以“.”开头的文件)。显示信息详细含义如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208195542.png)\n\n**（1）第一列**\n\n第一列代表这个文件的类型与权限(permission)，仔细看可以发现其中总共有10个字符。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208195618.png)\n\n\n\n任何设备在Linux下都是文件。上述10个字符的第一个字符表示文件的类型，文件的类型有：\n\n- 若是[d]则是目录（directory） \n- 若是[-]则是文件\n- 若是[l]则是链接文件（link）, 类似window系统下的快捷方式。\n- 块设备文件[b]\n- 字符设备文件[c]\n- 数据接口文件（sockets）[s]\n- 数据输送文件（FIFO，pipe）[p]\n\n\n\n  **与系统外设和存储等相关的一些文件，通常都集中在/dev这个目录中。分为两种：**\n\nA. 块（block）设备文件[b]：就是一些存储数据，以提供系统随机访问的接口设备，例如硬盘、软盘等。你可以随机的在硬盘的不同块读写，这种设备就是成组设备。\n\n例如：\n\n```bash\n[root@layne ~]# ls -al /dev | grep sda\nlrwxrwxrwx   1 root root           4 Feb  8 11:13 root -> sda3\nbrw-rw----   1 root disk      8,   0 Feb  8 11:13 sda\nbrw-rw----   1 root disk      8,   1 Feb  8 11:13 sda1\nbrw-rw----   1 root disk      8,   2 Feb  8 11:13 sda2\nbrw-rw----   1 root disk      8,   3 Feb  8 11:13 sda3\n```\n\nB. 字符（character）设备文件[c]：是一些串行端口的接口设备,例如键盘,鼠标。这些设备的特征就是“一次性读取”的，不能够截断输出。\n\n例如：\n\n```bash\n[root@layne ~]# ls -al /dev | grep vcs\ncrw-rw----   1 vcsa tty       7,   0 Feb  8 11:13 vcs\ncrw-rw----   1 vcsa tty       7,   1 Feb  8 11:13 vcs1\ncrw-rw----   1 vcsa tty       7,   2 Feb  8 11:14 vcs2\ncrw-rw----   1 vcsa tty       7,   3 Feb  8 11:14 vcs3\ncrw-rw----   1 vcsa tty       7,   4 Feb  8 11:14 vcs4\ncrw-rw----   1 vcsa tty       7,   5 Feb  8 11:14 vcs5\ncrw-rw----   1 vcsa tty       7,   6 Feb  8 11:14 vcs6\ncrw-rw----   1 vcsa tty       7, 128 Feb  8 11:13 vcsa\ncrw-rw----   1 vcsa tty       7, 129 Feb  8 11:13 vcsa1\ncrw-rw----   1 vcsa tty       7, 130 Feb  8 11:14 vcsa2\ncrw-rw----   1 vcsa tty       7, 131 Feb  8 11:14 vcsa3\ncrw-rw----   1 vcsa tty       7, 132 Feb  8 11:14 vcsa4\ncrw-rw----   1 vcsa tty       7, 133 Feb  8 11:14 vcsa5\ncrw-rw----   1 vcsa tty       7, 134 Feb  8 11:14 vcsa6\n```\n\n**总结：设备文件是我们系统很重要的文件，最好不要随意修改之外（通常它也不会让你修改的）；另外一个比较有趣的文件就是链接文件，类似window的桌面快捷方式，同样可以将linux下的连接文件简单的视为一个文件或目录的快捷方式。**\n\n数据接口文件（sockets）：\n\n既然被称为数据接口文件，这种类型ss的文件通常被用在网络上的数据承接了。我们可以启动一个程序来监听客户端的要求，而客户端就可以透过这个socket来进行数据的沟通了。第一个属性为[ s ]，最常在/var/run目录中看到这种文件类型了。\n\n数据输送文件（FIFO，pipe）：\n\nFIFO也是一种特殊的文件类型，他主要的目的在解决多个程序同时存取一个文件所造成的错误问题。FIFO是first-in-first-out的缩写（先进先出）。第一个属性为[p]。\n\n\n\n\n\n上述10个字符的后9个字符，三个一组,且均为[rwx]的3个参数组合。\n\n**其中[r]代表可读(read),[w]代表可写(write),[x]代表可执行(excute)。**\n\n这三个参数的出现顺序不会改变，**若没有某个权限，则会以[-]代替**。这三组参数中，第一组是文件所有者的权限；第二组是同用户组的权限；第三组是其他用户的权限。这三组权限均是针对某些账号而言的权限。另外，文件权限和目录权限意义不同，这是因为文件与目录记录的数据内容不相同，后面我们会详细叙述。\n\n\n\n**（2）第二列**\n\n 第二列表示有多少文件名链接到此节点(i-node)。\n\n每个文件都会将他的权限与属性记录到文件系统的i-node中，但是Linux所使用的目录树却是使用文件名来记录，因此每个文件名就会链接到一个i-node。**这个属性记录的就是有多少不同的文件名链接到相同的一个i-node号码**。\n\n**（3）第三列**\n\n第三列表示这个文件(或目录)的所有者账号\n\n**（4）第四列**\n\n第四列表示这个文件的所属用户组。\n\n在Linux系统下，每个账号会附属于一个或多个用户组中（一个主组，多个附加组）。\n\n**（5）第五列**\n\n第五列表述这个文件的容量大小，默认单位为bytes 。目录前面的数字都是4096，不是表示整个目录的大小。而文件前面的数字表示该文件的大小。\n\n**（6）第六列**\n\n 第六列为这个文件的创建文件日期或者是最近的修改日期\n\n如果想要显示完整的时间格式，可以使用ls参数，即”ls -l --full-time”，这样做就可以显示出完整的时间格式。\n\n**（7）第七列**\n\n第七列为该文件名\n\n注意，前缀为”.”的是隐藏文件。\n\nLinux文件权限最大的用途实在数据安全性上，它能根据不同用户的不同权限实现对不同文件的操作。为此，在我们设置Linux文件与目录的属性之前，需要弄清到底什么数据是可变的，什么数据是不可变的。\n\n## 2. Linux文件属性与权限的管理\n\n接下来,我们介绍几个常用于用户组，所有者，各种身份的权限的修改的命令：\n\n​    chgrp：改变文件所属用户组  （chgrp 即 change group）\n\n​    chown：改变文件所有者   （chang owner）\n\n​    chmod：改变文件的权限  (change mode)\n\n### 2.1 改变文件所属用户组\n\n语法：`chgrp [-R]  用户组  dirname/filename`\n\n参数：-R 表示如果为目录则递归修改组（该目录下的文件和目录都可以改）。\n\n**作用：**使用chgrp命令可以改变一个文件的用户组，它是changegroup的简称。\n\n**注意：**需要注意的是，要被改的组名必须要在/etc/group文件内存在才行，否则会报错。\n\n**案例：**将a.txt和b.txt所属用户组改为lucy\n\n```bash\n[root@layne testdir]# ll\ntotal 8\n-rw-r--r-- 1 root root   33 Feb  8 15:59 a.txt\n-rw-r--r-- 1 root root    0 Feb  7 18:35 b.txt\ndrwxr-xr-x 2 root root 4096 Feb  8 20:28 dir\n[root@layne testdir]# chgrp lucy a.txt b.txt\n[root@layne testdir]# ll\ntotal 8\n-rw-r--r-- 1 root lucy   33 Feb  8 15:59 a.txt\n-rw-r--r-- 1 root lucy    0 Feb  7 18:35 b.txt\ndrwxr-xr-x 2 root root 4096 Feb  8 20:28 dir\n```\n\n将dir目录及该目录下所有的文件所属用户组改为lucy\n\n```bash\n[root@layne testdir]# ll\ntotal 8\n-rw-r--r-- 1 root lucy   33 Feb  8 15:59 a.txt\n-rw-r--r-- 1 root lucy    0 Feb  7 18:35 b.txt\ndrwxr-xr-x 2 root root 4096 Feb  8 20:28 dir\n[root@layne testdir]# ll dir\ntotal 8\n-rw-r--r-- 1 root root 204 Feb  8 20:28 sed.txt\n-rw-r--r-- 1 root root 989 Feb  8 20:28 succ.log\n[root@layne testdir]# chgrp -R lucy dir\n[root@layne testdir]# ll dir\ntotal 8\n-rw-r--r-- 1 root lucy 204 Feb  8 20:28 sed.txt\n-rw-r--r-- 1 root lucy 989 Feb  8 20:28 succ.log\n[root@layne testdir]# ll\ntotal 8\n-rw-r--r-- 1 root lucy   33 Feb  8 15:59 a.txt\n-rw-r--r-- 1 root lucy    0 Feb  7 18:35 b.txt\ndrwxr-xr-x 2 root lucy 4096 Feb  8 20:28 dir\n```\n\n\n\n### 2.2 改变文件所有者\n\n**语法**：`chown [-R] 用户账号 dirname/filename`\n\n**或**\n\n`chown [-R] 用户账号：用户组名 dirname/filename`\n\n**作用：**使用chown命令可以改变一个文件的所有者，还可以直接修改群组的名称；它是changeowner的缩写。\n\n**注意：**用户必须是已经存在于系统中的账号，也就是在/etc/passwd这个文件中有记录的用户名称才能改变。如果要将目录下的所有子文件或目录同时改变文件所有者，加-R参数即可。\n\n案例：\n\n`chown -R root:root etc`  **将etc目录及目录下所有的目录和文件 的所有者和所属用户组改为root**\n\n`chown lucy install.log`  将install.log的所有者改为lucy用户\n\n通过实例我们可以修改文件的用户组和所有者，那么什么时候使用chown，什么时候使用chgrp呢？我们举个例子:复制文件给其他用户，我们使用cp命令:\n\n假设我们要将profile这个文件复制成为profile_test文件名，并且要将他给lucy这个用户使用，做法如下:\n\n```bash\n[root@layne tdir]# ll\ntotal 4\n-rw-r--r-- 1 root root 1896 Feb  8 20:39 profile\n[root@layne tdir]# cp profile profile_test\n[root@layne tdir]# ll\ntotal 8\n-rw-r--r-- 1 root root 1896 Feb  8 20:39 profile\n-rw-r--r-- 1 root root 1896 Feb  8 20:40 profile_test\n```\n\n不难发现，cp命令会复制执行者的属性与权限，profile_test还是归root所有。因此，lucy拿到该文件是无权进行操作的。所以你需要修改这个文件的所有者和用户组。\n\n\n\n### 2.3 改变文件权限\n\n**语法**：`chmod [-R] mode dirname/filename`\n\n**作用：**文件或目录权限的改变使用的是chmod(change file mode bits)这个命令。\n\n**注意：**权限的设置方法分两种，可以通过数字或符号进行修改。`-R`含义同上\n\n**（1）数字类型改变文件权限**\n\nLinux的基本权限有9个，分别是owner,group,others三种身份各自的read,write,excute权限,各个权限对应的数字如下:\n\n```tex\nr:4、w:2、x:1\n```\n\n为此每种身份各自的三个权限数字相加即可得出数字表示的权限。\n\n例如，`-rwxrwx`可以表示为：\n\n```bash\nowner = rwx = 4+2+1 = 7\ngroup = r-x = 4+1 = 5\nothers= --- = 0+0+0 = 0\n```\n\n最终得到的数字肯定是唯一的。\n\n数字分别表示为：\n\n- 7代表rwx\n- 6代表rw-\n- 5代表r-x\n- 4代表r--\n- 3代表-wx\n- 1代表--x\n- 0代表---\n\n范例：将文件a.txt的用户组权限改为r-x，所有者和other的权限不变\n\n```bash\n[root@layne testdir]# ll a.txt\n-rw-r--r-- 1 root lucy 33 Feb  8 15:59 a.txt\n[root@layne testdir]# chmod 654 a.txt\n[root@layne testdir]# ll a.txt\n-rw-r-xr-- 1 root lucy 33 Feb  8 15:59 a.txt\n```\n\n\n\n**（2）符号类型改变文件权限**\n\n另一种改变权限的方法就是通过符号了。上文提到，Linux总共9种权限，对应着三种身份，为此我们可以通过u,g,o代表三种身份，另外a代表全部身份。对应的权限可以写为r,w,x,如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208205322.png)\n\nu是user，g是group，o是others，a是all\n\n+是加一个权限，等号是设定最终的结果，-是在原来的基础上减一个。\n\n- `chmod g+w test.sh` 给该文件的用户组加一个w权限\n- `chmod u=rwx,go=rx a.txt` 给该文件所有者设定rwx权限，用户组和其它人设定rx权限。\n- `chmod u=rwx,g=rx,o=r b.txt`  给该文件分别设定权限\n\n**a-x等于 -x，a+等于+x**\n\n- `chmod a-x a.txt` 或 `chmod -x a.txt`  给该文件的所有者、用户组、其他人减去一个x权限\n- `chmod a+r a.txt` 或 `chmod +r a.txt`  给该文件的所有者、用户组、其他人加上一个r权限\n\n\n\n## 3. 目录与文件的权限意义\n\n\n\n### 3.1 权限对文件的重要性\n\n文件是实际含有数据的地方，包括一般文本文件，数据库内容文件，二进制可执行文件(binary program)等等，因此，文件权限有如下意义：\n\n- r（read）：可读取此文件的**实际内容**，如读取文本文件的文字内容等；\n- w（write）：可以编辑、新增或者是修改该文件的内容\n- x（execute）：该文件具有可以被系统执行的权限。\n\n**注意：在Linux中，文件是否能被执行是由是否具有”x”这个权限来决定，与拓展名无关。**\n\n\n\n### 3.2 权限对目录的重要性\n\n- r（read contents in directory）：表示具有读取目录结构清单的权限，所以当你具有读取（r）一个目录的权限时，表示你可以查询该目录下的文件名数据。**所以你就可以利用ls这个指令将该目录的内容列表显示出来。**\n\n- w（modify contents of directory）：这个可写入的权限对目录来说，**表示你具有改变目录结构清单的权限**，也就是底下这些权限：\n\t- 建立新的文件或子目录\n\t- 删除已经存在的文件或子目录（**不论该文件的权限**）\n\t- 将已存在的文件或目录进行重命名；\n\t- 移动该目录内的文件、目录位置。\n- x（access directory）：在Linux中，目录不可以被执行，目录的x代表的是使用者能否进入该目录成为工作目录的用途。所谓的工作目录（work directory）就是你目前所在的目录。举例来说，当你登入Linux时，你所在的Home目录就是你当下的工作目录。\n\n**如果你对一个目录用rw权限，没有x权限，即不能作为工作目录，则你可以在目录外面对目录里面的文件进行创建和删除，但是不能cd到里面里面操作。**\n\n综上：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208210508.png)\n","tags":["Linux"],"categories":["Linux"]},{"title":"Linux文本操作相关命令","url":"/2021/02/10/105900/","content":"\n本文带来Linux文本文件操作和文本内容操作常用方法。\n<!-- more -->\n\n\n\n## 一、文本文件操作\n\n\n\n## 1.1 cat&&tac\n\n直接查看一个文件的内容可以使用cat,tac,nl这几个指令\n\ncat(concatenate)参数：\n\n**-n 或 --number**：由 1 开始对所有输出的行数编号。\n\n**-b 或 --number-nonblank**：和 -n 相似，只不过对于空白行不编号。\n\n**-s 或 --squeeze-blank**：当遇到有连续两行以上的空白行，就代换为一行的空白行。\n\n**-v 或 --show-nonprinting**：使用 ^ 和 M- 符号，除了 LFD 和 TAB 之外。\n\n**-E 或 --show-ends** : 在每行结束处显示 $。\n\n**-T 或 --show-tabs**: 将 TAB 字符显示为 ^I。\n\n**-A, --show-all**：等价于 -vET。\n\n**-e：**等价于\"-vE\"选项；\n\n**-t：**等价于\"-vT\"选项；\n\ncat是concatenate的缩写，其功能时间一个文件的内容连续的输出。该命令适合看行数较少的文件。另外，需要查看一般DOS文件时，可以通过-A选项来显示换行符和[tab]**（及特殊符号）**。\n\n**tac(**反向输出)：\n\n```bash\n[root@layne laydir]# cat a.txt\n141232afsg\nzfghahhh\naaaaa\n[root@layne laydir]# tac a.txt\naaaaa\nzfghahhh\n141232afsg\n```\n\n与上面的cat命令进行比较，是由最后一行先显示。\n\ntac功能与cat类似，但是是由文件最后一行反向连续输出到屏幕上。\n\n\n\n## 1.2 more&less\n\nmore会在**最后一行输出目前按显示内容的所占百分比**，在现实过程中我们还可以输入以下命令进行进一步操作：\n\n- 向下翻页`ctrl+f`，向上翻页`ctrl+b`\n- 空格(space):向下翻页\n- 回车(Enter):向下翻一行\n- “/字符串”:在当前显示内容中线下查询字符串内容\n- “:f”:立即显示出文件名以及目前显示的行数\n- “q”:离开显示内容\n- “b”或[ctrl]-b:代表回翻,该操作仅对文件有效\n\n例如：`more /etc/profile`\n\n\n\n**less** 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。(**在查看大文件时或内存不够时，可用less命令)**\n\n\n\n## 1.3 head\n\n取出前面几行\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207194052.png)\n\n\n\n## 1.4 tail\n\ntail可以监控一个文件，**默认显示后10行**\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207194351.png)\n\n**-f 表示持续监测，这个功能可以用来监控日志或进行调试**\n\n\n\n## 1.5 管道\n\n“|”管道符，意思为管道左侧的输出作为右侧的输入，常和`grep`搭配使用\n\ngrep一般用作过滤。\n\n比如：\n\n- `ps -ef | grep vim` 显示带有vim的进程信息\n- `ps aux | head -n 10`显示前10个进程\n\n管道可以有多个，作用以此类推。\n\n`echo \"/\" | ls -l` 会显示当前目录下的内容，不会显示根目录下的内容\n\n## 1.6 Xargs\n\n**将前面输出作为后面命令的参数**\n\nxargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。\n\n之所以能用到这个命令，**关键是由于很多命令不支持|管道来传递参数**，而日常工作中有有这个必要，所以就有了 xargs 命令，例如：\n\n`echo \"/\" | xargs ls -l` 就可以输出根目录下的内容，而`echo \"/\" | ls -l`  则不行\n\n`echo \"a.txt\"|vim` 可以进入a.txt编辑，而`echo \"a.txt\"|vim`则不行\n\n\n\n## 1.7 数据重定向\n\n  标准输入（stdin）:编号为0\n\n  标准输出（stdout）:编号为1\n\n  标准错误输出（stderr）:编号为2\n\n  1>:以覆盖的方法,将正确的数据输出到文件;  \n\n  1>>:以累加的方法,将正确的数据输出到文件;  \n\n  2>:以覆盖的方法,将错误输出的数据输出到文件;  \n\n  2>>:以累加的方法,将错误输出的数据输出到文件;  \n\n\n\n**只有单独的> 和 >> 也是只能输出正确的信息**\n\n可自己尝试以下命令：\n\n```bash\nls  -l  >>  ok1.log\nls  -l  >  ok2.log\nls  hello 2>/root/err.log\nls  hello / 1>/root/log.log 2>/root/err.log\nls 1>/dev/null\nls  2>/tmp/err.log\n```\n\n既向控制台输出，也向文件写入\n\n`ls -l / | tee ok2.log`\n\ntee命令，将输入分成两个输出\n\ntee只能把**正确的数据**放入文件里面\n\ntee 把原来的数据清空，把最新的数据写到里面。\n\n\n\n## 二、文本内容操作\n\n### 2.1 cut\n\ncut：显示切割的行数据\n\n- -s：不显示没有分隔符的行（过滤脏数据）\n- -d：**指定分隔符**对源文件的行进行分割 \n- -f：选定显示哪些列\n  - m-n： m列到n列\n  - -n ：第一列到n列\n  - m- ：第m列到最后列\n  - n ：第n列\n\n\n\n例如（默认列数是从1开始）：\n\n- `cut -d \":\" -f 1 passwd` 对passwd用`:`分割，然后获取第一列\n- `cut -s -d \":\" -f 1 passwd` 对passwd用`:`分割，然后获取第一列，且不显示没有分隔符的行（如果有的行没有分隔符，不加-s会输出会包含脏数据，加了-s后就不会打印没有分隔符的行）\n- `cut -sd \":\" -f 1 passwd` 同 `cut -s -d \":\" -f 1 passwd`\n- 不能为`cut -ds \":\" -f 1 passwd`，因为d后面要跟分隔符\n- `cut -d \":\" -f 3-5 passwd` 以`:`作为分隔符切割passwd，输出从第3列到第5列\n- `cut -d \":\" -f 1,3,5 passwd` 以`:`作为分隔符切割passwd，输出1、3、5列\n- `cut -d \":\" -f -2 passwd` 以`:`作为分隔符切割passwd，输出前两列\n- `cut -d \":\" -f 3- passwd` 以`:`作为分隔符切割passwd，输出第三列到最后一列\n- `cut -d \":\" -f 3- --output-delimiter=\"..\" passwd` **指定输出的分隔符**\n\n```bash\n[root@layne laydir]# cp /etc/passwd ./\n[root@layne laydir]# cut -d \":\" -f 1 passwd\nroot\nbin\ndaemon\nadm\nlp\nsync\nshutdown\nhalt\nmail\nuucp\noperator\ngames\ngopher\nftp\nnobody\nvcsa\nsaslauth\npostfix\nsshd\nntp\nlucy\nliubei\ngem\nmysql\n[root@layne laydir]# cut -d \":\" -f 3- --output-delimiter=\"..\" passwd\n0..0..root../root../bin/bash\n1..1..bin../bin../sbin/nologin\n2..2..daemon../sbin../sbin/nologin\n3..4..adm../var/adm../sbin/nologin\n4..7..lp../var/spool/lpd../sbin/nologin\n5..0..sync../sbin../bin/sync\n6..0..shutdown../sbin../sbin/shutdown\n7..0..halt../sbin../sbin/halt\n8..12..mail../var/spool/mail../sbin/nologin\n10..14..uucp../var/spool/uucp../sbin/nologin\n11..0..operator../root../sbin/nologin\n12..100..games../usr/games../sbin/nologin\n13..30..gopher../var/gopher../sbin/nologin\n14..50..FTP User../var/ftp../sbin/nologin\n99..99..Nobody../../sbin/nologin\n69..69..virtual console memory owner../dev../sbin/nologin\n499..76..\"Saslauthd user\"../var/empty/saslauth../sbin/nologin\n89..89..../var/spool/postfix../sbin/nologin\n74..74..Privilege-separated SSH../var/empty/sshd../sbin/nologin\n38..38..../etc/ntp../sbin/nologin\n500..500..../usr/lucy../bin/bash\n501..501..../home/liubei../bin/bash\n502..0..../home/gem../bin/bash\n27..27..MySQL Server../var/lib/mysql../bin/false\n```\n\n\n\n### 2.2 sort\n\n排序：**字典序和数值序**\n\nsort：**排序文件的行**\n\n可选参数如下：\n\n\\-   n：按数值排序\n\n\\-   r：倒序  reverse\n\n\\-   t：自定义分隔符\n\n\\-   k：选择排序列\n\n\\-   f：忽略大小写\n\n\n\n现在，我们创建一个sort.txt，文件内容如下：\n\n```tex\na b 1\ndfdsa fdsa 15\nfds fds 6\nfdsa fdsa 8\nfda s 9\naa dd 10\nh h 11\n```\n\n`sort sort.txt`是默认按字典排序：\n\n```bash\n[root@layne laydir]# sort sort.txt\naa dd 10\na b 1\ndfdsa fdsa 15\nfda s 9\nfdsa fdsa 8\nfds fds 6\nh h 11\n```\n\n指定字段分隔符，按照第2个字段的字典序排序：\n\n```bash\n[root@layne laydir]# sort -t ' ' -k 2 sort.txt\na b 1\naa dd 10\nfds fds 6\ndfdsa fdsa 15\nfdsa fdsa 8\nh h 11\nfda s 9\n```\n\n指定字段分隔符，按照第3个字段的值**数值**序排序：\n\n```bash\n[root@layne laydir]# sort -t ' ' -k 3 -n sort.txt\na b 1\nfds fds 6\nfdsa fdsa 8\nfda s 9\naa dd 10\nh h 11\ndfdsa fdsa 15\n```\n\n指定字段分隔符，按照第3个字段的值数值倒序\n\n```bash\n[root@layne laydir]# sort -t ' ' -k 3 -nr sort.txt\ndfdsa fdsa 15\nh h 11\naa dd 10\nfda s 9\nfdsa fdsa 8\nfds fds 6\na b 1\n```\n\n将结果写入到新的文件中：\n\n```bash\n[root@layne laydir]# sort -t \" \" -k 3 -nr sort.txt >> test1.txt\n```\n\n\n\n### 2.3 wc\n\n格式：wc [选项列表]... [文件名列表]...\n\nDESCRIPTION 描述：\n\n对每个文件**输出行、单词、和字节统计数**，若不指定文件名称、或是所给予的文件名为\"-\"，则wc指令会从标准输入设备读取数据。\n\n​    **-c**, --bytes, --chars 输出字节统计数。\n\n​    **-l**, --lines 输出换行符统计数。\n\n​    -L, --max-line-length 输出最长的行的长度。\n\n​    **-w**, --words 输出单词统计数。\n\n例如：\n\n```bash\n[root@layne laydir]# cp sort.txt wc.txt\n[root@layne laydir]# cat wc.txt\na b 1\ndfdsa fdsa 15\nfds fds 6\nfdsa fdsa 8\nfda s 9\naa dd 10\nh h 11\n[root@layne laydir]# wc wc.txt\n 7 21 66 wc.txt\n```\n\n上面命令，输出结果为`7 21 66 wc.txt`，其中，7是行数，21是单词数，66是字节数\n\n```bash\n[root@layne laydir]# wc -l wc.txt\n7 wc.txt\n[root@layne laydir]# cat wc.txt | wc -l\n7\n[root@layne laydir]# wc -w wc.txt\n21 wc.txt\n[root@layne laydir]# wc -c wc.txt\n66 wc.txt\n```\n\n\n\n### 2.4 sed\n\nsed 在不打开文件的情况下执行交互，来修改文件。\n\nsed即行编辑器\n\nsed [选项] 如下：\n\n- -n：静默模式，不再默认显示模式空间的内容（看例子理解）\n- `-i`：**直接修改源文件**  \n-  `-e` Script -e Script：可以同时执行多个脚本\n- `-f` /path/to/sed_script：执行文件中的sed脚本\n- `-r`：表示使用扩展正则表达式\n- `a\\string`：在指定的行后追加新行，内容为string\n\nn \\n：用于换行\n\n-  `i\\string`：在指定行**前**添加新行，内容是string\n-  `r file`：将指定的文件内容添加到符合条件的行的位置\n- `w file`：将地址指定的范围内的行另存至指定文件\n-  `s/string1/string2/`：查找并替换，**默认只替换每行第一次模式匹配到的字符串**\n\nn `g`：行内全局替换 （看例子）\n\nn `i`：忽略大小写\n\nn s///，s###，s@@@：用于避免字符冲突\n\nn \\(\\) \\1\\2\n\n\n\n**动作说明**：\n\n- a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(**目前的下一行**)\n- c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行\n- d ：删除， **d 后面不接任何字符串**；\n- i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；\n- p ：**打印**，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行\n- s ：取代，可以直接进行取代的工作！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！\n\n \n\n让我们来尝试一下吧\n\n创建sed.txt，内容如下：\n\n```tex\nAuthentication improvements when using an HTTP proxy server.\nSupport for POSIX-style filesystem extended attributes.\nYARN's REST APIs now support write/modify operations.\n```\n\n**第一行下插入一行 `sed  \"1a\\hello world\" sed.txt`**（没有修改源文件）：\n\n```bash\n[root@layne laydir]# cat sed.txt\nAuthentication improvements when using an HTTP proxy server.\nSupport for POSIX-style filesystem extended attributes.\nYARN's REST APIs now support write/modify operations.\n[root@layne laydir]# sed  \"1a\\hello world\" sed.txt\nAuthentication improvements when using an HTTP proxy server.\nhello world\nSupport for POSIX-style filesystem extended attributes.\nYARN's REST APIs now support write/modify operations.\n```\n\n加上`-i`**直接修改源文件**：`sed -i \"1a\\hello world\" sed.txt`\n\n**删除第2行**`sed -i \"2d\" sed.txt` ：\n\n```bash\n[root@layne laydir]# cat sed.txt\nAuthentication improvements when using an HTTP proxy server.\nhello world\nSupport for POSIX-style filesystem extended attributes.\nYARN's REST APIs now support write/modify operations.\n[root@layne laydir]# sed -i \"2d\" sed.txt\n[root@layne laydir]# cat sed.txt\nAuthentication improvements when using an HTTP proxy server.\nSupport for POSIX-style filesystem extended attributes.\nYARN's REST APIs now support write/modify operations.\n```\n\n删除文档中的**所有内容**`sed \"d\" sed.txt` （不修改原文件）\n\n匹配行中包含0-9任意一个字符的行，**默认匹配的行打印（匹配的行打印两次），没匹配的行也打印（匹配的行打印一次**），命令为`sed \"/[0-9]/p\"  sed.txt` ：\n\n```bash\n[root@layne laydir]# cat sed.txt\nuthentication improvements when using an HTTP proxy server.\nSupport for POSIX-style filesystem extended attributes.\nhello2world\nYARN's REST APIs now support write/modify operations.\n[root@layne laydir]# sed \"/[0-9]/p\"  sed.txt\nuthentication improvements when using an HTTP proxy server.\nSupport for POSIX-style filesystem extended attributes.\nhello2world\nhello2world\nYARN's REST APIs now support write/modify operations.\n```\n\n匹配行中包含0-9任意一个字符的行，**只打印找到的行**，加`-n` 实现`sed -n \"/[0-9]/p\"  sed.txt` ：\n\n```bash\n[root@layne laydir]# cat sed.txt\nuthentication improvements when using an HTTP proxy server.\nSupport for POSIX-style filesystem extended attributes.\nhello2world\nYARN's REST APIs now support write/modify operations.\n[root@layne laydir]# sed -n \"/[0-9]/p\"  sed.txt\nhello2world\n```\n\n将filesystem替换为FS命令为`sed \"s/filesystem/FS/\" sed.txt` （默认区分大小写，且只替换每一行匹配到的第一个）：\n\n```bash\n[root@layne laydir]# cat sed.txt\nuthentication improvements when using an HTTP proxy server.\nSupport FILESysTem for POSIX-style filesystem extended filesystem  attributes.\nYARN's REST APIs now filesystem support write/modify operations.\n[root@layne laydir]# sed \"s/filesystem/FS/\" sed.txt\nuthentication improvements when using an HTTP proxy server.\nSupport FILESysTem for POSIX-style FS extended filesystem  attributes.\nYARN's REST APIs now FS support write/modify operations.\n```\n\n将filesystem替换为FS，忽略大小写`sed \"s/filesystem/FS/i\" sed.txt` ：\n\n```bash\n[root@layne laydir]# cat sed.txt\nuthentication improvements when using an HTTP proxy server.\nSupport FILESysTem for POSIX-style filesystem extended filesystem  attributes.\nYARN's REST APIs now filesystem support write/modify operations.\n[root@layne laydir]# sed \"s/filesystem/FS/i\" sed.txt\nuthentication improvements when using an HTTP proxy server.\nSupport FS for POSIX-style filesystem extended filesystem  attributes.\nYARN's REST APIs now FS support write/modify operations.\n```\n\n将filesystem替换为FS，不仅忽略大小写还要行内全局替换`sed \"s/filesystem/FS/gi\" sed.txt` ：\n\n```bash\n[root@layne laydir]# cat sed.txt\nuthentication improvements when using an HTTP proxy server.\nSupport FILESysTem for POSIX-style filesystem extended filesystem  attributes.\nYARN's REST APIs now filesystem support write/modify operations.\n[root@layne laydir]# sed \"s/filesystem/FS/gi\" sed.txt\nuthentication improvements when using an HTTP proxy server.\nSupport FS for POSIX-style FS extended FS  attributes.\nYARN's REST APIs now FS support write/modify operations.\n```\n\n新建一个文件sed1.txt，输入以下内容：\n\n```tex\nhello world,hahha,This is hiaccy2 ideas\nI am 24 years old,lalala\n```\n\n将sed1.txt的所有数字替换为5，`sed \"s/[0-9]/5/g\" sed1.txt `\n\n```bash\n[root@layne laydir]# sed \"s/[0-9]/5/g\" sed1.txt\nhello world,hahha,This is hiaccy5 ideas\nI am 55 years old,lalala\n```\n\n\n\n将hiaccy2改为hiaccy9，先看下面的命令：\n\n```bahs\n[root@layne laydir]# sed  \"s/2/9/\"  sed1.txt\nhello world,hahha,This is hiaccy9 ideas\nI am 94 years old,lalala\n```\n\n会发现，发现将所有匹配的都修改了（注意并未修改原文件），第二行的24被替换为了94。\n\n更精确匹配方案的写法应该为如下命令：\n\n```bash\n[root@layne laydir]# sed  \"s/hiaccy[0-4]/9/\"  sed1.txt\nhello world,hahha,This is 9 ideas\nI am 24 years old,lalala\n```\n\n**但是还存在问题，匹配后被修改内容问匹配出的部分，范围过大。**解决办法：**反向引用**\n\n用命令`sed \"s/\\(hiaccy\\)[0-6]/\\19/\" sed1.txt`\n\n```bash\n[root@layne laydir]# sed \"s/\\(hiaccy\\)[0-6]/\\19/\" sed1.txt\nhello world,hahha,This is hiaccy9 ideas\nI am 24 years old,lalala\n```\n\n分析：\n\n`sed \"s/\\(hiaccy\\)[0-6]/\\19/\" sed1.txt` 的结构划分为：\n\n- `sed` 命令\n- `s/` **替换的前缀**（第一个分割标志）\n- `\\(hiaccy\\)` 其实就是hiaccy，括号用`\\(`和`\\)`，就是转义字符，查找的还是hiaccy\n- `[0-6]` 通配符数字1~6\n- `/` **替换的第二个分割标志**\n- `\\1` 表示前面的第一个转义内容，即`\\(hiaccy\\)`\n- `9` 把在第一个分割标志和第二个分割标志中间 ，**除了`\\1`表示的之外的内容**，替换为9\n- `/`第三个分割标志\n\n再看一个例子，把sed1.txt的中的world替换为woABCld：\n\n```bash\n[root@layne laydir]# sed \"s/\\(wo\\)r\\(ld\\)/\\1ABC\\2/\" sed1.txt\nhello woABCld,hahha,This is hiaccy2 ideas\nI am 24 years old,lalala\n```\n\n如果加上`-r`，就不用转义字符了：\n\n```bash\n[root@layne laydir]# sed -r \"s/(wo)r(ld)/\\1ABC\\2/\" sed1.txt\nhello woABCld,hahha,This is hiaccy2 ideas\nI am 24 years old,lalala\n```\n\n**查找/etc/profile中包含PATH的行，将这些行写到指定的文件：hello.log中** ：`sed -n \"/PATH/w hello.log\" /etc/profile`\n\n\n\n### 2.5 awk\n\nawk是一个强大的文本分析工具，相对于grep查找，sed编辑，awk在对数据分析并生成报告时更为强大。\n\n**awk把文件逐行读入，以空格和制表符作为默认分隔符将每行切片**，切开的部分再进行各种分析处理。\n\n**awk -F '{pattern + action}' {filenames}**\n\n- 支持自定义分隔符\n\n- 支持正则表达式匹配\n\n- 支持自定义变量，数组 a[1] a[tom] map(key)\n\n- 支持内置变量\n\n  - ARGC        命令行参数个数\n  - ARGV        命令行参数排列\n  - ENVIRON       支持队列中系统环境变量的使用\n  - FILENAME      awk浏览的文件名\n  - FNR        浏览文件的记录数\n  - FS         设置输入域分隔符，等价于命令行 -F选项\n  - NF         浏览记录的域的个数\n  - NR         已读的记录数\n  - OFS        输出域分隔符\n  - ORS        输出记录分隔符\n  - RS         控制记录分隔符\n\n- 支持函数\n\n  - print、split、substr、sub、gsub（注意：关键字不要写在“”里面）\n\n- 支持流程控制语句，类C语言\n\n  - if、while、do/while、for、break、continue\n\n  \n\n现在，让我们尝试一下吧！\n\n显示全部内容，`awk -F ':' '{print $0}' passwd`\n\n```bash\n[root@layne laydir]# cp /etc/passwd ./\n[root@layne laydir]# awk -F ':' '{print $0}' passwd\nroot:x:0:0:root:/root:/bin/bash\nbin:x:1:1:bin:/bin:/sbin/nologin\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\nadm:x:3:4:adm:/var/adm:/sbin/nologin\nlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\nsync:x:5:0:sync:/sbin:/bin/sync\nshutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\nhalt:x:7:0:halt:/sbin:/sbin/halt\nmail:x:8:12:mail:/var/spool/mail:/sbin/nologin\nuucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin\noperator:x:11:0:operator:/root:/sbin/nologin\ngames:x:12:100:games:/usr/games:/sbin/nologin\ngopher:x:13:30:gopher:/var/gopher:/sbin/nologin\nftp:x:14:50:FTP User:/var/ftp:/sbin/nologin\nnobody:x:99:99:Nobody:/:/sbin/nologin\nvcsa:x:69:69:virtual console memory owner:/dev:/sbin/nologin\nsaslauth:x:499:76:\"Saslauthd user\":/var/empty/saslauth:/sbin/nologin\npostfix:x:89:89::/var/spool/postfix:/sbin/nologin\nsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin\nntp:x:38:38::/etc/ntp:/sbin/nologin\nlucy:x:500:500::/usr/lucy:/bin/bash\nliubei:x:501:501::/home/liubei:/bin/bash\ngem:x:502:0::/home/gem:/bin/bash\nmysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/false\n```\n\n只显示第一列`awk -F ':' '{print $1}' passwd`\n\n显示第一列与第5列`awk -F ':' '{print $1,$5}' passwd`\n\n显示第一列与第5列，中间用逗号隔开`awk -F ':' '{print $1 \",\" $5}' passwd`\n\n  ```bash\n[root@layne laydir]# awk -F ':' '{print $1 \",\" $5}' passwd\nroot,root\nbin,bin\ndaemon,daemon\nadm,adm\nlp,lp\nsync,sync\nshutdown,shutdown\nhalt,halt\nmail,mail\nuucp,uucp\noperator,operator\ngames,games\ngopher,gopher\nftp,FTP User\nnobody,Nobody\nvcsa,virtual console memory owner\nsaslauth,\"Saslauthd user\"\npostfix,\nsshd,Privilege-separated SSH\nntp,\nlucy,\nliubei,\ngem,\nmysql,MySQL Server\n  ```\n\n制表符拼接字段`awk -F':' ' { print $1\"\\t\" $7} ' passwd`\n\n在所有行开始前添加列名name,shell,在最后一行添加\"shell,end\" ：`awk -F ':' 'BEGIN{print \"name,shell\"} {print $1 \",\" $7} END{print \"shell,end\"}' passwd`\n\n  ```bash\n[root@layne laydir]# awk -F ':' 'BEGIN{print \"name,shell\"} {print $1 \",\" $7} END{print \"shell,end\"}' passwd\nname,shell\nroot,/bin/bash\nbin,/sbin/nologin\ndaemon,/sbin/nologin\nadm,/sbin/nologin\nlp,/sbin/nologin\nsync,/bin/sync\nshutdown,/sbin/shutdown\nhalt,/sbin/halt\nmail,/sbin/nologin\nuucp,/sbin/nologin\noperator,/sbin/nologin\ngames,/sbin/nologin\ngopher,/sbin/nologin\nftp,/sbin/nologin\nnobody,/sbin/nologin\nvcsa,/sbin/nologin\nsaslauth,/sbin/nologin\npostfix,/sbin/nologin\nsshd,/sbin/nologin\nntp,/sbin/nologin\nlucy,/bin/bash\nliubei,/bin/bash\ngem,/bin/bash\nmysql,/bin/false\nshell,end\n  ```\n\n可以先在txt文本上写出来，再粘贴进去\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210208140020.png)\n\n  **模板**\n\n```bash\nawk `BEGIN {\n\t\t...\n}\n{\n\t...\n}\nEND {\n\t...\n}`\nfilename\n\t\n```\n\n**搜索passwd有root关键字的所有行** `awk '/root/ {print $0}' passwd`\n\n**搜索passwd有root关键字的所有行**，并在后面打印`ok`，再打印passwd所有的内容\n\n`awk ' /root/ {print $0,\"ok\"} {print $0}' passwd`  \n\n```bash\n[root@layne laydir]# awk  ' /root/  {print $0,\"ok\"}  {print $0}'  passwd\nroot:x:0:0:root:/root:/bin/bash ok\nroot:x:0:0:root:/root:/bin/bash\nbin:x:1:1:bin:/bin:/sbin/nologin\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\nadm:x:3:4:adm:/var/adm:/sbin/nologin\nlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\nsync:x:5:0:sync:/sbin:/bin/sync\nshutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\nhalt:x:7:0:halt:/sbin:/sbin/halt\nmail:x:8:12:mail:/var/spool/mail:/sbin/nologin\nuucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin\noperator:x:11:0:operator:/root:/sbin/nologin ok\noperator:x:11:0:operator:/root:/sbin/nologin\ngames:x:12:100:games:/usr/games:/sbin/nologin\ngopher:x:13:30:gopher:/var/gopher:/sbin/nologin\nftp:x:14:50:FTP User:/var/ftp:/sbin/nologin\nnobody:x:99:99:Nobody:/:/sbin/nologin\nvcsa:x:69:69:virtual console memory owner:/dev:/sbin/nologin\nsaslauth:x:499:76:\"Saslauthd user\":/var/empty/saslauth:/sbin/nologin\npostfix:x:89:89::/var/spool/postfix:/sbin/nologin\nsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin\nntp:x:38:38::/etc/ntp:/sbin/nologin\nlucy:x:500:500::/usr/lucy:/bin/bash\nliubei:x:501:501::/home/liubei:/bin/bash\ngem:x:502:0::/home/gem:/bin/bash\nmysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/false\n```\n**搜索passwd有root关键字的所有行的第一列**，并在后面打印`ok`，再打印passwd所有的内容\n\n`awk -F ':'  ' /root/  {print $1,\"ok\"}  {print $0}'  passwd`\n\n```bash\n[root@layne laydir]# awk -F ':'  ' /root/  {print $1,\"ok\"}  {print $0}'  passwd\nroot ok\nroot:x:0:0:root:/root:/bin/bash\nbin:x:1:1:bin:/bin:/sbin/nologin\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\nadm:x:3:4:adm:/var/adm:/sbin/nologin\nlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\nsync:x:5:0:sync:/sbin:/bin/sync\nshutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\nhalt:x:7:0:halt:/sbin:/sbin/halt\nmail:x:8:12:mail:/var/spool/mail:/sbin/nologin\nuucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin\noperator ok\noperator:x:11:0:operator:/root:/sbin/nologin\ngames:x:12:100:games:/usr/games:/sbin/nologin\ngopher:x:13:30:gopher:/var/gopher:/sbin/nologin\nftp:x:14:50:FTP User:/var/ftp:/sbin/nologin\nnobody:x:99:99:Nobody:/:/sbin/nologin\nvcsa:x:69:69:virtual console memory owner:/dev:/sbin/nologin\nsaslauth:x:499:76:\"Saslauthd user\":/var/empty/saslauth:/sbin/nologin\npostfix:x:89:89::/var/spool/postfix:/sbin/nologin\nsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin\nntp:x:38:38::/etc/ntp:/sbin/nologin\nlucy:x:500:500::/usr/lucy:/bin/bash\nliubei:x:501:501::/home/liubei:/bin/bash\ngem:x:502:0::/home/gem:/bin/bash\nmysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/false\n```\n\n\n\n\n\n\n\n统计passwd文件中，（按 “:” 分）每行的行号，每行的列数，对应的完整行内容（ $0对应完成的内容）\n\n`awk -F ':' '{print NR \"-\" NF \"-\" $0}' passwd`\n\n```bash\n[root@layne laydir]# awk -F ':' '{print NR \"-\" NF \"-\" $0}' passwd\n1-7-root:x:0:0:root:/root:/bin/bash\n2-7-bin:x:1:1:bin:/bin:/sbin/nologin\n3-7-daemon:x:2:2:daemon:/sbin:/sbin/nologin\n4-7-adm:x:3:4:adm:/var/adm:/sbin/nologin\n5-7-lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\n6-7-sync:x:5:0:sync:/sbin:/bin/sync\n7-7-shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\n8-7-halt:x:7:0:halt:/sbin:/sbin/halt\n9-7-mail:x:8:12:mail:/var/spool/mail:/sbin/nologin\n10-7-uucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin\n11-7-operator:x:11:0:operator:/root:/sbin/nologin\n12-7-games:x:12:100:games:/usr/games:/sbin/nologin\n13-7-gopher:x:13:30:gopher:/var/gopher:/sbin/nologin\n14-7-ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin\n15-7-nobody:x:99:99:Nobody:/:/sbin/nologin\n16-7-vcsa:x:69:69:virtual console memory owner:/dev:/sbin/nologin\n17-7-saslauth:x:499:76:\"Saslauthd user\":/var/empty/saslauth:/sbin/nologin\n18-7-postfix:x:89:89::/var/spool/postfix:/sbin/nologin\n19-7-sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin\n20-7-ntp:x:38:38::/etc/ntp:/sbin/nologin\n21-7-lucy:x:500:500::/usr/lucy:/bin/bash\n22-7-liubei:x:501:501::/home/liubei:/bin/bash\n23-7-gem:x:502:0::/home/gem:/bin/bash\n24-7-mysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/false\n```\n\n\n\n**案例：**\n\n统计报表：合计每人1月总消费，0：manager（管理员），1：worker（工人）\n\nemp.txt中的内容如下：\n\n```tex\nTom\t 0   2020-12-11      car     3000\nJohn\t 1   2020-01-13      bike    1000\nvivi\t 1   2020-01-18      car     2800\nTom\t 0   2020-01-20      car     2500\nJohn\t 1   2020-01-28      bike    3500\n```\n\n解决方案：\n\n```bash\nawk  '{\nsplit($3, date, \"-\")\nif (date[2] == \"01\") {\nmap_name_sala[$1]+=$5 \nif($2==\"0\"){\nmap_name_role[$1]=\"Manager\"\n}else{\nmap_name_role[$1]=\"Worker\"\n}\n}\n}  \nEND{\nfor(name in map_name_sala){\nprint name\"\\t\"map_name_sala[name]\"\\t\"map_name_role[name]\n}\n}'  emp.txt\n```\n\n解释：\n\n`split($3, date, \"-\")` 意思是将第三列按照`-`分割，赋值给date数组，这样date[2]就是月份。\n\n`map_name_sala` 和 `map_name_role` 都是map[key]类型。\n\n执行上述命令后，输出结果为：\n\n```tex\nvivi\t2800\tWorker\nTom\t2500\tManager\nJohn\t4500\tWorker\n```\n\n","tags":["Linux"],"categories":["Linux"]},{"title":"使用Xftp和lrzsz在Linux与Windows之间互传文件","url":"/2021/02/04/212039/","content":"\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210204212602.png)\n<!-- more -->\n\n\n一般，我们使用Xftp向Linux中传输文件，该软件以可视化的形式显示Linux中的文件目录，真的是对新手非常友好，下面就让我们来看看吧！\n\n首先，安装Xftp软件（可以向博主获取）并打开，点击弹出框右上角的“新建”按钮：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203183652.png)\n\n依次输入名称、主机（用IP地址）、协议（用SFTP）、端口（22）、用户名（root）、密码，然后点击“确定”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203183805.png)\n\n再双击新建好的连接，就可以成功连接到Linux系统了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203184044.png)\n\n想要将本地的文件上传到Linux系统中，只需要将文件从左侧拖到右侧即可。\n\n同样的，如果想要把Linux中的文件下载到Windows平台下，只需要将文件从右侧拖到左侧即可。\n\n另外，有一个非常小巧的工具lrzsz，也非常适合在Linux和Windows之间互传文件。\n\n输入`yum install lrzsz -y` 安装 lrzsz，想要上传文件，只需要将文件拖到连接Linux的Shell中即可。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203184450.png)\n\n当把文件拖到Shell中时，会自动调用lrzsz的`rz -E`命令。\n\n同样，如果想要把文件从Linux下载到本地，只需要输入`sz 文件名`。如下图，我下载一个etc目录下的my.cnf文件到本地：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203184757.png)\n\n其实，lrzsz的原理是客户端和服务器模式，服务器是我们的Linux系统，客户端是我们本地的PC机。`rz`是receive的意思，即服务器收到文件，`sz`是send的意思，即服务器向客户端发送文件。\n\n有了lrzsz，我们就不需要专门打开Xftp软件了，直接用命令就可以完成文件的传输工作。不过，还是建议小文件用lrzsz，大文件用Xftp，毕竟lrzsz还只是个小工具，大文件伤不起呀！\n\n\n\n","tags":["Linux","Xftp","lrzsz"],"categories":["Linux"]},{"title":"Linux切换运行级别、关闭防火墙、禁用selinux、关闭sshd、时间同步、修改时区、拍摄快照、克隆操作、修改语言环境","url":"/2021/02/04/211133/","content":"\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210204212150.png)\n<!-- more -->\n\n\n\n@[TOC](文章目录)\n\n\n\n## 1. Linux运行级别及切换\n\n在系统输入`vim /etc/inittab`可以查看系统的运行级别：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203105614.png)\n\n可以看到，Linux有7个运行级别，分别是：\n\n- 0： 系统停止（关机）\n- 1：单用户模式\n- 2：无网络的多用户模式\n- 3：有网络的多用户模式（完整多用户模式）\n- 4：未使用\n- 5：图形化界面\n- 6：重启\n\n`id:3:initdefault`表示系统初始化的运行级别为3，一般常用的运行级别是3 和 5。如果要修改系统初始化的运行级别，那么只需修改 “id：” 后面的数字即可。\n\n**查看当前运行级别**\n\n输入`runlevel`可以查看系统当前的运行级别：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203110129.png)\n\n上图，`N`表示自系统启动后运行级别尚未更改，`3`表示系统的当前运行级别。\n\n**切换运行级别**\n\n除了通过修改`inittab`文件中的默认运行级别之外，管理员还可以通过`init `命令来任意切换 7个级别，值得一提的是，`init 0`和`init 6`等效于 shutdown 和 reboot 。\n\n执行`init`命令，将系统级别切换到5，再将级别切换回3：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203110555.png)\n\n上图，`5 3`表示系统更改之前的运行级别为5，当前运行级别为3。\n\n**查看系统中所有服务在各运行级别中的启动状态**\n\n输入`chkconfig`：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203110752.png)\n\n上图0代表关闭，1代表启用。\n\n**查看network服务在各运行级别中的启动状态**\n\n输入`chkconfig --list network`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203121539.png)\n\n\n\n\n\n\n\n## 2. 关闭防火墙并禁止开机启动\n\n在Linux系统中，`iptables`表示防火墙的服务名，输入`chkconfig --list iptables`查看防火墙在系统各运行级别的启动状态。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203122603.png)\n\n可以看到，iptables在运行级别2，3，4，5是on，\n\n输入命令`chkconfig iptables off`全部**禁止开机启动**：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203123324.png)\n\n现在我们只是禁止了开机启动，并没有关闭当前运行的防火墙服务，可以输入`service iptables status`查看防火墙的状态：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203123824.png)\n\n可以看到，防火墙还在运行状态，输入命令`service iptables stop`以关闭防火墙服务：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203124002.png)\n\n到了这里，防火墙服务以及防火墙开机启动都关闭成功了。\n\n## 3. 禁用selinux\n\n安全增强型 Linux（Security-Enhanced Linux）简称 seLinux，它是一个 Linux 内核模块，也是 Linux 的一个安全子系统。selinux可能会带来一些权限方面的问题，这是我们在开发过程中不愿看到的，一般将其关闭。\n\n首先在命令行输入`getenforce` 查看selinux的状态：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203130235.png)\n\n可以看到，selinux在运行中。\n\n在命令行输入 `vim /etc/selinux/config` 编辑selinux的配置文件：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203130621.png)\n\n将`SELINUX=enforcing`注释，增加一行`SELINUX=disabled`，保存并退出，输入`init 6`重启虚拟机才能生效。\n\n\n\n## 4. 关闭sshd服务\n\n一般我们通过**关闭sshd服务的DNS以加快SSH登录速度**。\n\n在命令行输入`vim /etc/ssh/sshd_config` ，在vim的命令模式下输入`/UseDNS`查找UseDNS的位置（可以使用n选择下一个匹配的项目），然后将去掉UseDNS的注释，并将yes改为no，保存退出。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203131358.png)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203131637.png)\n\n输入`service sshd restart`重启sshd服务：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203131815.png)\n\n重启之后，修改的配置项就加载了。\n\n\n\n## 5. 时间同步\n\n输入`date`可以查看当前时间：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203132029.png)\n\n发现时间不对，这时输入`yum install ntp -y`，安装完ntp之后系统中多了两个服务：`ntpd`和`ntpdate`，如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203132432.png)\n\n可以看到，ntpd的状态都是off，也就是这个服务不会开机启动，现在设置ntp为开机启动。\n\n输入`chkconfig ntpd on`，再次通过`chkconfig`查看：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203132717.png)\n\n这时，ntpd服务在系统的2、3、4、5级别上已设置为开机启动。\n\n输入 `service ntpd status`查看ntpd的服务状态，如果显示stopped表示没有启动，此时输入`service ntpd start`启动服务，**过几分钟**（5分组左右）就通过网络自动同步到当前时间了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203133012.png)\n\n## 6. 修改时区\n\n输入`date`查看当前系统的时间和时区：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203150127.png)\n\n可以看到，当前时区为EST（英国时间），现在我们要改为CST（北京时间），可以采用如下步骤：\n\n（1）拷贝时区文件\n\n输入 `cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime`\n\n（2）修改/etc/sysconfig/clock\n\n输入`vim /etc/sysconfig/clock`将文件里的内容改为：\n\n```bash\nZONE=\"Asia/Shanghai\"\nUTC=true\nARC=false\n```\n\n这里补充一下，ZONE、UTC、ARC、SRM：\n\n- ZONE：指定时区，ZONE的值是一个文件的相对路径名，这个文件是相对 /usr/share/zoneinfo 目录下的一个时区文件。比如ZONE的值可以是：“Asia/Shanghai\"、\"US/Pacific\"、\"UTC\" 等。\n- UTC：指定BIOS中保存的时间是否是GMT/UTC时间，true表示BIOS里面保存的时间是UTC时间，false表示BIOS里面保存的时间是本地时间。\n- ARC：这个选项一般配置false，在一些特殊硬件（Alpha）下才配置该选项为true。\n- SRM：同ARC，该选项一般配置false，在一些特殊硬件下才配置该选项为false。\n\n（3）修改/etc/profile\n\n输入以下命令追加写入到/etc/profile中：\n\n```bash\necho \"TZ='Asia/Shanghai'; export TZ  \">>/etc/profile\nsource /etc/profile #编译立即生效\n```\n\n需要注意的是：\n\n- **/etc/profile**：影响所有用户\n- **~/.bashrc**：影响当前用户\n\n想要了解更多source和profile的内容，可参考[source /etc/profile作用](https://blog.csdn.net/llzhang_fly/article/details/104980029)\n\n（4）再次输入`date`查看，发现已经修改成功了\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203153352.png)\n\n\n\n\n\n## 7. 删除70-persistent-net.rules文件\n\n我们先来看看为什么要删除70-persistent-net.rules文件：\n\n- 许多Linux distribution使用udev来动态管理**设备文件**，并根据设备的信息对其进行持久化命名。\n- udev 会在系统引导的过程中识别网卡，**将mac地址和网卡名称对应起来**，并记录在udev的规则脚本中。\n- 当我们在克隆新的虚拟机时，会自动为虚拟机的网卡生成MAC地址。由于你使用的是以前系统虚拟硬盘的信息，而该系统中已经有eth0（表示第一块有线网卡）的信息，对于这个新的网卡，udev会自动将其命名为eth1 （累加的原则）。\n- udev记录网络规则的脚本为：/etc/udev/rules.d/70-persistent-net.rules，当克隆新虚拟机打开该文件后，会发现，里面有eth0，eth1 这两个网卡的信息，但实际上输入`ifconfig`时只能发现eth1这一个网卡的信息，这是因为eth0根本就不存在。\n\n我在第一次克隆完虚拟机后，输入`vim /etc/udev/rules.d/70-persistent-net.rules` 确实发现有两条mac地址和网卡名称的对应记录，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203134321.png)\n\n此时，系统使用的是新的网卡eth1，，输入 `vi /etc/sysconfig/network-scripts/ifcfg-eth0`查看当前虚拟机的网络配置文件，可以发现该虚拟机使用是eth0，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203134641.png)\n\n这样就对应不上了，此时解决的方法有两个。\n\n**方法一：将70-persistent-net.rules中eth0的那条记录删除，将eht1改为eth0**\n\n但是，如果我们要克隆多台虚拟机，采用方法一，岂不是要修改多次，所以最好采用方法二。\n\n**方法二：输入`rm -rf /etc/udev/rules.d/70-persistent-net.rules`删除70-persistent-net.rules**\n\n删除70-persistent-net.rules后，再输入`init 0`关机，这时再克隆虚拟机。启动新克隆的虚拟机后，系统会自动产生70-persistent-net.rules文件，并且只有eth0这一个网卡记录，这样就对应上了，可以愉快的联网了。\n\n\n\n## 8. 拍摄快照\n\n虽然开机的时候也可以拍摄快照，但为了防止系统的不稳定，最好在关机之后拍，因为系统在运行过程中也会产生数据。\n\n拍摄快照的步骤如下：\n\n（1）点击下图按钮\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203154109.png)\n\n（2）点击“拍摄快照”，输入快照名称，再点击弹出框的拍摄快照。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203154314.png)\n\n这样就完成了一个快照的拍摄了。\n\n## 9. 克隆虚拟机\n\n虚拟机的克隆有两种方式**链接克隆**和**完整克隆**。\n\n- **链接克隆**就是在被克隆的虚拟机上创建了一个链接指向新的虚拟机，但是如果被克隆的虚拟机系统损坏，则在此基础上所有链接克隆的虚拟机都将不能用。但是链接克隆一个好处是克隆出来的虚拟机不会占用整个虚拟机的磁盘空间，同时链接克隆速度比较快，不用经历安装虚拟机要命的超长等待时间。\n- **完整克隆**是对被克隆虚拟机的一个完整副本，此副本虚拟机完全独立，但需要较多的存储磁盘空间。\n\n这里为了速度和存储空间，我们选择链接克隆的方式。步骤如下：\n\n（1）点击下图按钮\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203154109.png)\n\n（2）选择一个虚拟机的快照，点击“克隆”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203160606.png)\n\n（3）点击下一步，选择“现有快照”，再点下一步。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203160701.png)\n\n（4）选择创建链接克隆，点击下一步。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203160805.png)\n\n（5）填写克隆的虚拟机名称，选择位置，点击完成。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203160852.png)\n\n此时，克隆操作已经完成，但是新克隆出来的虚拟机还不能上网，必须进程配置。\n\n启动新克隆的虚拟机并登陆，用户名和密码与原来的虚拟机一样。\n\n登陆完毕后，输入 `vim /etc/sysconfig/network`修改计算机名称（hostname）：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203161605.png)\n\n然后输入 `vim /etc/sysconfig/network-scripts/ifcfg-eth0` 配置网络：\n\n这是修改之前的网络配置\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203161822.png)\n\n这是修改之后的网络配置\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203162011.png)\n\n把网络的IP地址修改一下，别和之前的冲突就行。\n\n重启layne1虚拟机，使用XShell连接，在XShell中输入`ssh root@192.168.218.51`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203162457.png)\n\n点击“接受并保存”，输入密码，连接成功！\n\n输入`ifconfig`查看网络配置，可以看到，刚刚配置已经生效。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203162612.png)\n\n输入`ping www.baidu.com`，连网成功\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203162718.png)\n\n输入`vim /etc/hosts`编辑CentOS的hosts文件，将`192.168.218.50`改为`192.168.218.51`，将`layne`改为`layne1`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203163146.png)\n\n输入`ping layne1`，可以看到，layne1被解析为了`192.168.218.51`，大功告成。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203163316.png)\n\n在自己的windows上，进入`C:\\Windows\\System32\\drivers\\etc`目录修改host文件：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210203163650.png)\n\n然后，windows平台就可以和克隆的虚拟机进行通信了。\n\n\n\n## 10. 修改语言环境\n\n首先，查看当前语言环境：\n\n```bash\n[root@layne ~]# echo $LANG\nen_US.UTF-8\n```\n\n所以需要修改系统的语言环境：\n\n```bash\n[root@layne ~]# LANG=zh_CN.UTF-8\n[root@layne ~]# echo $LANG\nzh_CN.UTF-8\n```\n\n这样就修改为中文环境了，但是这样修改**在断开连接或者下次重启系统**，就无效了。如何才能长期有效呢？\n\n输入`vim /etc/sysconfig/i18n`，将之前的语言变量注释，加入以下信息即可：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210207113321.png)\n\n\n\n\n\n\n\n\n\n参考文档\n\n[修改系统时间为UTC时间](https://blog.csdn.net/weixin_34346099/article/details/93569068)\n\n","tags":["Linux","selinux"],"categories":["Linux"]},{"title":"把Linux系统中的yum替换为清华源，速度瞬间从5KB/s变到3.5MB/s","url":"/2021/02/04/211126/","content":"\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210204211713.png)\n<!-- more -->\n\n\n\n在Linux系统中，使用yum安装软件特别方便，可恨的是，yum原本镜像为`mirror.centos.org`，服务器在国外，下载速度5KB/s，真的等的我花儿都谢了。\n\n我网上找了很多资料，首先找到了阿里云的镜像，奈何阿里云官方把CentOS-6的yum源给删了，最低支持CentOS-7的，我用的是CentOS-6.5（大家可以输入`cat /etc/centos-release`查看版本当前操作系统版本） ，无奈，那就换个源吧！\n\n功夫不负有心人，终于在[清华大学开源软件镜像站](https://mirrors.tuna.tsinghua.edu.cn/)找到了下面一句话：\n\n> 该文件夹只提供 CentOS 7 与 8，架构仅为 `x86_64` ，如果需要较早版本的 CentOS，请参考 centos-vault 的帮助，若需要其他架构，请参考 centos-altarch 的帮助。\n\n我点到`centos-vault`下，尝试了一下，还真的可以，每秒几MB，真的发大了，哈哈。\n\n下面，我们来一步一步配置CentOS-6的yum源吧！\n\n（1）在Linux命令行中输入如下命令，备份`CentOS-Base.repo`文件\n\n```bash\ncp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak\n```\n\n（2）输入`vi /etc/yum.repos.d/CentOS-Base.repo`编辑`CentOS-Base.repo`文件，将 在 `mirrorlist=` 开头行前面加 `#` 注释掉；并将 `baseurl=` 开头行取消注释（如果被注释的话），把该行内的域名及路径（例如`mirror.centos.org/centos`）替换为 `mirrors.tuna.tsinghua.edu.cn/centos-vault`。以上步骤可以被下方的命令一步完成：\n\n```bash\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://mirror.centos.org/centos/|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos-vault/|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/CentOS-Base.repo\n```\n\n按理说，到这里应该就可以了，我试着执行`yum install vim`试了一下，出现了`404 Not Found`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202231401.png)\n\n\n\n到底怎么回事呢？我试着访问了下`https://mirrors.tuna.tsinghua.edu.cn/centos-vault/6/`，还真访问不到。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202231606.png)\n\n我再试着把上述链接的6改为了6.0，访问`https://mirrors.tuna.tsinghua.edu.cn/centos-vault/6.0/`，竟然访问到了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202231800.png)\n\n找到了出错原因，那就好办了，依次执行以下命令：\n\n```bash\ncp /etc/yum.repos.d/CentOS-Base.repo.bak /etc/yum.repos.d/CentOS-Base1.repo.bak #为了以防万一，再次备份CentOS-Base.repo文件\nmv /etc/yum.repos.d/CentOS-Base.repo.bak /etc/yum.repos.d/CentOS-Base.repo #将CentOS-Base.repo.bak重命名为CentOS-Base.repo\nvi /etc/yum.repos.d/CentOS-Base.repo\n:%s/$releasever/6.0/g #将文件中$releasever全部改成6.0\nyum clean all && yum makecache # 清除和缓存\n```\n\n再次执行`yum install vim -y`，嘻嘻，每秒3.5MB，赚大发了。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202232628.png)\n\n\n\n最后，补充几个其它的源：\n\n**企业贡献：**\n搜狐开源镜像站：http://mirrors.sohu.com/\n网易开源镜像站：http://mirrors.163.com/\n\n**大学教学：**\n北京理工大学：\n[http://mirror.bit.edu.cn](http://mirror.bit.edu.cn/) (IPv4 only)\n[http://mirror.bit6.edu.cn](http://mirror.bit6.edu.cn/) (IPv6 only)\n北京交通大学：\n[http://mirror.bjtu.edu.cn](http://mirror.bjtu.edu.cn/) (IPv4 only)\n[http://mirror6.bjtu.edu.cn](http://mirror6.bjtu.edu.cn/) (IPv6 only)\n[http://debian.bjtu.edu.cn](http://debian.bjtu.edu.cn/) (IPv4+IPv6)\n兰州大学：http://mirror.lzu.edu.cn/\n厦门大学：http://mirrors.xmu.edu.cn/\n清华大学：\nhttp://mirrors.tuna.tsinghua.edu.cn/ (IPv4+IPv6)\nhttp://mirrors.6.tuna.tsinghua.edu.cn/ (IPv6 only)\nhttp://mirrors.4.tuna.tsinghua.edu.cn/ (IPv4 only)\n天津大学：http://mirror.tju.edu.cn/\n中国科学技术大学：\nhttp://mirrors.ustc.edu.cn/ (IPv4+IPv6)\nhttp://mirrors4.ustc.edu.cn/\nhttp://mirrors6.ustc.edu.cn/\n东北大学：\nhttp://mirror.neu.edu.cn/ (IPv4 only)\nhttp://mirror.neu6.edu.cn/ (IPv6 only)\n电子科技大学：http://ubuntu.uestc.edu.cn/\n\n\n\n亲测，只有搜狐源、东北大学、清华大学维护CentOS6，其它的应该最低从CentOS7开始维护。\n\n\n\n\n\n","tags":["Linux","yum"],"categories":["Linux"]},{"title":"在vmware中安装CentOS虚拟机，保姆式教学！","url":"/2021/02/04/205940/","content":"\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210204211554.png)\n\n<!-- more -->\n\n\n\n## 1. 安装vmware、Xshell、Xftp\n\n现在vmware用的最多的两个版本是12和15：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202170216.png)\n\n这两个版本，根据系统的不同来安装，建议windows7安装vmware12.5.6，windows10安装vmware15.1.0（本文所有软件可联系博主获取软件）。\n\nXftp和Xshell可以方便在windows平台上操作虚拟机：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202170644.png)\n\nXshell用于连接本地和远程的虚拟机，Xftp可以方便在windows和vmware之间传输文件。安装的时候要注意，选择家庭/教育版安装，不要选择商业版。\n\n安装软件时直接点下一步即可！\n\n## 2. 创建vmware虚拟机\n\n（1）启动vmware软件，点击“创建新的虚拟机”，或者右上角”文件“新建虚拟机。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202171714.png)\n\n\n\n（2）选择“自定义”，点击“下一步”\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202171923.png)\n\n（3）选择“WorkStation12.x”硬件兼容性，该版本兼容的产品最多。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202172021.png)\n\n（4）选择“稍后安装操作系统”，点击下一步。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202172213.png)\n\n（5）在客户机操作系统中选择“Linux”，“版本”建议选择为**CentOS 6 64位**，点击下一步。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202172404.png)\n\n（6）填写“虚拟机名称”，选择“位置”，“位置”是将新建的虚拟机文件安装到哪个目录，建议选择安装在大一点的磁盘目录，路径不要出现中文和空格（注意创建二级目录）。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202172652.png)\n\n（7）选择“处理器数据”和“每个处理器的内核数量”，如果你的处理器有8核，可以把每个处理器的内核数量设置为2，一般的电脑处理器为4核，建议两个都设置为1。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202173122.png)\n\n（8）虚拟机的内存建议设置为1024MB，如果你的电脑内存很大（超过16GB），可设置为2048MB。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202173513.png)\n\n（9）网络类型选择“使用网络地址转换(NAT)”，关于vmware的三种模式，可以参考[VMware的三种网络模式 — NAT模式、桥接模式、仅主机模式](https://blog.csdn.net/qq_37555071/article/details/113575483)。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202173632.png)\n\n（10）I/O控制器类型，选择默认值\"LSI Logic(L)\"，点击下一步。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202182204.png)\n\n（11）虚拟磁盘类型也选择默认值“SCSI”，点击下一步。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202182243.png)\n\n（12）选择默认选项“创建新虚拟磁盘”，点击下一步。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202182438.png)\n\n（13）磁盘容量可以设置为20GB，选择“将虚拟磁盘存储为单个文件”以提高性能，这里不要勾选“立即分配所有磁盘空间”，否则会立即分配20GB空间，不勾选意味着所占空间会随着虚拟机中的文件增多而逐渐增大。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202182649.png)\n\n（14）虚拟机磁盘文件的命名默认即可。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202183034.png)\n\n（15）这一步会显示新建虚拟机的信息，直接点击完成。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202183218.png)\n\n## 3. 配置虚拟机镜像文件\n\n（1）点击“编辑虚拟机设置”\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202184318.png)\n\n（2）选中“CD/DVD(IDE)”，选择右侧的“使用ISO映像文件”，点击“浏览”，选择自己的ISO文件（我的为CentOS-6.5），点击“确定”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202184209.png)\n\n（3）点击“开启虚拟机”\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202184511.png)\n\n（4）把鼠标点进去，使用“↑”、“↓”键移动，这里选择默认的“Intall or upgrade an existing system”安装镜像文件。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202190512.png)\n\n（5）使用“←”、“→”键选择“Skip”跳过磁盘检查，按回车键。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202184858.png)\n\n（6）直接点击回车。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202184954.png)\n\n（7）在CentOS图形安装界面，点击右下角的“Next”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202185935.png)\n\n（8）选择“English\"，不要选择中文。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202190119.png)\n\n（9）选择\"U.S.English\"美式英语，点击”Next\"。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202190229.png)\n\n（10）选择磁盘的类型，这里选择默认值“Basic Storage Devices”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202190712.png)\n\n（11）第一次使用，在磁盘分区时不需要保留磁盘上的数据，所以点击“Yes, discard any data”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202190852.png)\n\n（12）这里配置操作系统主机名和网络，直接默认即可（默认主机名localhost，网络为localdomain)，点击“Next”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202191139.png)\n\n（13）时区选择“Asia/Shanghai”，点击右下角“Next”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202191302.png)\n\n（14）设置密码，直接填123456即可，点击“Next\"会提示密码太简单，再点击“Use Anyway”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202191411.png)\n\n（15）这里选择磁盘分区方式，选择最后一个”Create Custom Layout“，自定义磁盘分区，然后点击右下角”Next“。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202191742.png)\n\n（16）创建标准分区，选择第一个“Standard Partition”，然后再点击右下角“Create”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202192346.png)\n\n（17）选择挂载点“/boot”，文件类型“ext4”，大小设置为200MB，选择“Fixed Size”（确定大小），点击右下角“OK”创建第一个分区。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202192645.png)\n\n（18）继续点击Create，选择“Standard Partition”，设置swap分区（交换分区），大小设置为2048MB，选择“Fixed Size”（确定大小），点击右下角“OK”创建第二个分区。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202193124.png)\n\n（19）再次继续点击Create，选择“Standard Partition”，设置最后一个分区，将新分区挂载到“/”（根分区），文件类型设置为“ext4”，选择“Fill to maximum allowable size”（充满磁盘剩余总容量），点击右下角“OK”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202193409.png)\n\n（20）三个分区创建完毕后，点击右下角“Next”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202193650.png)\n\n（21）点击“Format”格式化磁盘分区。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202193744.png)\n\n（22）点击“Write changes to disk”进行格式化\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202193918.png)\n\n（23）这里按照默认配置在/dev/sda设备安装boot loader，用于启动/dev/sda3上的CentOS系统，直接点击右下角\"Next\"即可。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202194022.png)\n\n（24）现在开始安装系统，一共205个包需要安装。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202194217.png)\n\n（25）最后点击reboot重启虚拟机\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202203147.png)\n\n（26）重启虚拟机后，输入用户名root，按回车键，再输入密码（123456），再点击回车进入系统。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202203418.png)\n\n\n\n## 4. 配置虚拟机网络\n\n（1）使用vi编辑\"**/etc/sysconfig/network**\"，设置计算机的名称\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202204047.png)\n\n（2）按小写的i进入编辑模式，修改HOSTNAME为自己设置的主机如，如`HOSTNAME=layne`，第一行不变，修改完之后，按ESC键退出编辑模式，输入`:wq`保存并退出。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202204345.png)\n\n出现下面的提示就表示保存成功了：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202205115.png)\n\n这里补充几个vi的常用命令，后面会经常用到。\n\n- `u` 撤销\n- `dd` 删除一行\n- `yy p` 复制粘贴\n- `:wq` 保存并退出\n- `:q!`  只退出不保存\n- `o` 另开一行\n- `r` 替换\n- `:.,$d` 清空文档\n\n更多vi命令，可参考[Linux——vi命令详解](https://blog.csdn.net/cyl101816/article/details/82026678)。\n\n（3）输入`vi /etc/sysconfig/network-scripts/ifcfg-eth0`编辑网络配置文件，在输入过程中可以按`Tab`键提示和补全。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202205405.png)\n\n上图中：\n\n- DEVICE表示硬件的名称，eth0表示第一块有线网卡\n- HWADDR表示该有线网卡的MAC地址\n- TYPE表示该网卡的类型，Ethernet指以太网\n- UUID表示该网卡设备的编号，操作系统中每个设备都有一个唯一的编号\n- ONBOOT表示该网卡是否随系统启动而启动\n- BOOTPROTO表示网卡连接网络的类型，默认是dhcp的\n\n为了配置IP地址、子网掩码、网关、DNS，我们需要首先查看vmware的网络编辑器。\n\n（4）在vmware中选择“编辑”，点击“虚拟网络编辑器”\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202205840.png)\n\n（5）在虚拟网络编辑器中，点击“更改设置”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202210049.png)\n\n（6）选中“NAT模式”，查看子网IP和子网掩码。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202210211.png)\n\n上图中，我的子网IP是`192.168.218.0`，子网掩码是`255.255.255.0`，也可以点击上图右侧的”NAT设置“查看详情。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202210444.png)\n\n上图中，可以看到子网IP、子网掩码、网关IP（不要更改），所以在虚拟机中可配置：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202211110.png)\n\n在上图中，我做的操作是：\n\n- 删除了MAC地址行和UUID行\n- 将ONBOOT修改为yes，随系统启动\n- BOOTPROTO修改为static，表示静态IP地址配置\n- 配置IP地址`IPADDR=192.168.218.50`（前三位是自己的网络号，后一位可以自己设置）\n- 配置子网掩码`NETMASK=255.255.255.0`\n- 配置网关`GATEWAY=192.168.218.2`\n- 配置首选DNS解析地址`DNS1=192.168.218.2`\n- 配置备选DNS解析地址`DNS2=11.114.114.114`\n\n然后，按ESC键退出编辑模式，按`:wq`保存退出。\n\n（7）输入`service network restart`重启网络，然后输入`ping www.baidu.com`看看网络是否配置成功。需要注意的是，Linux中的ping默认不会停止，可以使用`Ctrl+C`停止该命令的执行。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202211925.png)\n\n（8）输入`vi /etc/hosts`编辑CentOS的hosts文件，添加本地解析条目。下图`192.168.218.50`是刚刚设置的IP地址，`layne`是之前配置的主机名。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202212217.png)\n\n保存退出后，输入`ping layne`，可以看到，layne被解析为了`192.168.218.50`\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202212413.png)\n\n## 5. 用Xshell连接虚拟机\n\n（1）打开安装的Xshell软件，输入`ssh root@192.168.218.50`回车。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202213011.png)\n\n（2）在弹出的对话框中，在弹出的对话框中输入密码123456，点击“确定”登陆成功。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202213346.png)\n\n注意，此时用户名为root，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202213445.png)\n\n\n\n（3）登录成功后，输入`reboot`重启虚拟机，也可以输入`init 6`重启虚拟机（`init 0`是关闭虚拟机）。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202213620.png)\n\n\n\n（4）重启后再次输入`ssh root@192.168.218.50`，此时用户名为`layne`。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202213739.png)\n\n（5）输入ifconfig查看网络配置，下图可以看到，配置生效\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202213930.png)\n\n再ping百度试一下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202214029.png)\n\nOK，大功告成了。\n\n\n","tags":["Linux","vmware","CentOS"],"categories":["Linux"]},{"title":"VMware的三种网络模式 — NAT模式、桥接模式、仅主机模式","url":"/2021/02/02/221724/","content":"\n我们在配置vmware时，有三种模式，分别是**NAT模式**、**桥接模式**、**仅主机模式**，现在让我们来看看这三种模式吧。\n<!-- more -->\n\n## 1. NAT模式\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202174313.png)\n\n在NAT网络中，会用到VMware Network AdepterVMnet8**虚拟网卡**，主机上的VMware Network AdepterVMnet8虚拟网卡被直接连接到VMnet8虚拟交换机上与虚拟网卡进行通信。虚拟网卡只是作为主机与虚拟机通信的接口，并不是依靠虚拟网卡VMware Network AdepterVMnet8来联网的，它仍然依靠于宿主机器所在的网络来访问公网。NAT模式下虚拟机的TCP/IP配置信息是由VMnet8虚拟网卡的DHCPserver提供的，无法进行手工改动。该模式下，虚拟机可以和宿主机可以互相访问，但不可访问宿主机所在网络的其它计算机。采用NAT模式最大的优势是将虚拟系统接入互联网非常简单，只要求宿主机器能访问互联网即可，不需要配置IP地址、子网掩码、网关等，但DNS地址还是要根据实际情况填的。\n\n下面是简化后的NAT拓扑图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202180908.png)\n\n上面可能不太好理解，先来打个比方。我们将局域网看成是一个有若干居民居住的小区，这个小区对外面的世界来说有一个或者几个能被识别的地址。路由器就是该小区的收发室，也是小区与外界联络的唯一通道；而局域网和外网之间的数据就看成是各种各样的快递包裹，小区居民与外界的联络均是靠这些进出的包裹进行的。\n\n小区里每个居民都有小区内部地址（局域网IP），但这个地址对外并不公开。外来的包裹只能送到收发室这个统一地址（路由器的公网IP），由收发室来决定哪个包裹送到哪个居民家去；而小区内所有居民发出去的包裹不能直接发到外面，只能送到收发室，由收发室贴上小区统一地址（公网IP）以后再往外发送。所以对外界来说，收发到小区居民的包裹是统一收自或发送到小区收发室的，并不知道具体来自/发送给哪个小区居民。路由器的这种工作模式就叫做NAT模式。\n\n## 2. 桥接模式\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202175608.png)\n\n\n\n桥接模式是通过虚拟网桥将主机上的网卡与虚拟交换机 VMnet0连接在一起，虚拟机上的虚拟网卡(并不是 VMware Network Adapter VMnet1和 VMware Network Adapter VMnet8)都连接在虚拟交换机Vmnet0上，所以桥接模式的虚拟机必须与主机在同一网段且子网掩码，网关与DNS也要与主机网卡一致。所以虚拟机与宿主机和物理机处于同一个局域网中，可以和本局域网中的其它真实主机进行通讯。在桥接模式下，需要手工为虚拟系统配置IP地址、子网掩码，并且还要和宿主机器处于同一网段中。\n\n简化后的桥接模式拓扑图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202181047.png)\n\n同样的，我们仍然以小区的例子来解释这个工作模式。\n\n桥接模式的小区呢和前面的NAT模式小区有所不同，桥接模式小区每个居民都拥有一个对外界公开的地址，居民们收取和发送包裹都直接使用自己的地址（公网IP），而不是收发室的地址（路由器的公网IP）。这样一来，外界就能看到他们收发到小区的包裹具体是来自/给到哪个居民了，在这种模式下，收发室（路由器）并不需要对进出包裹（数据包）进行公私地址的转换，只是提供了一个收发的通道。\n\n## 3. 仅主机模式\n\n仅主机模式，是一种比NAT模式更加封闭的的网络连接模式，它将创建完全包含在主机中的专用网络，使用的虚拟网卡是VMnet1。仅主机模式的虚拟网卡仅对主机可见，并在虚拟机和主机系统之间提供网络连接。在默认情况下，使用仅主机模式网络连接的虚拟机无法连接到Internet(在主机上安装合适的路由或代理软件，或者在Windows系统的主机上使用Internet连接共享功能，仍然可以让虚拟机连接到Internet或其他网络)。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210202181232.png)","tags":["vmware"],"categories":["工具"]},{"title":"我花了72小时，用了近4万字，总结了65道操作系统知识点！","url":"/2021/01/28/163906/","content":"\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210128164145.png)\n<!-- more -->\n\n> 本人是北京航空航天大学21软件学院的一名在读硕士，下面是我研究生复试和面试整理的操作系统知识点，覆盖了操作系统的全部内容，我相信无论是企业面试和还是考试，都不会超出里面的范围。现在发布出来，供大家参考，希望大家能有所收获。\n\n\n\n话不多说，直接来干货。\n\n@[TOC](文章目录)\n\n\n\n## 1、操作系统的目标\n\n**方便性** 方便用户使用\n\n**有效性** 提高系统资源的利用率和系统的吞吐量（系统的吞吐量表示在单位时间内实际处理的数据量）\n\n**可扩充性** 能够方便地增加新的功能和模块\n\n**开放性** 能够遵循国际标准，凡遵循国际标准所开发的硬件和软件，都能彼此兼容，方便地实现互联。\n\n## 2、操作系统的作用\n\n**操作系统提供了用户与计算机硬件之间的接口** 用户在操作系统的帮助下能够方便快捷地操作计算机硬件运行自己的程序。\n\n**操作系统实现了对计算机系统资源的管理** 操作系统能够合理分配计算机系统的各种资源，提高系统资源的利用率和系统的吞吐量\n\n**操作系统实现了对计算机硬件的抽象** 操作系统屏蔽了计算机硬件物理接口的具体细节，用户只需要了解操作系统提供的操作命令，就可以实现对计算机硬件的操作。\n\n## 3、脱机输入/输出(Off-Line I/O)技术\n\n为了缓和CPU和I/O设备之间速度不匹配的矛盾，而引入了脱机输入、脱机输出技术。 该技术是利用专门的外围控制机， 先将低速IO 设备上的数据传送到高速磁盘上，或者相反。这样当处理机需要输入数据时，再从磁盘上高速地调入内存，极大地提高了输入速度。 反之，当处理机需要输出数据时，也可以由处理机把数据从内存输出到高速磁盘上，处理机便可去做自己的事情。由于数据的输入和输出是在脱离主机的情况下进行的，故称之为**脱机输入/输出技术**。这种脱机I/O方式的主要优点为：\n\n- 减少了CPU的空间时间\n- 提高了I/O速度\n\n反之，将在主机的直接控制下进行输入/输出的IO方式称之为**联机输入/输出（On-Line I/O）技术**。另外，如果用一道程序来模拟脱机输入时的外围控制机功能，则将该技术称之为**假脱机技术**即**SPOOLing**（Simultaneaus Periphernal Operating OnLine)技术，**SPOOLing**技术详情见下面题目。\n\n## 4、分时、实时、分布式、批处理系统的区别？\n\n（1）批处理系统\n\n批处理系统的特征是**将磁带上的一批作业能自动地逐个地调入内存依次运行**，而无需人工干预。批处理系统分为单道批处理系统和多道批处理系统。\n\n**单道批处理系统**的工作方式：每次从外存上只调入一道程序进入内存运行，各道作业的完成顺序在正常情况下与它们进入内存的顺序完全相同，只有当程序完成或发生异常情况时，才换入其后继程序进入内存运行。\n\n**多道批处理系统**的工作方式：用户所提交的作业都**先存放在外存上**并排成一个队列，称为“后备队列”；然后，由**作业调度程序按一定的算法**从后备队列中选择若干个作业调入内存，使它们共享CPU和系统中的各种资源。由于同时在内存中存在若干道程序，这样便可以利用一个程序执行I/O操作的空档时间，让CPU去运行另一道程序。\n\n（2）分时系统\n\n分时系统的引入是为了满足用户对人机交互的需要。\n\n分时系统的工作方式：在一台主机上连接了多个带有显示器和键盘的终端，同时允许多个用户通过自己的终端，以交互式方式使用计算机，共享主机的资源。**分时系统为了满足人机交互，必须做到及时接受和及时处理**。\n\n要做到及时接收多个用户键盘输入的命令，只需配置一个多路卡。多路卡的作用是，实现分时多路复用，以很快的速度周期性地扫描各个终端，接收各用户从终端上输入的数据。此外，为了能使从终端输入的数据被依次逐条地进行处理，还需要为每个终端配置一个缓冲区，用来暂存用户键入的命令（或数据）。\n\n要做到及时处理用户的作业（程序），可将用户提交的作用直接调入内存（不像批处理系统那样，把用户提交的作业依次从磁盘调入内存，这时仍有作业在外存上驻留等待调用，分时系统为了及时处理用户的作业，需要把所有用户提交的作业直接调入内存），并使处理机采用轮转的运行方式。为了避免一个作业长期独占处理机，引入了时间片的概念，系统规定每个作业只能运行一个时间片（如30ms），然后就暂停该作业的运行，并立即调度下一个作业运行。\n\n（3）实时系统\n\n实时系统是指系统能及时(或实时)响应外部事件的请求，在规定的时间内完成对该事件的处理，并控制所有实时任务协调一致地运行。实时操作系统要追求的目标是：对外部请求在严格时间范围内做出反应，有高可靠性和及时性。常见实时系统有工业控制系统、信息查询系统、多媒体系统、嵌入式系统等。\n\n工业控制系统能够实时采集现场数据，能对采集的数据及时处理，进而自动的控制相应设备执行。如火炮的自动控制系统，导弹的制导系统。\n\n信息查询系统能够接收从远程终端发来的服务请求，根据用户提出的请求，对信息进行检索和处理，并能及时对用户做出正确的回答。如飞机和火车的订票系统。\n\n多媒体系统是指用于播放音频和视频的系统，为了保证有好的视觉和听觉感受，它必须是实时信息处理系统。\n\n嵌入式系统用于对设备进行控制或对其中的信息做出处理，除了将各种类型的芯片嵌入到各种仪器和设备中，还需要配置嵌入式OS，它同样需要具有实时控制和处理的功能。\n\n（4）分布式操作系统\n\n分布式操作系统是支持分布式处理的软件系统，是在由通信网络互联的多处理机体系结构上执行任务的系统。它将负载分散到多个计算机硬件服务器上，能够提供更好的性能和可用性。\n\n分布式操作系统通常配置为共享内存和任务的服务器群集。这些服务器协同工作，提供比单个大型计算机服务器更大功率、更大容量、更高性能。\n\n## 5、操作系统的基本特性\n\n操作系统的基本特性是并发、共享、虚拟、异步\n\n（1）并发\n\n并发性和并行性是既相似又有区别的两个概念。并发性是指两个或多个事件在同一时间间隔内发生；而并行性是指两个或多个事件在同一时刻发生。\n\n倘若在计算机系统中有多个处理机，处理多个可并发执行的程序，这样，多个程序便可同时执行。\n\n而在单处理机系统，每一时刻只能运行一个程序，比如说0~20ms，20~40ms，40~60ms分别运行A、B、C程序，宏观上有三道程序同时执行，我们可以说这个程序是并发执行的。\n\n（2）共享\n\n共享是指系统中的资源，可供内存中多个并发执行的进程（或线程）**共同**使用。共享有两种工作方式，互斥共享方式和同时访问方式。\n\n互斥共享方式：规定在一段时间内只允许一个进程（或线程）访问某资源，如临界资源，当某一进程访问完并释放该资源后，才允许另一进程进行访问。\n\n同时访问方式：允许在一段时间内由多个进程同时访问某资源。\n\n（3）虚拟\n\n”虚拟“技术通过空分复用或时分复用，将一条物理信道变为若干条逻辑信道，使原来只能供一对用户通话的物理信道，变为能够供多个用户同时通话的逻辑信道。一般地，把通过某种技术将一个物理实体变为若干个逻辑上的对应物的功能称为”虚拟“。空分复用或时分复用技术见题目6。\n\n（4）异步\n\n在内存中的每个进程，在何时能获得外理机运行，又以怎样的速度向前推进，都是不可预知的。内存中的每个进程有的侧重于计算而较少需要I/0，有的其计算少而I/O多，这样，很可能是先进入内存的作业后完成，而后进入内存的作业先完成。或者说，进程是以不可预知的速度向前推进的，即为进程的异步性。\n\n## 6、时分复用和空分复用技术\n\n（1）时分复用技术\n\n时分复用技术是利用处理机的空闲时间去运行其它程序，提高了处理机的利用率。时分复用技术广泛用于实现虚拟处理机和虚拟设备，使处理机的利用率得以提升。\n\n虚拟处理机技术。利用多道程序设计技术，为每道程序建立至少一个进程，让多道程序并发执行。此时，虽然系统中只有一台处理机，但它能同时为多个用户服务，使每个终端用户都认为是有一个处理机（CPU）在专门为他服务。亦即，利用多道程序设计技术，把一台物理上的处理机虚拟为多台逻辑上的处理机，在每台逻辑处理机上运行一道程序，我们把用户所感觉到的处理机称为**虚拟处理机**。\n\n虚拟设备技术。将一台物理I/O设备虚拟为多台逻辑上的I/O设备，并允许每个用户占用一台逻辑上的I/O设备，这样便可使原来仅允许在一段时间内由一个用户访问的设备（即临界资源），变为允许多个用户“同时”访问的共享设备，即宏观上能“同时”为多个用户服务。例如原来的打印机输入临界资源，而通过虚拟设备技术又可以把它变为多台逻辑上的打印机，供多个用户“同时”打印。\n\n（2）空分复用技术\n\n空分复用技术是利用存储器的空闲空间分区域存放和运行其它程序，提高了内存的利用率。\n\n但是，单纯的空分复用存储器只能提高内存的利用率，并不能实现在逻辑上扩大存储器容量和功能，还必须引入**虚拟存储技术**才能达到此目的，虚拟存储技术在本质上是实现内存的分时复用，即它可以通过分时复用内存的方式，使一道程序仅在远小于它的内存空间中运行。例如，一个100MB的应用程序之所以可以运行在30MB的内存空间，实质上就是每次只把用户程序的一部分调入内存运行，运行完成后将该部分换出，再换入另一部分到内存中运行，通过这样的置换功能，便实现了用户程序的各个部分分时地进入内存运行。\n\n虚拟的实现，可以采用时分复用，也可以采用空分复用。将一台物理设备对应多个N个逻辑设备，如果是利用空分复用方法来实现虚拟，此时一台虚拟设备平均占用的空间必然也等于或低于物理设备所拥有空间的1/N。如果是利用时分复用方法来实现虚拟，则每台虚拟设备的平均速度必然等于或低于物理设备速度的1/N。\n\n## 7、操作系统的主要功能有哪些？\n\n（1）处理机管理功能\n\n在传统的多道程序系统中，处理机的分配和运行都是以进程为基本单位的，因而对处理机的管理可归结为对进程的管理。\n\n主要功能有\n\n- 进程控制：创建和撤销进程\n- 进程同步：为使多个进程余条不紊地运行，可采用进程互斥方式和进程同步方式的协调方式\n- 进程通信：实现进程之间的信息交换\n- 调度：作业调度和进程调度\n\n（2）存储器管理功能\n\n存储器管理的主要任务，是为多道程序的运行提供良好的环境，提高存储器的利用率，方便用户使用，并能从逻辑上扩充内存。\n\n主要功能有\n\n- 内存分配：为每道程序分配内存空间，可采取静态和动态两种方式\n- 内存保护：确保每道用户程序都仅在自己的内存空间内运行，彼此互不干扰\n- 地址映射：将地址空间中的逻辑地址转化为内存空间中与之对应的物理地址\n- 内存扩充：借助虚拟存储技术，从逻辑上扩充内存容量\n\n（3）设备管理功能\n\n设备管理的任务是，完成用户进程提出的I/O请求，为用户进程分配所需的I/O设备，并完成指定的I/O操作，提高CPU和I/O设备的利用率，提高I/O速度。\n\n主要功能有\n\n- 缓冲管理：在I/O设备和CPU之间引入缓冲，可有效缓和CPU和I/O设备速度不匹配的矛盾，提高CPU利用率\n- 设备分配：根据用户进程的I/O请求，为之分配所需的I/O设备\n- 设备处理：实现CPU和设备控制器之间的通信\n\n（4）文件管理功能\n\n文件管理功能的主要任务，是对用户文件和系统文件进行管理以方便用户使用，并保证文件的安全性\n\n主要功能有\n\n- 文件存储空间的管理：由文件系统对诸多文件及文件的存储空间实施统一的管理\n- 目录管理：为每个文件建立一个目录项，目录项包括文件名、文件属性、文件在磁盘上的物理位置等，并对众多的目录项实现按名存取\n- 文件的读/写管理和保护：从外存读取数据，或将数据写入外存，使用存取控制功能对文件进行保护\n\n（5）操作系统与用户之间的接口\n\n为了方便用户使用，操作系统提供了**用户与操作系统的接口**，有两大类：\n\n- 用户接口：用户与操作系统直接交互的接口，便于用户直接或间接地控制自己的作业\n- 程序接口：用户程序与操作系统的接口，方便用户程序在执行中访问系统资源，是用户程序取得操作系统服务的唯一途径\n\n## 8、微内核OS结构\n\n所谓的微内核技术，是指精心设计的、能实现现代OS核心功能的小型内核，其并非是一个完整的OS，只是将操作系统中最基本的部分放入微内核，而将操作系统的绝大部分功能都放在微内核外面的一组服务器中实现的，比如提供进程管理的服务器、提供存储器管理的服务器、提供I/O设备管理的服务器。微内核与服务器之间的消息传递是通过消息传递机制（即网络通信的方式）来实现信息交互的。\n\n在微内核OS中，一般采用“机制与策略分离”的原理。所谓机制，是指实现某一功能的具体执行机构。而策略，则是在机制的基础上借助于某些参数和算法来实现该功能的优化，或达到不同的功能目标。在传统的OS中，将机制放在OS的内核的较低层，把策略放在内核的较高层次中。在微内核OS中，通常将机制放在OS的微内核，将策略放在微内核外面的一组服务器中。\n\n## 9、进程和程序的区别和联系\n\n（1）程序是指令的集合，是静态的概念。**进程是程序在处理机上的一次执行的过程**，是动态的概念。\n\n（2）当程序没有运行时，程序作为软件资料可长期保存，而进程存在生命周期的，进程只有在运行的时候存在。\n\n（3）一个程序在运行时可能对应多个进程。\n\n## 10、进程的基本状态及转换\n\n进程有三种基本状态：就绪状态、执行状态、阻塞状态\n\n- **就绪状态**：已经获得投入运行所必需的一切资源，一旦分配到CPU，就可以立即执行。\n- **执行状态**：当进程由调度／分派程序分派后，得到CPU控制权，正在CPU上运行着。\n- **阻塞状态**：一个进程正在等待某个事件的发生（如等待 I/O 的完成），而暂停执行。\n\n进程的三种基本状态及其转换图如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126120846.jpg)\n\n上图的四种中间之间过程：\n\n- 就绪到执行：当处理机空闭时，由调度／分派程序从就绪进程队列中选择一个进程占用CPU\n- 执行到就绪： 时间片用完\n- 执行到阻塞：等待某事件的发生（如等待I/O完成）\n- 阻塞到就绪：事件已经发生（如I/O完成）\n\n为了满足进程控制块对数据及操作的完整性要求以及增强管理的灵活性，通常在系统中又为进程进入了两种常见的状态：创建状态和终止状态。\n\n- 创建状态：首先由进程申请PCB，并向PCB填写控制和管理进程的信息；然后，为该进程分配运行时所必须的资源；最后，把该进程转入就绪状态并插入就绪队列之中。如果进程所需的资源尚不得到满足，则创建工作未完成，进程不能被调度，此时进程所处的状态称为创建状态。\n- 终止状态：首先，等待操作系统进行善后处理，最后将其PCB清零，并将PCB空间返还系统。当一个进程达到了自然结束点，或是出现了无法克服的错误，或是被操作系统所终结，或是被其他有终止权的进程所终结，它将进入终止状态。进入中止状态的进程在操作系统中依然保留一个记录，其中保存状态码和一些计时统计数据，供其他进程收集。一旦其他进程完成了对其信息的提取之后，操作系统将删除该进程，即将其PCB清零。\n\n进程的五种基本状态及其装换图如下所示：\n\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126115642.png)\n\n\n\n为了系统和用户观察分析进程的需要，许多系统还引入一个对进程的重要操作——挂起操作，与挂起操作对应的操作是激活操作。为了方便操作，也引入了相对应的原语，即挂起原语Suspend和激活原语Active。此时，进程会有**增加**如下状态的转化：\n\n（1）活动就绪->静止就绪：当进程处于未被挂起的状态时，称此为活动就绪状态，此时进程可以接受调度。当用挂起原语Suspend将该进程挂起后，该进程便转为静止就绪状态，此时进程不再被调度执行。\n\n（2）执行->静止就绪：当进程正在执行时，用挂起原语Suspend将该进程挂起后，该进程将暂停执行，进入静止就绪状态。\n\n（3）活动阻塞->静止阻塞：当进程处于未被挂起的阻塞状态时，称它是处于活动阻塞状态。当用挂起原语Suspend将该进程挂起后，进程便转为静止阻塞状态。\n\n（4）静止阻塞->静止就绪：当处于静止阻塞的进程在其所期待的事件出现之后，它将从静止阻塞变为静止就绪状态。\n\n（5）静止就绪->活动就绪：处于静止就绪的进程用激活原语Active激活后，该进程便转为活动就绪状态。\n\n（6）静止阻塞->活动阻塞：处于静止阻塞的进程用激活原语Active激活后，该进程便转为活动阻塞状态。\n\n进程有三种基本状态的中间过程有4个，再加上述6个中间过程，共10个中间过程。\n\n引入挂起和激活操作的进程状态及其装换图如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126150438.png)\n\n## 11、进程控制块PCB中的信息和作用\n\n（1）进程控制块中的信息\n\n- 进程标识符：用于唯一地标识一个进程\n- 处理机状态：处理机的上下文信息，主要是由处理器各种寄存器中的内容组成的\n- 进程调度信息：包括进程的当前状态，进程优先级，引起进程由执行状态变为阻塞状态的事件，进程调度所需的其他信息，如进程调度算法，进程等待CPU时间，进程已执行的时间\n- 进程控制信息：包括程序和数据的地址，进程同步和通信机制，进程运行期间所需的资源清单，已分配到该进程的资源清单，PCB首地址大的链接指针\n\n（2）进程控制块的作用\n\n- 作为独立运行基本单位的标志：PCB是进程存在于系统中的唯一标志\n- 能实现间断性的运行方式：进程被阻塞后，PCB能保留CPU现场信息，再次被调度时，PCB供改进程恢复CPU现场\n- 提供进程管理所需要的信息：操作系统总是根据PCB中的进程控制信息实施对进程的控制和管理\n- 提供进程调度所需要的信息：操作系统根据PCB中的进程调度信息实施进度调度\n- 实现由于其它进程的同步与通信：操作系统根据根据PCB中的进程同步和通信机制实施进程的同步与通信\n\n## 12、理解进程同步机制的基本概念\n\n进程同步机制的任务是，对多个相关进程在执行次序上进行协调，使并发执行的进程能按按照一定的规则共享系统资源，并能很好的合作，从而使程序的执行具有可再现性。**因此同步机制包含进程同步和进程互斥两个概念**。\n\n（1）进程同步和进程互斥\n\n- 进程同步：相互合作的进程能够互通消息、彼此协调运行，比如生产者和消费者问题，生产者生产一个商品后，消费者才能消费。\n- 进程互斥：两个或多个进程访问同一资源，当有一个进程访问时，其他进程不能访问。如生产者和生成者、消费者和消费者。\n\n（2）进程同步机制的两个制约关系\n\n- 间接相互制约：源于资源共享。（进程互斥）\n- 直接相互制约：源于进程间的合作。（进程同步）\n\n可以使用信号量机制实现进程同步和进程互斥。\n\n## 13、理解临界资源、临界区的概念\n\n临界资源：一次仅允许一个进程访问的资源，如打印机、磁带机，都属于临界资源，各个进程间应采用互斥访问方式。\n\n临界区：每个进程中访问临界资源的那段代码称为临界区。显然，若能保证各个进程互斥地进入自己的临界区，便可实现诸进程对临界资源的互斥访问。\n\n```tex\nwhile(TRUE)\n{\n   进入区\n   临界区\n   退出区\n   剩余区\n}\n```\n\n\n\n## 14、进程同步机制应遵循的规则\n\n所有的同步进制都应遵循下述四条准则：\n\n- **空闲让进**：当无进程处于临界区时，表明临界资源处于空闲状态，应允许一个请求进入临界区的进程立即进入自己的临界区，以有效地利用临界资源。\n- **忙则等待**：当已有进程进入临界区时，表明临界资源正在被访问，因而其它试图进入临界区的进程必须等待，以保证对临界资源的互斥访问。\n- **有限等待**：对要求访问临界资源的进程，应保证在有限时间内能进入自己的临界区。\n- **让权等待**：当进程不能进入自己的临界区时，应立即释放处理机。这里的“权”指处理机。\n\n## 15、用信号量机制解决经典进程同步问题\n\n（1）生产者-消费者问题\n\n生产者-消费者问题（多人多缓问题），有多个生产者和多个消费者，生产者向缓冲区中放数据，消费者从缓冲区中取数据。当缓冲区满时，生产者不能再产生数据，当缓冲区空时，消费者不能从缓冲区中取走数据。缓冲池是临界资源，**同一时刻只能有一个进程进入缓冲区**，可以利用互斥信号量mutex实现诸进程对缓冲池的互斥作用；利用信号量empty和full分别表示缓冲区中空缓冲区和满缓冲区的数量。只要缓冲区未满，生产者可将数据放入缓冲区，只要缓冲区非空，消费者便可从缓冲区中取走一个数据。其中，生产者和生产者之间、消费者和消费者之间是互传进程，生产者和消费者是同步进程。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126152544.png)\n\n\n\n```c\n/*生产者-消费者问题（多人多缓问题）伪代码*/\n// semaphore指信号量的类型\nsemaphore Sbuf=n, Sdata =0；\nsemaphore mutex=1;\nitem buffer[n]；\nint in=0, out=0；\n\nproceducer( ) {\n　　do{\n\t\tproducer an item nextp；\n\t\tP(Sbuf)；\n\t\tP(mutex)；\n\t\tbuffer[in]=nextp；\n\t\tin=(in+1) % n；\n\t\tV(mutex)；\n\t\tV(Sdata)；\n\t\t\n\t}while(TRUE);\n}\n \nconsumer( ) {\n　　do{\n\t\tP(Sdata)； \n\t\tP(mutex)；\n\t\tnextc=buffer[out]；\n\t\tout=(out+1) % n；\n\t\tV(mutex)；\n\t\tV(Sbuf)；\n\t\t\n\t}while(TRUE);\n}\n\nvoid main(){\n       cobegin\n         proceducer( ); consumer( ) ;\n       coend\n}\n\n```\n\n（2）读者-写者问题\n\n读者写者( Reader- Writer)问题也是一个著名的进程同步问题。一个数据文件或记录可被多个进程共享。把只要求\n读该文件的进程称为“读进程”，修改文件的进程称为“写进程”。有如下要求：\n\n- 允许多者读\n- 不允许多者写\n- 不允许读/写并进\n\n```c\n/*读者-写者问题伪代码*/\nsemaphore wmutex=1；\nsemaphore rmutex=1；\nint readcount=0；\nReader( ){\n  do{ \n\t\tP(rmutex);　　\n\t\tif (readcount==0)  P(wmutex)； \n\t\tV(rmutex)；\n\t\tperform read operation；\n\t\tP(rmutex);　\n\t\treadcount--;\n\t\tif (readcount==0)  V(wmutex)；\n\t\tV(rmutex)；\n\t\t\n\t}while(TRUE);\n}\n\nWriter( ){\n　do{\n\t\tP(wmutex)；\n　　　   perform write operation；\n　　　   V(wmutex)；\n\n\t}while(TRUE);\n}\n\nvoid main(){\n       cobegin\n         Reader( ); Writer( ) ;\n       coend\n}\n```\n\n在读者-写者问题中，写进程与写进程是互斥进程，读进程与写进程是互斥进程，可以看到，上面代码中声明了两个互斥信号量，并没有声明同步信号量，即读者-写者并没有同步进程。\n\n那么，如何判断两个进程是互斥或同步的呢？我认为，判断是否同步或互斥要看这两个进程对资源的使用情况，如果两个进程共享同一临界资源，那么肯定是互斥的，如果一个进程所需的资源依赖于另一个进程的输出，那么这两个进程是同步的。\n\n另外，常见的进程同步问题还有奇偶数问题、黑白棋子问题、和尚提水问题，限于篇幅，这里就不具体介绍了。\n\n## 16、理解进程通信及其方式\n\n进程通信：指进程之间的信息交换，其所交换的信息量，少者是一个状态或数值，多者则是成千上万个字节。由于进程的同步与互斥也要在进程间交换一定的信息，故也称进程通信，但它们是低级进程通信，因为它们信号量机制实现进程通信，通信效率低，信息量少。\n\n另外，有一种基于共享数据结构的通信方式，它要求诸进程共用某些数据结构，借以实现诸进程间的信息交换。如在生产者—消费者问题中，就是用有界缓冲区这种数据结构来实现通信的。这里，公用数据结构的设置及对进程间同步的处理，都是程序员的职责。这种通信方式是低效的，只适于传递相对少量的数据，也属于低级通信。 \n\n**进程的高级通信可以实现进程间大量数据的传送**，具体的高级通信方式有共享存储器系统、管道通信系统、消息传递系统、客户机/服务器系统。\n\n- 基于共享存储区的通信方式：为了传输大量数据，在存储器中划出了一块共享存储区，诸进程可通过对共享存储区中数据的读或写来实现通信，属于高级通信。\n- 管道通信系统：建立在文件系统基础上，适用于大量变化着的信息交换。所谓“管道”，是指用于连接一个读进程和一个写进程以实现它们之间通信的一个共享文件，又名pipe文件。向管道(共享文件)提供输入的发送进程(即写进程)，以字符流形式将大量的数据送入管道；而接受管道输出的接收进程(即读进程)，则从管道中接收(读)数据。为了协调双方的通信，管道机制必须提供三种协调能力，即互斥、同步、确定对方是否存在。\n- 消息传递系统：在该机制中，进程间的数据交换是以格式化的消息(message)为单位的；在计算机网络中，又把message称为报文。消息传递系统因其实现方式的不同，可进一步分为两类：\n  a. 直接通信方式：发送进程利用OS提供的发送原语，直接把消息发送给目标进程。\n  b. 间接通信方式：进程之间通过一个作为共享数据结构的中间实体（称为邮箱）的方式进行消息的发送和接收。\n- 客户机/服务器系统：通过计算机网络进行通信。\n\n## 17、进程与线程的区别与联系\n\n（1）在传统的OS中，进程是一个独立拥有资源，能够独立运行，同时又是可独立调度和分派的基本单位。\n\n（2）在OS引入线程之后，把线程一个独立调度和分派的基本单位，线程本身并不拥有系统资源，仅有一点不可缺少的、能保证独立运行的资源。引入线程的目的是减少程序在并发执行时带来的时空开销，\n\n（3）同一个进程中，线程的切换不会引起进程的切换，但从一个线程切换到另一个进程中的线程时，必然会引起进程的切换。线程的上下文切换比进程的上下文切换带来的开销小得多。\n\n（4）一个进程中含有若干个相对独立的线程，多个线程可并发执行。在多线程OS中，所谓进程的执行状态，实际上是指该进程中的某些线程正在执行。\n\n## 18、作业调度的概念及其调度算法\n\n作业调度（又称高级调度）是指根据某种算法，决定将外存上处于后备队列中的哪几个作业调入内存，为它们创建进程、分配必要的资源，并将它们放入就绪队列。作业调度主要用于多道批处理系统中，而在分时和实时系统中不设置这种调度。\n\n作业调度算法有：\n\n（1）先来先服务(FCFS)调度算法\n\n按照作业到达的先后次序来进程调度，该调度算法也适用于进度调度\n\n（2）短作业优先(SJF)调度算法\n\n根据作业的长短来计算优先级，作业越短，其优先级越高\n\n（3）优先级调度算法(PSA)\n\n对于先来先服务算法，作业的等待时间就是作业的优先级；对于短作业优先调度算法，作业的长短就是作业的优先级。\n\n（4）高响应比优先（HRRN）调度算法\n\n既考虑了作业的等待长度，又考虑了运行时间的调度算法，是一个动态的优先级，响应比=优先级=(等待时间+要求服务时间)/要求服务时间\n\n## 19、进程调度的概念及其调度算法\n\n进程调度（又称低级调度）是指，根据某种算法，**决定就绪队列中的哪个进程应获得处理机**，并由分派/调度程序将处理机分配给被选中的进程。在多道批处理系统、分时系统和实时系统中，都必须配置这种调度。除了高级调度和低级调度之外，还有中级调度，它实际上是完成内外存的换入和换出，提高内存的利用率。\n\n进程调度算法有：\n\n（1）轮转(RR)调度算法\n\n系统将所有的就绪进程按FCFS策略排成一个就绪队列，系统可设每隔一定时间(如30ms)便产生一次中断，按照队列中的顺序，把CPU分给进程，并令其执行一个时间片。\n\n（2）优先级调度算法\n\n把处理机分配给就绪队列中优先级最高的进程。\n\n（3）多级反馈队列调度算法\n\n设置多个队列，为每个队列设置不同的优先级，第一个队列优先级最高，第二个次之，最后一个队列优先级最小。\n\n每个队列都采用FCFS算法，当新进程进入内存后，首先将它放入第一队列的末尾，按照FCFS算法调度。当轮到该进程执行时，如果它能在该时间片内完成，便可撤离系统，否则，将它放入第二个队列的末尾，依次类推。当进程被降到最后一个队列后，便采取RR方式进行调度。\n\n队列间按照优先级调度。首先调度最高优先级队列中的各个进程，仅当第一队列空闲才调度第二队列中的进程运行。\n\n多级反馈队列示意图如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126165210.png)\n\n\n\n（4）基于公平原则的调度算法\n\n以上的几种进程调度算法保证的只是优先级运行，并不能保证占用了多少处理机时间，而基于公平原则的调度算法考虑的是调度的公平性，常见有两种相对公平的调度算法。\n\n- 保证调度算法：让每个进程都获得相同的处理机时间。\n- 公平分享调度算法：让每个用户获得相同的处理机时间。\n\n\n\n## 20、实时调度及其调度算法\n\n（1）实现实时调度的基本条件\n\n- 提供必要的信息：就绪时间、开始截止时间和完成截止时间、处理时间、资源要求、优先级。\n- 系统处理能力强：在实时系统中，若处理机的处理能力不够强，则有可能因处理机忙不过，而致使某些实时任务不能得到及时处理，从而导致发生难以预料的后果。\n- 采用抢占式调度机制：在含有HRT（硬实时任务）的实时系统中，广泛采用抢占机制。这样便可满足HRT任务对截止时间的要求。\n- 具有快速切换机制：对中断的快速响应能力、快速的任务分派能力。\n\n（2）实时调度算法的分类\n\n  ① 根据实时任务性质，可将实时调度的算法分为硬实时调度算法和软实时调度算法；② 按调度方式，则可分为非抢占调度算法和抢占调度算法。\n\n**非抢占式调度算法**：\n\n- 非抢占式轮转调度算法：用于要求不太严格的实时控制系统。\n- 非抢占式优先调度算法：用于有一定要求的实时控制系统。\n\n**抢占式调度算法**：\n\n- 基于时钟中断的抢占式优先级调度算法：时钟中断发生时才抢占处理机，可用于大多数的实时系统。\n- 立即抢占的优先级调度算法：能获得非常快的响应。\n\n（3）两种常见的实时调度算法\n\n①**最早截止时间优先EDF（Earliest Deadline First）算法** \n\n算法根据任务的截止时间来确定任务的优先级。截止时间愈早，其优先级愈高。该算法要求在系统中保持一个实时任务就绪队列，该队列按各任务截止时间的早晚排序。算法既可用于抢占式调度，也可用于非抢占式调度方式中。 \n\n② **最低松弛度优先LLF（Least Laxity First）算法**\n\n该算法在确定任务的优先级时，根据的是任务的紧急(或松弛)程度。任务紧急程度愈高，赋予该任务的优先级就愈高，以使之优先执行。**该算法主要用于可抢占调度方式中**。 \n\n\n\n\n\n\n\n## 21、作业平均周转时间和平均带权周转时间\n\n（1）平均周转时间短\n\n作业的周转时间包含四个部分：作业在外存后备队列上等待(作业)调度的时间，进程在就绪队列上等待进程调度的时间，进程在CPU上执行的时间，以及进程等待I/O操作完成的时间。其中的后三项在一个作业的整个处理过程中，可能发生多次。\n\nn个作业的平均周转时间=n个作业的周转时间之和/n。\n\n（2）带权周转时间\n\n作业的周转时间T与系统为它提供服务的时间$T_s$之比，即$W=T/T_s$\n\n（3）平均带权周转时间\n\n平均带权周转时间可表示为：$W{\\rm{ = } }\\frac{1}{n}\\sum\\limits_{i = 1}^n {\\frac{ { {T_i} } }{ { {T_s} } } }$\n\n## 22、CPU利用率和CPU最小时钟周期\n\n（1）CPU利用率\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126171358.png)\n\n（2）CPU最小时钟周期\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126171425.png)\n\n**时钟发生器发出的脉冲信号做出周期变化的最短时间称之为震荡周期**，也称为 CPU 时钟周期，它是计算机中最基本的、最小的时间单位。每一次震荡周期到来，芯片内的晶体管就改变一次状态，让整个芯片完成一定任务。也就是说在一个时钟周期内，**CPU仅完成一个最基本的动作**，一条指令的执行可能需要好几个时钟周期。由此，更小的时钟周期就意味着更高的工作频率。CPU时钟周期的倒数就是时钟频率。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126171621.png)\n\n## 23、死锁及死锁产生的条件\n\n如果一组进程中的每一个进程都在等待仅由该组进程中的其它进程才能引发的事件，那么该组进程是是死锁的。\n\n产生死锁必须具备四个条件，只要任一条件不成立，死锁就不会发生。它们分别是互斥条件、请求和保持条件、不可抢占条件，循环等待条件。\n\n- 互斥条件：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求该资源，则请求者只能等待，直至占有该资源的进程用毕释放。\n- 请求和保持条件：指进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源又已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。\n- 不可抢占条件：指进程已获得的资源在未使用完之前不能被抢占，只能在使用完时由自己释放。\n- 循环等待条件：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，…，Pn}中的P0正在等待一个P1占用的资源； P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。\n\n## 24、处理死锁的方法\n\n目前处理死锁的方法可归结为四种：\n\n（1）预防死锁。一种较简单和直观的事先预防方法。该方法是通过设置某些限制条件，去破坏产生死锁的四个必要条件中的一个或几个条件，来预防发生死锁。\n（2）避免死锁。在资源的动态分配过程中，用某种方法去防止系统进入不安全状态，从而避免发生死锁。同样是属于事先预防的策略，但它并不是事先采取各种限制措施，去破坏死锁的四个必要条件。\n\n（3）检测死锁。允许系统在运行过程中发生死锁。但可通过系统所设置的检测机构，及时地检测出死锁的发生，然后采取适当的措施，把进程从死锁中解脱出来。\n\n（4）解除死锁。这是与检测死锁相配套的一种措施。当检测到系统中已发生死锁时，须将进程从死锁状态中解脱出来。\n\n为了更清楚理解处理死锁四种方法，下面对这四种方法采取的具体措施进行介绍：\n\n（1）预防死锁\n\n预防死锁的方法是通过破坏产生死锁的四个必要条件中的一个或几个，以预防发生死锁。由于互斥条件是非共享设备所必须的，不仅不能改变，还应加以保证，因此主要是破坏产生死锁的后三个条件。  \n\n**破坏“请求和保持” 条件（静态分配资源法）**\n该方法规定，所有进程在开始运行之前，必须一次性地申请其在整个运行过程中所需的全部资源。如果系统有足够资源，则分配，破坏了“请求”条件；如果资源不足，则一个资源也不分配，破坏了“保持”条件。\n\n**破坏“不可抢占” 条件（剥夺式分配资源法）**\n进程是逐个地提出对资源的要求的。当一个已经保持了某些资源的进程，再提出新的资源请求而不能立即得到满足时，必须释放它已经保持了的所有资源，待以后需要时再重新申请。这意味着某一进程已经占有的资源，在运行过程中会被暂时地释放掉，也可认为是被抢占了，从而破坏了“不可抢占”条件。\n\n**破坏“循环等待” 条件（按序分配资源法）**\n按顺序分配资源，比如规定每个进程必须按序号递增的顺序请求资，这样，在所形成的资源分配图中，不可能再出现环路，因而破坏了“循环等待”条件。\n\n（2）避免死锁\n\n在资源的动态分配过程中，用某种方法去防止系统进入不安全状态，从而避免发生死锁，但它并不是事先采取各种限制措施。\n\n由于在资源动态分配的过程中，要确保系统始终处于安全状态，可以利用**银行家算法**避免死锁。\n\n银行家算法：当进程在请求一组资源时，首先确定系统中是否有足够的资源分配给他。若有，再进一步计算系统分配这些资源给该进程后，是否会出现不安全状态(是否满足产生死锁的条件)，如果不满足产生死锁的条件，才将资源分配给它。\n\n（3）检测死锁\n\n用资源分配图检测死锁\n\n圆圈代表进程，方框代表资源一类资源，方框中的点代表这一类资源中的一个资源，由进程出发的边是资源请求边，由资源出发的边是资源分配边。**寻找既不阻塞又非独立的进程节点**Pi，如果Pi能够获取所需的资源，则消去Pi的请求边和分配边，依次类推，直到所有的进程节点都是孤立这点，则称该图是可完全简化图。\n\n一个资源分配图中的进程产生**死锁的**充分条件是：当且仅当该资源分配图是不可完全简化图。\n\n例如，下面的资源分配图\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126181401.png)\n\n首先，找到寻找既不阻塞又非独立的进程节点P1，消去P1的请求边和分配边，P1释放资源后，便可使P2获得资源而继续运行，直至P2完成后又释放出它所占有的全部资源，再消去P2的请求边和分配边，此时资源分配图中所有的进程结点都成为孤立结点，该图是可完全简化的，不会发生死锁。\n\n用资源分配图检测死锁更详细的过程可参考[死锁检测-资源分配图的简化](https://blog.csdn.net/qq_39328436/article/details/111123779)\n\n（4）解除死锁\n\n主要有两种方法解决死锁，分别是抢占资源和终止（撤消）进程的方式。\n\n- 抢占资源。从其它进程抢占足够数量的资源给死锁进程，以解除死锁状态。\n- 终止（撤消）进程。终止（或撤销）系统中的一个或多个死锁进程，直至打破循环环路，使系统从死锁状态中解脱出来。\n\n## 25、存储器的多层结构\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126182822.png)\n\n存储器分为三个层次，分别是CPU寄存器、主存、辅存。\n\n- CPU寄存器：寄存器具有与处理机相同的速度，完全能与CPU协调工作，但价格十分昂贵。\n- 主存：包括高速缓存、主存储器、磁盘缓存 。\n  高速缓存：介于寄存器和主存贮器之间，主要用于备份主存中常用的数据，以减少处理机对主存储器的访问次数。\n  主存储器：简称主存或内存，用于保存进程运行时的程序和数据。\n  磁盘缓存：为了缓和磁盘的I/O和主存访问速度的不匹配，从而设置磁盘缓存。\n- 辅存：磁盘，可移动存储介质。\n\n## 26、程序装入内存转变为可以执行程序的过程\n\n用户程序要在系统中运行，必须先将它装入内存，然后再将其转变为一个可以执行的程序，通常要经过以下步骤：编译、 链接、 装入。\n\n（1）编译：由编译程序对用户源程序进行编译，形成若干个**目标模块**。\n\n（2）链接：由链接程序将编译后形成的一组目标模块以及它们需要的库函数链接在一起，形成一个完整的装入模块(可执行目标程序)。\n\n（3）装入：由装入程序将装入模块装入内存。\n\n这三步的示意图如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126183251.png)\n\n\n\n\n\n## 27、程序的装入和链接方式有哪些？\n\n（1）程序的装入\n\n为了阐述方便，先介绍无需进行链接的单个目标模块的装入过程。该目标模块也就是装入模块。在将一个转入模块装入内存时，可以有如下装入方式：\n\n- 绝对装入方式。当计算机系统很小，且仅能运行单道程序时，完全有可能知道程序将驻留在内存的什么位置。此时可以采用绝对装入方式。用户程序经编译后，将产生绝对地址(即物理地址)的目标代码。 装入模块被装入内存后，由于程序中的逻辑地址与实际内存地址完全相同，故不须对程序和数据的地址进行修改。\n- 可重定位装入方式（静态重定位）。**所谓的重定位是指，在装入时对目标程序中指令和数据的修改过程，即地址变换。**可重定位装入方式的地址变换通常是在装入时一次完成的，以后不再改变，故称为静态重定位。\n- 动态运行时的装入方式（动态重定位）。当把装入程序装入内存后，并不立即把装入模块中的逻辑地址转换为物理地址（即不立即进行地址变换），而是把这种地址变换推迟到程序真正要执行时才进行。因此，装入内存后所有的地址都仍是逻辑地址。\n\n（2）程序的链接\n\n源程序经过编译后，可得到一组目标模块。链接程序的功能是将这组目标模块以及它们所需要的函数库装配成一个完成的装入模块。在对目标模块进行链接时，根据链接的时间不同，可把链接分成如下三种：\n\n- 静态链接(Static Linking)方式：在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的装配模块，以后不再拆开。\n- 装入时动态链接(Load-time Dynamic Linking)：这是指将用户源程序编译后所得到的一组目标模块，在装入内存时，采用边装入边链接的链接方式。\n- 运行时动态链接(Run-time Dynamic Linking)：这是指对某些目标模块的链接，是在程序执行中需要该(目标)模块时，才对它进行的链接。\n\n## 28、存储器（内存）的管理策略或方式\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126212226.png)\n\n## 29、连续分配存储管理方式\n\n连续分配存储管理方式是指为某一程序分配连续的内存空间，程序在内存中的逻辑地址相邻，体现在内存空间分配时物理地址相邻。连续分配方式可分为四类：\n\n（1）单一连续分配（单用户连续分配）\n\n用于单用户、单任务OS中。把内存分为系统区和用户区两部分，系统区仅提供给OS使用，它通常是放在内存的低址部分。而在用户区内存中，仅装有一道用户程序，即整个内存的用户空间由该程序独占。程序装入时一般采用静态重定位的方式。\n\n（2）固定分区分配\n\n在多道程序系统出现以后，为了能在内存中装入多道程序，且使这些程序之间又不会发生相互干扰，于是将整个用户空间划分为若干个固定大小的区域，在每个分区中只装入一道作业，这样就形成了最早的、也是最简单的种可运行多道程序的分区式存储管理方式。如果在内存中有四个用户分区，便允许四个程序并发运行。当有一空闲分时，便可以再从外存的后备作业队列中选择一个适当大小的作业，装入该分区。当该作业结束时，又可再从后备作业队列中找出另一作业调入该分区。\n\n对于分区的划分，可以大小相等(指所有的内存分区大小相等)，也可以分为若干大小不等的分区。\n\n为了便于内存分配，通常将分区按其大小进行排队，并为之建立一张分区使用表，其中各表项包括每个分区的起始地址、大小及状态(是否已分配)，如下图所示。当有一用户程序要装入时，由内存分配程序依据用户程序的大小检索该表，从中找出一个能满足要求的、尚未分配的分区，将之分配给该程序，然后将该表项中的状态置为“已分配”。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126214010.png)\n\n每个分区只能装一个作业，作业不一定占满分区，所以会产生碎片、零头。需要注意的是，固定分区分配在程序装入内存时采用的也是静态重定位的方式。\n\n（3）动态分区分配（可变分区分配）\n\n动态分区分配是根据进程的实际需要，动态地为之分配内存空间。其分区的大小可以改变。\n\n为了实现动态分区分配，系统中必须配置相应的数据结构，用以描述空闲分区和己分配分区的情况，为分配提供依据。常用的数据结构有以下两种形式：①空闲分区表，在系统中设置一张空闲分区表，用于记录每个空闲分区的情况。每个空闲分区占一个表目，表目中包括分区号、分区大小和分区始址等数据项，如下图所示。②空闲分区链。为了实现对空闲分区的分配和链接，在每个分区的起始部分设置一些用于控制分区分配的信息以及用于链接各分区所用的前向指针，在分区尾部则设置一后向指针。通过前、后向链接指针，可将所有的空闲分区链接成一个双向链，如下图所示。为了检索方便，在分区尾部重复设置状态位和分区大小表目。当分区被分配出去以后。把状态位由“0”改为“1”，此时，前、后向指针已无意义。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126215149.png)\n\n在动态分区管理中，主要的操作是分配内存和回收内存。\n\n**分配内存**：设请求的分区大小为u.size，表中每个空闲分区的大小可表示为m.size，而size是事先规定的不在切割的剩余分区的大小。 分配内存的流程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126220559.png)\n\n**回收内存**：\n\n- 回收区与插入点的前一个空闲分区F1相邻接。应将回收区与插入点的前一分区合并，不必为回收分区分配新表项，而只需修改其前一分区F1的大小。\n- 回收分区与插入点的后一空闲分区F2相邻接。可将两分区合并，形成新的空闲分区，但用回收区的首址作为新空闲区的首址，大小为两者之和。\n- 回收区同时与插入点的前、后两个分区邻接。此时将三个分区合并，使用F1的表项和F1的首址，取消F2的表项，大小为三者之和。\n- 回收区既不与F1邻接，又不与F2邻接。这时应为回收区单独建立一个新表项，填写回收区的首址和大小，并根据其首址插入到空闲链中的适当位置。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126220856.png)\n\n**动态分区分配采用的是动态重定位的方式。**\n\n（4）动态可重定位分区分配\n\n连续分配方式的一个重要特点是，一个系统或用户程序必须被装入一片连续的内存空间中。当一台计算机运行了一段时间后，它的内存空间将会被分割成许多小的分区，而缺乏大的空闲空间。即使这些分散的许多小分区的容量总和大于要装入的程序，但由于这些分区不相邻接，也无法把该程序装入内存。\n\n如下图所示，图a中示出了在内存中现有四个互不邻接的小分区，它们的容量分别为10KB、30KB、14KB和26KB，其总容量是80KB。但如果现在有一个作业到达，要求获得40KB的内存空间，**由于必须为它分配一个连续空间**，故此作业无法装入。这种不能被利用的小分区即是前已提及的“碎片”，或称为“零头“。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126230014.png)\n\n若想把大作业装入，可采用的一种方法是：将内存中的所有作业进行移动，使它们全都相邻接。这样，即可把原来分散的多个空闲小分区拼接成一个大分区，可将一个作业装入该区。这种通过移动内存中作业的位置，把原来多个分散的小分区拼接成一个大分区的方法,称为“拼接”或“紧凑”，如上图b所示。\n\n虽然“紧凑”能获得大的空闲空间，但也带来了新的问题，即经过紧凑后的用户程序在内存中的位置发生了变化，此时若不对程序和数据的地址加以修改(变换)，则程序必将无法执行。为此，在每次“紧凑”后，都必须对移动了的程序或数据进行重定位。为了提高内存的利用率，系统在运行过程中是经常需要进行“紧凑”的。每“紧凑”一次，就要\n对移动了的程序或数据的地址进行修改，这不仅是一件相当麻烦的事情，而且还大大地影响到系统的效率，下面要介绍的动态重定位方法将能很好地解决此问题。\n\n**动态重定位**\n\n在前面所介绍的**动态运行时装入的方式**中，作业装入内存后的所有地址仍然都是相对(逻辑)地址，而将相对地址转换为绝对(物理)地址的工作被推迟到程序指令要真正执行时进行。为使地址的转换不会影响到指令的执行速度，必须有硬件地址变换机构的支持，即须在系统中增设一个重定位寄存器，用它来存放程序(数据)在**内存中的起始地址**。程序在执行时，真正访问的内存地址是相对地址与重定位寄存器中的地址相加而形成的。下图显示出了动态重定位的实现原理。地址变换过程是在程序执行期间，随着对每条指令或数据的访问自动进行的，故称为**动态重定位**。当系统对内存进行了“紧凑”，而使若干程序从内存的某处移至另一处时，不需对程序做任何修改，只要用该程序在内存的新起始地址去置换原来的起始地址即可。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126231200.png)\n\n**动态重定位分区分配算法**\n\n**动态重定位分区分配算法与动态分区分配算法基本上相同**，差别仅在于：在动态重定位分区分配算中，增加了紧凑的功能。通常，当该算不能找到一个足够大的空闲分区以满足用户需求时，如果所有小的空闲分区的容量总和大于用户的要求，这时便须对内存进行“紧凑”，将经“紧凑”后所得到的大空闲分区分配给用户。如果所有小的空闲分区的容量总和仍小于用户的要求，则返回分配失败信息。下图是动态分区分配算法流程图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126231750.png)\n\n\n\n## 30、常见的动态分区分配算法\n\n（1）基于顺序搜索的动态分区分配算法\n\n①首次适应(first fit，FF)算法\n\nFF算法要求空闲分区链以**地址递增**的次序链接。\n\n在分配内存时，从链首开始顺序查找，直至找到一个大小能满足要求的空闲分区为止。然后再按照作业的大小，从该分区中划出一块内存空间，分配给请求者，余下的空闲分区仍留在空闲链中。\n\n②循环首次适应(next fit，NF)算法（下次适应）\n\nNF算法要求空闲分区链以**地址递增**的次序链接成循环链。\n\n算法是从上次找到的空闲分区的下一个空闲分区开始查找，直至找到一个能满足要求的空闲分区，从中划出一块与请求大小相等的内存空间分配给作业。\n\n③最佳适应(best fit，BF)算法\n\nBF算法要求空闲分区链以**容量大小递增**的次序链接。\n\n为一作业选择分区时总是寻找其大小最接近于作业大小的存储区域。\n\n④最坏适应(worst fit，WF)算法\n\nWF算法要求空闲分区链以**容量大小递减**的次序链接。\n\n在为作业选择存储区域时，总是挑选一个最大的空闲区。在划分后剩下的空闲区也是最大的，对以后的分配很可能是有用的。\n\n（2）基于索引搜索的动态分区分配算法\n\n①快速适应(quick fit)算法（分类搜索法）\n\n将空闲分区根据其**容量大小进行分类**，对于每一类具有相同容量的所有空闲分区，单独设立一个空闲分区链表，这样系统中存在多个空闲分区链表。\n\n空闲分区的分类是根据进程常用的空间大小进行划分，如2 KB、4 KB、8 KB等，对于其它大小的分区，如7 KB这样的空闲区，既可以放在8 KB的链表中，也可以放在一个特殊的空闲区链表中。\n\n在分配内存时，根据进程的长度，寻找到能容纳它的最小空闲区链表，并取下第一块进行分配即可。\n\n②伙伴系统(buddy system)\n\n该算法规定，无论已分配分区或空闲分区，其大小均为2的k次幂，k为整数，1≤k≤m，其中：$2^1$表示分配的最小分区的大小，$2^m$表示分配的最大分区的大小，通常$2^m$是整个可分配内存的大小。 \n\n假设系统的可利用空间容量为$2^m$个字，则系统开始运行时，整个内存区是一个大小为$2^m$的空闲分区。在系统运行过程中，由于不断地划分，将会形成若干个不连续的空闲分区，将这些空闲分区按分区的大小进行分类。对于具有相同大小的所有空闲分区，单独设立一个空闲分区双向链表，这样，不同大小的空闲分区形成了k个空闲分区链表。\n\n设分配一个长度为n的存储空间，首先计算一个i值，使$2^{i-1}<n\\le 2^i$。在空闲分区大小为$2^i$的空闲分区链表中查找。若找到，即把该空闲分区分配给进程。\n\n否则，在分区大小为$2^{i+1}$的空闲分区链表中寻找。若找到，则把该空闲分区分为相等的两个分区，这两个分区称为**一对伙伴**，其中的一个分区用于分配，而把另一个加入分区大小为$2^i$的空闲分区链表中。\n\n若大小为$2^{i+1}$的分区也不存在，则需要查找大小为$2^{i+2}$的空闲分区，若找到则对其进行两次分割。若仍然找不到，则继续查找大小为$2^{i+3}$的空闲分区，依此类推。\n\n在最坏的情况下，可能需要对2k的空闲分区进行k次分割才能得到所需分区。\n\n在回收内存时，一次回收也可能要进行多次合并，如回收大小为$2^i$的空闲分区时，若已存在$2^i$的空闲分区时，则应将其与伙伴分区合并大小为$2^{i+1}$的闲分区；若已存在$2^{i+1}$的空闲分区时，又应继续与其伙伴分区合并为大小为$2^{i+2}$的空闲分区，依此类推。\n\n（3）哈希算法\n\n在快速适应算法和伙伴系统算法中，都是将空闲分区根据分区大小进行分类，对于每一类空闲分区，设立一个空闲分区链表。在为进程分配空间时，需要在一张管理索引表中查找到所需空间大小所对应的表项，从中得到对应的空闲分区链表表头指针，从而通过查找得到一个空闲分区。\n\n哈希算法是利用哈希快速查找的优点，建立哈希函数，构造一张**以空闲分区大小为关键字**的哈希表，该表的每一个表项记录了一个对应的空闲分区链表表头指针。　　\n\n当进行空闲分区分配时，根据所需空闲分区大小，通过哈希函数计算，即得到在哈希表中的位置，从中得到相应的空闲分区链表，实现最佳分配策略。\n\n## 31、分页存储管理方式的基本概念\n\n我们知道，离散分配方式分为分页存储管理方式、分段存储管理方式、段页式存储管理方式。这一题介绍分页存储管理方式。\n\n**（1）引入分页的好处**\n\n①没有外碎片，每个内碎片不超过页面大小。\n\n②一个程序的页面不必连续存放\n\n③以页面作为内存分配的基本单位，能有效地提高内存利用率\n\n**（2）分页存储管理方式**\n\n分页存储管理方式是指将**用户程序的地址空间**分为若干个固定大小的区域，称为页或页面（典型大小一般为4KB）。相应地，也将内存空间分为若干个物理块，页和块的大小相同，这样可将用户程序的任一页放入一物理块中，实现离散分配。分页的地址结构包含页号和页内地址。分页地址结构如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127102037.png)\n\n*图中地址长度为32位，其中0~11位为页内地址，即每页大小为4KB；12~31位页号，地址空间最多允许有1M页，即2的20次方个页面*\n\n在分页系统中，允许将进程的各个页离散地存储在内存的任一物理块中，为保证进程仍然能够正确地运行，即能在内存中找到每个页面所对应的物理块，**系统为每个进程建立了一张页面映像表**，简称**页表**，从而实现从页号到物理块号的地址映射。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126233741.png)\n\n（3）**分页存储管理方式中的地址变换机构**\n\n为了实现进程从逻辑地址到物理地址的变换功能，在系统设置了页表寄存器，用于存放页表在内存中的始址和页表的长度，当进程要访问某个逻辑地址中的数据时，地址变换机构会先将要访问的页号与页表长度进行比较，如果页号大于页表长度，则表示本次所访问的地址已超越进程的地址空间，将产生地址**越界中断**。如果未出现越界错误，则**将页号与页表项长度相乘再和页表始址相加**，便得到该表项在页表中的位置，于是可从中得到该页的物理块号，再与页内地址拼接，得到物理地址，再根据物理地址获取所需数据。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126235007.png)\n\n（4）**具有快表的地址变换机构**\n\n由于页表是存放在内存中的，这使CPU在每存取一个数据时，都要两次访问内存。第一次是访问内存中的页表，从中找到指定页的物理块号，再将块号与页内偏移量W拼接，以形成物理地址。第二次访问，才是从第一次所得地址中获得所需数据(或向此地址中写入数据)。\n\n为了提高地址变换速度，可增加一个快表(寄存器)，先从快表读出该页所对应的物理块号。如果未在快表中找到所对应的表项，则再从页表中去寻找。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210126235222.png)\n\n\n\n（5）**两级和多级页表**\n\n现代的计算机系统中，都支持非常大的逻辑地址空间。例如，对于一个具有32位逻辑地址空间的分页系统，规定页面大小为4KB即$2^{12}$B，在每个进程页表中的页表数最大可达$2^{20}$个即1M个，又因为每个页表项在内存中占一个字节，故每个进程仅仅其页表就要占用1MB的内存空间（分页中的页表在内存中占据一片连续的空间），而且还要求是连续的，多个进程情况下，这简直就是灾难。显然这是不现实的，为此可以将页表的存储也采用离散分配方式，即：\n\n①对于页表所需的内存空间，采用离散分配方式，以解决难以找到一块连续的大内存空间的问题。\n\n②只将当前需要的部分**页表项**调入内存，其余的**页表项**仍驻留在磁盘上，需要时再调入。\n\n**两级页表**\n\n针对难于找到大的连续的内存空间来存放页表的问题，可利用将页表进行分页，并离散地将各个页面分别存放在不同的物理块中的办法来加以解决，即两级页表的方式。下图是两级页表的结构\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127000147.png)\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127000217.png)\n\n\n\n具有两级页表的地址变换机构\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127000319.png)\n\n**多级页表**\n\n在二级页表基础上再将页表进行分页，即采用三级页表结构。依次类推，可进行多级分页。\n\n## 32、分段存储管理方式的基本概念\n\n我们知道，离散分配方式分为分页存储管理方式、分段存储管理方式、段页式存储管理方式。这一题介绍分段存储管理方式。\n\n**（1）为什么要引入分段**\n\n一方面是由于通常的程序都可分为若干个段，如主程序段、子程序段A、子程序段B、…、数据段以及栈段等，每\n个段大多是一个相对独立的逻辑单位；另一方面，实现和满足信息共享、信息保护、动态链接以及信息的动态增长等需要，也都是以段为基本单位的。所以，要引入分段存储管理方式。\n\n**（2）分段存储管理方式**\n\n分段存储管理方式把用户程序的地址空间分为若干个大小**不同**的段，每段定义一组相对完整的信息，(内存空间)分配时以段为单位进行分配。分段的地址结构包含段号和段内地址。分段的地址结构如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127104958.png)\n\n*在该地址结构中，允许一个作业最长有64K个段，每个段最大长度为64KB*\n\n分段存储管理方式为使程序能正常运行，亦即，能从物理内存中找出每个逻辑段所对应的位置，应像分页系统那样，**在系统中为每个进程建立一张段映射表**，简称**段表**，从而实现从逻辑段到物理内存区的映射。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127105313.png)\n\n\n\n（3）**分段存储管理方式中的地址变换机构**\n\n为了实现进程从逻辑地址到物理地址的变换功能，在系统设置了段表寄存器，用于存放段表在内存中的始址和段表的长度，在进行地址变换时，会先将要访问的段号与段表长度进行比较，如果段号大于段表长度，则表示本次所访问的地址已超越进程的地址空间（逻辑空间），将产生地址**越界中断**。如果未出现越界错误，则根据段表的始址和该段的段号，计算出该段对应段表项的位置，从中读出该段在内存的起始地址。然后，再检查段内地址是否超过该段的段长。若超过，同样发生越界中断。若未越界，则将该段的基址与段内地址相加，即可得到要访问的内存的物理地址。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127112132.png)\n\n（4）**具有快表的地址变换机构**\n\n像分页系统一样，当段表放在内存中时，每访问一个数据，都须**访问两次内存**，从而极大地降低了计算机的速率。解决的方法也和分页系统类似，再增设一个联想存储器（**快表**），用于保存最近常用的段表项。由于一般段比页大，因而段表项的数目比页表项的数目少，其所需的联想存储器也相对较小，便可以显著地减少存取数据的时间，比起没有地址变换的常规存储器的存取速度来仅慢约10%～15%。\n\n## 33、分页和分段的主要区别\n\n（1）页是信息的物理单位，分页是由于系统管理的需要。段则是信息的逻辑单位，分段的目的是为了能更好地满足用户的需要。\n\n（2） 页的大小固定且由系统决定，在系统中只能有一种大小的页面；而段的长度却不固定，决定于用户所编写的程序。\n\n（3）分页的用户程序地址空间（逻辑空间）是一维的。由于分页完全是系统的行为，故在分页系统中，用户程序的地址是属于单一的线性地址空间，程序员只需利用一个记忆符即可表示一个地址。而分段是用户的行为，故在分段系统中，用户程序的地址空间是二维的，程序员在标识一个地址时，既需给出段名，又给出段内地址。\n\n## 34、段页式存储管理方式的基本概念\n\n我们知道，离散分配方式分为分页存储管理方式、分段存储管理方式、段页式存储管理方式。这一题介绍段页式存储管理方式。\n\n（1）为什么要引入段页式\n\n分页系统以页面作为内存分配的基本单位，能有效地提高内存利用率，而分段系统以段作为内存分配的基本单位，它能够更好地满足用户多方面的需要。如果能对两种存储管理方式“各取所长”，则可形成一种新的存储器管理方式——段页式存储管理方式。这种新的系统既具有分段系统的便于实现、分段可共享、易于保护、可动态链接等一系列优点，又能像分页系统那样，很好地解决内存的外部碎片问题。\n\n（2）段页式存储管理方式\n\n段页式存储管理方式是分段和分页原理结合的产物，即先将用户程序分为若干个段，再把每个段分成若干个页，并为每个段赋予一个段名。段页式的地址结构包含段号和段内页号以及页内地址。段页式的地址结构如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127133546.png)\n\n*在该地址结构中，允许一个作业最多有256个段，每段最多有4K个页，每页大小为4KB*\n\n（3）段页式存储管理方式中的地址变换\n\n在段页式系统中，为了实现从逻辑地址到物理地址的变换，**系统中需要同时配置段表和页表**。**段表的内容不再是内存始址和段长，而是页表始址和页表长度**。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127134135.png)\n\n地址变换过程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127134213.png)\n\n其实就是先进行分段的地址变换，根据段号从段表中找到对应的页表，再进行分页的地址变换，根据页号从页表中找到相应页面在内存中的起始地址（即物理块号），最后再结合页内地址，找到在内存中的物理地址，\n\n（4）具有快表的地址变换机构\n\n**在段页式系统中，为了获得一条指令或数据，须三次访问内存。第一次访问是访问内存中的段表，从中取得页表始址；第二次访问是访问内存中的页表，从中取出该页所在的物理块号，并将该块号与页内地址一起形成指令或数据的物理地址；第三次访问才是真正从第二次访问所得的地址中，取出指令或数据。**\n\n为了提高执行速度，在地址变换机构中增设一个联想存储器（快表）。\n\n\n\n## 35、虚拟存储器系统的基本概念\n\n存储器（内存）的管理策略除了连续分配方式、离散分配方式之外，还有虚拟存储系统。\n\n（1）为什么要引入虚拟存储器\n\n前面所介绍的各种存储器管理方式有一个共同的特点，即**它们都要求将一个作业全部装入内存后方能运行**。于是，出现了下面这样两种情况：\n\n①有的作业很大，其所要求的内存空间超过了内存总容量，作业不能全部被装入内存，致使该作业无法运行。\n\n②有大量作业要求运行，但由于内存容量不足以容纳所有这些作业，只能将少数作业装入内存让它们先运行，而将其它大量的作业留在外存上等待。\n\n出现上述两种情况的原因都是由于内存容量不够大。一个显而易见的解决方法是从物理上增加内存容量，但这往往会受到机器自身的限制，而且无疑要增加系统成本，因此这种方法是受到一定限制的。另一种方法是从逻辑上扩充内存容量，这正是虚拟存储技术所要解决的主要问题。\n\n（2）局部性原理\n\n局部性原理是指，在一个较短的时间内，程序的执行仅局限于**某个部分**，相应地，它所访问的存储空间也局限于某个区域。\n\n局限性表现在时间局限性和空间局限性。\n\n①时间局限性。 如果程序中的某条指令被执行，则不久以后该指令可能再次执行；如果某数据被访问过，则不久以后该数据可能再次被访问。 产生时间局限性的典型原因是在程序中存在着大量的循环操作。\n\n②空间局限性。 一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，其典型情况便是程序的顺序执行。\n\n（3）虚拟存储器\n\n**基于局部性原理可知，应用程序在运行之前没有必要将之全部装入内存**，而仅须将那些当前要运行的少数页面或段先装入内存便可运行，其余部分暂留在盘上。程序在运行时，如果它所要访问的页(段)已调入内存，便可继续执行下去；但如果程序所要访问的页(段)尚未调入内存(称为缺页或缺段)，便发出**缺页(段)中断请求**，此时OS将利用**请求调页(段)功能**将它们调入内存，以使进程能继续执行下去。如果此时内存已满，无法再装入新的页(段)，OS还须再利用**页(段)的置换功能**，将内存中暂时不用的页(段)调至盘上，腾出足够的内存空间后，再将要访问的页(段)调入内存，使程序继续执行下去。这样，便可使一个大的用户程序在较小的内存空间中运行，也可在内存中同时装入更多的进程，使它们并发执行。\n\n总结一下，所谓虚拟存储器，是指具有**请求调入功能和置换功能**，**能从逻辑上**对内存容量加以扩充的一种存储器系统。其逻辑容量由内存容量和外存容量之和所决定，其运行速度接近于内存速度，而每位的成本却又接近于外存。\n\n虚拟存储器的实现方法有：请求分页存储管理方式、请求分段存储管理方式、请求段页式存储管理方式。这些将在后面的题目中介绍。\n\n## 36、请求分页存储管理方式\n\n请求分页系统是建立在基本分页基础上的，为了能支持虚拟存储器功能，而增加了**请求调页功能和页面置换功能**。相应地，每次调入和换出的基本单位都是长度固定的页面，这使得请求分页系统在实现上要比请求分段系统简单(后者在换进和换出时是可变长度的段)。因此，请求分页便成为目前最常用的一种实现虚拟存储器的方式\n\n为了实现请求分页，系统必须提供一定的硬件支持。计算机系统除了要求一定容量的内存和外存外，还需要有**请求页表机制、缺页中断机构以及地址变换机构**。\n\n（1）请求页表机制\n\n为了满足页面换进换出的需要，在**请求页表**中又增加了四个字段。这样，在请求分页系统中的每个页表应含以下诸项：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127142533.png)\n\n- 状态位P：用于指示该页是否已调入内存。\n- 访问字段A：用于记录本页在一段时间内被访问的次数，或记录本页最近已有多长时间未被访问。\n- 修改位M：表示该页在调入内存后是否被修改过。\n- 外存地址：用于指出该页在外存上的地址，通常是**磁盘块号**，供调入该页时参考。 \n\n（2）缺页中断机构\n\n缺页中断是一种特殊的中断，它与一般的中断相比，有着明显的区别：\n\n-  在指令执行期间产生和处理中断信号。而一般中断是在一条指令执行完后，检查是否有中断请求到达。\n- 一条指令在执行期间可能产生多次缺页中断。\n\n缺页中断就是要访问的页不在主存中，需要操作系统将页调入主存后再进行访问，此时会暂时停止指令的执行，产生一个缺页中断异常，对应的异常处理程序就会从选择一页调入到内存，调入内存后之前被中断的的异常指令就可以继续执行。\n\n缺页中断的处理过程如下：\n\n1. 如果内存中有空闲的物理块，则分配一物理块，然后转第4步，否则转第2步；\n2. 选择某种页面置换算法，选择一个将被替换的物理块，它所对应的页面号为q，将q页面换出，如果该页在内存期间被修改过，则需把它写回到外存；\n3. 将q所对应的页表项进行修改，把其状态位P置为0；\n4. 将需要访问的页装入到空闲的物理块中；\n5. 修改访问的页所对应的页表项的内容，把状态位P位置1，物理块号置为该页装入的物理块所对应的块号，访问字段A加1，外存地址置为访问的页在外存的磁盘块号；\n6. 重新运行被中断的指令。\n\n（3）地址变换机构\n\n请求分页系统中的地址变换机构是在分页系统地址变换机构的基础上，为实现虚拟存储器，再增加了某些功能所形成的，如产生和处理缺页中断，以及从内存中换出一页的功能等等，具体过程如下图所示：\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127144617.png)\n\n## 37、请求分页存储管理方式的页面置换算法\n\n（1）最佳置换算法\n\n最佳置换算法是一种理论上的算法，其所选择的被淘汰页面将是以后永不使用的，或许是在最长(未来)时间内不再被访问的页面。\n\n（2）先进先出(FIFO)页面置换算法\n\n先进先出(FIFO)页面置换算法总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面予以淘汰。\n\n（3）LRU(Least Recently Used)最近最久未使用置换算法\n\nLRU算法利用“最近的过去”作为“最近的将来”的近似，因此，LRU置换算法是选择最近最久未使用的页面予以淘汰。\n\n（4）最少使用(Least Frequently Used，LFU)置换算法\n\n在采用最少使用置换算法时，应为在内存中的**每个页面设置一个移位寄存器**，用来记录该页面被访问的频率。该置换算法选择在最近时期使用最少的页面作为淘汰页。\n\n（5）Clock置换算法\n\n为每页设置一位访问位，再**将内存中的所有页面都通过链接指针链接成一个循环队列**。当某页被访问时，其访问位被置1。\n\nClock置换算法在选择一页淘汰时，检查页的访问位。**如果是0，就选择该页换出**；若为1，则重新将它置0，暂不换出，再按照循环队列次序检查下一个页面。\n\n（6）页面缓冲算法（Page Buffering Algorithm PBA）\n\n页面缓冲算法建立了一个已修改换出页面的链表，对每一个要被换出的页面(已修改)，系统暂不把它们写回磁盘，而是将它们挂在已修改换出页面的链表上，仅当换出页面数目达到一定值时，再将它们**一起写回磁盘上**。\n\n## 38、“抖动”发生原因及预防方法\n\n每个进程在运行时，频繁地出现缺页，造成每个进程的大部分时间都用于页面的换进/换出，而几乎不能再去做任何有效的工作，从而导致发生处理机的利用率急剧下降并趋于0的情况。称此时的进程是处于“抖动”状态。\n\n预防抖动的方法：\n\n（1）采取局部置换策略\n\n当某进程发生缺页时，只能在分配给自己的内存空间里进行置换，不允许它从别的进程空间获得新的物理块，这样即使本进程发生了“抖动”，也不会对其它进程产生影响。\n\n（2）把工作集融入到处理机调度中\n\n调度程序从外存调入作业之前，必须检查每个进程在内存的驻留页面是否足够多。\n\n（3）利用“L=S”准则调节缺页率\n\n其中L是缺页之间的平均时间，S是平均缺页服务时间，即用于置换一个页面所需的时间。如果是L远比S大，说明很少发生缺页，磁盘的能力尚未得到充分的利用；反之，如果是L比S小，则说明频繁发生缺页，缺页的速度已超过磁盘的处理能力。只有当L与S接近时，磁盘和处理机都可达到它们的最大利用率。\n\n（4）选择暂停某些进程，释放它们的物理块。\n\n## 39、什么是工作集\n\n所谓工作集，是指在某段时间间隔Δ里，进程实际所要访问页面的集合。**由于进程发生缺页率的时间间隔与进程所获得的物理块数有关**，虽然程序只需要少量的几页在内存便可运行，但为了较少地产生缺页，应将程序的全部工作集装入内存中。\n\n缺页率与物理块数之间的关系如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127150640.png)\n\n\n\n然而无法事先预知程序在不同时刻将访问哪些页面，即无法事先预知程序的工作集，故仍只有像页面置换算法那样，用程序的过去某段时间内的行为作为程序在将来某段时间内行为的近似。\n\n## 40、请求分段存储管理方式\n\n在分页基础上建立的请求分页式虚拟存储器系统，是以页面为单位进行换入、换出的。而在分段基础上所建立的请求分段式虚拟存储器系统，则是以分段为单位进行换入、换出的。在请求分段系统中,程序运行之前，只需先调入少数几个分段(不必调入所有的分段)便可启动运行，当所访问的段不在内存中时，可请求OS将所缺的段调入内存。像请求分页系统一样，为实现请求分段存储管理方式，同样需要一定的硬件支持和相应的软件。\n\n与请求分页系统相似，在请求分段系统中所需的硬件支持有**段表机制**、**缺段中断机构**,以及**地址变换机构**。\n\n（1）段表机制\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127151614.png)\n\n- 存取方式：用于标识本分段的存取属性是只执行、只读，还是允许读/写。\n- 访问字段A：其含义与请求分页的相应字段相同，用于记录该段被访问的频繁程度。\n- 修改位M：用于表示该段在进入内存后是否已被修改过，供置换段时参考。\n- 存在位P：指示本段是否已调入内存，供程序访问时参考。\n- 增补位：是请求分段式管理中所特有的字段，用于表示本段在运行过程中是否做过动态增长。\n- 外存始址：指示本段在外存中的起始地址，即起始盘块号。\n\n（2）缺段中断机构\n\n与缺页中断机构类似，缺段中断机构同样需要在一条指令的执行期间产生和处理中断，以及在一条指令执行期间，可能产生多次缺段中断。\n但由于分段是信息的逻辑单位，因而不可能出现一条指令被分割在两个分段中，和一组信息被分割在两个分段中的情况。\n\n请求分段系统中的中断处理过程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127151944.png)\n\n（3）地址变换机构\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127152058.png)\n\n## 41、I/O系统的基本功能\n\n（1）隐藏物理设备的细节\n\nI/O系统通过对I/O设备进行抽象，以隐藏物理设备的实现细节，用户只需通过I/O系统提供的读、写命令就可以操作相应的I/O设备。\n\n（2）与设备的无关性\n\n与设备无关的基本含义：应用程序中所用的设备，不局限于使用某个具体的物理设备。比如，当用户要输出打印时，只须提供读/写命令和逻辑设备名，不必指名是哪一台打印机。另一方面，也可以有效提高OS的可移植性和易适应性，对于OS而言，应允许在不需要将整个操作系统进行重新编译的情况下，增添新的设备驱动程序。如Windows中，系统可以为新I/O设备自动安装和寻找驱动程序，从而做到即插即用。\n\n引入该层的目的：为了方便用户和提高OS的可适应性和可扩展性。\n\n（3）提高处理机和I/O设备的利用率\n\nI/O系统的第三个功能是要尽可能地让处理机和I/O设备并行操作，以提高它们的利用率。为此，一方面要求处理机能快速响应用户的I/O请求，使I/O设备尽快地运行起来；另一方面，也应尽量减少在每个I/O设备运行时处理机的干预时间。\n\n（4）对I/O设备进行控制\n\n对I/O设备进行控制是驱动程序的功能。目前对I/O设备有四种控制方式：①采用轮询的可编程I/O方式；②采用中断的可编程I/O方；③直接存储器访问方式；④I/O通道方式。后面的题目会具体介绍这四种方式。\n\n（5）确保对设备的正确共享\n\n从设备的共享属性上，可将系统中的设备分为如下两类:\n①独占设备，进程应互斥地访问这类设备，即系统一旦把这类设备分配给了某进程后，便由该进程独占，直至用完释放。典型的独占设备有打印机、磁带机等。\n②共享设备，是指在一段时间内允许多个进程同时访问的设备。典型的共享设备是磁盘，当有多个进程需对磁盘执行读、写操作时，可以交叉进行，不会影响到读、写的正确性。\n\n（6）错误处理\n\n从处理的角度，可将错误分为临时性错误和持久性错误。对于临时性错误，可通过重试操作来纠正，只有在发生了持久性错时，才需要向上层报告。例如，在磁盘传输过程中发生错误，系统并不认为磁盘发生了故障，而是可以重新再传，一直要重传多次后，若仍有错，才认为磁盘发生了故障。**由于多数错误是与设备紧密相关的**，因此对于错误的处理，应该尽可能在接近硬件的层面上进行，即**在低层软件能够解决的错误就不向上层报告，因此高层也就不能感知；只有底层软件解决不了的错误才向上报告，请求高层软件解决**。\n\n## 42、I/O系统的层次结构\n\n为使十分复杂的I/O软件能具有清晰的结构、更好的可移植性和易适应性，目前已普遍采用**层次式结构**的I/O系统。这是将系统中的**设备管理模块**分为若干个层次，每一层都是利用其下层提供的服务，完成输入输出功能中的某些子功能，并屏蔽这些功能实现的细节，向高层提供服务。\n\nI/O软件的层次结构如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127201122.png)\n\nI/O系统中各模块之间的层次视图如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127202038.png)\n\n与前面的I/O软件组织的层次结构相对应，I/O系统本身也可分为如下三个层次:\n\n（1）**中断处理程序**\n\n它处于I/O系统的底层，直接与硬件进行交互。当有I/O设备发来中断请求信号时，在中断硬件做了初步处理后，便转向中断处理程序。它首先保存被中断进程的CPU环境，然后转入相应设备的中断处理程序进行处理，在处理完成后，又恢复被中断进程的CPU环境，返回断点继续运行。\n\n（2）**设备驱动程序**\n\n它处于I/O系统的次底层，是进程和设备控制器之间的通信程序，其主要功能是，将上层发来的抽象I/O请求转换为对I/O设备的具体命令和参数，并把它装入到设备控制器中的命令和参数寄存器中，或者相反。由于设备之间的差异很大，每类设备的驱动程序都不相同，故必须由设备制造厂商提供，而不是由OS设计者来设计。\n\n（3）**设备独立性软件**\n\n现代OS中的I/O系统基本上都实现了与设备无关性，也称为与**设备无关的软件**。其基本含义是：I/O软件独立于具体使用的物理设备。由此带来的最大好处是，提高了I/O系统的可适应性和可扩展性，使它们能应用于许多类型的设备，而且在每次增加新设备或替换老设备时，都不需要对I/O软件进行修改，这样就方便了系统的更新和扩\n展。设备独立性软件的内容包括**设备命名、设备分配、数据缓冲**和数据高速缓冲一类软件等。\n\n另外，用户层软件实现与用户交互的接口，用户通过该层发出I/O命令与I/O系统接口交互，从而操作相应的I/O设备。\n\n## 43、I/O系统接口的类型有哪些\n\n根据设备类型的不同，I/O系统接口进一步分为块设备接口、流设备接口和网络接口。\n\n（1）块设备接口\n\n快设备接口是块设备管理程序与高层之间的接口。\n\n所谓块设备，数据的存取和传输都是以数据块为单位的设备。典型的块设备是磁盘，该设备的基本特征是传输速率较高，通常每秒钟为数MB到数十MB。另一特征是可寻址，即能指定数据的输入源地址及输出的目标地址，可随机地读门写磁盘中任一块；磁盘设备的I/O常采用DMA方式。\n\n（2）流设备接口\n\n流设备接口是流设备管理程序与高层之间的接口，该接口又称为字符设备接口。\n\n所谓字符设备，是指数据的存取和传输是以字符为单位的设备，如键盘、打印机等，字符设备的基本特征是传输速率较低，通常为每秒几个字节至数千字节。另一特征是不可寻址，即不能指定数据的输入源地址及输出的目标地址，字符设备在输入/输出时，常采用中断驱动方式。\n\n（3）网络通信接口\n通过网络进行通信，接入该接口的设备成为通信设备。\n\n## 44、设备控制器的基本功能\n\n设备控制器的主要功能是，控制一个或多个I/O设备，以实现I/O设备和计算机之间的数据交换。它是CPU与I/O设备之间的接口，接收从CPU发来的命令，去控制I/O设备工作，使处理机能够从繁杂的设备控制事务中解脱出来。设备控制器是一个可编址的设备，当它仅控制一个设备时，它只有一个唯一的设备地址；若控制器可连接多个设备，则应含有多个设备地址，每一个设备地址对应一个设备。\n\n设备控制器的基本功能如下：\n\n- 接收和识别处理机发来的命令\n- 实现CPU与控制器之间、控制器与设备之间的数据交换\n- 标识和报告I/O设备的状态\n- 识别其所控制的每个I/O设备的地址\n- 提供主机与I/O设备之间的数据缓冲\n- 对I/O设备传来的数据进行差错控制\n\n\n\n## 45、I/O通道基本概念和常用类型\n\n（1）为什么要引入I/O通道\n\n虽然在CPU与I/O设备之间增加了设备控制器后，已能大大减少CPU对I/O的干预，但当主机所配置的外设很多时，CPU的负担仍然很重。为此，在CPU和设备控制器之间又增设了I/O通道，其主要目的是为了建立独立的I/O操作，不仅使数据的传送能独立于CPU，而且也希望有关对I/O操作的组织、管理及其结束处理尽量独立，以保证CPU有更多的时间去进行数据处理；或者说，其目的是使一些原来由CPU处理的I/O任务转由通道来承担，从而把CPU从繁杂的I/O任务中解脱出来。在设置了通道后，CPU只需向通道发送一条I/O指令。通道在收到该指令后，便从内存中取出来本次要执行的通道程序，然后执行该通道程序，仅当通道完成了规定的I/O任务后，才向CPU发中断信号。\n\n（2）什么是I/O通道\n\nI/O通道是一种特殊的处理机，它具有执行I/O指令的能力，并通过执行通道(I/O)程序来控制I/O操作。\nI/O通道与一般的处理机不同：\n\n- 指令类型单一。其所能执行的命令主要局限于与I/O操作有关的指令。\n- 通道没有自己的内存。通道所执行的通道程序是放在主机的内存中的，即通道与CPU共享内存。\n\n（3）I/O通道的类型\n\n根据信息交换方式的不同，可把通道分为以下三种类型：\n\n①字节多路通道(Byte Multiplexor Channel)\n\n这是一种按字节交叉方式工作的通道。它通常都含有许多非分配型子通道，其数量可从几十到数百个，每一个子通道连接一台I/O设备，并控制该设备的I/O操作。这些子通道按时间片轮转方式共享主通道。字节多路通道的原理图如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127205934.png)\n\n② 数组选择通道(Block Selector Channel)\n\n这种通道虽然可以连接多台高速设备，但由于它只含有一个分配型子通道，在一段时间内只能执行一道通道程序，控制一台设备进行数据传送。\n\n③ 数组多路通道(Block Multiplexor Channel)\n\n数组多路通道是将数组选择通道传输速率高和字节多路通道能使各子通道(设备)**分时并行**操作的优点相结合而形成的一种新通道。它含有多个非分配型子通道，用于连接多台高、中速的外围设备，其数据传送是按数组方式进行的。 \n\n这里，不得不提由于通道而引发的**“瓶颈问题”**。由于通道加个昂贵，致使机器中所设置的通道数量势必较少，这往往由使它成了I/O的瓶颈，进而造成整个系统吞吐量的下降。单通道I/O系统如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127210619.png)\n\n解决瓶颈问题最有效的方法，便是增加设备到主机间的通路而不增加通道，如下图所示。换言之，就是把一个设备连接到多个控制器上，而一个控制器又连接到多个通道上。多通路方式不仅解决了瓶颈问题，而且提高了系统的可靠性，因为个别通道或控制器的故障不会使设备和存储器之间没有通路。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127210958.png)\n\n## 46、中断(interrup)和陷入阱(trap)的区别\n\n（1）中断\n中断是指CPU对I/O设备发来的中断信号的一种响应。CPU暂停正在执行的程序，保留CPU环境后，自动地转去执行该I/O设备的中断处理程序。执行完后，再回到断点，继续执行原来的程序。I/O设备可以是字符设备，也可以是块设备、通信设备等，由于中断是由外部设备引起的，故又称**外中断**。\n（2）陷入\n另外还有一种由CPU内部事件所引起的中断，例如进程在运算中发生了上溢或下溢，又如程序出错，如非法指令、地址越界，以及电源故障等。通常把这类中断称为**内中断或陷入(tuap)**。与中断一样，若系统发现了有陷入事件，CPU也将暂停正在执行的程序，转去执行该陷入事件的处理程序。**中断和陷入的主要区别是信号的来源，即是来自CPU外部，还是CPU内部。**\n\n## 47、软中断与硬中断的区别\n\n（1）软中断是软件实现的中断，也就是程序运行时其他程序对它的中断；而硬中断是硬件实现的中断，是程序运行时外接设备对它的中断。\n（2）软中断是程序安排好的，不具有随机行。硬中断是由外部事件引起的，具有随机性和突发性。\n（3）软中断并不会直接中断CPU，接收软中断的进程只有在得到处理机之后才会产生软中断。由于处理硬中断的驱动是需要运行在CPU上的，因此，当中断产生的时候，CPU会中断当前正在运行的任务，来处理中断。\n（4）软中断的中断信号由中断指令直接发出的，是不可屏蔽的；硬中断的中断信号是由中断控制器提供的，是可屏蔽的。\n\n## 48、对I/O设备的控制方式有哪些\n\n（1）使用轮询的可编程I/O方式（程序I/O方式）\n\n在处理机向控制器发出一条I/O指令，启动输入设备输入数据时，要同时把状态寄存器中的忙/闲标志busy置为1，然后便不断地循环测试busy(称为轮询)。当busy=1时，表示输入机尚未输完一个字（符），处理机应继续对该标志进行测试，直至busy=0，表明输入机已将输入数据送入控制器的数据寄存器中。于是处理机将数据寄存器中的数据取出，送入内存指定单元中，这样便完成了个字(符)的I/O。\n\n它的特点是：\n\n- CPU和I/O设备串行\n-  每次传送一个字（节）\n\n（2）使用中断的可编程I/O方式（中断驱动I/O方式）\n\n当某进程要启动某个I/O设备工作时，便由CPU向相应的设备控制器发出一条I/O命令，然后立即返回继续执行原来的任务。设备控制器于是按照该命令的要求去控制指定I/O设备。此时，CPU与I/O设备并行操作。例如，在输入时，当设备控制器收到CPU发来的读命令后，便去控制相应的输入设备读数据。一旦数据进入数据寄存器，控制器便通过控制线应CPU发送一中断信号，由CPU检查输入过程中是否出错。若无，便向控制器发送取走数据的信号，然后再通过控制器及数据线，将数据写入内存指定单元中。\n\n（3）直援存储器访问方式（DMA方式）\n\n为了进一步减少CPU对I/O的干预，引入了直接存储器访问方式，该方式的特点是：\n\n- 数据传输的基本单位是数据块，即在CPU与I/O设备之间，每次传送至少一个数据块。\n- 所传送的数据是从设备直接送入内存的，或者相反。\n- 仅在传送一个或多个数据块的开始和结束时，才需CPU干预，整块数据的传送是在控制器的控制下完成的。可见，DMA方式较之中断驱动方式又进一步提高了CPU与I/O设备的并行操作程度。\n\n它的特点是：\n\n- CPU和I/O设备并行。\n- 每次传送一个至少一个数据块。所传送的多个数据块是连续的，从设备直接送入内存，或者相反。\n\n（4）I/O通道控制方式\n\n虽然DMA方式比起中断方式来已经显著地减少了CPU的干预，即已由以字(节)为单位的干预减少到以数据块为单位的干预，但CPU每发出一条I/O指令，也只能去读(或写)一个连续的数据块。而当我们需要一次去读多个数据块且将它们分别传送到不同的内存区域，或者相反时，则须由CPU分别发出多条I/O指令及进行多次中断处理才能完成。\n\nI/O通道方式是DMA方式的发展，它可进一步减少CPU的干预，即把对一个数据块的读(或写)为单位的干预，减少为对一组数据块的读(或写)及有关的控制和管理为单位的干预。同时，又可实现CPU、通道和I/O设备三者的并行操作，从而更有效地提高整个系统的资源利用率。例如，当CPU要完成一组相关的读(或写)操作及有关控制时，只需向I/O通道发送一条I/O指令，以给出其所要执行的通道程序的首址和要访问的I/O设备，通道接到该指令后，通过执行通道程序便可完成CPU指定的I/O任务。\n\n它的特点是：\n\n-  CPU、通道、I/O设备并行。\n- 每次传送多个不同方向且不连续的数据块。\n\n##  49、什么是假脱机(Spooing)技术？\n\n当系统中引入了多道程序技术后，完全可以利用其中的一道程序，来模拟脱机输入时的外围控制机功能，把低速I/O设备上的数据传送到高速磁盘上。再用另一道程序模拟脱机输出时外围控制机的功能，把数据从磁盘传送到低速I/O设备上。 这样，便可在主机的直接控制下，实现以前的脱机输入、输出功能。我们把这种在联机情况下利用多道程序实现数据输入和输出的技术称为 SPOOLing（Simultaneaus Periphernal Operating OnLine)技术，或称为假脱机技术。\n\n## 50、磁盘的访问时间有哪些\n\n磁盘设备在工作时以恒定速率旋转。为了读或写，磁头必须能移动到所指定的磁道上，并等待所指定的扇区的开始位置旋转到磁头下，然后再开始读或写数据。故可把对磁盘的访问时间分成以下三部分：\n\n- **寻道时间**：把磁头移动到指定磁道上所经历的时间\n- **旋转延迟时间** ：指定扇区移动到磁头下面所经历的时间\n- **传输时间** ：数据的传输时间（数据读出或写入的时间）\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127225002.png)\n\n## 51、常见的磁盘调度算法\n\n为了减少对文件的访问时间，应采用一种最佳的磁盘调度算法，**以使各进程对磁盘的平均访问时间最小**。**由于在访问磁盘的时间中主要是寻道时间**，因此，磁盘调度的目标是使磁盘的平均寻道时间最少。目前常用的磁盘调度算法有先来先服务、最短寻道时间优先及扫描等算法。\n\n（1）先来先服务(FCFS)\n根据进程请求访问磁盘的先后次序进行调度。\n（2）最短寻道时间优先(SSTF)\n该算法选择的进程，其要求访问的磁道与当前磁头所在的磁道距离最近，以使每次的寻道时间最短。\n（3）扫描(SCAN)算法（电梯调度算法）\n该算法不仅考虑到欲访问的磁道与当前磁道间的距离，更优先考虑的是磁头当前的移动方向。这是典型电梯调度算法。\n（4）循环扫描(CSCAN)算法\n电梯调度算法的改进，即将最小磁道号紧接着最大磁道号构成循环，进行循环扫描。\n（5）NStepSCAN算法\n“**磁臂粘着**”(Armstickiness)：前几种算法中，如果有一个或几个进程对某一磁道频率进行I/O操作，从而垄断了整个磁盘设备。我们把这一现象称为磁臂粘着。\nN步SCAN算法是将磁盘请求队列分成若干个长度为N的子队列，磁盘调度将按FCFS算法依次处理这些子队列。而每处理一个队列时又是按SCAN算法。当系统（磁盘调度）正在处理某子队列的请求时，如果又出现新的磁盘I/O请求，便将新请求进程放入其他队列，这样就可避免出现粘着现象。\n（6）FSCAN算法\nFSCAN算法是N步SCAN算法的简化，即FSCAN只将磁盘请求队列分成两个子队列。一个是由当前所有请求磁盘I/O的进程形成的队列，由磁盘调度按SCAN算法进行处理。在扫描期间，将新出现的所有请求磁盘I/O的进程，放入另一个等待处理的请求队列。\n\n## 52、文件系统中的三级数据组织形式\n\n在文件系统中，通常把数据分为数据项、记录和文件三级。\n\n（1）数据项\n\n最低级的数据组织形式，分为：\n\n- 基本数据项：原子数据，最小逻辑单位，即数据元素、字段。如学号、姓名\n- 组合数据项：由若干个基本数据项组成，简称组项。如工资（基本工资、奖励工资）\n\n数据项的型和值：型指数据项名字和类型，实体在数据项上的数据则称为值。\n\n（2）记录\n\n一组相关数据项的集合，用于描述一个对象在某方面的属性。一个记录应包含哪些数据项，取决于需要描述对象的哪个方面。由于对象所处的环境不同可把他作为不同的对象。例如，一个学生，当把他作为班上的一名学生时，对他的描述应使用学号、姓名、年龄及所在系班，也可能还包括他所学过的课程的名称、成绩等数据项。但若把学生作为个医疗对象时，对他描述的数据项则应使用诸如病历号、姓名、性别、出生年月、身高、体重、血压及病史等项。\n在诸多记录中，为了能唯一地标识一个记录，必须在一个记录的各个数据项中确定出一个或几个数据项，把它们的集合称为关键字(key)。或者说，关键字是唯一能标识一个记录的数据项。通常，只需用一个数据项作为关键字。例如，前面的病历号或学号便可用来从诸多记录中标识出唯一的一个记录。然而有时找不到这样的数据项，只好把几个数据项定为能在诸多记录中唯一地标识出某个记录的关键字。\n\n（3）文件\n\n文件是指由创建者者所定义的、具有文件名的一组相关元素的集合，可分为有结构文件和无结构文件两种。在有结的文件中，文件由若干个相关记录组成，而无结构文件则被看成是一个字符流。文件在文件系统中是一个最大的数据单位，它描述了一个**对象集**。例如，可以将一个班的学生记录作为一个文件。\n\n文件、记录和数据项之间的层次关系如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127230905.png)\n\n## 53、文件系统模型的三个层次\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127231021.png)\n\n**文件系统接口(最高层)**：命令接口和程序接口。命令接口是指用户与文件系统直接交互的接口，程序接口是**用户程序**与文件系统的接口。\n\n**对对象操纵和管理的软件集合(中间层)**：对文件读写的管理、对文件的保护和共享、对文件目录的管理、对存储空间的管理。\n\n对象及属性(最底层)：对象有文件、目录、磁盘存储空间。\n\n## 54、文件的逻辑结构和物理结构\n\n（1）文件的逻辑结构\n\n文件的逻辑结构：从用户角度出发所看到的文件组织形式，是**用户可以直接处理**的数据及其结构。\n\n文件的逻辑结构有：\n\n①按有无结构可分为有结构文件和无结构文件两种。在有结构的文件中，文件有若干个相关记录组成，而无结构文件则被看成是一个字符流。\n\n②按文件的组织方式分类可分为顺序文件、索引文件、索引顺序文件。\n\n- 顺序文件：按照某种顺序排列所形成的文件，其记录可以是定长记录或可变长记录。\n- 索引文件：（定长记录可以通过简单计算，实现随机查找）当记录为可变长时，通常为之建立一张索引表，并**为每一个记录设置一个表项**，以加快检索记录的速度。\n- 索引顺序文件：这是顺序文件和索引文件相结合的产物，这里，在为每个文件建立一张索引表时，并不是为每一个记录建立一个索引表项，而是**为每一组记录中的第一个记录设置一个表项**。\n\n（2）文件的物理结构\n\n文件在外存上的存储组织形式，又称为文件的存储结构**。文件在外存上的存储组织形式可分为以下形式**：\n\n①连续组织方式\n为每个文件分配相邻的物理块（盘块/扇区），将分配给文件的首物理块的地址登记在它的文件目录项内，这样的文件结构是顺序式的文件结构。\n②链接组织方式\n通过每个盘块上的链接指针，将同一个文件的多个离散的盘块链接成一个链表，这样的文件结构是链接式文件结构。\n③索引组织方式\n通过给每个文件的盘块建立索引，形成的是索引式文件结构。\n\n## 55、文件目录和目录文件\n\n通常，在现代计算机系统中，都要存储大量的文件，为了能对这些文件实施有效的管理，必须对它们加以妥善组织，这主要是通过**文件目录**实现的。文件目录也是一种数据结构，用于标识系统中的文件及其物理地址，供检索时使用。对目录管理的要求如下：\n（1）实现“按名存取”。用户只须向系统提供所需访同文件的名字，便能快速准确地找到指定文件在外存上的存储位置。这是目录管理中最基本的功能。\n（2）提高对目录的检索速度。通过合理地组织目录结构加快对目录的检索速度，从而提高对文件的存取速度。\n（3）文件共享，在多用户系统中,应允许多个用户共享一个文件，这样就只须在外存中保留一份该文件的副本供不同用户使用，以节省大量的存储空间，并方便用户和提高文件利用率。\n（4）允许文件重名。系统应允许不同用户对不同文件采用相同的名字，以便于用户按照自己的习惯给文件命名和使用文件。\n\n文件控制块(FCB，File Controller Block)：为正确存取文件，而为文件设置的用于描述和控制文件的数据结构。文件与文件控制块一一对应。\n**文件控制块的有序集合构成文件目录。**\n**文件目录以文件的形式存放，所以叫做目录文件。**\n\n## 56、文件的链接组织形式\n\n前面也提到了，文件的物理结构也即文件在外存上的存储组织形式，又称为文件的存储结构。文件在外存上的存储组织形式有连续组织方式、链接组织方式、索引组织方式，这里介绍文件的链接组织方式。\n\n链接组织方式：通过每个盘块上的链接指针，将同一个文件的多个离散的盘块链接成一个链表，这样的文件结构是链接式文件结构。可分为隐式链接和显式链接两种方式。\n\n（1）隐式链接\n\n将一文件离散地存放在外存上，并将下一个物理块（磁盘块）的地址登记在分配给它的前一个物理块中。\n\n磁盘空间的链接式分配如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127234257.png)\n\n（2）显式链接\n\n将文件离散地存放，并将链接各个物理块的指针显式地登记在内存的一张文件分配表FAT(File Allocation Table)中。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127234450.png)\n\n## 57、文件分配表FAT技术\n\n在MS-DOS中，最早使用的是12位的FAT12，后来为16位的FAT16。在Windows 95和Windows 98操作系统中升级为32位的FAT32，Windows NT/2000/XP及以后的操作系统中又进一步发展为新技术文件系统NTFS。 \n\n在FAT中引入了“卷”的概念，支持将一个物理磁盘分成四个逻辑磁盘，每个逻辑磁盘就是一个**卷**(也称为**分区**)，也就是说每个卷都是一个能够被单独格式化和使用的逻辑单元，供文件系统分配空间时使用。一个卷中包含了文件系统信息、一组文件以及空闲空间。每个卷都专门划出一个单独区域来存放自己的目录和FAT表，以及自己的逻辑驱动器字母。通常对仅有一个硬盘的计算机，最多可将其硬盘分为“C:”、“D:”、“E:\"、“F:”四个卷。需要指出的是。在现代OS中，一个物理磁盘可以划分为多个卷，一个卷也可以由多个物理磁盘组成。\n\n（1）FAT12 \n\nFAT12是以盘块为基本分配单位的，每个盘块大小一般为512B，即0.5K，最多能够存放$2^{12}$方个盘块。为了安全起见，**在每个分区中**都配有两张相同的文件分配表FAT1和FAT2。在FAT的每个表项中存放下一个盘块号，实际上用于盘块之间链接的指针，**通过它可以将一个文件的所有盘块链接起来**。FAT12技术将文件的第一个盘块号放在自己的文件控制块(FCB)中。\n\n为了适应磁盘容量不断增大的需要，在进行盘块分配时，不再以盘块而是以**簇**(cluster)为基本单位，簇是一组连续的扇区，大小一般盘块的整数倍。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210127235301.png)\n\n（2）FAT16\n\n具有16位表宽的FAT表称为FAT16，最大表项数将增至65536 ($2^{16}$)个，可以存放更多的盘块。\n\n（3）FAT32\n\n具有32位表宽的FAT表称为FAT32，FAT12一般以簇为基本分配单位的，FAT32的每个簇都固定为4 KB，管理的最大磁盘空间可达4KB*$2^{32}$，即16TB。FAT32有最小管理空间的限制，不支持容量小于512M的分区，并且FAT32的单个文件长度不能大于4GB。\n\n## 58、NTFS的文件组织方式\n\nNTFS(New Technology File System)是一个专门为Windows NT开发的、全新的文件系统，并适用于Windows 2000/XP及后续的Windows OS。\n\n（1）NTFS磁盘组织\n\nNTFS是以簇作为磁盘空间分配和回收的基本单位的。一个文件占用若干个簇，一个簇只属于一个文件。这样，在为文件分配磁盘空间时，就无须知道盘块的大小，只要根据不同的磁盘容量，选择相应大小的簇，即使NTFS具有了与磁盘物理块大小无关的独立性。\n\n在NTFS文件系统中，把卷上簇的大小称为“**卷因子**”，卷因子是在磁盘格式化时确定的，其大小也是物理磁盘扇区的整数倍。簇的大小可由格式化命令按磁盘容量和应用需求来确定，可以为512B、1KB、…，最大可达64KB。对于小磁盘(≤512MB)，默认簇大小为512字节；对于1GB磁盘，默认簇大小为1KB；对于2GB的磁盘，则默认簇为4KB。事实上，为了在传输效率和簇内碎片之间进行折中，NFS在大多数情况下都是使用4KB。\n\n（2）NTFS文件的组织\n\n在NTFS中，**以卷为单位**，将一个卷中的所有文件信息、目录信息以及可用的未分配空间信息，都以文件记录的方式记录在一张**主控文件表 MFT**(Master File Table)中，该表是NTFS卷结构的中心，从逻辑上讲，卷中的每个文件作为一条记录，在MFT表中占有一行，其中还包括MFT自己的这一行。每行大小固定为1 KB，每行称为该行所对应文件的**元数据**(metadata)，也称为文件控制字。\n\n在MFT表中，每个元数据都将其所对应文件的所有信息(包括文件的内容等)组织在所对文件的一组属性中。由于文件大小相差悬殊，其属性所需空间大小也相差很大。当文件较小时，其属性值所占空间也较小，可以将文件的所有属性直接记录在元数据中。而当文件较大时，元数据仅能记录文件的一部分属性，其余属性，如文件的内容等，只好记录到卷中的其它可用簇中，并将这些簇按其所记录文件的属性进行分类，分别链接成多个队列，并将指向这些队列的指针保存在元数据中。\n\n例如，对于一个真正的数据文件，即属性为DATA的文件，如果很小，就直接将其存储在MFT表中对应的元数据中，这样对文件数据的访问仅需要对MFT表进行访问即可，减少了磁盘访问次数，显著地提高了对小文件存取的效率。如果文件较大，则文件的真正数据往往保存在其它簇中。此时通过元数据中指向文件DATA属性的队列指针，可以方便地查找到这些簇，完成对文件数据的访问。\n\nNTFS使用64位磁盘地址，并且支持长文件名，单个文件名限制在255个字符以内。它和FAT技术一样，也是链接组织形式的一种。\n\n## 59、文件存储空间的管理\n\n文件在外存上的存储组织形式有连续组织方式、链接组织方式、索引组织方式。为了实现任何一种文件组织方式，都需要为文件分配盘块，**因此必须知道磁盘上哪些盘块是可用于分配的**。**故在为文件分配磁盘时，除了需要文件分配表外，系统还应为可分配存储空间设置相应的数据结构**，即设置一个磁盘分配表 (Disk Allocation Table)，用于\n记住可供分配的存储空间情况。此外，还应提供对盘块进行分配和回收的手段。不论哪种分配和回收方式，存储空间的基本分配单位都是磁盘块而非字节。下面介绍几种常用的文件存储空间的管理方法：\n\n（1）空闲表法\n\n属于连续分配方式，为每个文件分配一块连续空间。系统为外存上的所有区建立一张空闲表，每个空闲区对应一个空闲表项，每个表项包括表项序号、空闲区的第一个盘块号和空闲区的盘块数。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210128113805.png)\n\n**分配与回收**\n\n空闲盘区的分配与内存的分区(动态)分配类似，同样是采用首次适应算法和最佳适应算法等。\n\n系统在对用户所释放的存储空间进行回收时，也采取类似于内存回收的方法，即要考虑回收区是否与空闲表中插入点的前区和后区相邻接，对相邻接者应予以合并。\n\n优点：空闲区分配与回收容易。\n缺点：空闲表也会浪费很大存储空间。\n\n（2）空闲链表法\n\n将文件存储空间中的所有空闲区拉成一条空闲链表。根据构成链所用基本元素的不同，可以把链表分为两种形式：空闲盘块链和空闲盘区链。\n\n①空闲盘块链：将磁盘上所有空闲空间，以盘块为单位拉成一条链。\n\n优点：分配和回收一个盘块的过程简单。\n缺点：空闲盘块链可能很长。分配盘块时，可能要重复操作多次。\n\n②空闲盘区链：将磁盘上所有空闲盘区拉成一条链。和空闲表法类似。\n\n（3）位示图法\n\n利用二进制的一位来表示文件存储空间中的一个盘块的使用情况。其值为0表示空闲，为1表示分配，这样由所有盘块所对应的位构成一个集合，称为位示图。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210128114835.png)\n\n在盘块分配时，将相应位置1，在回收时，将相应位置0。\n\n（4）成组链接法\n\n①空闲盘块的组织\n\n将一个文件卷的所有空闲盘块分成固定大小的组（如100个盘块），将每一组的盘块号和盘块数记入前一组的最后一个盘块中，第一组的盘块数和盘块号记入空闲盘块号栈中。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210128120327.png)\n\n②空闲盘块的分配\n\n首先检查空闲盘块号栈是否上锁，如未上锁，便从栈顶取出一空闲盘块号，将与之对应的盘块分配给用户，然后将栈顶指针下移一格。把栈中的空闲盘块数减1。\n\n若该盘块号已是栈底，即S.free(0)，这是当前栈中最后一个可分配的盘块号。调用磁盘读过程，将栈底盘块号所对应盘块的内容读入栈中，作为新的盘块号栈的内容，并把原栈底对应的盘块分配出去(其中的有用数据已读入栈中)。\n\n③空闲盘块的回收\n\n将回收盘块的盘块号记入空闲盘块号栈的顶部，并执行空闲盘块数加1操作。\n\n当栈中空闲盘块号数目已达100时，表示栈已满，便将现有栈中的100个盘块号记入新回收的盘块中，再将其盘块号作为新栈底。空闲盘块数记为1。\n\n## 60、廉价磁盘冗余阵列（RAID)\n\n人们于1987年开发出由多个小磁盘组成一个大容量的康价碰盘冗余阵列( Redundant Array of Inexpensive Disk,RAID)，该系统是利用一台磁盘阵列控制器来统一管理和控制一组(几台到几十台)磁盘驱动器组成一个大型磁盘系统。RAID不仅是大幅度地增加了磁盘的容量，而且也极大地提高了磁盘的I/O速度和整个磁盘系统的可靠性。故该系统一经推出便很快被许多大型系统所采用。\n\n（1）并行交叉存取\n\n把在大、中型机中，用于提高访问内存速度的并行交叉存取技术应用到磁盘存储系统中，以提高对磁盘的I/O速度。在该系统中，有多台磁盘驱动器，系统将每一盘块中的数据分为若干个子盘块数据，再把每一个子盘块的数据分别存储到各个不同磁盘中的**相同位置**上。以后当要将一个盘块的数据传送到内存时，采取并行传输方式，将各个盘块中的子盘块数据同时向内存中传输，从而使传输时间大大减少。 \n\n磁盘并行交叉存取方式如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210128122310.png)\n\n（2）RAID的分级\n\n- RAID 0级。仅提供了并行交叉存取。 无冗余校验功能。\n- RAID 1级。具有磁盘镜像功能。\n- RAID 3级。具有并行传输功能的磁盘阵列。利用其中一台磁盘做奇偶校验盘完成数据的校验功能。\n- RAID 5级。具有独立传送功能的磁盘阵列。每个磁盘都有自己独立的数据通路，独立地进行读/写，无专门的校验盘。\n- RAID 6级和RAID 7级。RAID 6级的阵列中，设置了一个专用的、可快速访问的异步校验盘，该盘有独立的数据访问同路。RAID 7级是对RAID 6级的改进，在该阵列中的所有磁盘，都具有较高的传输速率和优异的性能，是目前最高档次的磁盘阵列。 \n\n## 61、常见的磁盘容错技术\n\n磁盘容错技术，又称系统容错技术SFT(System Fault Tolerance)，主要通过设置冗余部件来提高磁盘系统可靠性。SFT常被分为 低、中、高三个级别。第一级是低级磁盘容错技术；第二级是中级磁盘容错技术；第三级是系统容错技术，它基于集群技术实现容错。\n\n（1）第一级容错技术SFT-Ⅰ\n\n主要用于防止因磁盘表面缺陷所造成的数据丢失。它包含双份目录和双份文件分配表、热修复重定向、写后读校验等措施。\n\n①双份目录和双份文件分配表\n\n可在不同的磁盘上或在磁盘的不同区域中，分别建立(双份)目录表和FAT。其中，一份被称为主目录及主FAT； 把另一份称为备份目录及备份FAT。 \n\n②热修复重定向\n\n系统将磁盘的一部分作为热修复重定向区，用于存放当发现磁盘有缺陷时的待写入数据，并对写入该区的数据进行登记，以便以后对数据进行访问。\n\n③写后读校验\n\n在每次从内存缓冲区向磁盘中写入数据块后，又立即读出该磁盘块，并送至另一缓冲区与内存中的数据进行比较，若一致则认为写入成功。\n\n（2）第二级容错技术SFT-Ⅱ\n\n第二级容错技术主要用于防止由磁盘驱动器和磁盘控制器故障所导致的系统不能正常工作，它具体分为磁盘镜像和磁盘双工。\n\n①磁盘镜像(Disk Mirroring)\n\n为了避免磁盘驱动器发生故障而丢失数据，便增设了磁盘镜像功能。为实现该功能，须在同一磁盘控制器下，再增设一个完全相同的磁盘驱动器。 \n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210128123550.png)\n\n②磁盘双工(Disk Duplexing)\n\n即将两台磁盘驱动器分别接到两个磁盘控制器上，同样使这两台磁盘机镜像成对。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210128123650.png)\n\n（3）第三级容错技术——基于集群技术的容错功能\n\n主要工作模式有三种：热备份模型、互为备份模式和公用磁盘模式。\n\n①双机热备份模式\n\n系统中备有两台服务器，两者的处理能力通常是完全相同的，一台作为主服务器，另一台作为备份服务器。 \n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210128124108.png)\n\n②双机互为备份模式\n\n两台服务器均为在线服务器，它们各自完成自己的任务。例如，一台作为数据库服务器，另一台作为电子邮件服务器，为了实现两者互为备份的功能，在两台服务器之间，应通过某种专线将其连接起来。如果希望两台服务器之间能相距较远，最好利用FDDI单模光纤来连接两台服务器。在此情况下，最好再通过路由器将两台服务器互连起来，作为备份通信线路。下图是双机互为备份系统的情况：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210128124142.png)\n\n在互为备份的模式中，最好在每台服务器内都配置两台硬盘，一个用于装载系统程序和应用程序，另一个用于接收由另一台服务器发来的备份数据，作为该服务器的镜像盘。在正常运行时，镜像盘对本地用户是锁死的，这样就较易于保证在镜像盘中数据的正确性。如果仅有一个硬盘，则可用建立虚拟盘的方式或分区方式来分别存放系统程序和应用程序以及另一台服务器的备份数据。\n\n③公用磁盘模式\n\n为了减少信息复制的开销，可以将多台计算机连接到一台公共的磁盘系统上去。该公共磁盘被划分为若干个卷。每台计算机使用一个卷。如果某台计算机发生故障，此时系统将重新进行配置，根据某种调度策略来选择另一台替代机器，后者对发生故障的机器的卷拥有所有权，从而可接替故障计算机所承担的任务。\n\n## 62、特权指令和非特权指令、管态和目态\n\n（1）特权指令和非特权指\n\n**特权指令**是指不允许用户程序中直接使用的指令，是关系到系统全局的指令。其对内存空间的访问范围基本不受限制，不仅能访问用户存储空间，也能访问系统存储空间。包括I/O指令、设置系统时钟时间、关中断、清主存、修改存储器管理寄存器、执行停机指令、转换执行状态等。 \n\n**非特权指令**只能完成一般性的操作和任务，不能对系统中的硬件和软件直接进行访问，其对内存的访问范围也局限于用户空间。 \n\n（2）管态和目态\n\n**管态（系统态、核心态）**：当CPU处于管态时可执行包括特权指令在内的一切机器指令。当**OS程序**占用中央处理器时应让CPU在管态工作。\n\n**目态（用户态）**：不允许执行特权指令。当**应用程序**占用中央处理器时应让CPU在目态工作。\n\n\n\n## 63、什么是系统调用？和库函数调用有何区别？\n\n（1）系统调用是操作系统提供的一组用于实现各种系统功能的子程序(过程)，并将它们提供给应用程序调用。库函数调用可以认为是系统调用的封装（并不是所有的库函数都是），有可能包含一个系统调用，有可能包含几个系统调用，也有可能不包含系统调用。\n（2）系统调用是面向硬件的，调用速度明显快于库函数，但是缺乏移植性。库函数的调用是面向开发的，虽然速度要慢于系统调用，但是解决了一致性的问题。\n（3）系统调用发生在内核空间，如果一般应用程序使用系统调用来进行文件操作，会有用户态到系统态切换的开销。事实上，如果使用库函数调用进行文件操作也会有系统调用带来的开销，但是使用库函数可以大大减少系统调用的次数。因为由于双缓冲的存在(双缓冲区肯定比单个缓冲区强)，在用户态和内核态，都应用了缓冲技术，当用户使用fwrite()写文件，都是先将内容写到用户空间缓冲区，当用户空间缓冲去写满或者写操作结束时，才将用户缓冲区的内容写到内核缓冲区，同样的道理，当内核缓冲区写满或者写结束时才将内核缓冲区的内容写到文件对应的硬件媒介上。\n\n## 64、系统调用和一般的过程调用的区别\n\n(1) 运行在不同的系统状态。系统调用中，调用程序是运行在用户态，而被调用程序是运行在系统态。一般的过程调用，其调用程序和被调用程序都运行在相同的状态——系统态或用户态。\n\n (2) 状态的转换。系统调用中，涉及到**由用户态到系统态的转换**。而一般的过程调用并不涉及到系统状态的转换。\n\n(3) 返回问题。系统调用中，在被调用过程执行完后，不一定返回原进程执行，而是重新做进程优先级分析，当原进程仍具有最高优先级时，才返回到调用程序继续执行。但一般的过程调用仍返回原进程。\n\n(4) 嵌套调用。系统调用中，每个系统对嵌套调用的深度都有一定的限制，例如最大深度为6。但一般的过程对嵌套的深度则没有什么限制。\n\n##  65、逻辑地址空间和物理地址空间\n\n程序经过编译后，每个目标模块都是从0号单元开始编址，称为目标模块的相对地址(或逻辑地址)。当链接程序将各个目标模块链接成一个完整的可执行目标程序时，依次按各个模块的相对地址构成一个统一的**从0号单元开始编址的逻辑地址空间**。\n\n物理地址空间是指内存中物理单元的集合，它是地址转换的最终地址，进程在运行时执行指令和访问数据都要通过物理地址从主存中获取。当装入程序(Loader)将可执行目标程序装入内存时，必须通过地址转换将逻辑地址转换成物理地址(因为要分配内存空间)，这个过程称为**地址重定位**，这样形成的地址空间就是**物理地址空间**。\n\n\n\n> 如需转发请联系本人同意，谢谢！","tags":["操作系统","面经"],"categories":["知识储备"]},{"title":"使用HybridSN进行高光谱图像分类","url":"/2021/01/05/173233/","content":"\n\n\n## 一、前言\n\n高光谱图像（Hyperspectral image，以下简称HSI）分类广泛应用于遥感图像的分析，随着深度学习和神经网络的兴起，越来越多的人使用二维CNN对HSI进行分类，而HSI分类性能却高度依赖于空间和光谱信息，由于计算复杂度增加，很少有人将三维CNN应用于HSI分类中。这篇 [Exploring 3-D–2-D CNN Feature Hierarchy for Hyperspectral Image Classification](https://ieeexplore.ieee.org/document/8736016)构建一种混合网络(HybridSN)解决了HSI分类所遇到的问题，它首先用三维CNN提取空间-光谱的特征，然后在三维CNN基础上进一步使用二维CNN学习更多抽象层次的空间特征，这与单独使用三维CNN相比，混合的CNN模型既降低了复杂性，也提升了性能。经实验证明，使用HybridSN进行HSI分类，能够获得非常不错的效果。\n\n\n\n<!-- more -->\n\n\n\n## 二、高光谱图像\n\n在进行高光谱图像分类之前，我认为有必要了解什么是高光谱图像。从计算机的角度来说，高光谱图像（Hyperspectral image）就是由多通道（几十甚至几百个）的数组构成的图像，每个像素点都有很多的数来描述，**单个通道上的“灰度值”反映了被拍摄对象对于某一波段的光的反射情况。**\n\n我们知道，常见的RGB彩色图像只有三个通道，而高光谱图像有几十甚至几百个通道，所以高光谱图像包含包含更多的目标信息，利用高光谱图像进行目标的分类识别也必然比采用RGB图像具有更高的准确度。如下图所示，利用高光谱相机可以拍摄出由不同波长组成的空间立方体图像（即高光谱图像），用一个光谱曲线将其显示出来，横轴表示波长，纵轴表示反射系数，由于同一物体对不同波长的光反射因子不一样，因此利用高光谱图像更能反映出不同物体的差异性。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210103152030.png)\n\n其实高光谱成像技术在很早以前就已经被广泛应用了，天上的卫星拍摄到的就是高光谱图像，通过分析每个像素点的光谱曲线，可以把不同地面目标对应的像素点分类，从而在高光谱图像中把地面、建筑物、草坪、江河等等区分开。\n\n\n\n## 三、HybridSN模型\n\n对于HSI分类问题，我们在提取空间信息的同时，也希望能获取到不同波长的光谱信息，而二维CNN是无法处理光谱信息的，也就无法提取到更具有判别性的特征图。幸运的是，三维CNN能够同时提取光谱和空间的特征，但代价是增加计算复杂度。为了充分发挥二维和三维CNN的优势，Swalpa Kumar Roy等人提出了HSI分类模型HybridSN，其模型图如下图所示，它由三个三维卷积、一个二维卷积和三个全连接层组成。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210103230613.png)\n\n在HybridSN模型中，三维卷积核的尺寸分别为`8×3×3×7×1`(即图中$K_1^1$=3 ，$K_2^1$=3 ，$K_3^1$=7)、`16×3×3×5×8`(即图中$K_1^2$=3 ，$K_2^2$=3 ，$K_3^2$=5)和`32×3×3×3×16`(即图中$K_1^3$=3 ，$K_2^3$=3 ，$K_3^3$=3），分别位于第一、第二和第三卷积层中。其中，`16×3×3×5×8`表示输入特征图的个数为8，输出特征图个数为16，三维卷积核大小为`3x3x5`，可理解为有两个空间维度和一个光谱维度。**二维卷积在flatten之前被应用一次，它能有效的判别空间信息，也不会大量损失光谱信息，这是对HSI数据非常重要**。\n\n模型的详细参数配置如下表所示，可以看出，第一个FC层（即dense1)参数量最多，最后一个全连接层（dense3）的输出为16，这是因为Indian Pines (IP)数据集的类别数为16。HybridSN中可训练的权重参数总数为5122176，所有参数都是随机初始化的，使用Adam优化器，交叉熵损失函数，学习率为0.001，batch大小为128，训练100个epoch。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210103232652.gif)\n\n\n\n下面是我实现的HybridSN模型：\n\n```python\nclass_num = 16\nclass HybridSN(nn.Module):  \n  def __init__(self, in_channels=1, out_channels=class_num):\n    super(HybridSN, self).__init__()\n    self.conv3d_features = nn.Sequential(\n        nn.Conv3d(in_channels,out_channels=8,kernel_size=(7,3,3)),\n        nn.ReLU(),\n        nn.Conv3d(in_channels=8,out_channels=16,kernel_size=(5,3,3)),\n        nn.ReLU(),\n        nn.Conv3d(in_channels=16,out_channels=32,kernel_size=(3,3,3)),\n        nn.ReLU()\n    )\n\n    self.conv2d_features = nn.Sequential(\n        nn.Conv2d(in_channels=32 * 18, out_channels=64, kernel_size=(3,3)),\n        nn.ReLU()\n    )\n\n    self.classifier = nn.Sequential(\n        nn.Linear(64 * 17 * 17, 256),\n        nn.ReLU(),\n        nn.Dropout(p=0.4),\n        nn.Linear(256, 128),\n        nn.ReLU(),\n        nn.Dropout(p=0.4),\n        nn.Linear(128, 16)\n    )\n \n  def forward(self, x):\n    x = self.conv3d_features(x)\n    x = x.view(x.size()[0],x.size()[1]*x.size()[2],x.size()[3],x.size()[4])\n    x = self.conv2d_features(x)\n    x = x.view(x.size()[0],-1)\n    x = self.classifier(x)\n    return x\n```\n\n带有Batch Normalization的HybridSN模型：\n\n```python\nclass HybridSN_BN(nn.Module):  \n  def __init__(self, in_channels=1, out_channels=class_num):\n    super(HybridSN_BN, self).__init__()\n    self.conv3d_features = nn.Sequential(\n        nn.Conv3d(in_channels,out_channels=8,kernel_size=(7,3,3)),\n        nn.BatchNorm3d(8),\n        nn.ReLU(),\n        nn.Conv3d(in_channels=8,out_channels=16,kernel_size=(5,3,3)),\n        nn.BatchNorm3d(16),\n        nn.ReLU(),\n        nn.Conv3d(in_channels=16,out_channels=32,kernel_size=(3,3,3)),\n        nn.BatchNorm3d(32),\n        nn.ReLU()\n    )\n\n    self.conv2d_features = nn.Sequential(\n        nn.Conv2d(in_channels=32 * 18, out_channels=64, kernel_size=(3,3)),\n        nn.BatchNorm2d(64),\n        nn.ReLU()\n    )\n\n    self.classifier = nn.Sequential(\n        nn.Linear(64 * 17 * 17, 256),\n        nn.ReLU(),\n        nn.Dropout(p=0.4),\n        nn.Linear(256, 128),\n        nn.ReLU(),\n        nn.Dropout(p=0.4),\n        nn.Linear(128, 16)\n    )\n \n  def forward(self, x):\n    x = self.conv3d_features(x)\n    x = x.view(x.size()[0],x.size()[1]*x.size()[2],x.size()[3],x.size()[4])\n    x = self.conv2d_features(x)\n    x = x.view(x.size()[0],-1)\n    x = self.classifier(x)\n    return x\n```\n\n上面我实现了两种模型，一种是原始的HybridSN模型，另一种是带有Batch Normalization的HybridSN模型，下面还会再实现另外两种模型。\n\n## 四、注意力机制\n\n为了提升HSI分类模型的性能，我也实现了带有注意力机制的HybridSN模型进行训练，这里我采用[CBAM: Convolutional Block Attention Module](https://arxiv.org/pdf/1807.06521.pdf)的空间注意力和通道注意力机制。\n\n（1）Channnel attetion module(通道注意力模块)\n\n通道注意力模是解决**look what**的问题，主要是探索不同通道之间特征图的关系，通过分配各个卷积通道上的资源，使模型更应该注意哪一部分特征。通道注意力的过程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210104232604.png)\n\n1. 首先使用MaxPool和AvgPool聚合两个空间维度上的特征，实现时可以用`AdaptiveAvgPool2d`和`AdaptiveMaxPool2d`保证尺寸不变\n2. 然后通过共享的MLP层，即FC+Relu+FC层，学习每个通道的权重，再将两个特征图相加，后接一个sigmoid函数。\n3. 最后将结果与未经channel attention的原始输入相乘，从而得到的新的特征图。\n\nChannnel attetion module实现如下：\n\n```python\n# 参考 https://github.com/luuuyi/CBAM.PyTorch\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n```\n\n（2）Spatial attention module(空间注意力模块)\n\n空间注意力模块解决的是**look where**的问题，通过对特征图每个位置进行二维调整（即attention调整），使模型关注到值得更多关注的区域上。空间注意力的过程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210104222745.png)\n\n1. 首先对不同特征图上相同位置的像素值进行全局的MaxPooling和AvgPooling操作，分别得到两个spatial attention map。\n2. 将这两个特征图concatenate，通过7\\*7的卷积核对这个feature map进行卷积操作，后接一个sigmoid函数。\n3. 最后把得到的空间注意力特征图与未经Spatial attention的原始输入相乘，得到的新的特征图。\n\nSpatial attention module实现如下：\n\n```python\n# 参考 https://github.com/luuuyi/CBAM.PyTorch\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n```\n\n**加上注意力机制的HybridSN模型如下：**\n\n```python\nclass_num = 16\nclass HybridSN_Attention(nn.Module):  \n  def __init__(self, in_channels=1, out_channels=class_num):\n    super(HybridSN_Attention, self).__init__()\n    self.conv3d_features = nn.Sequential(\n        nn.Conv3d(in_channels,out_channels=8,kernel_size=(7,3,3)),\n        nn.ReLU(),\n        nn.Conv3d(in_channels=8,out_channels=16,kernel_size=(5,3,3)),\n        nn.ReLU(),\n        nn.Conv3d(in_channels=16,out_channels=32,kernel_size=(3,3,3)),\n        nn.ReLU()\n    )\n\t# 通道和空间注意力\n    self.ca = ChannelAttention(32 * 18)\n    self.sa = SpatialAttention()\n\n    self.conv2d_features = nn.Sequential(\n        nn.Conv2d(in_channels=32 * 18, out_channels=64, kernel_size=(3,3)),\n        nn.ReLU()\n    )\n\n    self.classifier = nn.Sequential(\n        nn.Linear(64 * 17 * 17, 256),\n        nn.ReLU(),\n        nn.Dropout(p=0.4),\n        nn.Linear(256, 128),\n        nn.ReLU(),\n        nn.Dropout(p=0.4),\n        nn.Linear(128, 16)\n    )\n \n  def forward(self, x):\n    x = self.conv3d_features(x)\n    x = x.view(x.size()[0],x.size()[1]*x.size()[2],x.size()[3],x.size()[4])\n\n    x = self.ca(x) * x\n    x = self.sa(x) * x\n\n    x = self.conv2d_features(x)\n    x = x.view(x.size()[0],-1)\n    x = self.classifier(x)\n    return x\n```\n\n**加上Batch Normalization、注意力机制的HybridSN模型如下：**\n\n```python\nclass HybridSN_BN_Attention(nn.Module):  \n  def __init__(self, in_channels=1, out_channels=class_num):\n    super(HybridSN_BN_Attention, self).__init__()\n    self.conv3d_features = nn.Sequential(\n        nn.Conv3d(in_channels,out_channels=8,kernel_size=(7,3,3)),\n        nn.BatchNorm3d(8),\n        nn.ReLU(),\n        nn.Conv3d(in_channels=8,out_channels=16,kernel_size=(5,3,3)),\n        nn.BatchNorm3d(16),\n        nn.ReLU(),\n        nn.Conv3d(in_channels=16,out_channels=32,kernel_size=(3,3,3)),\n        nn.BatchNorm3d(32),\n        nn.ReLU()\n    )\n\n    self.ca = ChannelAttention(32 * 18)\n    self.sa = SpatialAttention()\n\n    self.conv2d_features = nn.Sequential(\n        nn.Conv2d(in_channels=32 * 18, out_channels=64, kernel_size=(3,3)),\n        nn.BatchNorm2d(64),\n        nn.ReLU()\n    )\n\n\n    self.classifier = nn.Sequential(\n        nn.Linear(64 * 17 * 17, 256),\n        nn.ReLU(),\n        nn.Dropout(p=0.4),\n        nn.Linear(256, 128),\n        nn.ReLU(),\n        nn.Dropout(p=0.4),\n        nn.Linear(128, 16)\n    )\n \n  def forward(self, x):\n    x = self.conv3d_features(x)\n    x = x.view(x.size()[0],x.size()[1]*x.size()[2],x.size()[3],x.size()[4])\n\n    x = self.ca(x) * x\n    x = self.sa(x) * x\n\n    x = self.conv2d_features(x)\n    x = x.view(x.size()[0],-1)\n    x = self.classifier(x)\n    return x\n```\n\n\n\n## 五、开始实验\n\n上面，我共实现了四种HybridSN模型，分别是：\n\n- 原始的HybridSN模型\n- 加上Batch Normalization的HybridSN模型：HybridSN_BN\n- 加上通道和空间注意力机制的HybridSN模型：HybridSN_Attention\n- 加上Batch Normalization、通道和空间注意力机制的HybridSN模型：HybridSN_BN_Attention\n\n下面我将分别用这四种模型测试Indian Pines数据集，并分析结果。(考虑到篇幅，下面我只写出了主要的方法，全部实现过程请看我的colab)\n\n\n\n### 5.1 下载数据集\n\nIndian Pines 是最早的用于HSI分类的数据集，该数据集有尺寸为145×145 的空间图像和224个波长范围为400～2500nm的光谱反射谱带，由于第 104~108、150-163 和第 220 个波段不能被水反射，因此一般使用的是剔除了这 20 个波段后剩下的 200 个波段作为测试的对象。该数据集共有16类庄稼，用不同的颜色标出。可通过如下方式下载数据集：\n\n```python\n! wget http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n! wget http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n! pip install spectral\n```\n\n### 5.2 PCA降维\n\n可以把HSI数据立方体表示为$I\\ \\in R^{M \\times N \\times D}$ ，其中*I*为原始输入，*M*为宽度，*N*为高度，*D*为光谱带数（即深度）。**I**中的每一个HSI像素都包含*D个*光谱量，并形成一个one-hot 标签向量，$Y = ({ {\\rm{y}}_1},{ {\\rm{y} }_2}, \\cdots ,{ {\\rm{y} }_C}) \\in {R^{1 \\times 1 \\times C} }$，其中，C表示谱带覆盖类别（land-cover categories）。然而，高光谱像素谱带覆盖类别的混合特性，使得类内差异性和类间相似性较高，这对任何模型来说都具有很大的挑战，为了消除光谱冗余，我们采用主成分分析(PCA)的方法将谱带的数量从*D*减少到*B*，同时保持相同的空间尺寸（即宽度*M*和高度*N*）。因为只减少了光谱带的数量，从而保留了空间信息，这对识别任何物体都是非常重要的。这里将PCA降维后的数据表示为$X \\in {R^{M \\times N \\times B}}$， 其中X为PCA后的修正输入，*M*为宽度，*N*为高度，*B*为PCA后的谱带数。\n\n为了更好的进行HSI分类，下面将HSI数据立方体划分为一个个小的有重叠的3D-patch，其真值由中心像素的标签决定。对于上面**X**中的3D neighboring patches $P \\in {R^{S \\times S \\times B}}$ ，其空间位置的中心为$(\\alpha ,\\beta )$，覆盖S×S窗口或空间范围和所有*B*谱段。因此，在位置$(\\alpha ,\\beta )$处的3D-patch，用${P_{\\alpha ,\\beta }}$表示，涵盖了宽度从$\\alpha  - (S - 1)/2$ 到 $\\alpha  + (S - 1)/2$ ，高度从$\\beta  - (S - 1)/2$ 到 $\\beta  + (S - 1)/2$，以及PCA降维后的数据立方体*X*的所有*B*谱段。\n\n下面是PCA降维及3D-patch的实现过程：\n\n```python\n# 对高光谱数据 X 应用 PCA 变换\ndef applyPCA(X, numComponents):\n    newX = np.reshape(X, (-1, X.shape[2]))\n    pca = PCA(n_components=numComponents, whiten=True)\n    newX = pca.fit_transform(newX)\n    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n    return newX\n\n# 对单个像素周围提取 patch 时，边缘像素就无法取了，因此，给这部分像素进行 padding 操作\ndef padWithZeros(X, margin=2):\n    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n    x_offset = margin\n    y_offset = margin\n    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n    return newX\n\n# 在每个像素周围提取 patch ，然后创建成符合 keras 处理的格式\ndef createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n    # 给 X 做 padding\n    margin = int((windowSize - 1) / 2)\n    zeroPaddedX = padWithZeros(X, margin=margin)\n    # split patches\n    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n    patchIndex = 0\n    for r in range(margin, zeroPaddedX.shape[0] - margin):\n        for c in range(margin, zeroPaddedX.shape[1] - margin):\n            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n            patchesData[patchIndex, :, :, :] = patch\n            patchesLabels[patchIndex] = y[r-margin, c-margin]\n            patchIndex = patchIndex + 1\n    if removeZeroLabels:\n        patchesData = patchesData[patchesLabels>0,:,:,:]\n        patchesLabels = patchesLabels[patchesLabels>0]\n        patchesLabels -= 1\n    return patchesData, patchesLabels\ndef splitTrainTestSet(X, y, testRatio, randomState=345):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState, stratify=y)\n    return X_train, X_test, y_train, y_test\n```\n\n然后，创建数据集加载类：\n\n```python\n\"\"\" Training dataset\"\"\"\nclass TrainDS(torch.utils.data.Dataset): \n    def __init__(self):\n        self.len = Xtrain.shape[0]\n        self.x_data = torch.FloatTensor(Xtrain)\n        self.y_data = torch.LongTensor(ytrain)        \n    def __getitem__(self, index):\n        # 根据索引返回数据和对应的标签\n        return self.x_data[index], self.y_data[index]\n    def __len__(self): \n        # 返回文件数据的数目\n        return self.len\n\n\"\"\" Testing dataset\"\"\"\nclass TestDS(torch.utils.data.Dataset): \n    def __init__(self):\n        self.len = Xtest.shape[0]\n        self.x_data = torch.FloatTensor(Xtest)\n        self.y_data = torch.LongTensor(ytest)\n    def __getitem__(self, index):\n        # 根据索引返回数据和对应的标签\n        return self.x_data[index], self.y_data[index]\n    def __len__(self): \n        # 返回文件数据的数目\n        return self.len\n```\n\n\n\n### 5.3 训练模型\n\n为了更高效的分析训练结果，我创建了一个训练和测试的方法，然后将上面提到的四种模型作为参数进行训练。\n\n（1）训练方法\n\n```python\ndef train(net):\n  current_loss_his = []\n  current_Acc_his = []\n\n  best_net_wts = copy.deepcopy(net.state_dict())\n  best_acc = 0.0\n\n  criterion = nn.CrossEntropyLoss()\n  optimizer = optim.Adam(net.parameters(), lr=0.001)\n\n  # 开始训练\n  total_loss = 0\n  for epoch in range(100):\n      net.train()  # 将模型设置为训练模式\n      for i, (inputs, labels) in enumerate(train_loader):\n          inputs = inputs.to(device)\n          labels = labels.to(device)\n          # 优化器梯度归零\n          optimizer.zero_grad()\n          # 正向传播 +　反向传播 + 优化 \n          outputs = net(inputs)\n          loss = criterion(outputs, labels)\n          loss.backward()\n          optimizer.step()\n          total_loss += loss.item()\n\n      net.eval()   # 将模型设置为验证模式\n      current_acc = test_acc(net)\n      current_Acc_his.append(current_acc)\n\n      if current_acc > best_acc:\n        best_acc = current_acc\n        best_net_wts = copy.deepcopy(net.state_dict())\n\n      print('[Epoch: %d]   [loss avg: %.4f]   [current loss: %.4f]  [current acc: %.4f]' %(epoch + 1, total_loss/(epoch+1), loss.item(), current_acc))\n      current_loss_his.append(loss.item())\n\n  print('Finished Training')\n  print(\"Best Acc:%.4f\" %(best_acc))\n\n  # load best model weights\n  net.load_state_dict(best_net_wts)\n\n  return net,current_loss_his,current_Acc_his\n\n```\n\n（2）测试方法\n\n```python\ndef test_acc(net):\n  count = 0\n  # 模型测试\n  for inputs, _ in test_loader:\n      inputs = inputs.to(device)\n      outputs = net(inputs)\n      outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n      if count == 0:\n          y_pred_test =  outputs\n          count = 1\n      else:\n          y_pred_test = np.concatenate( (y_pred_test, outputs) )\n\n  # 生成分类报告\n  classification = classification_report(ytest, y_pred_test, digits=4)\n  index_acc = classification.find('weighted avg')\n  accuracy = classification[index_acc+17:index_acc+23]\n  return float(accuracy)\n```\n\n### 5.4 可视化结果\n\nHybridSN、HybridSN_BN、HybridSN_Attention、HybridSN_BN_Attention的训练结果如下：\n\n（1）四种模型的Loss下降曲线\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210104232504.png)\n\n（2）四种模型的Accuracy变化曲线\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210104232720.png)\n\n（3）四种模型最佳Precision、Recall、F1-Score\n\n|         模型          | Accuracy | Recall | F1-Score |\n| :-------------------: | :------: | :----: | :------: |\n|       HybridSN        |  0.9790  | 0.9788 |  0.9786  |\n|      HybridSN_BN      |  0.9897  | 0.9888 |  0.9888  |\n|  HybridSN_Attention   |  0.9807  | 0.9806 |  0.9805  |\n| HybridSN_BN_Attention |  0.9885  | 0.9884 |  0.9884  |\n\n\n\n### 5.5 分析结论\n\n从收敛速度上看，HybridSN_BN和HybridSN_BN_Attention远快于另外两种模型，说明加上Batch Normalization之后，模型的收敛速度大大提升，而HybridSN_BN和HybridSN_BN_Attention的收敛速度几乎一致，说明Attention并没有提升收敛速度的作用。四种模型大约在25个epoch以后就不怎么收敛了，说明HybridSN本身的收敛速度还是很快的，只不过加了BN以后这种效果更明显了。\n\n从准确度上看，HybridSN_BN、HybridSN_Attention、HybridSN_BN_Attention均高于HybridSN，说明Attention和Batch Normalization都有提升模型分类准确度的作用，相比HybridSN_Attention而言，HybridSN_BN提升的效果更明显，这就说明了BN不仅提升收敛速度方面效果显著，在提升准确度方面也是很不错的。但是，HybridSN_BN_Attention相比HybridSN_BN，无论是Precision、Recall还是F1-Score，都下降了，说明了BN和Attention这两种机制并不搭配，在下面的思考中，我也给出了为什么会出现这种情况个人理解。\n\n可以看到，HybridSN模型的收敛速度很快，能在25个epoch内实现，准确度也很高，四种模型的准确度都达到了97%以上，说明采用三维和二维卷积的混合网络，是非常有助于解决高光谱图像分类问题的。\n\n## 六、思考\n\n（1）二维卷积和三维卷积的区别\n\n二维卷积是最常见、用途最广泛的卷积，主要用于提取空间特征，对于一张图片来说，可以根据设定的卷积核的不同，提取不同的特征，如数字图像处理中模糊、锐化、去噪操作都是用特定卷积核实现的。在CNN中，二维卷积的卷积核权重是可学习的，再加上激活函数的非线性变换，几乎可以拟合出任何想要的模型。三维卷积的卷积核可以看作一个数据立方体，因此三维卷积处理的对象是一个立方体图像（或其它相似的数据），三维卷积的思想与二维卷积相同，只不过多了一个维度，所以三维卷积不仅提取处理空间特征，也可以提取除了空间特征以外的另一维度的特征。由于高光谱图像不仅有空间信息，还有不同波长的光谱信息，因此使用三维卷积来提取高光谱图像的特征就再好不过了，但是三维卷积有一个致命的缺点，就是计算复杂度太高，参数量也比二维卷积多出了一维维度的倍数，所以采用三维和二维卷积的混合网络即降低了模型的复杂性，也提升了模型的性能。\n\n\n\n（2）为什么Batch Normalization能够加快收敛速度\n\nBatch Normalization，顾名思义，以进行学习时的batch为单位，按batch进行规范化。具体而言，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布，用数学式表示的话，如下所示：\n\n![img](https://img-blog.csdnimg.cn/2020072322071327.png)\n\n*m代表batch的大小，$μ_B$为批处理数据的均值，$σ^2_B$为批处理数据的方差。*\n\nBN层可以让激活函数(非线性变化函数)的输入数据落入比较敏感的区域，缓解了梯度消失问题，加速了网络收敛速度。同时，BN层将每一个batch的均值与方差引入到网络中，由于每个batch的这两个值都不相同，可看做为训练过程增加了随机噪音，可以起到一定的正则效果，防止过拟合。\n\n\n\n（3）为什么多测试几次网络会发现每次分类的结果都不一样？\n\n我认为导致网络每次测试的结果都不一样原因是：没有使用net.eval()将模型切换至测试模式。我们知道，dropout的本质通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。比如，以概率 p=0.6 随机将神经元置0，就相当于在10个神经元选4个神经元输出(4个神经元在工作，另外6神经元置0)，这时我们就相当于训练了 $C_{10}^4$ 个模型，只是每个模型的参数量更少了。如果测试时仍处理训练模式，那么也会随机将神经元置为0，这就带来了不确定的结果，而在测试模式下，**dropout层会让所有的激活单元都通过**，最后的输出再乘以 (1-p) 作为模型的测试结果，这样得到的数据就是确定的了。同样的，是否处于测试模式也会影响BN层的工作机制，从而影响测试的结果。因此，合理的运用model.train()和model.eval()对测试的结果是至关重要的。\n\n（4）如果想要进一步提升高光谱图像的分类性能，可以如何使用注意力机制？\n\n在上述实验中，给HybridSN加上注意力机制后模型的性能有了提升，我也尝试了把注意力机制加在HybridSN模型不同位置上作比较（结果没有在上面呈现），发现把Attention加在第三个三维卷积后，二维卷积之前比加在二维卷积后效果更好。我认为这是因为高光谱图像经过二维卷积之后会损失一部分光谱信息，如果将注意力机制加在二维卷积之后，那么Attention抽取关键信息的效果就不明显了（会忽略少部分光谱信息的关键区域），所以应该把Attention加在第三个三维卷积后，这样会保留更多的光谱信息，从而进一步提升高光谱图像的分类性能。从上面的结果中也看到了，加上Attention之后，模型的Precision、Recall、F1-Score都提升了。\n\n（5）为什么HybridSN_BN_Attention的性能不如HybridSN_BN？\n\n我刚开始也很疑惑，为什么两个被公认性能优越的模块加在一起后性能反而下降了呢？我想了很久，得出了自己的一点理解：因为BN是固定每一次输入Batch的分布，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布，而Attention是为了获取更加显著的区域，经过Attention后显著的区域会变得更为突出，这在一定程度上打乱了原来输入的分布，从这方面来讲，这两个模块似乎是一种相互互斥的存在，效果自然就下降了。这相当于一个精通python的人认为python是世界上最好的语言，而另一个精通java的人认为java是最好世界上的语言，当一个人即会java也会python时，它可能任何一个都不精通。当然，得到这样的结果也可能是如下原因造成：\n\n- 由于Indian Pines数据集自身特点，模型对其分类性能的评估并没有广义性，可能在另外一个数据集上BN+Attenion的性能又比只有BN的模型好了\n- 模型本身的结果可能就存在问题，如果调整一下各个模块的顺序，改变各个模块内卷积核的参数，结果会不会更好呢？\n\n由于时间原因及个人知识水平的限制，并没有做更多的探索，还望见谅。\n\n\n\n## 七、结语\n\nHybridSN是一种用于HSI分类的混合网络模型，以三维和二维卷积结合的方式既提升了模型的性能，也降低了复杂度。另外，分别加上BN、Attention之后，模型的性能都有了提升，BN提升更加明显，但是把BN和Attention一起加在HybridSN中，会发现HybridSN_BN_Attention的性能虽然比原始的HybridSN有所提升，但不如HybridSN_BN，我认为这是因为两个模块是一种互斥的存在，从而导致HybridSN_BN的性能降低，上面也给出了自己的解释。这次实验让我学到了很多，不仅熟练掌握了HSI分类，也对BN和Attention的印象更加深刻，更明白了不是所有优秀的模块组合起来就能取得好的结果，只有不断的分析、实践、思考，才能更好的理解其本质。\n\n\n\n最后，附上本文在colab的实现过程：[https://colab.research.google.com/drive/12QbZqcxfplFEm7yp3X9wgi7jYXmqRjz9?usp=sharing](https://colab.research.google.com/drive/12QbZqcxfplFEm7yp3X9wgi7jYXmqRjz9?usp=sharing)\n\n\n\n【参考文档】\n\n[HybridSN: Exploring 3-D–2-D CNN Feature Hierarchy for Hyperspectral Image Classification](https://ieeexplore.ieee.org/document/8736016)\n\n[CBAM: Convolutional Block Attention Module](https://github.com/luuuyi/CBAM.PyTorch)\n\n[高光谱数据集](https://blog.csdn.net/qq_38290648/article/details/80596543)\n\n[高光谱图像](https://www.sohu.com/a/308158769_394987)\n\n\n\n","tags":["HybridSN"],"categories":["神经网络"]},{"title":"SalBiNet360-Saliency Prediction on 360° Images with Local-Global Bifurcated Deep Network","url":"/2020/12/23/111602/","content":"\n> 这是我第一篇精读的IEEE2020论文，**我认为这一篇论文非常有意义，因为它是目前为止最先进显著性预测模型**，在理解论文过程中，我查阅了许多相关资料，其中一些放在文章最后的补充概念部分，这对理解SalBiNet360的原理非常重要。另外，如果翻译过程中有什么不妥之处，欢迎在评论区留言指正。\n\n英文名称：SalBiNet360: Saliency Prediction on 360° Images with Local-Global Bifurcated Deep Network\n\n中文名称：基于局部-全局二叉式深度网络的360°全景图显著性预测\n\n论文会议：2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)\n\n论文地址：[https://ieeexplore.ieee.org/abstract/document/9089519](https://ieeexplore.ieee.org/abstract/document/9089519)\n\n<!-- more -->\n\n# 摘要\n\n随着虚拟现实应用的发展，在360°全景图上预测人类的**视觉注意**对理解用户的行为至关重要。基于360°全景图全局和局部视觉显著性的特点，论文提出了一个用于360°全景图显著性检测的框架(**SalBiNet360**)。在全局深度子网络中，利用多个多尺度的上下文模块和一个多级解码器来整合网络中层和深层的特征。在局部深层子网络中，只利用一个多尺度上下文模块和一个单级解码器来减少局部显著性特征的冗余。最后利用线性组合方法，结合全局和局部显著图的特点，生成最终的融合显著图。在两个公开数据集上进行的定性和定量实验表明，**SalBiNet360的性能优于当前最先进的方法，特别是能够更好地预测低显著性区域和图像左右边界附近的区域**。\n\n关键词： 360°度全景图，SalBiNet360，虚拟现实 (VR)\n\n> 关于视觉注意及视觉显著性检测见本文最后的补充概念\n\n# 1 引言\n\n360°全景图在VR技术中扮演着至关重要的角色。与2D图像不同，360°全景图允许观察者从各个方向观察场景，因此其特性吸引了研究者更多的兴趣。此外，预测观察者在360°图像上的视觉注意力，可以帮助研究人员了解观察者佩戴VR设备时的行为，它还可以应用在计算机视觉的各个方面，如图像压缩和图像裁剪。\n\n**显著性模型是在人类没有明确意图的情况下，观察图像所注意的物体或区域**。随着神经网络的兴起和发展，对传统二维图像上的显著性预测模型进行了深入的研究。许多优秀的模型已经出现，并建立了大量的数据集，如MIT Saliency Benchmark（一个显著性预测框架）。然而，到目前为止，关于360°全景图的显著性预测的研究还比较少，尤其是以神经网络为框架的研究。一般情况下，我们通常会通过**ERP投影**的方式将360°全景图投射到一个平面上，但这种投影方式会使360°全景图对应球面的两个极点被转换为上界和下界，从而导致图像过度变形。\n\n> 关于ERP投影见本文最后的补充概念\n\n如果将传统二维图像上的显著性预测模型直接应用于ERP图像上，效果将不理想。图1(c)显示了在传统二维图像上，由显著性模型预测的全局ERP显著图。与ground truth（图1（b））相比，全局显著图大致预测了显著性区域（赤道区域）。但是对于其它低显著性的区域（观察者不太关注的区域），传统模型无法预测它们。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223124655.gif)\n\n*图1：(a)ERP图像，(b)ground truth显著图，(c)(d)分别是全局和局部的显著图，(e)-(j)通过立方体投影的直线显著图。*\n\n为了解决这个问题，利用**立方体投影**将360°全景图投射到立方体的六个面，每个面的视场角（FOV）为90°，显示六个方向的景物，而且比ERP图像失真小，这些立方体图被命名为rectilinear  images，我将其译为**直线图像**。图1(e)-(j)说明了这六张直线显著图，而图1(d)则说明了由这些直线显著图重新投影出来的局部显著图。与全局显著图相比，局部显著图可以精细地预测显著性区域，即使是一些显著性较低的区域也可以预测。（这里全局和局部特征我理解为：如果从整张ERP图片提取特征就是全局特征，如果从立方体投影的六个面分别提取特征，再重新投影到一张图上，那么得到的就是局部特征）\n\n> 关于立方体投影及视场角（FOV）见本文最后的补充概念\n\n然而，在预测缺乏全局性的直线显著图时，模型增强了低显著性区域，**甚至一些不被人类关注的区域也被错误地预测出来**。在图1(i)和图1(j)中，除了云层和坑洞，几乎没有什么物体，它们显然是突出的。所以在局部显著性图（图 1(d)）中，这两类物体很容易被预测出来。但是预测整幅图像时，在其它物体的存在下，云层和坑洞就显得不那么突出了。因此，模型对云层和坑洞的敏感度较低，所以，在缺乏全局性的直线图像中，模型对低显著性的区域是很敏感的。\n\n为了解决上述问题，作者提出了一种基于360全景图的全局和局部显著性预测的新框架（SalBiNet360）。这是一个包含全局和局部显著性预测子网络的（二叉式）深度网络框架。在全局深度子网络中，我们利用多个多尺度的上下文模块从网络的中层和深层提取上下文特征，然后，这些特征由一个多级解码器整合，预测全局显著性图。对于局部深层子网络，为了降低网络对一些低显著性区域的敏感性，只利用一个多尺度上下文模块，然后利用单级解码器来还原图像的分辨率，生成直线显著图。最后，将多张直线显著图重新投影成局部显著图（如图1（d）），利用线性组合策略将全局显著图与局部显著图融合。综上所述，我们做出了以下贡献：\n\n- 本文提出了一种全新的二叉式深度网络，该网络有两个子网络，分别预测360°全景图的全局和局部显著性特征，通过线性组合将全局显著图和局部显著图融合生成最终的显著图。\n- 提出了一种新的多尺度上下文模块，以提取360°全景图多尺度的上下文特征。\n- 采用全局子网络中的多级解码器，以减小不同层网络的判别特征之间的差距。\n- 在两个公开可用的数据集上进行的广泛实验说明，所提出的SalBiNet360优于经过测试的最先进的方法。\n\n\n\n# 2 相关工作\n\n到目前为止，大多数360°全景图的显著性预测模型都是在传统二维图像模型基础上改进的（所谓的传统二维图像模型是指用二维图形训练和测试的模型），主要有三个研究方向，分别是赤道偏移、凝视位置的数据映射和投影变换。\n\n赤道偏移。当观察者观看VR全景图像时，他们往往会更多地关注赤道附近的内容，这被命名为赤道偏移。Battisti等人利用这一特性，将360°全景图划分为多个区域，每个区域赋予它们不同的显著性权重，区域越靠近图像的赤道，权重越大。Ding等人从图像层面细化到像素层面，利用高斯分布模型来模拟赤道偏差，从而提高了精度。但他们都只是基于360°全景图的全局属性设计模型，并没有考虑到局部的显著性。\n\n凝视位置的数据映射。一些显著性预测方法，是基于凝视位置的数据映射，就是利用观察者的凝视位置数据来建立显著图。这类方法将观察者有限的注视位置扩展为一般的显著图。Upenik等人利用头部运动轨迹来预测显著性，根据头部角度速度来确定观察者观察图像某些区域时所花费的时间。Abreu等人根据观察者的固定时间来区分观察者的视觉注意（即根据观察者看某一部分区域的大概时间来区分显著性区域）。\n\n对于投影变换，最常用的方法是ERP投影。为了解决ERP图像的边界失真问题，Lebreton等人将几个不同经度边界的ERP投影的显著图均匀地整合在一起，从而得到最终的显著图。但是，他们没有解决图像上下边界附近发生严重失真的问题。Startsev等人将360°全景图投射到ERP图像和反ERP图像上，然后对这两幅图像进行预测，并将其融合，解决了等ERP图像左右边界不连续的问题。此外，他们还单独预测了图像上下边界附近的显著性。但是他们直接使用了预训练的显著性模型，而不是通过360°全景图来训练它们，所以这些模型无法提取360°全景图的具体特征。而球面投影则是将360°全景图投射到球面上，使图像具有连续性，图像上的景物不易失真，可以提高模型的精度。但是，应用这些模型预测立体空间中的图像是复杂的。Bogdanova等人提出了一个基于球面投影的显著模型，构建球形金字塔模块，从而输出球面显著图。\n\n除以上几种投影变换方法外，还有立方体、八面体、正金字塔投影等。立方体投影是比较常用的，Maugey等人提出了双立方体投影法，以解决立方体投影中的边界畸变和不连续问题。该方法将360°全景图投影到两个水平和垂直相差45°的立方体上，然后通过旋转将两个显著图融合。虽然它可以精细地预测局部的显著图，但由于直线图像缺乏全局性，导致一些低显著性的区域被错误地预测。Chao等人提出的SalGAN360在上述方法基础上扩展到多立方投影，也分别预测了全局和局部的显著图。但是，它始终利用SalGAN预测全局和局部的显著性特征，而没有考虑到360°全景图的全局和局部属性，以提高网络。\n\n本文根据360°全景图的全局和局部属性，将提出的网络分为两个不同的深度子网络，分别预测全局和局部的显著性特征，通过融合得到最终的显著图。\n\n\n\n# 3 方法\n\n在本节中，构建了包含全局和局部深度子网络的二叉式网络（SalBiNet360）。在全局深度子网络中，生成了一个全局的显著图，可以检测所有方向的视觉注意力。在局部深度子网络中，生成了一个以精细方式检测视觉注意力的局部显著图。最后，通过与全局和局部显著图的线性组合，生成融合的显著图。\n\n## 3.1 SalBiNet360机制\n\n### 3.1.1 Framework\n\n所提出的模型框架如图2所示，它利用ResNet50作为骨架。ResNet50有四个卷积块（输入层还有一个卷积）。对于一个输入图像，在第四个卷积块中，分辨率降低了32倍。这里从网络中第二个卷积块的最后一层开始，我们将网络分成两个深度子网络，其中一个子网络预测全局的显著性，而另一个子网络预测局部的显著性。这两个子网络都有ResNet50最后两个卷积块。图2蓝色的方块就是ResNet50的convBlock块，一个convBlock包含多个卷积操作，若想对ResNet50有更深的理解，[ResNet残差网络及变体详解](https://blog.csdn.net/qq_37555071/article/details/108258862?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522160870193316780308367044%252522%25252C%252522scm%252522%25253A%25252220140713.130102334.pc%25255Fblog.%252522%25257D&request_id=160870193316780308367044&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v1~rank_blog_v1-1-108258862.pc_v1_rank_blog_v1&utm_term=resnet)，相信会让你有所收获。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223134214.gif)\n\n\n\n*图2：SalBiNet360的框架。该网络分为全局和局部深度子网络，分别用来预测360°图像的全局和局部显著性特征。*\n\n在全局显著性预测中，将整个ERP图像纳入全局子网络，输出全局显著性图。一般来说，神经网络的第一个卷积块会提取一些低级特征，如形状、纹理、轮廓等，但这些特征对显著性预测的贡献较小。此外，不同尺度的卷积层可以保留精细的上下文信息，尤其是中间*的特征图的尺度相对较大，这使得更多的全局信息得以保留。因此，将最后三个卷积块的特征被提取出来，分别输入到三个相同的多尺度上下文模块(MCM)中，然后生成三个不同层次的特征图。之后，采用多级解码器对三个特征图进行整合，生成全局显著图。\n\n在局部显著性预测中，首先通过立方体投影将360°全景图投射到立方体的六个面上，每个面的FOV为90°，可以得到多个直线图像。然后，把这些直线图像输入到局部子网络，局部子网络输出相应的直线显著图。最后，ResNet50的后三个卷积块的特征也在局部子网络中被提取出来。如果在局部子网络中加入多个多尺度的上下文模块（就像全局子网络的架构一样），会增加局部显著图的冗余度。在图3中，沟渠和灯具是ground truths中显著性较低的对象，与只有一个多尺度上下文模块的局部子网络相比，有多个多尺度上下文模块的局部子网络会错误地将这些显著性较低的对象预测为高显著性的对象。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223135821.gif)\n\n*图3：具有一个和多个多尺度上下文模块的地面真相和局部显著图的视觉比较。(\"MCM\"表示多尺度上下文模块)*\n\n因此，与全局子网络中的操作不同，局部子网络的最后三个卷积块中的特征图是连在一起的，只是输入到一个多尺度的上下文模块中。之后，采用卷积-上采样操作的单级解码器，将多尺度上下文模块输出的结果恢复到与输入的分辨率一样的大小。然后，利用重投影法将这些直线显著图重新投影成多个ERP显著图，通过使用平均法将所有的ERP显著图重叠，得到局部显著图。\n\n最后，将全局和局部的显著图进行线性组合融合，得到融合的显著图。多尺度上下文模块和多级解码器的具体内容将在3.2节和3.3节中介绍。\n\n\n\n### 3.1.2 预训练\n\n在没有360°全景图的大型数据集的情况下，SalBiNet360必须由SALICON进行预训练(SALICON是一个显著性检测数据集)。该数据集包含20 000张二维图像以及相应的显著图，其中10,000张用于训练，5,000张用于验证，5 000张用于测试。将二维图像放入模型中，相应地同时生成两张预测的二维显著图，一个是来自全局子网络，另一个是来自局部子网络。然后利用 ground truth的显著图，根据**二元交叉熵损**失来监督这两个子网络的训练。\n\n### 3.1.3 微调\n\n在使用SALICON预训练模型后，将全局和局部深度子网络进行联合微调。做法是将训练的直线图像分别放入模型中，相应地同时生成两个预测的直线显著图，一个是来自全局子网络，另一个是来自局部子网络。在360°全景图显著性预测模型中，经常使用皮尔逊相关系数（Pearson’s Correlation Coefficient，简称CC） 和归一化扫描路径显著性（Normalized Scanpath Saliency，简称NSS）这两个指标来衡量模型的有效性。因此，在二元交叉熵中加入这两个指标来优化模型训练的过程。对于全局子网络，损失可以定义如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223141413.png)\n\n其中，ܵ$S_g$、$S_{sal}$、$S_{fix}$ 分别是全局子网络预测的直线显著图、ground truth的直线显著图和二元直线定点图，$\\mu_{BCE}$和$\\sigma_{BCE}$表示SalBiNet360对预测的二维图像计算出的$L_{BCE}$的平均值和标准差，$L_{BCE}$表示二元交叉熵损失，$L_{normal}$是CC和NSS的归一化函数。\n\n$L_{BCE}$定义如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223142528.png)\n\n其中*N*为像素数，*j*为像素坐标。\n\n$L_{normal}$定义如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223142732.png)\n\n其中$L^{CC}_{metric}$是CC的评价指标，$L^{NSS}_{metric}$是NSS的评价指标。$\\mu_{CC}$、$\\sigma_{CC}$、$\\mu_{NSS}$、$\\sigma_{NSS}$表示SalBiNet360对预测的二维图像计算的这两个指标（即CC和NSS）的平均值和标准差。\n\n对于局部子网络来说，$lossL_L$与全局子网络中的损失相同，只是用局部子网络预测的直线显著图$S_l$代替了$S_g$。因此，SalBiNet360的总损失定义为：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223143234.png)\n\n在微调步骤中，我们首先固定Resnet50网络的权重。然后，我们对SalBiNet360中其他部分的权重进行微调。最后，随机初始化两个解码器的最后两个卷积层的权重（我认为这两个卷积层的权重占比很小，虽然进行随机初始化了，也影响不了整个模型的性能，也提高了模型的鲁棒性）。这样在微调的过程中就可以充分提取360°全景图的特征。\n\n\n\n\n\n\n\n### 3.1.4 测试\n\n在测试步骤中，预测局部显著性时，首先将直线图像放入模型中，在局部子网络中生成直线显著图（忽略全局子网络中的输出）。然后，将直线显著图重新投影为ERP格式，得到局部显著图图。\n\n在预测全局显著性特征时，将ERP图像放入模型中，模型直接在全局子网络中生成全局显著图（忽略局部子网络中的输出）。虽然网络是由失真较小的直线图像进行微调，但在预测全局显著性时，由于赤道附近的区域失真较小，所以模型在等ERP图像的赤道附近区域可以很好地预测。另外，**利用Fused Saliency Method（FSM，融合显著性方法）作为后处理方法，解决了360°全景图左右边界不连续的问题，在这个方法中四个全局显著图分别赋予0.25的权重**。\n\n最后，将全局和局部显著图进行线性组合融合，得到融合后的显著图。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223143752.png)\n\n其中，$\\alpha$和$\\beta$分别是全局和局部显著图的权重，且$\\alpha$+$\\beta$=1\n\n> 我第一次看到FSM方法，非常疑惑，作者文中并没有过多的解释，我查阅了其引用的论文，才理解了具体用到的方法，见本文最后的补充概念。\n\n\n\n## 3.2 多尺度上下文模块\n\n当360°全景图通过ERP投影到一个平面上时，图像上不同尺度的上下文信息是不同的。如图 4 所示，**360°全景图比传统的二维图像尺寸大得多，可以捕捉到更多的场景信息**，在此基础上，设计了多尺度的上下文模块，有效地提取不同尺度的上下文特征。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223145815.gif)\n\n*图4：多尺度上下文模块的架构。*\n\n如图4所示，所提出的多尺度上下文模块共包含四个分支{$b_n$，n=1,2,3,4}。每个分支都有不同大小的卷积核，可以在不同尺度的360°全景图上进一步提取更多的上下文特征。最初，在每个分支中利用1X1卷积层来减少特征图的通道数。对于݊n>1的分支，我们分别增加一个卷积层，其内核大小为（2n-1)x(2n-1)，然后，在四个分支中分别增加一个3x3卷积层，再将每个分支的特征图Concatenate，再做一次3x3卷积操作。最后，将提取的判别特征与原始特征通过shortcut与Relu激活函数结合，从而得到不同尺度的判别特征。\n\n\n\n## 3.3 多层解码器\n\n在全局子网络中，利用多尺度的上下文模块，得到三个判别性特征图{$f_i$，i=2,3,4}。**为了缩小这些特征之间的差距，采用多级解码器**。如图5所示，在这个解码器中有四层（c=1,2,3,4)，$f_i^c$示 c 层中的第 i 个特征图，$f_i$在第二层中更新了以下内容公式：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223154812.png)\n\n其中Up是对特征图进行因子为$2^{k-2}$的上采样操作，Conv是3X3卷积层。 圈表示元素化的乘积（点积）。\n\n> 我认为这里每一层的特征图从2开始是应该方便些公式，如上面的$2^{k-2}$，k从2开始，第一幅特征图上采样因子就是2\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223150711.gif)\n\n*图5：多级解码器的架构。图中顶部的\"c\"表示第c层。*\n\n对于$f_4^2$，作者设置$f_4^2$ = $f_4^1$，对于$f_2^3$，作者设置$f_2^3$=$f_2^2$（即这些位置的特征图采用一样的上采样和卷积操作）。至于$f_3^3$ 和 $f_2^4$，作者采用upsampling-convolution-concatenation策略来整合多个特征，公式如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223151427.png)\n\n其中，Up为上采样因子为2的操作，Conv是3x3卷积层，Concat是两个特征图的通道连接。最终，通过对 $f_2^4$使用upsampling-convolution策略，解码器生成与输入的ERP图像大小相同的全局显著图。\n\n至于单层解码器，主要由5个3×3卷积层和1个填充层以及5个系数为2的双线性上采样层组成（就是图2的那五个橙色方块）。\n\n\n\n# 4 实验\n\n## 4.1 实验配置\n\n### 4.1.1评估数据集\n\nSalBiNet360是在两个公开的数据集上进行评估的：Salient360! 和Grand Challenge\n\nSalient360! 是国际会议ICME2017的360°全景图显著性预测挑战赛数据集，由65张360°全景图组成，观察者的头部和眼部运动被记录下来，其中40张图像用于训练，25张用于测试。\n\nVR Saliency是虚拟现实全景图显著性预测数据集，由22张图像组成，观察者的头部动作被记录下来。\n\n两个数据集中的图像都由室内和室外场景内容组成。\n\n### 4.1.2 评价指标\n\nSalBiNet360的性能是通过四个常用的指标来评估的：KL、CC、NSS、AUC。KL和CC评价的是两个显著图之间的**显著性密度分布**（主要是面向局部信息的），NSS和AUC评价的是显著图和二元定点图之间的眼动位置分布（主要是面向全局信息的）。\n\n> 关于KL、CC、NSS、AUC四个指标，作者并没有详细介绍，我查阅了许多资料，大致知道其具体的衡量标准，见见本文最后的补充概念部分。\n\nSalient360! 数据集的性能是在KL、CC、NSS和AUC上评估的，而对于VR Saliency数据集，只采用CC来评估性能，如下所述。在下面的小节中，每个实验的最佳结果以红色粗体显示。\n\n\n\n### 4.1.3 实施细节\n\n**网络结构** 。在全局子网络中，对于多尺度上下文模块，每个分支的特征图通道减少到32个。在局部子网中，经过多尺度上下文模块的处理，特征图的通道减少到256个。\n\n**微调与测试**。Salient360!数据集中40幅训练图像分为30幅用于微调，10幅用于验证。在立方体投影中，立方体在水平和垂直方向上每45°旋转一次，可以得到2×2=4种旋转方式。因此，每幅360°全景图有4×6=24幅直线图像，完全可以生成30×24=720幅直线图像用于微调，10×24=240幅用于验证。在测试SalBiNet360模型时，为了使重新投影后的局部显著图更加平滑，每隔10度旋转一个立方体，因此会产生9×9×6×25=12150张整线图像（每次转10度，即水平方向9次，垂直方向9次，一次360°全景图有6张直线图像，Salient360!共25张360全景测试图）。当预测全局显著性时，预测25个ERP图像得到全局显著图。当预测局部显著性时，预测12150张直线图像，并重新投影为25张局部显著性图。最后，通过融合这些全局和局部的显著图，可以得到25个融合的显著图。\n\n对于VR Saliency，根据其实验设置，用SALICON数据集预训练模型后直接测试22张360°全景图。同样，立方体也是每10°旋转一次，就会产生9x9x6x22=10692个直线图像。预测全局和局部的显著图和Salient360!中的一样。\n\n在预训练和微调步骤中，所提出的模型均采用Adam优化器进行训练，初始学习率为$10^{-4}$，batch size为10。此外，当损失下降到一定程度时，学习率会下降10%。SalBiNet360基于PyTorch框架，部署在GeForce GTX1080Ti GPU上。源代码可在https://github.com/githubcbob/SalBiNet360（访问404，可能是作者删了，哈哈）\n\n\n\n## 4.2 SalBiNet360的分析\n\n### 4.2.1 融合策略中权重的选择\n\n在融合全局与局部显著图时，我们做了多组全局和局部权重选择的实验。表1说明了在测试步骤中，两个数据集上不同权重的融合结果。可以看出，随着$\\alpha$的增加( $\\beta$减少)，SalBiNet360的指标得分在两个数据集上都出现了先增加后减少的现象。当全局权重 $\\alpha$设置为0.55时Salient360!表现最佳，$\\alpha$在上设置为0.50时VR Saliency表现最佳。因此，在下面的实验中，我们在Salient360!上的融合策略中设置 $\\alpha$=0.55($\\beta$=0.45)，在VR Saliency上设置 $\\alpha$=0.50($\\beta$=0.50)。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223154513.png)\n\n*表1：两个数据集上不同权重选择的比较。*\n\n\n\n### 4.2.2 多尺度上下文模块的有效性\n\n在下面的实验中，**将SalBiNet360的两个子网络中的多尺度上下文模块全部去掉**，以测试多尺度上下文模块对显著性预测的影响。表2显示了SalBiNet360在有上下文模块和无上下文模块的两个数据集上测试的量化结果。显然，带有多尺度上下文模块的SalBiNet360的性能优于不带有多尺度上下文模块的SalBiNet360，这说明带有多尺度上下文模块的SalBiNet360能够有效地预测360°图像上的显著性。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223154733.png)\n\n*表2：SalBiNet360在两个数据集上有无上下文多尺度模块（\"MCM\"表示多尺度上下文模块）的比较。*\n\n### 4.2.3 全局、局部和融合显著性预测的表现\n\n为了研究全局、局部和融合的显著性预测对SalBiNet360整体性能的影响，我们分别在两个数据集上做了两个子网络和融合策略的显著性预测实验。从表3中可以看出，全局显著性在NSS和AUC上的表现都优于局部显著性，而局部显著性在KL和CC上的表现都优于全局显著性。这说明全局显著图对图像凝视点（gaze points）的识别能力优于局部显著图，而局部显著图在减少显著性信息的丢失上效果更好。此外，融合后的显著图在NSS和AUC上都优于全局图和局部图，而在其他两个指标上略逊于局部图。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223155102.png)\n\n*表3：全局、局部和融合的显著性在两个数据集上的比较。*\n\n图6和图7通过箱形图说明了全局、局部和融合显著性在两个数据集上的得分分布。很明显，在Salient360!数据集上，融合显著性在KL和CC上的得分分布与局部显著性相似，NSS和AUC的得分分布与全局显著性相似。但在VR Saliency上，融合显著性的得分分布普遍大于全局和局部显著性的得分分布。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223155431.gif)\n\n*图6：全局、局部和融合显著性在Salient 360上的得分分布对比*\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223155530.gif)\n\n*图7：全局、局部和融合显著性在VR Saliency上的得分分布对比*\n\n图8和图9显示了全局、局部和融合的显著图在两个数据集上的一些可视化结果。可以观察到全局子网络预测的显著性区域大致集中在图像的赤道部分，局部显著性预测可以精细地预测一些显著对象，如阶梯。但是，由于缺乏全局性，某些物体被错误地预测为高显著性（如坐具和树木）。结合全局和局部显著图的特性，融合后的显著图可以恰到好处地预测这些显著性区域，从而提高SalBiNet360的预测质量。在VR Saliency上，融合的显著图的效果稍差（因为模型没有进行微调就直接测试，主要是由于其数据集很少，只有22张图片），但融合的显著图也可以削弱一些物体的显著性。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223155754.gif)\n\n*图8：Salient 360上的全局、局部、融合的显著图和ground truths 的视觉对比*\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223155853.gif)\n\n*图9：VR Saliency上的全局、局部、融合的显著图和ground truths 的视觉对比*\n\n\n\n## 4.3 与最新技术的比较\n\n表4显示了SalBiNet360与现有的7种经过测试的最先进的显著性模型在Salient360!上的量化结果，包括GBVS360、SalNet360、Maugey、Startsev、SJTU、CDSR和SalGAN360。很明显，SalBiNet360在大多数情况下优于所有其他方法。只有SalGAN360获得了与SalBiNet360相同的AUC分数。值得注意的是，与CC、NSS和AUC相比，SalBiNet360在KL方面有明显的提高，这说明SalBiNet360在减少**显著性密度分布**的信息损失方面效果更好。表5是VR Saliency中的量化比较。比较的三种模型（EB、SalNet+EB、ML-Net+EB）的性能结果，可以看出，SalBiNet360得到的性能最好。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223160455.png)\n\n*表4：不同方法在Salient360!上的比较。*\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223160542.png)\n\n*表5：不同方法在VR Saliency上的比较。*\n\nSalient360!上一些案例的视觉对比如图10所示。与SalGAN360相比，SalBiNet360不仅可以精细预测图像赤道附近的显著性区域，还可以预测一些SalGAN360忽略的低显著性区域（如树木和地板）。此外，SalBiNet360还可以预测图像左右边界附近的显著性区域，从而减少了显著性密度分布中的信息损失。这与表4中SalBiNet360在KL得分上有所提高的结果是一致的。这一现象可以解释为利用多尺度的上下文模块从多个尺度中提取上下文特征，包括边界的特征。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223160909.gif)\n\n*图10：SalBiNet360和最先进的模型在Salient360!的视觉对比*\n\n\n\n图11是SalBiNet360在VR Saliency上的视觉效果。在不对网络进行微调的情况下，SalBiNet360只能大概的做到预测一些显著性区域。但图像左右边界附近的显著性区域还是可以预测的。 它证明了SalBiNet360能够有效地解决等角ERP图像左右边界的不连续视觉信息问题。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223161046.gif)\n\n*图11：SalBiNet360对VR Saliency的视觉效果*\n\n## 4.4 讨论\n\n在两个360°全景图数据集上的大量实验表明，提出的SalBiNet360实现了具有竞争力的性能。它是一个包含全局和局部显著性预测子网络的二叉式深度网络。该网络仅通过直线图像进行联合微调。如果将这两个子网络作为两个独立的分支分别进行微调，则可能会浪费很多时间。\n\n在缺乏大的360°图像saliency数据集的情况下，必须利用失真较小的直线图像和相应的ground truths来微调模型，这样可以忽略ERP图像上下边界的尺度和严重失真。显然，在测试步骤中，由于等角图像的赤道附近失真较小，全局子网络可以很好地预测赤道附近的显著性（如图8、9中的\"全局显著性图\"）。**但是，两极的预测效果并不好。因此，需要将其与局部显著性相结合，以弥补全局显著性的不足。**\n\n与表4和表5中的其他模型相比，SalBiNet360构建了两个子网络，分别适合预测360°图像的全局和局部显著性。**全局子网络可以大致预测360°全景图的赤道附近和左右边界的显著性区域，而局部子网络可以精细地预测一些显著性高的区域和观察者不太关注的区域，从而弥补了全局显著性的劣势**。SalBiNet360的分叉架构可以应用于其他利用360°全景图的全局和局部显著性的显著性模型。\n\n# 5 总结\n\n本文提出了一种全新的360°全景图的显著性预测框架，命名为SalBiNet360。所提出的框架在ResNet50的第二个卷积块之后分为两个深度子网络，这两个子网络分别预测全局和局部的显著图。在全局深度子网络中，上下文特征从网络的中层和深层中提取，**并由三个多尺度的上下文模块和一个多层次的解码器进行整合**。而在局部深度子网络中，为了减少图像低显著性区域的冗余，**只利用一个多尺度的上下文模块和一个单层解码器来生成局部显著图**。最后，通过线性组合融合全局和局部显著图，生成最终的显著图。在两个可用的数据集上进行了广泛的实验表明，SalBiNet360的性能优于已测试的最先进的方法。\n\n\n\n\n\n# 补充概念\n\n为了更好的理解本文，我查阅了许多资料，补充了文章中的作者所提到一些术语和概念：\n\n（1）**视觉显著性检测**(Visual saliency detection)指通过智能算法模拟人的视觉特点，提取图像中的显著区域***(***即人类感兴趣的区域)。\n\n（2）**视觉注意机制**(Visual Attention Mechanism，VA)，即面对一个场景时，人类自动地对感兴趣区域进行处理而选择性地忽略不感兴趣区域，这些人们感兴趣区域被称之为显著性区域。如下图所示，当看到这幅图像时，图中的四个人最能引起人的注意，而其它区域（如地面）就不会引起人们注意。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223123107.png)\n\n*来源：https://www.cnblogs.com/ariel-dreamland/p/8919541.html*\n\n（3）**ERP**全称equirectangular projection，又可以称为equidistant cylindrical projection，中文译为等距柱状投影。这种投影方式将经线映射为恒定间距的垂直线，将纬线映射为恒定间距的水平线，映射关系相对简单，但既不是等面积的也不是保角的，引入了相当大的失真。下图是一幅用ERP方式投影的360全景图，长宽比为2:1，可以明显看出，两极区域拉伸严重，造成了极大的冗余，增加了编码负担。 \n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223124401.png)\n\n*来源：https://blog.csdn.net/lin453701006/article/details/71173090*\n\n（4）**立方体投影**就是将360°全景图被投影到各面都是正方形的六面体上（面积未保持不变），经线和纬线都是直线，在纬度 +45° 和 -45° 之间，东南西北方向是准确的，但常规方向不准确。在极面上，由中心确定的方向是真实的。在纬度 +45° 到 -45° 之间，比例尺是正确的。如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223125906.png)\n\n*来源：https://desktop.arcgis.com/zh-cn/arcmap/10.7/map/projections/cube.htm*\n\n（5）**视场角（FOV）**。在光学仪器中，以光学仪器的镜头为顶点，以被测目标的物像可通过镜头的最大范围的两条边缘构成的夹角，称为视场角。如下图。 视场角的大小决定了光学仪器的视野范围，视场角越大，视野就越大。通俗地说，目标物体超过这个角就不会被收在镜头里，就只能看到局部物体。 \n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223130328.png)\n\n*来源：http://www.colorspace.com.cn/kb/2013/02/24/fov/*\n\n（6）**Fused Saliency Maps**（FSM，融合显著性方法）。其作者认为（引用论文的作者），在传统情况下，摄影师倾向于将感兴趣的对象定格在赤道近旁，对于边界的区域其实是不敏感的，因此，使用传统图像训练显著性模型很可能不包含这些区域的特征，尤其是基于深度神经网络的模型。为了解决显著性模型的中心先验限制，提出了Fused Saliency Maps（FSM）方法，该方法过程如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223144925.png)\n\n\n\n1. 首先，输入ODI图像（omnidirectional images，全向图像，球面投影产生的图像）。\n2. 将其随机翻译成不同的版本，具体做法是将里面的场景、物体随机移动、随机拼接，这里的T即翻译的版本数，可根据实际情况设置。\n3. 用相应的显著性模型预测这T幅图像的显著性图\n4. 采用线性组合的方式将3产生的T幅显著性图融合，其中$W_1$、$W_2$、$W_3$、$W_4$为这4幅图的权重。\n\nFSM方法可以解决全景图左右边界不连续的问题。\n\n*来源：Look around you: Saliency maps for omnidirectional images in VR applications. Ninth International Conference on Quality of Multimedia Experience (QoMEX). IEEE, 1-6, 2017*\n\n（7）Kullback-Leibler Divergence，简称**KL散度**，通常用来衡量两个分布之间的距离， KL散度越小说明，两个分布之间的距离越小，该模型检测性能越好。\n\n（8）Pearson’s Correlation Coefficient（**CC**）是指皮尔逊相关系数，用来评价预测的眼关注点显著图和ground truth之间的线性相关性，其值越大，相关性越强，算法性能越好，值为正表示正相关，值为负表示负相关。下面是其一个公式（CC有很多变种）：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201223152740.png)\n\n\n\n（9）Normalized Scanpath Saliency （**NSS**）是指标准化扫描路径显着性，用来评价预测的人眼凝视位置与ground truth之间的差异性，NSS越大说明差异越小，模型性能越好。\n\n（10）Area Under Curve（**AUC**），显示了模型预测人眼注视点的能力大小，最理想的预测对应的score是1，AUC越大说明算法检测性能越好。\n\n*参考：https://www.cnblogs.com/hSheng/archive/2012/12/05/2803424.html*\n\n","categories":["论文精读"]},{"title":"破解DeePL翻译ppt后下载下来的加密文件","url":"/2020/12/20/163913/","content":"\n在DeePL中把ppt翻译后再下载下来不能进行编辑，只能以只读方式打开，只有输入密码后才能进行编辑，这里破解密码，步骤如下：\n\n<!-- more -->\n\n\n\n1、首先将ppt的名称加上.zip，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164232.png)\n\n2、解压缩后，打开文件夹，进入ppt文件夹下并打开presentation.xml文件，如下图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164425.png)\n\n3、删除里面``cy=\"10058400\" /></p:presentation>`之间的内容，如下图：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164546.png)\n\n\n\n4、全选下图这些文件压缩为zip文件文件\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164853.png)\n\n\n\n5、然后再把压缩后的zip文件后面`.zip`删除，再次打开ppt，即可进行编辑了\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201220164748.png)","categories":["工具"]},{"title":"使用matplotlib绘制各种图像","url":"/2020/11/25/125802/","content":"\n最近由于论文的需要，要用到matplotlib绘制图像，这里顺便总结一下！\n\n<!-- more -->\n\n\n\n> 我是用cv2读取图片，最近发现了一个奇异的问题，在一个目录下用相对路径读取读片（相对路径没有英文），新建一个notebook文件可以读取，但是把别的地方的notebook文件拷贝过来就不行了，一直返回None，虽然不知道是什么原因，但以后需要注意啊！！！\n\n\n\n\n\n## 1. 基本matplotlib设置\n\n```python\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# 设置中文字体为黑体\nplt.rcParams['font.sans-serif'] = ['SimHei']\n# 用来正常显示负号\nplt.rcParams['axes.unicode_minus'] = False\ndef BGRTORGB(img):\n    return cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\ndef BGRTOGRAY(img):\n    return cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n```\n\n\n\n## 2. 同时显示多个图像\n\n因为matplotlib显示是从小到大排列，比如要显示10张图像，并分2为行显示，显示的子图序号是`251`、`252`、...、`2510`，这是不行的，因为`2510`不被识别，因此采用这种方式显示图片有个数限制。\n\n所以，用虽然用下面的方法可以同时显示多个图像，但是最多不超过10张。\n\n\n\n```python\n# 同时显示多个图像\n\"\"\"\nimgs 源图片列表，类型为list（或numpy数组）\nnames 源图片标题，类型为list（或numpy数组）\nuse_color 是否彩色显示，None为全部灰度显示，如果指定某一张图片彩色显示，也可以设置，例如[False,True,False]，即执行第二种图片彩色显示\ntfigsize 整个画布的尺寸\nrow 显示的行数，行数一定要被图片数整除\n\"\"\"\ndef show_mutli_img(imgs,names,use_color=None,tfigsize=(10,10),row=1):\n    if use_color is None :\n        use_color = [False]*len(imgs)\n    plt.figure(figsize=tfigsize)\n    show_len=len(imgs)\n    gird=int(str(row)+str(show_len//row))*10;\n    r,i,c=0,0,0;\n    for r in range(row):\n        for i in range(show_len//row):       \n            plt.subplot(gird+c+1)\n            if use_color[i]:\n                plt.imshow(imgs[c])\n            else: plt.imshow(imgs[c],cmap='gray')\n            plt.title(names[c])\n            c = c+1\n    plt.show()\n```\n\n\n\n## 3. 同时显示多个图像（无个数限制）\n\n```python\n\"\"\"\nplt.subplot2grid创建子图\nrow,col分别为行、列，类型为int\ndata_img为存储图片的数组列表（或numpy数组），必须是(row,col)大小\ndata_title为存储对应图片的标题列表（或numpy数组），大小同上\n\"\"\"\ndef show_data_img(data_img,data_title,row,col,tfigsize=(12,5)):\n    plt.figure(figsize=tfigsize)\n    plots = []\n    for i in range(row):\n        for j in range(col):\n            ax = plt.subplot2grid((row,col), (i,j))\n            ax.imshow(data_img[i][j],cmap='gray')\n            ax.set_title(data_title[i][j])\n    plt.show()   \n```\n\n\n\n## 4. 多个大小不同的图像\n\n```python\n# 显示不同级别的小波图像\n\"\"\"\n使用plt.subplot2grid来创建多小图\n- grid 表示将整个图像窗口分成几行几列，类型为tuple\n- pos 表示从第几行几列开始作图，类型为list\n- spans 表示这个图像行列跨度，类型为list\n- titles 表示子图的标题，类型为list\n\"\"\"\ndef show_levelImg(show_imgs,grid,pos,spans,titles,tfigsize=(10,10)):\n    plt.figure(figsize=tfigsize)\n    i = 0;\n    for i in range(len(pos)):\n        #行，列\n        trowspan,tcolspan = spans[i]\n        ax1 = plt.subplot2grid(grid, pos[i], rowspan=trowspan, colspan=tcolspan);\n        ax1.imshow(show_imgs[i],cmap='gray');ax1.set_title(titles[i]);ax1.axis('off')\n```\n\n可以运行一下看看效果：\n\n```python\n# len(imgs) =7\ngrid = (4,4)\npos = [(0, 2),(2,0),(2,2),(0,0),(0,1),(1,0),(1,1)]\nspans = [(2,2),(2,2),(2,2),(1,1),(1,1),(1,1),(1,1)]\ntitles = ['person1','person2','person3','person4','person5','ca1','car2']\nshow_levelImg(imgs,grid,pos,spans,titles)  \n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201125140304.png)\n\n\n\n\n\n\n\n## 5. 直方图\n\n```python\n# 读取图像\nimg = cv2.imread(\"resource/lina.jpg\")\nimg = BGRTORGB(img)\ngray_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\nplt.hist(gray_img.ravel(), 256);plt.title(\"直方图\")\nplt.show()\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201125140529.png)\n\n\n\n```python\ndef show_hist_quantized(histogram_quantized):\n    plt.figure(figsize=(7,7))\n    plt.title(u\"等分量化后的直方图\")\n    plt.xlabel(u\"等分index\")\n    plt.ylabel(u\"nums\")\n    plt.bar(x=list(range(len(histogram_quantized))), height=histogram_quantized)\n    plt.show()\nhistogram_quantized = [50,40,30,60,70]    \nshow_hist_quantized(histogram_quantized)\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201125141415.png)\n\n\n\n\n\n## 6. 坐标轴为整数的折线图\n\n```python\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nplt.title(u\"a plot\")\nplt.xlabel(u\"xxx\")\nplt.ylabel(u\"yyy\")\ndata = [13.4,4.6,7.8,9.9,5.5]\nplt.xticks(range(len(data)))\nplt.plot(data)\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201125141022.png)\n\n","tags":["matplotlib"],"categories":["python"]},{"title":"C++学习笔记——函数提高","url":"/2020/11/25/105331/","content":"\n本文讲解了C++函数的各种用法，如函数的默认参数、占位参数、函数重载等。\n\n<!-- more -->\n\n\n\n## 1 函数默认参数\n\n在C++中，函数的形参列表中的参数是可以有默认值大的\n语法：`返回值类型 函数名 (参数= 默认值){ }`\n\n示例：\n\n```cpp\n#include<iostream>\nusing namespace std;\n\n//函数默认参数\n//如果我们自己传入数据，就用自己的数据，如果没有，那么用默认值\nint func(int a, int b=50, int c=70) {\n\treturn a + b + c;\n}\n\n//注意事项\n//1、如果某个位置已经有了默认参数，那么从这个位置往后，从左到右都必须有默认值\n//int func2(int a, int b = 50, int c) {\n//\treturn a + b + c;\n//}\n//2、如果函数声明有默认参数，函数实现就不能有默认参数\n//准确的说，是声明和实现只能有一个有默认参数，否则，可能会二义性\nint func2(int a, int b);\n\nint func2(int a=20, int b=20) {\n\treturn a + b;\n}\n\nint  main() {\n\t//cout << func(10, 20, 30) << endl;\n\t//cout << func(10,20) << endl;\n\t//cout << func(10) << endl;\n\tcout<<func2(10, 10)<<endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n## 2 函数占位参数\nC++中函数的形参列表里可以有占位参数，用来做占位，调用函数时必须填补该位置\n\n语法：`返回值类型 函数名 (函数类型){}`\n\n在现阶段函数的占位参数存在意义不大，但是以后可能会用到该技术\n\n示例：\n```cpp\n//占位参数\n//目前阶段占位参数，我们还用不到，以后可能会用到\n\nvoid func(int a,int) {\n\tcout << \"this is func\" << endl;\n}\n//占位参数 还可以有默认参数\nvoid func1(int a, int = 10) {\n\tcout << \"this is func1\" << endl;\n}\nint  main() {\n\tfunc(10,10);\n\tfunc1(10);\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 3 函数重载\n### 3.1 函数重载概述\n\n**作用**：函数名可以相同，提高复用性\n\n函数重载满足条件：\n- 同一作用域下\n- 函数名称相同\n- 函数参数**类型不同** 或者 **个数不同** 或者 **顺序不同**\n\n**注意**：函数的返回值不可以作为函数重载的条件\n\n示例：\n\n```cpp\n//函数重载\n//可以让函数名相同，提高复用性\n\n//函数重载的满足条件1\n//1、同一作用域下\n//2、函数名称相同\n//3、函数参数类型不同，或者个数不同，或者顺序不同\n\nvoid func() {\n\tcout << \"func 调用\" << endl;\n}\nvoid func(int a) {\n\tcout << \"func(int a) 调用\" << endl;\n}\nvoid func(double a) {\n\tcout << \"func(double a) 调用\" << endl;\n}\nvoid func(int a,double b) {\n\tcout << \"func(int a,double b) 调用\" << endl;\n}\nvoid func(double a, int b ) {\n\tcout << \"func(double a, int b ) 调用\" << endl;\n}\n//注意事项\n//函数的返回追不可以作为函数重载的条件\n//int func(double a, int b) {\n//\tcout << \"func(double a, int b ) 调用\" << endl;\n//}\nint  main() {\n\n\t//func();\n\t//func(1);\n\t//func(10.0);\n\t//func(1, 10.0);\n\tfunc(10.0, 1);\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n### 3.2 函数重载注意事项\n\n- 引用作为重载条件\n- 函数重载碰到函数默认参数\n\n示例：\n\n```cpp\n//函数重载的注意事项\n//1、引用作为重载的条件\nvoid func(int &a) {// int &a=10;不合法的，因为这个字面量10放在常量区里面，不可写\n\tcout << \"fun(int &a) 调用\" << endl;\n}\nvoid func(const int &a) {//const int &a=10 只读的引用，是合法的\n\tcout << \"fun(const int &a) 调用\" << endl;\n}\n\n//2、函数重载碰到默认参数\nvoid func2(int a,int b=10) {\n\tcout << \"func2(int a,int b)的调用\" << endl;\n}\nvoid func2(int a) {\n\tcout << \"func2(int a)的调用\" << endl;\n}\n\nint  main() {\n\t//int a = 10;\n\t//const引用也可以重载\n\t//func(a);//调用无const函数\n\t//func(10);//调用有const函数\n\n\tfunc2(10);//当函数重载碰到默认参数，出现二义性，报错，尽量避免这种情况\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n\n\n","tags":["c++"],"categories":["c&c++"]},{"title":"C++学习笔记——引用","url":"/2020/11/25/105203/","content":"\n\n引用作为c++的一个重要功能，起到了不可忽视的作用，那么，就让我们一起来看看吧！\n\n<!-- more -->\n\n\n\n## 1 引用的基本使用\n\n- **作用**：给变量起别名\n- **语法**：`数据类型 &别名 = 原名`\n- 注意：引用的数据类型要和原类型一样\n\n原理图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20201004193426381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n示例：\n\n```cpp\n#include<iostream>\n\nusing namespace std;\n\n\nint  main() {\n\n\tint a = 10;\n\t//创建引用\n\tint &b = a;\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\tb = 100;\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 2 引用的注意事项\n- 引用必须初始化\n- 引用在初始化后，不可以改变\n\n原理图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20201004194926459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n示例：\n\n```cpp\nint  main() {\n\tint a = 10;\n\n\t//1、引用必须初始化\n\tint &b=a;//错误，必须要初始化\n\n\t//2、引用在初始化后，不可以改变\n\tint c = 20;\n\tb = c;//赋值操作，而不是更改引用,即这里只是把b指向的内置赋值成20\n\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\tcout << \"b = \" << b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 3 引用做函数参数\n- 作业：函数传参时，可以利用引用的技术让形参修饰实参\n- 优点：可以简化指针修改实参\n\n示例：\n\n```cpp\n//交换函数\n//1、值传递\nvoid mySwap01(int a, int b) {\n\tint temp = a;\n\ta = b;\n\tb = temp;\n\t//cout << \"mySwap01 a = \" << a << endl;\n\t//cout << \"mySwap01 b = \" << b << endl;\n}\n//2、地址传递\nvoid mySwap02(int *a, int *b) {\n\tint temp = *a;\n\t*a = *b;\n\t*b = temp;\n\n}\n//3、引用传递\nvoid mySwap03(int &a, int &b) {\n\tint temp = a;\n\ta = b;\n\tb = temp;\n}\n\nint  main() {\n\tint a = 10;\n\tint b = 20;\n\t//mySwap01(a, b);//值传递，实参不会改变\n\tmySwap02(&a, &b);//地址传递，实参会改变\n\tmySwap03(a, b);//地址传递，实参会改变\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n> 总结：通过引用参数产生的效果同地址传递是一样的。引用的语法更清楚简单\n\n## 4 引用做函数返回值\n\n作用：引用是可以作为函数的返回值存在的\n\n注意：**不要返回局部变量的引用**\n用法：函数调用作为左值\n\n示例：\n\n```cpp\n//引用做函数的返回值\n//1、不要返回局部变量的引用\nint& test01() {\n\tint a = 10;//局部变量存放在四区中的栈区\n\treturn a;\n}\n\n//2、函数的调用可以作为左值\nint& test02() {\n\tstatic int a = 10;//静态变量，存放在全局区，全局区上的数据在程序结束后由系统释放\n\treturn a;\n}\n\n\nint  main() {\n\n\tint &ref = test01();\n\n\t//cout << \"ref = \" << ref << endl;//第一次结果正确，是因为编译器做了保留\n\t//cout << \"ref = \" << ref << endl;//第二次结果错误，是因a的内存已经释放\n\n\tint &ref2 = test02();\n\tcout << \"ref2 = \" << ref2 << endl;\n\tcout << \"ref2 = \" << ref2 << endl;\n\ttest02() = 1000;//如果函数的返回值是引用，这个函数调用可以作为左值\n\tcout << \"ref2 = \" << ref2 << endl;\n\tcout << \"ref2 = \" << ref2 << endl;\n\tcout << \"ref2 = \" << ref2 << endl;\n\n\n\t//这个就不是变量的引用了，这里相对于直接在新的内存copy函数的返回值\n\tint ref3 = test02();\n\tcout << \"ref3 = \" << ref3 << endl;\n\tcout << \"ref3 = \" << ref3 << endl;\n\ttest02() = 999;\n\tcout << \"ref3 = \" << ref3 << endl;\n\tcout << \"ref3 = \" << ref3 << endl;\n\tcout << \"ref3 = \" << ref3 << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 5 引用的本质\n\n本质：**引用的本质在c++内部实现是一个指针常量**，即指针的指向不可以修改，指针指向的值是可以修改的\n\n\n示例：\n\n```cpp\n//发现是引用，转换为int* const ref = &a;\nvoid func(int &ref) {\n\tref = 100;//ref是引用，转换为*ref = 100;\n}\n\nint  main() {\n\n\tint a = 10;\n\n\t//自动转化为int *const ref = &a;指针常量是指针的指向不可以改，也说明为什么引用不可更改\n\tint &ref = a;\n\tref = 20; //内部发现ref是引用1，自动帮我们转换为：*ref = 20;\n\n\tcout << \"a = \" << a << endl;\n\tcout << \"ref = \" << ref << endl;\n\n\tfunc(a);\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n原理图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20201004204328249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n> 结论：C++推荐用引用技术，因为语法方便，引用本质是指针常量，但是所有的指针操作编译器都帮我们做了\n\n## 6 常量引用\n\n作用：常量引用主要用来修饰形参，防止误操作\n\n在函数形参列表中，可以加==const修饰形参==，防止形参改变实参\n\n示例：\n\n```cpp\n//打印数据\nvoid showValue(const int &val) {//加上const就不能修改了\n\t//val = 1000;\n\tcout << \"val = \" << val << endl;\n}\n\nint  main() {\n\t//常量引用\n\t//使用场景：用来修饰形参，防止误操作\n\t//int a=10;\n\t//int &ref = 10;//错误，引用必须引一块合法的内存空间，可以是栈区，也可以是堆区，所以这里字面量是不可以的\n\tconst int &ref = 10;//加上const之后，编译器将代码修改为 int temp = 10; const int &ref = temp;\n\t//ref = 20;//加入const之后变为只读，不可以修改\n\n\tint a = 100;\n\tshowValue(a);\n\tcout << \"a = \" << a << endl;\n\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n","tags":["c++"],"categories":["c&c++"]},{"title":"C++学习笔记——程序的内存模型","url":"/2020/11/25/105031/","content":"\n本文带来C++的内存模型，干货多多！\n\n<!-- more -->\n\n\n\n## 1 内存分区模型\n\nC++程序在执行时，将内存大方向划分为4个区域\n- 代码区：存放函数体的二进制代码，由操作系统进行管理的（所有的代码及注释）\n- 全局区：存放全局变量和静态变量以及常量\n- 栈区：由编译器自动分配释放，存放函数的参数值，局部变量等\n- 堆区：有程序员分配和释放，若程序员不释放，程序结束时由操作系统回收\n\n\n**内存四区的意义**：\n不同区域存放的数据，赋予不同的生命周期，给我们更大的灵活编码\n\n### 1.1 程序运行前\n在程序编译后，生成了exe可执行程序，**未执行该程序前**分为两个区域：代码区和全局区\n\n\n**代码区：**\n- 存放CPU执行的机器指令（就是你写的代码）\n- 代码区是**共享的**，共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可\n- 代码区是**只读的**，使其只读的原因是防止程序意外地修改了它的指令\n\n**全局区：**\n- 全局变量和静态变量存放在此\n- 全局区还包含了常量区，字符串常量和其他常量（const修饰的变量即常量）也存放在此\n- ==该区域的数据在程序结束后由操作系统释放==\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020092320063968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\n//全局变量\nint g_a = 10;\nint g_b = 10;\n\n//const修饰的全局变量，即全局常量，全局常量在常量区\nconst int c_g_a = 10;\nconst int c_g_b = 10;\n\nint main() {\n\n\t// 全局区\n\t// 全局变量、静态变量、常量（字符串常量和全局常量）\n\n\t//创建普通局部变量\n\tint a = 10;\n\tint b = 10;\n\tcout << \"局部变量a的地址为：\" << (int)&a << endl;\n\tcout << \"局部变量b的地址为：\" << (int)&b << endl;\n\tcout << \"全局变量g_a的地址为：\" << (int)&g_a << endl;\n\tcout << \"全局变量g_b的地址为：\" << (int)&g_b << endl;\n\n\t// 静态变量 在普通变量前面加static，数据静态变量\n\tstatic int s_a = 10;\n\tstatic int s_b = 10;\n\tcout << \"静态变量s_a的地址为：\" << (int)&s_a << endl;\n\tcout << \"静态变量s_b的地址为：\" << (int)&s_b << endl;\n\t\n\t//常量\n\t//字符串常量\n\tcout << \"字符串常量hello地址为：\" << (int)&\"hello\" << endl;\n\tcout << \"字符串常量world地址为：\" << (int)&\"world\" << endl;\n\n\t//const修饰的变量\n\t//const修饰的全局变量，const修饰的局部变量\n\tcout << \"全局常量c_g_a的地址为：\" << (int)&c_g_a << endl;\n\tcout << \"全局常量c_g_b的地址为：\" << (int)&c_g_b << endl;\n\n\t//const修饰的局部变量，即局部常量,局部常量不在常量区，也不在全局区\n\t//局部常量存放在栈区\n\tconst int c_l_a = 10;// c - const g - global  l - local\n\tconst int c_l_b = 10;\n\tcout << \"局部常量c_l_a的地址为：\" << (int)&c_l_a << endl;\n\tcout << \"局部常量c_l_b的地址为：\" << (int)&c_l_b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n\n```\n\n总结：\n- c++中在程序运行前分为全局区和代码区\n- 代码区特点是共享和只读\n- 全局区中存放全局变量、静态变量、常量\n- 常量区中存放const修饰的全局常量和字符串常量\n- const修饰的局部常量放在栈区（同普通局部变量一个位置）\n\n### 1.2 程序运行后\n**栈区：**\n- 由编译器自行分配和释放，存放函数的参数值，局部变量等\n- 注意事项：不要返回局部变量的地址，栈区开辟的数据由编译器自行释放（在函数执行完后自行释放）\n\n```cpp\n//栈区数据注意事项 -- 不要返回局部变量的地址\n//栈区的数据由编译器管理开辟和释放\n\n\nint * func(int b) {//形参数据也会放在栈区\n\tb = 100;\n\tint a = 10;//局部变量存放在栈区，存放在栈区，栈区的数据在函数执行完后自行释放\n\treturn &a;//返回局部变量的地址\n}\n\nint main() {\n\t//接受func函数的返回值\n\tint * p = func(1);\n\n\tcout << *p << endl;//第一次可以打印正确的数字，是因为编译器给我们做了一次保留\n\tcout << *p << endl;//第二次这个数据就不再保留了\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n**堆区：**\n- 由程序员分配释放，若程序员不释放，程序运行期间，该内存不会被释放，程序结束时由操作系统回收\n- 在C++中利用new在堆区开辟内存\n\n```cpp\nint * func() {\n\t//利用new关键字，可以将数据开辟到堆区\n\t//指针 本质也是变量，放在栈上，指针保存的数据是放在堆区\n\tint * p = new int(10);//返回的是地址\n\treturn p;\n}\n\nint main() {\n\t\n\t//在堆区开辟数据\n\tint * p = func();//得到func返回的地址，保存在main函数区部变量中（也在堆区），当main函数运行结束后，这个func返回的局部变量地址才会消除\n\tcout << *p << endl;\n\tcout << *p << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923205713998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n总结：\n- 堆区数据由程序员管理开辟和释放\n- 堆区数据利用new关键字进行开辟内存\n\n### 1.3 new操作符\n- C++中利用==new==操作符在堆区开辟数据\n- 堆区开辟的数据，由程序员手动开辟，手动释放，释放利用操作符==delete==\n\n语法：`new 数据类型`\n利用new创建的数据，会返回该数据对应的类型的指针\n\n```cpp\n//1、new的基本语法\nint * func() {\n\t//在堆区创建整型数据\n\t//new返回的是 该数据类型的指针\n\tint * p = new int(10);\n\treturn p;\n}\n\nvoid test01() {\n\tint * p = func();\n\tcout << *p << endl;\n\tcout << *p << endl;\n\t//堆区的数据 由程序员管理开辟，程序员管理释放\n\t//如果释放堆区的数据，利用关键字delete\n\tdelete p;\n\tcout << *p << endl; //内存已经被释放，再次访问就是非法操作，会报错\n}\n\n//2、在堆区利用new开辟数组\nvoid test02() {\n\t//创建10整型数据的数组，在堆区\n\tint * arr = new int[10];//10代表数组有10个元素\n\tfor (int i = 0; i < 10; i++)\n\t\tarr[i] = i + 100;\n\tfor (int i = 0; i < 10; i++)\n\t\tcout << arr[i] << endl;\n\t//释放堆区数组\n\t//释放数组的时候，要加[]才可以\n\tdelete[] arr;\n}\n\nint main() {\n\t\n\t//test01();\n\ttest02();\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 2 几个重要结论\n### 2.1 结论1\n结论1：当调用函数时，其实就相当于对传入实参进行了拷贝，存放在栈中（局部变量），具体如下（见示例1）：\n- 当传入值时，拷贝值存放在栈中\n- 当传入指针时，拷贝指针的地址存放在栈中\n- 当传入引用时，和传入指针原理是一样的，因为引用本身是由指针常量实现的（即指针的指向不可以修改，指针指向的值是可以修改的）\n\n\n另外，需要注意的是：**简短的赋值操作不属于拷贝，赋值操作只是把原有内存空间换成了别的值，所以拷贝只是在函数调用时存在**，可以看下面的实例2.\n\n示例1：\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\nvoid func(int a) {\n\tcout << \"func(int a)中a的地址：\" << (int)&a << endl;\n}\nvoid func1(int *a) {\n\tcout << \"func1(int *a)中a的地址：\" << (int)a << endl;\n\tcout << \"func1(int *a)中指针a本身存放的地址：\" << (int)&a << endl;\n\n}\nvoid func2(int &a) {\n\tcout << \"func2(int &a)中a的地址：\" << (int)&a << endl;\n}\n\nint g_a = 10;\nint main() {\n\tcout << \"（全局区）全局变量g_a的地址：\" << (int)&g_a << endl;\n\tint l_a = 10;\n\tcout << \"（栈区）局部变量l_a的地址：\" <<(int)&l_a<< endl;\n\tfunc(g_a);\n\tfunc(l_a);\n\tfunc1(&l_a);\n\tfunc2(l_a);\n\t/*\n\t结论1：当调用函数时，其实就相当于对传入实参进行了拷贝，存放在栈中（局部变量），具体有以下情况：\n\t- 当传入值时，拷贝值存放在栈中\n\t- 当传入指针时，拷贝指针的地址存放在栈中\n\t- 当传入引用时，和传入指针原理是一样的，因为引用本身是由指针常量实现的（即指针的指向不可以修改，指针指向的值是可以修改的）\n\t*/\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n示例2：\n\n```cpp\nint main() {\n\n\t//下面赋值前后a的地址是一样的\n\tint a = 10;\n\tcout << \"a的地址为：\" << (int)&a << endl;\n\n\ta = 20;\n\tcout << \"值更改后a的地址为：\" << (int)&a << endl;\n\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n### 2.2 结论2\n结论2，**全局区中全局变量和静态变量在一个区域，全局常量和字符串常量在一个区域。全局变量和静态变量可以读写（即可以更改），全局常量和字符串常量只能读，不能写。在全局区的常量区（全局常量和字符串常量）中，数值只存在一份，而全局变量区（全局变量和静态变量），相同数值可以有多个（方法是多创建几个数值相同的变量）。见示例1**\n\n除了作用域外，全局变量区（全局变量和静态变量）和局部变量区（栈区）是一样的，都可以读写，多个值相同的变量拥有**不同**的内存空间。\n\n另外，**所有的字面量（包括字符串字面量和其他字面量）都是常量，存放在全局区中的常量区中，不能进行写操作，而引用是为了给变量其别名，为了对变量的内存空间进行读写操作（必须有一个合法的内存空间，可以读写的空间都可以，堆区，栈区，全局变量区），所以把变量赋给引用，而不能是字面量。如果是只读的引用，则把字面量赋给引用也是合法的，见示例2**。\n\n**要记住，创建一个局部变量（指针变量、数值变量，后面结论3会更详细说），存放在栈区，而引用只是给变量起别名，不是创建变量，所以引用的右侧位置只能放变量**\n\n```cpp\n// 全局区\n// 全局变量、静态变量、常量（字符串常量和全局常量）\n\n//全局变量\nint g_a = 10;\nint g_b = 10;\n\n//const修饰的全局变量，即全局常量\nconst int c_g_a = 10;\nconst int c_g_b = 10;\nint main() {\n\n\tcout << \"全局区变量地址为：\" << endl;\n\tcout << \"全局变量g_a的地址为：\" << (int)&g_a << endl;\n\tcout << \"全局变量g_b的地址为：\" << (int)&g_b << endl;\n\n\tstatic int s_a = 10;\n\tstatic int s_b = 10;\n\tcout << \"全局区静态变量地址为：\" << endl;\n\tcout << \"静态变量s_a的地址为：\" << (int)&s_a << endl;\n\tcout << \"静态变量s_b的地址为：\" << (int)&s_b << endl;\n\n\n\tcout << \"全局区普通常量地址为：\" << endl;\n\tcout << \"全局常量c_g_a的地址为：\" << (int)&c_g_a << endl;\n\tcout << \"全局常量c_g_b的地址为：\" << (int)&c_g_b << endl;\n\n\tcout << \"全局区字符串常量地址为：\" << endl;\n\tcout << \"字符串常量hello地址为：\" << (int)&\"hello\" << endl;\n\tcout << \"字符串常量world地址为：\" << (int)&\"world\" << endl;\n\tcout << \"字符串常量hello地址为：\" << (int)&\"hello\" << endl;\n\n\t//结论2，全局区中全局变量和静态变量在一个区域，全局常量和字符串常量在一个区域。\n\t//全局变量和静态变量可以读写（即可以更改），全局常量和字符串常量只能读，不能写\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n示例2：\n\n```cpp\nint main() {\n\t//int &p = 10;//读写的引用，不合法的，因为这个字面量10放在常量区里面，不可以进度写\n\t//只读的引用，合法的\n\tconst int &c = 10;\n\n\tint a = 10;\n\t//引用右侧放变量，合法\n\tint &b = a;\n\n\tcout << b << endl;\n\tcout << c << endl;\n\t\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n\n\n### 2.3 结论3\n**结论3（很重要）**：\n- 所有的局部变量(一般为指向一片内存空间的指针的内存地址)存放在栈区，当然数据型局部变量也在栈区\n- 所有的全局变量(一般为指向一片内存空间的指针的地址)存放在全局变量区，当然数据型全局变量也在全局变量区\n- 如果在局部作用域中new一个数据(数值或数组），new出来的数据的内存空间在堆区，而指向该内存空间的指针的地址在栈区\n- 如果在全局作用域中new一个数据(数值或数组），new出来的数据的内存空间在堆区，而指向该内存空间的指针的地址在全局变量区\n- 在局部作用域中用如`char * s = \"hello\"`创建出来的字符串，该字符串的内存空间在全局常量区，而指向该字符串内存空间的指针的地址(即s的地址)在栈区，故该字符串的内存空间是只读不可写的，而指向该字符串内存空间的指针的地址是可以被重新写入的（即重新赋值，看结论1的注意事项）\n- **在局部作用域中用如`char arr[] = \"hello\"`创建出来的数组字符串，该字符串的内存空间在栈区，该字符串内存空间的指针的地址(即arr的地址)也在栈区**，这个是需要注意的地方，所以，该字符串的内存空间是**可读可写的**，并且内存空间由编译器自动释放。\n- 在局部作用域中用如`char *newarr = new char[6]`创建的数组，该数组的内存空间在堆区，而指向数组内存空间的指针的地址在栈区，该数组的内存空间需要由程序员释放，指向数组内存空间的指针的地址(内存空间)由编译器自动释放。\n- 在全局作用域用如`int * global_newArr = new int[20]`创建的数组，该数组的内存空间在堆区，而指向数组内存空间的指针的地址在全局变量区，该数组的内存空间需要由程序员释放，指向数组内存空间的指针的内存空间由编译器自动释放。\n\n示例：\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\nint * global_newArr = new int[20];\n\nint main() {\n\n\t//int型局部变量\n\tint a = 10;\n\tcout << \"栈区的地址为：\" << (int)&a << endl;\n\n\tcout << \"全局常量区的地址为：\" << (int)&\"hello\" << endl;\n\t\n\n\tint * p = new int(10);\n\tcout << \"堆区的地址为：\" << (int)p << endl;\n\tcout << \"指向堆区内存空间的指针(变量)的地址为：\" << (int)&p << endl;\n\n\n\tchar * s = \"hello\";\n\tcout << \"字符串的地址为：\" << (int)s << endl;\n\tcout << \"指向该字符串内存空间的指针的地址为：\" << (int)&s << endl;\n\t//s[3] = 'z'; 报错，全局常量区只能读，不能写\n\n\tchar arr[] = \"hello\";\n\tcout << \"数组字符串的地址为：\" << (int)arr << endl;\n\tcout << \"指向该数组字符串内存空间的指针的地址为：\" << (int)&arr << endl;\n\t//arr[3] = 'z';不报错，数组字符串存放在堆区，可读可写\n\n\n\tchar *newarr = new char[6];\n\tcout << \"new数组的地址为：\" << (int)newarr << endl;\n\tcout << \"指向new数组内存空间的指针的地址为：\" << (int)&newarr << endl;\n\n\tcout << \"global_newArr的地址为：\" << (int)global_newArr << endl;\n\tcout << \"指向global_newArr内存空间的指针的地址为：\" << (int)&global_newArr << endl;\n\n\tdelete p;\n\tdelete[] newarr;\n\tdelete[] global_newArr;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n### 2.4 结论4(sizeof统计空间大小)\n\n**关于数组字符串和指针字符串所占空间大小**，无论指针字符串长度多少，使用`sizeof`统计的大小始终为4个字节，因为统计的是指针的大小。而对于数组字符串，使用`sizeof`统计的是整个字符串空间的大小。\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\nint main() {\n\tchar * a = \"abcefg\";\n\tchar b[] = \"abcefg\";\n\tcout << \"a:\" << sizeof(a) << endl;\n\tcout << \"b：\" << sizeof(b) << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n","tags":["c++"],"categories":["c&c++"]},{"title":"C++学习笔记——结构体","url":"/2020/11/25/104732/","content":"\n## 1. 结构体基本概念\n\n结构体属于用户==自定义的数据类型==，允许用户存储不同的数据类型\n\n<!-- more -->\n\n## 2. 结构体定义和作用\n\n语法：`struct 结构体名 { 结构体成员列表 };`\n通过结构体创建变量的方式有三种：\n- struct 结构体名变量名\n- struct 结构体名 = {成员1值，成员2值...}\n- 定义结构体时顺便创建变量\n\n示例：\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\n// 1、创建学生数据类型：学生包括(姓名，年龄，分数)\n// 语法 struct 类型名称 {成员列表}\nstruct Student {\n\n\t//成员列表\n\tstring name;\n\tint age;\n\tint score;\n\n} s3;//顺便常见结构体变量\n\n// 2、通过学生类型创建具体学生\n\nint main() {\n\n\t// 2.1 struct Student s1\n\t//struct 关键字可以省略\n\tStudent s1;\n\ts1.age = 20;\n\ts1.name = \"张三\";\n\ts1.score = 100;\n\tcout << \"姓名：\" << s1.name << \"\\t年龄：\" << s1.age << \"\\t分数：\" << s1.score << endl;\n\t// 2.2 struct Student s2 = {...}\n\tstruct Student s2 = { \"李四\",21,101 };\n\tcout << \"姓名：\" << s2.name << \"\\t年龄：\" << s2.age << \"\\t分数：\" << s2.score << endl;\n\t// 2.3 在定义结构体时顺便创建结构体变量 （不建议）\n\ts3.age = 22;\n\ts3.name = \"王五\";\n\ts3.score = 102;\n\tcout << \"姓名：\" << s3.name << \"\\t年龄：\" << s3.age << \"\\t分数：\" << s3.score << endl;\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n> - 总结1：定义结构体时的关键字是struct，不可省略\n> - 总结2：创建结构体变量时，关键字struct可以省略\n> - 总结3：结构体变量利用操作符\".\"访问成员\n\n## 3. 结构体数组\n\n作用：将自定义的结构体放入到数组中方便维护\n语法：`struct 结构体名 数组名[元素个数]={ {} , {} , {} , ... , {}}`\n示例：\n\n```cpp\n#include<iostream>\n#include<string>\nusing namespace std;\n\nstruct Student {\n\n\tstring name;\n\tint age;\n\tint score;\n\n};\n\nint main() {\n\n\tstruct Student stus[3] = { {\"张三\",20,100},{\"李四\",21,101},{\"王五\",22,102} };\n\tfor (int i = 0; i < 3; i++)\n\t\tcout << \"名字：\" << stus[i].name << \"\\t年龄：\" << stus[i].age << \"\\t分数：\" << stus[i].score << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 4. 结构体指针\n\n作用：通过指针访问结构体中的成员\n\n- 利用操作符`->`可以通过结构体指针访问结构体属性\n\n示例：\n\n```cpp\nstruct Student {\n\n\tstring name;\n\tint age;\n\tint score;\n\n};\n\nint main() {\n\n\t//1、创建结构体变量\n\tstruct Student s = { \"张三\",18,100 };\n\n\t//2、通过指针指向结构体变量（struct可以省略）\n\tstruct Student * p = &s;\n\n\t//3、通过指针访问结构体变量中的数据\n\t\n\tcout << \"名字：\" << p->name << \"\\t年龄：\" << p->age << \"\\t分数：\" << p->score << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 5. 结构体嵌套结构体\n\n作用：结构体中的成员可以是另一结构体\n例如：每个老师辅导一个学员，一个老师的结构体中，记录一个学生的结构体\n\n```cpp\nstruct Student {\n\tstring name;\n\tint age;\n\tint score;\n\n};\nstruct Teacher {\n\tint id;\n\tstring name;\n\tint age;\n\tstruct Student stu;\n};\n\nint main() {\n\tstruct Teacher t;\n\tt.id = 1000;\n\tt.name = \"老王\";\n\tt.age = 50;\n\tt.stu.name = \"小王\";\n\tt.stu.age = 20;\n\tt.stu.score = 60;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 6. 结构体做函数参数\n\n作用：将结构体作为参数向函数中传递\n\n传递方式有两种：\n- 值传递\n- 地址传递\n\n```cpp\n// 定义学生结构体\nstruct Student {\n\tstring name;\n\tint age;\n\tint score;\n\n};\n\n\n//打印学生信息的函数\n//1、值传递 不会改变实参的值\nvoid printStudent1(struct Student s) {\n\ts.age = 100;\n\tcout << \"值传递函数打印 姓名：\" << s.name << \" 年龄：\" << s.age << \" 分数：\" << s.score << endl;\n}\n\n//2、地址传递 会改变实参的值\nvoid printStudent2(struct Student * p) {\n\tp->age = 200;\n\tcout << \"地址传递函数打印 姓名：\" << p->name << \" 年龄：\" << p->age << \" 分数：\" << p->score << endl;\n}\n\nint main() {\n\t//结构体做函数参数\n\t//将学生传入到一个参数中，打印学生身上的所有信息\n\t\n\t//创建结构体变量\n\tstruct Student s;\n\ts.name = \"张三\";\n\ts.age = 20;\n\ts.score = 85;\n\t\n\t//printStudent1(s);\n\tprintStudent2(&s);\n\n\tcout << \"main函数打印 姓名：\" << s.name << \" 年龄：\" << s.age << \" 分数：\" << s.score << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 7. 结构体中const使用场景\n\n作用：用const来防止误操作\n\n```cpp\n// const使用场景\n\n// 定义学生结构体\nstruct Student {\n\tstring name;\n\tint age;\n\tint score;\n\n};\n\n// 将函数中的形参改为指针，可以减少内存空间，而且不会复制新的副本出来\n// 传入指针，只占4个字节，比值传递大幅节省空间\nvoid printStudent(const struct Student * s) {\n\t//s->age = 150; 加入const之后，只能读不能写，一旦有修改的操作就会报错，可以防止我们误操作\n\tcout << \"值传递函数打印 姓名：\" << s->name << \" 年龄：\" << s->age << \" 分数：\" << s->score << endl;\n}\n\nint main() {\n\n\t//创建结构体变量\n\tstruct Student s;\n\ts.name = \"张三\";\n\ts.age = 20;\n\ts.score = 85;\n\t\n\tprintStudent(&s);\n\n\tcout << \"main函数打印 姓名：\" << s.name << \" 年龄：\" << s.age << \" 分数：\" << s.score << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n","tags":["c++"],"categories":["c&c++"]},{"title":"C++学习笔记——指针","url":"/2020/11/25/100029/","content":"\n## 1. 指针的基本概念\n\n指针的作用：可以通过指针**间接访问**内存，**指针就是一个地址**\n\n- 内存编号是从0开始记录的，一般用十六进制数字表示\n- **可以利用指针变量保存地址**\n\n<!-- more -->\n\n## 2. 指针变量的定义和使用\n\n指针变量定义语法：`数据类型 * 变量名`\n\n\n实例如下：\n```cpp\n#include<iostream>\nusing namespace std;\n\nint main() {\n\t//1.定义zhizhen\n\tint a = 10;\n\t// 指针定义的语法：数据类型 * 指针变量名\n\tint * p;\n\t// 让指针记录变量a的地址\n\tp = &a;\n\tcout << \"a的地址为：\" << &a << endl;\n\tcout << \"指针p为：\" << p << endl;\n\n\t//2.使用指针\n\t//可以通过解引用的方式来找到指针指向的内存\n\t// 指针前加 * 代表解应用，找到指针指向内存中的数据，然后进行读和写的操作\n\t*p = 1000;\n\tcout << \"a = \" << a << endl;\n\tcout << \"*p = \" << *p << endl;\n\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n示意图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923150324142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n## 3. 指针所占用内存空间\n\n- 在32位操作系统下，指针是占4个字节空间大小，不管是什么类型\n- 在64位操作系统下，指针是占8个字节空间大小\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923145719788.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n实例：\n\n```cpp\n#include<iostream>\nusing namespace std;\n\nint main() {\n\n\t//指针所占用内存空间\n\tint a = 10;\n\tint * p = &a;\n\n\t// 在32位操作系统下，指针是占4个字节空间大小，不管是什么类型\n\t// 在64位操作系统下，指针是占8个字节空间大小\n\t// sizeof(p)和sizeof(int *)一样\n\tcout << \"sizeof （int *) = \" << sizeof(int *) << endl;\n\tcout << \"sizeof （float *) = \" << sizeof(float *) << endl;\n\tcout << \"sizeof （double *) = \" << sizeof(double *) << endl;\n\tcout << \"sizeof （char *) = \" << sizeof(char *) << endl;\n\tsystem(\"pause\");\n\treturn 0;\n}\n\n```\n\n## 4. 空指针和野指针\n\n- **空指针**：指针变量指向内存中编号为0的空间\n- 用途：初始化指针变量\n- 注意：空指针指向的内存是不可以访问的\n\n示例1：空指针\n\n```cpp\nint main() {\n\n\t//空指针\n\t// 1、空指针用于给指针变量进行初始化\n\tint * p = NULL;\n\t// 2、空指针是不可以进行访问的\n\t// 0~255之间的内存编号是系统占用的，因此不可以访问\n\t*p = 100;\n\t// 会报错\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n- **野指针**：指针变量指向非法的内存空间\n\n示例2：野指针\n\n```cpp\nint main() {\n\n\t//野指针\n\t// 在程序中，尽量避免出现野指针\n\n\t// 指针变量p指向内存地址编号为0x1100的空间\n\tint * p = (int *)0x1100;\n\t\n\t// 访问野指针报错\n\tcout << *p << endl;\n\t// 引发了异常: 读取访问权限冲突。因为你没有申请这个地址所指向的内存空间，却又访问它\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n> 总结：空指针和野指针都不是我们申请的空间，因此不要访问，否则会出错。\n\n## 5. const修饰指针\n\nconst修饰指针三种情况：\n1. const修饰指针 ---常量指针\n2. const修饰常量 ---指针常量\n3. const即修饰指针，又修饰常量\n\n示例1：常量指针（红线标注的是错误的方式）\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923153102951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n示例2：指针常量（红线标注的是错误的方式）\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923153331852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n示例3：const即修饰指针，又修饰常量（红线标注的是错误的方式）\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020092316462361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n示例：\n\n```cpp\nint main() {\n\t// 1、const修饰指针\n\tint a = 10;\n\tint b = 10;\n\n\tconst int * p = &a;\n\t// 指针指向的值不可以改，指针的指向可以改\n\t// *p = 20; 错误\n\tp = &b; //正确\n\n\t// 2、const修饰常量 指针常量\n\tint * const p2 = &a;\n\t*p2 = 100; //正确\n\t// p2 = &b; //错误，指针的指向不可以改\n\t// 3、const修饰指针和常量\n\tconst int * const p3 = &a;\n\t// 指针的指向和指针指向的值 都不可以改\n\t// *p3 = 100; //错误\n\t// p3 = &b; //错误\n}\n```\n\n> 技巧：看const右侧紧跟着的是指针还是常量，是指针就是常量指针，是常量就是指针常量\n\n## 6. 指针和数组\n\n- 作用：利用指针访问数据数据中元素\n\n示例：\n\n```cpp\nint main() {\n\t//指针和数组\n\t//利用指针访问数组中的元素\n\n\tint arr[10] = { 1,2,3,4,5,6,7,8,9,10 };\n\tcout << \"第一个元素为：\" << arr[0] << endl;\n\tint * p = arr;//arr就是数组首地址\n\tcout << \"利用指针访问第一个元素：\" << *p << endl;\n\t//p++不是把地址加1，是指针偏移一个单位，在32位下是偏移4个字节，在64位下，偏移8个字节\n\tp++;\n\tcout << \"利用指针访问第二个元素：\" << *p << endl;\n\tsystem(\"pause\");\n\treturn 0;\n}\n```\n\n## 7. 指针和函数\n\n作用：利用指针作函数参数，可以修改实参的值\n\n\n\n\n\n```cpp\n#include<iostream>\nusing namespace std;\n\n\nvoid swap01(int a, int b) {\n\tint temp = a;\n\ta = b;\n\tb = temp;\n}\n\nvoid swap02(int *p1, int *p2) {\n\tint temp = *p1;\n\t*p1 = *p2;\n\t*p2 = temp;\n}\n\nint main() {\n\n\t// 指针和函数\n\t//1、值传递 不会改变实参的值\n\tint a = 10;\n\tint b = 20;\n\tswap01(a, b);\n\tcout << \"值传递：\\n\";\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\n\t//2、地址传递 会改变实参的值\n\tcout << \"地址传递：\\n\";\n\tswap02(&a, &b);\n\tcout << \"a = \" << a << endl;\n\tcout << \"b = \" << b << endl;\n\n\tsystem(\"pause\");\n\treturn 0;\n}\n\n```\n\n示例图如下\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923160126957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n## 8. 指针、数组、函数组合案例\n\n案例描述：封装一个函数，利用冒泡排序，实现对整型数组的升序排序\n\n例如数组：int arr[10] = {4,3,6,8,1,2,10,8,7,5}\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200923160823956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n","tags":["c++"],"categories":["c&c++"]},{"title":"机器学习概述","url":"/2020/11/25/095842/","content":"\n\n在这里将学到：\n- 什么是机器学习\n- 为什么需要机器学习\n- 机器学习中的基本概念：包括**样本、特征、标签、模型、学习算法**\n- 机器学习的三要素：模型、评价准则、优化算法\n- 训练集、测试集、样本集的概念\n\n<!-- more -->\n\n\n\n\n\n\n## 什么是机器学习\n\n通俗地讲， 机器学习 （Machine Learning， ML）让计算机从数据中进行自动学习，得到某种知识，而不是人为指定且明显的去编程。以手写体数字识别为例，我们需要让计算机能自动识别手写的数字，由于每个人的写法都不相同，我们很难总结每个数字的手写体特征，因此设计一套识别算法几乎是一项几乎不可能的任务。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802172618567.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70 =500x300)\n在现实生活中，很多问题都类似于手写体数字识别这类问题，比如物体识别、语音识别等。对于这类问题，我们不知道如何设计一个计算机程序来解决。因此，**人们开始尝试采用另一种思路，即让计算机“看”大量的样本，并从中学习到一些经验，然后用这些经验来识别新的样本**。要识别手写体数字，首先通过人工标注大量的手写体数字图像（即每张图像都通过人工标记了它是什么数字），这些图像作为训练数据，然后通过学习算法自动生成**模型**，并依靠它来识别新的手写体数字。这和人类学习过程也比较类似，我们教小孩子识别数字也是这样的过程。这种通过数据来学习的方法就称为**机器学习**的方法。\n\n机器学习自动生成的**模型**也称为**决策函数$f$**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802190614894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70 =500x300)\n\n\n\n## 机器学习的基本概念\n\n机器学习中的一些基本概念：包括**样本、特征、标签、模型、学习算法**等。以一个生活中的经验学习为例，假设我们要到市场上购买芒果，但是之前毫无挑选芒果的经验，那么我们如何通过学习来获取这些知识？\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802174401531.gif)\n\n\n首先，我们从市场上随机选取一些芒果，列出每个芒果的**特征 (Feature)**，包括颜色，大小，形状，产地，品牌，以及我们需要预测的**标签 (Label)**。标签可以是**连续值**（比如关于芒果的甜度、水分以及成熟度的综合打分），也可以是**离散值**（比如好、坏两类标签）。\n\n一个标记好特征以及标签的芒果可以看作是一个**样本 (Sample)**。一组样本构成的集合称为**数据集 (Data Set)**。一般将数据集分为两部分：训练集和测试集。 **训练集 (Training Set)**中的样本是用来训练模型的，也叫**训练样本**(Training Sample)，而**测试集 (Test Set)**中的样本是用来检验模型好坏的，也叫**测试样本**(Test Sample)。\n\n> 特征也可以称为属性（Attribute），样本（Sample），也叫示例（Instance）。\n\n我们用一个 d 维向量 $X = [x_1, x_2, · · · , x_d]^T$ 表示一个芒果的所有特征构成的向量，称为**特征向量** （Feature Vector），其中每一维表示一个特征。\n\n假设训练集由 N 个样本组成，其中每个样本都是独立同分布 （Identically and Independently Distributed， IID）的，即独立地从相同的数据分布中抽取的，记为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802191715788.png)\n其中，${X^{ { {\\left( 1 \\right)}^{ } } } }$表示第一个样本的特征向量，${y^{ { {\\left( 1 \\right)}^{ } } } }$第一个样本的标签\n\n给定训练集 $D$，我们希望让计算机自动寻找一个**函数**$f (X; θ)$ 来建立每个样本特性向量 X 和标签 y 之间的映射。对于一个样本 $X$，我们可以通过决策函数来预测其标签的值\n$$\\hat y=f (X; θ)$$\n或标签的条件概率\n$$p(y|X)=f_y(x;θ)$$\n其中 $θ$ 为可学习的参数\n\n通过一个**学习算法** (Learning Algorithm) ${\\rm A}$，在训练集上找到一组参数 $θ^∗$，使得函数 $f (X; θ^*)$ 可以近似真实的**映射关系**。这个过程称为学习 （Learning）或训练 （Training）过程，函数 $f (X; θ)$ 称为**模型** （Model）。\n\n> 在有些文献中，学习算法也叫做学习器 （Learner）。\n\n下次从市场上买芒果（测试样本）时，可以根据芒果的特征，使用学习到的模型 $f (X; θ^*)$ 来预测芒果的好坏。为了评价的公正性，我们还是独立同分布地抽取一组样本作为测试集 $D^′$，并在测试集中所有样本上进行测试，计算预测结果的准确率。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802193511664.png)\n其中 $I (·)$ 为指示函数， $|D^′|$ 为测试集大小。(对于分类，一般是求出总测试集样本的个数和预测正确的个数)\n\n下图给出了机器学习的基本概念。对一个预测任务，输入特征向量为 $X$，输出标签为 $y$，我们选择一个函数 $f (X; θ)$，通过学习算法 ${\\rm A}$ 和一组训练样本 $D$，找到一组最优的参数 $θ^∗$，得到最终的模型 $f (X; θ^*)$。这样就可以对新的输入 $X$ 进行预测。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802194158846.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n## 机器学习的三要素\n机器学习方法可以分为三个基本要素：模型、学习准则、优化算法。\n\n### 模型\n> 输入空间默认为样本的特征空间\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802200142159.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n> $R$为实数，$m$为参数的数量，表示有m个实数要学习\n\n**线性模型：**\n\n**线性模型**的假设空间为一个参数化的**线性函数族**，\n$$f (X; θ)=W^T*X+b$$\n\n其中，$X$是特征向量，$W$是权重向量，$b$是偏置\n\n**非线性模型：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802200931903.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n比如，卷积神经网络中的卷积运算本身就是一个可以学习**非线性基函数**\n\n\n## 学习准则\n学习准则就是找到可以评价学习成果好坏的准则，如果用函数表示，则成为**损失函数**\n\n**损失函数**是一个非负实数函数，，用$L(y, f (X; θ))$ 来表示，用来量化模型预测和真实标签之间的差异。\n\n常见回归损失函数有：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200802202112854.png)\n常见分类损失函数有：**交叉熵损失函数 （Cross-Entropy Loss Function）**\n\n## 优化算法\n梯度下降\n\n## 关于验证集的一点补充\n\n将数据分为训练集和测试集对于简单的模型和样本来说就够了，但是对于复杂的模型（如CNN）和样本（如图片），就需要将数据集分类三部分：训练集、验证集、测试集\n\n- 训练集（Training Set）：用来训练模型的\n- 验证集（Validation set）：用于对模型的能力进行初步评估，可以作为调参、选择特征等算法相关的选择的依据。\n- 测试集 （Test Set）：用来评估模最终模型的泛化能力，不能作为调参、选择特征等算法相关的选择的依据\n\n就好比考试一样，我们平时做的题相当于训练集，测试集相当于最终的考试，我们通过最终的考试来检验我们最终的学习能力，将测试集信息泄露出去，相当于学生提前知道了考试题目，那最后再考这些提前知道的考试题目，当然代表不了什么，你在最后的考试中得再高的分数，也不能代表你学习能力强。所以说，如果通过**测试集**来调节模型，相当于不仅知道了考试的题目，学生还都学会怎么做这些题了（因为我们肯定会人为的让模型在测试集上的误差最小，因为这是你调整超参数的目的），那再拿这些题考试的话，人人都有可能考满分，但是并没有起到检测学生学习能力的作用。原来我们通过测试集来近似泛化误差，也就是通过考试来检验学生的学习能力，但是由于**信息泄露**，此时的测试集即考试无任何意义，现实中可能学生的能力很差。所以，我们在学习的时候，老师会准备一些小测试来帮助我们查缺补漏，**这些小测试也就是要说的验证集**。我们通过验证集来作为调整模型的依据，这样不至于将测试集中的信息泄露。\n\n- 训练集-----------学生的课本；学生 根据课本里的内容来掌握知识。\n- 验证集------------作业，通过作业可以知道 不同学生学习情况、进步的速度快慢。\n- 测试集-----------考试，考的题是平常都没有见过，考察学生举一反三的能力。\n\n也就是说我们将数据划分训练集、验证集和测试集。在训练集上训练模型，在验证集上评估模型，一旦找到的最佳的参数，就在测试集上最后测试一次，测试集上的误差作为泛化误差的近似。关于验证集的划分可以参考测试集的划分，其实都是一样的，这里不再赘述。\n\n总结\n\n- 机器学习让计算机从数据中进行自动学习，得到某种知识，而不是人为指定且明显的去编程\n- 之所以机器学习，是因为现实世界的问题都比较复杂，很难通过规则来手工实现\n- 机器学习中的基本概念：包括**样本、特征、标签、模型、学习算法**\n- 机器学习的三要素：模型、评价准则、优化算法\n- 训练集、测试集、样本集的概念\n\n> 声明：本文大部分摘录自神邱老师的《神经网络与深度学习》，加上一小部分个人的理解和其他博客的资料，因此将本文声明为**转载**，本文只做学习和交流使用，如有侵权请联系博主删除。\n\n参考文档\n【神经网络与深度学习-邱锡鹏著】\n[训练集、验证集、测试集以及交验验证的理解](https://blog.csdn.net/kieven2008/article/details/81582591)\n[训练集、验证集和测试集](https://zhuanlan.zhihu.com/p/48976706)\n\n\n\n\n","categories":["机器学习"]},{"title":"hexo写博客常见错误及解决方法","url":"/2020/11/24/231533/","content":"\n主要有两个问题：\n\n- hexo复杂latex公式无法显示并报错的问题。\n- hexo关于`#`的转义问题。\n\n<!-- more -->\n\n## 问题1\n\n在写hexo博客的时候，遇到markdown中公式太长太复杂的时候，就老是显示出错，部分错误信息如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124231718.png)\n\n\n\n我报错的地方如下:\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124231907.png)\n\n\n\n**解决方法**\n\n这是一个hexo中的latex公式渲染问题。不能在公式内连续使用两个`左花括号`！，把两个左花括号中间加`空格`分开就行了，即改为：`$\\frac{ {\\partial L} }{ {\\partial W} }$`\n\n你就会看到成功的显示公式啦！！！\n\n另外，用这种方法也可以：[Nunjucks Error: 解决方案](https://blog.csdn.net/weixin_45333934/article/details/108274320)，该方法好像执行`hexo g`速度快一点，可能是心理作用，哈哈！\n\n\n\n## 问题2\n\n当我输入`hexo clean && hexo g`时，出现以下报错信息：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20210210131847.jpg)\n\n这是因为我的文章出现如下字符：\n\n```bash\n`${#name[subscript]}`  \n```\n\n**解决方法**\n\n```bash\n`${#name[subscript]}`     # 报错\n`${&#35;name[subscript]}` # 不会报错，但是&#35;不会被识别为#\n\\${&#35;name[subscript]}   # 报错\n`${`&#35;name[subscript]}  #正确，用这个就行, &#35会被识别为#\n```\n\n\n\n参考文档\n\n[hexo复杂latex公式无法显示问题](https://blog.csdn.net/ALexander_Monster/article/details/106178094)\n\n[Hexo错误”expected end of comment, got end of file”](http://xuejiangtao.com/posts/11a14c34/)\n\n\n\n","categories":["工具"]},{"title":"Google Colab的一些常用命令","url":"/2020/11/24/224504/","content":"\n在此记录一些我遇到Google Colab常用命令，以便以后查阅！该命令colab在线地址为：[colab的一些常用命令](https://drive.google.com/file/d/1JRO9afK-2PQwvudxaDLqcuawZRxZWT-c/view?usp=sharing)\n\n\n<!-- more -->\n\n\n\n\n\n# 基本操作\n（1）colab挂载drive上的数据文件\n推荐参考我的博客[Google Colab挂载drive上的数据文件](https://blog.csdn.net/qq_37555071/article/details/107544680)，这里不就过多赘述了。\n\n（2）查看当前所在路径\n```bash\n!pwd\n```\n\n（3）切换目录\n```bash\n# 后面为要切换的路径，支持相对、绝对路径\n%cd /content/drive/Colab/   \n```\n\n（4）查看当前目录的所有文件名称\n```bash\n!ls  也可以 ls\n```\n\n（5）拷贝文件\n```bash\n# 前面是要拷贝的文件名，后面是拷贝后的文件目录\n!cp -i /content/drive/cat_dog_result.csv /content/\n```\n\n（6）创建文件或文件夹\n```bash\n# 创建dirabc文件夹\nmkdir dirabc\n# 创建test1、test2、test3文件\ntouch test1.txt test2.txt test3.txt\n```\n\n（7）删除文件\n```bash\n#  删除文件夹或文件，后面跟文件夹或文件名称\n!rm -rf test3.txt\n# 也可以删除多个文件\n!rm -rf test1.txt test2.txt test3.txt\n# 删除除了drive的所有文件\nls | grep -v drive | xargs rm -rf\n```\n\n# 解压缩操作\n（1）解压rar文件\n```bash\n! apt-get install rar\n!apt-get install unrar\n# x参数是保存原来的文件架构，e参数是把里面的文件都解压到当前路径下\n# 注意压缩文件时要右键，添加到xxx.rar，不要添加到压缩文件\n! unrar x cat_dog.rar\n```\n\n（2）压缩rar文件\n```bash\n# !rar 压缩后的文件名 要压缩的文件名或文件夹名\n!rar a 123.rar  wxl.jks\n```\n\n（3）解压zip文件\n```bash\n!unzip FileName.zip \n```\n\n（4）压缩zip文件\n```bash\n# !zip 压缩后的文件名 要压缩的文件名或文件夹名\n!zip FileName.zip DirName \n```\n更多解压缩方式可参考：[Unrar, Unzip in colab](https://colab.research.google.com/drive/17Jtj0Mrs0lgWo4zi8jQoiAKunjR0GzwQ?usp=sharing)\n\n\n# 阻止Colab自动掉线\n在colab上训练代码，页面隔一段时间无操作之后就会自动掉线，之前训练的数据都会丢失。不过好在最后终于找到了一种可以让其自动保持不离线的方法，用一个js程序自动点击连接按钮。代码如下：\n\n```js\nfunction ClickConnect(){\n  console.log(\"Working\"); \n  document\n    .querySelector(\"#top-toolbar > colab-connect-button\")\n    .shadowRoot\n    .querySelector(\"#connect\")\n    .click()\n}\n \nsetInterval(ClickConnect,60000)\n```\n\n使用方式是：按快捷键`ctrl+shift+i`，并选择`Console`，然后复制粘贴上面的代码，并点击回车，该程序便可以运行了，如下所示：\n\n![](https://img-blog.csdnimg.cn/img_convert/a812f4f6edd1c69db0f38e73e2ac11d1.png)\n\n\n参考文档\n[linux下解压命令大全](https://blog.csdn.net/xsfqh/article/details/89448976)","tags":["colab"],"categories":["工具"]},{"title":"为什么MobileNet及其变体（如ShuffleNet）会变快？","url":"/2020/11/24/224323/","content":"\n> 本文是转载文章，转载自[深入剖析：为什么MobileNet及其变体（如ShuffleNet）会变快？](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247506133&idx=2&sn=1082d154907cedb3ebea028df3787d8e&chksm=ec1c352cdb6bbc3af142fe776b2c9ac73d831d946eda8cc45c6abf9cdd6c4abaef82eeb0c250&mpshare=1&scene=1&srcid=0828Av9UvPz9AbUBnyi3wnAc&sharer_sharetime=1599125590114&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=c45d238be947117fcf9d09c202668aad75e244db66c43fc3a6c9ec8bcc9fc50c2cea193203b229ce4e107aea09fe33bfdf0d8528a563618300689838ca76ab39f9ee3393d565a0a091b9713ef47b7273dd8c99b377abc7d00d8cbc662ebffd68d8fafa2b606af324cd1df8fed9a07a6d523be8a08c4323a195123680a9aa8a75&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AcmLpoaSOHY8/ugr1Vi8OdE=&pass_ticket=3cYbF3kWoNM5K54gjR4tlKnJ8nzIvZSHzY/x9JiBuxWbw3Pu9NJ1BcsIB%2byab7wA&wx_header=0)，**删除了文中冗余的部分，加入许多自己的理解，有些部分也通过pytorch进行了实现，并通过引入具体的计算更清晰的反映出轻量级神经网络的本质**。\n\n\n<!-- more -->\n\n\n\n\n\n\n## 前言\n\n从MobileNet等CNN模型的组成部分出发，概述了**高效CNN模型**（如MobileNet及其变体）中**使用的组成部分**（building blocks），并解释了**它们如此高效的原因**。特别地，我提供了关于如何在**空间和通道域**进行卷积的直观说明。\n\n## 高效CNN模型的组成部分\n在解释具体的高效CNN模型之前，我们先检查一下高效CNN模型中使用的组成部分的计算量，看看卷积在空间和通道域中是如何进行的。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903215104929.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n假设 H x W 为输出feature map的空间大小，N为输入通道数，K x K为卷积核的大小，M为输出通道数，则标准卷积的计算量为 H\\*W\\*N\\*K\\*K\\*M 。这里重要的一点是，**标准卷积的计算量与(1)输出特征图H x W的空间大小，(2)卷积核K x K的大小，(3)输入输出通道的数量N x M成正比**。当在空间域和通道域进行卷积时，需要上述计算量。通过分解这个卷积，可以加速 CNNs，如上图所示。\n\n## 卷积\n首先，我提供了一个直观的解释，关于空间和通道域的卷积是如何对进行标准卷积的，它的计算量是H\\*W\\*N\\*K\\*K\\*M。\n\n这里连接输入和输出之间的线，以可视化输入和输出之间的依赖关系。直线数量大致表示空间（spatial）和通道（channel）域中卷积的计算量。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903215447602.png#pic_center)\n例如，最常用的卷积——conv3x3，可以如上图所示。我们可以看到，输入和输出在**空间域是局部连接的，而在通道域是全连接的**。你可能看的不太明白，那我们换张图试试，下面是空间域中输入和输出的关系，可以看出空间域确实是局部连接的。**空间域可以想象为，把特征图输入和输出的神经元拉平，然后进行连接，事实上，在计算机内部就是这么做的。**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904092416818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n下面是通道域中输入和输出的关系（隐藏红线框部分），下图输入通道是3，输出通道是1，输入的3个通道都连接在了输出的1个通道上，这也证明了通道域是全连接的。如果输出通道有多个，也能想象出来吧。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731150746946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n接下来，如上所示用于改变通道数的conv1x1，或pointwise convolution。由于kernel的大小是1x1，所以这个卷积的计算量是 H\\*W\\*N\\*M，（H x W 为输出feature map的空间大小，N为输入通道数，M为输出通道数），计算量比conv3x3降低了1/9。**这种卷积被用来“混合”通道之间的信息**。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903220728682.png#pic_center)\n## 分组卷积（Grouped Convolution）\n分组卷积是卷积的一种变体，将输入的feature map的**通道分组**，对每个分组的通道独立地进行卷积。\n\n假设 G 表示组数，分组卷积的计算量为 H\\*W\\*N\\*K\\*K\\*M/G，计算量变成标准卷积的1/G。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903221031750.png#pic_center)\n举个例子，假如，卷积核大小为3x3，输入通道数为10，输出通道数为20，输出特征图大小为15x15，对于不分组情况下，计算量是`15*15*10*3*3*20=405000`，如果把输入的feature map的通道分为2组，即输入通道数为10分为2组，每组的输入通道数为5，每组输出通道数为10，输出的总通道数为20，则计算量为`15*15*5*3*3*10*2=202500`，可以看到计算量变成标准卷积的1/2。\n\n在conv3x3 而且 G=2的情况。我们可以看到，通道域中的连接数比标准卷积要小，说明计算量更小。\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903222640830.png#pic_center)\n在 conv3x3，G=3的情况下，**连接变得更加稀疏**。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090322265421.png#pic_center)\n在 conv1x1，G=2的情况下，conv1x1也可以被分组。**这种类型的卷积被用于ShuffleNet中**。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903222726588.png#pic_center)\n## 深度卷积（Depthwise Convolution）\n在**深度卷积**中，对每个输入通道分别进行卷积。**它也可以定义为分组卷积的一种特殊情况，其中输入和输出通道数相同，G等于通道数**。不得不提的是，**Depthwise Convolution在通道域上把计算量降低到了极致**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904093441110.png#pic_center)\n如上所示，depthwise convolution 通过省略通道域中的卷积，大大降低了计算量。\n\n## Channel Shuffle\nChannel shuffle是一种操作(层)，它改变 ShuffleNet 中**使用的通道的顺序**。这个操作是通过张量reshape和 transpose 来实现的。\n\n\n假设G表示分组卷积的组数，N表示输入通道的数量，首先将输入通道的维数reshape 为(G, N ')，即G\\*N '=N，然后将(G, N ')转置为(N '， G)，最后将其view成与输入相同的形状。pytorch实现如下：\n\n```python\nimport torch\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n \n    channels_per_group = num_channels // groups\n \n    # reshape\n    x = x.view(batchsize, groups,\n               channels_per_group, height, width)\n \n    # transpose\n    # - contiguous() required if transpose() is used before view().\n    #   See https://github.com/pytorch/pytorch/issues/764\n    x = torch.transpose(x, 1, 2).contiguous()\n \n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n\nx = torch.randn(1,10,224,224)\nx = channel_shuffle(x,2)\nx.size()\n# 输出：torch.Size([1, 10, 224, 224])\n```\n\n虽然Channel Shuffle的操作不能像计算卷积那样来定义计算量，但应该有一些开销。\n\nG=2时的channel shuffle 情况如下，这里无卷积操作，只是改变了通道的顺序。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904111231327.png#pic_center)\n打乱的通道数 G=3，情况如下\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904111255450.png#pic_center)\n\n\n\n\n## 高效的CNN模型\n\n下面，对于高效的CNN模型，我将直观地说明为什么它们是高效的，以及如何在空间和通道域进行卷积。\n\n### ResNet (Bottleneck Version)\nResNet 中使用的**带有bottleneck 架构的残差单元**是与其他模型进行进一步比较的良好起点。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904095643782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n如上所示，具有bottleneck架构的残差单元由conv1x1、conv3x3、conv1x1组成。第一个conv1x1减小了输入通道的维数，降低了随后的conv3x3的计算量。最后的conv1x1恢复输出通道的维数。\n\n### ResNeXt\nResNeXt是一个高效的CNN模型，可以看作是ResNet的一个特例，将conv3x3替换为**成组**的conv3x3。**通过使用有效的分组conv**，与ResNet相比，conv1x1的通道减少率变得适中（可以让conv1x1不用降维那么多，因为用分组conv已经降低了很多计算量了），从而在相同的计算代价下获得更好的精度。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904095826862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n### MobileNet (Depthwise Separable Conv)\nMobileNet是一个**深度可分离卷积**模块的堆叠，由depthwise conv和conv1x1 (pointwise conv)组成。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090410015854.png#pic_center)\n**深度可分离卷积分别在空间域和通道域独立执行卷积**。这种卷积分解显著降低了计算量，从**H\\*W\\*N\\*K\\*K\\*M** 降低到 **H\\*W\\*N\\*K\\*K**(depthwise) + **H\\*W\\*N\\*M**(conv1x1)=**HWN(K² + M)**。一般情况下，输出通道M远远大于卷积核大小K(如K=3和M≥32)，减小率约为1/8-1/9。\n\n这里你可能对于上述计算比较懵，那我们分解一下吧。\n\n- 假设 H x W 为输出feature map的空间大小，N为输入通道数，K x K为卷积核的大小，M为输出通道数，则标准卷积的计算量为 H\\*W\\*N\\*K\\*K\\*M。\n- depthwise conv可以看做分组卷积的一种特殊情况，其中输入和输出通道数相同，分组数G等于通道数，其计算量为H\\*W\\*N\\*K\\*K\\*N/N=H\\*W\\*N\\*K\\*K。\n- conv1x1的输入通道数是N，输出通道数是M，计算量为H\\*W\\*N\\*M。\n\n### ShuffleNet\nShuffleNet的动机是如上所述，**conv1x1是在空间域上已经把计算量降低到了极致**，所以在空间域上已经没有改进的空间，而**分组conv1x1可以在通道域上再次降低计算量**。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904102338403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n上图说明了用于ShuffleNet的模块。这里重要的使用的组成部分（building blocks）是channel shuffle层，它在分组卷积中对通多在组间的顺序进行“shuffles”。**如果没有channel shuffle，分组卷积的输出在组之间就不会被利用，导致精度下降**。\n\n### MobileNet-v2\nMobileNet-v2采用类似ResNet中带有bottleneck架构残差单元的模块架构；并用深度卷积（depthwise convolution）代替conv3x3，是残差单元的改进版本。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904112511274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n从上面可以看到，与标准的 bottleneck 架构相反，第一个conv1x1增加了通道维度，然后执行depthwise conv，最后一个conv1x1减少了通道维度。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904112551109.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n通过如上所述对组成部分（building blocks） 进行重新排序，将其与MobileNet-v1(Depthwise Separable Conv)进行比较，我们可以看到这个体系结构（MobileNet-v2）是如何工作的 (这种重新排序不会改变整个模型体系结构，因为MobileNet-v2是这个模块的**堆叠**，所以不会有影响的)。\n\n也就是说，上述模块可以看作是深度可分离卷积的一个改进版本，其中可分离卷积中的单个conv1x1被分解为两个conv1x1。让T表示通道维数的**扩展因子**，两个 conv1x1 的计算量为 2\\*H\\*W\\*N\\*N/T ，而深度可分离卷积下的conv1x1的计算量为 HWN²。如果T = 6，将 conv1x1 的计算成本降低了3倍(一般为T/2)。\n\n这里可能有人看的不太理解，我们来详细算一下吧（以上图为例子）。\n\n上图中第一个depthwise conv，第二个和第三个为conv1x1，假设第二个Conv1x1的输入通道为N，则第二个Conv1x1输出通道就为N/T，因为第二个Conv1x1是**经过扩展因子扩大了T倍**，则第三个Conv1x1的输入通道是N/T，则第三个Conv1x1的输出通道是N。\n\n这样就可以算出，第二个Conv1x1的计算量为`H*W*N*N/T`，第三个Conv1x1的计算量为`H*W*(N/T)*N`，两个Conv的总计算量为**2\\*H\\*W\\*N\\*N/T**，而深度可分离卷积下的conv1x1的计算量为 HWN²，如果T = 6，成本计算成本就是降低了3蓓。\n\n可能有人要问，PW升维不是增加参数量了么，你这么一算咋减小了？是的，用PW**升维**是增加了一部分参数量，不过正因为是1x1Conv，所以增加的参数量并不多。上面，我们对组成部分（building blocks） 进行重新排序并进行了计算，在**高维度下**两个conv1x1比**低维度下**深度可分离卷积下的conv1x1参数可降低了不少呢！\n\n### FD-MobileNet\n\n最后，介绍 Fast-Downsampling MobileNet (FD-MobileNet)。在这个模型中，与MobileNet相比，下采样在较早的层中执行。这个简单的技巧可以降低总的计算成本。\n\n从VGGNet开始，许多模型采用相同的下采样策略：**执行向下采样，然后将后续层的通道数增加一倍**。对于标准卷积，下采样后计算量不变，因为根据定义得 H\\*W\\*N\\*K\\*K\\*M 。而对于深度可分离卷积，下采样后其计算量减小：由 **HWN(K² + M)** 降为 **(H/2)\\*(W/2)\\* (2N)\\*(K² + 2M)** = HWN(K²/2 + M)。当M不是很大时(即较早的层)，这是相对占优势的。注意：这里的2N和2M是因为执行向下采样后，后续层的通道数了增加一倍。\n\n\n**下面是对全文的总结：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200905112522639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n","tags":["MobileNet"],"categories":["神经网络"]},{"title":"DenseNet密集卷积网络详解（附代码实现）","url":"/2020/11/24/224154/","content":"\n## 前言\nDenseNet是CVPR2017的最佳论文，由康奈尔大学黄高博士（Gao Huang）、清华大学本科生刘壮（Zhuang Liu）、Facebook 人工智能研究院研究科学家 Laurens van der Maaten 及康奈尔大学计算机系教授 Kilian Q. Weinberger 所作，有兴趣的同学可以结合[原文](https://arxiv.org/pdf/1608.06993.pdf)阅读。\n\n<!-- more -->\n\n\n\n\nResNet通过前层与后层的“**短路连接**”（Shortcuts），加强了前后层之间的信息流通，在一定程度上缓解了梯度消失现象，从而可以将神经网络搭建得很深，具体可以参考[ResNet残差网络及变体详解](https://blog.csdn.net/qq_37555071/article/details/108258862)。更进一步，这次的主角DenseNet最大化了这种前后层信息交流，**通过建立前面所有层与后面层的密集连接，实现了特征在通道维度上的复用，不但减缓了梯度消失的现象，也使其可以在参数与计算量更少的情况下实现比ResNet更优的性能**。连接方式可以看下面这张图：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903105033190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n标准的 L 层卷积网络有 $L$ 个连接，即每一层与它的前一层和后一层相连，而DenseNet将前面所有层与后面层连接，故有 $(1+2+...+L)*L=(L+1)*L/2$ 个连接。这里看完有些摸不着头脑没关系，接下来我们会具体展开。\n\n## Dense Block\nDense Block是DenseNet的一个基本模块，这里我们从一般的神经网络说起：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090311000558.png#pic_center)\n上图是标准神经网络的一个图，输入和输出的公式是$X_l = H_l(X_{l-1})$，其中$H_l$是一个组合函数，通常包括BN、ReLU、Pooling、Conv操作，$X_{l-1}$是第 $l$ 层输入的特征图，$X_{l}$是第 $l$ 层输出的特征图。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903110339759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n上图则是ResNet的示意图，我们知道ResNet是跨层相加，输入和输出的公式是$X_l = H_l(X_{l-1})+X_{l-1}$\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903110523502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n而对于DesNet，则是采用跨通道concat的形式来连接，用公式来说则是$X_l = H_l(X_0,X_1,...,X_{l-1}$)，这里要注意所有的层的输入都来源于前面所有层在channel维度的concat，我们用一张动图体会一下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090311071451.gif#pic_center)\n特征传递方式是**直接将前面所有层的特征concat后传到下一层，而不是前面层都要有一个箭头指向后面的所有层**，这与具体代码实现是一致的，后面会具体的实现。\n\n这里要注意，**因为我们是直接跨通道直接做concat，所以这里要求不同层concat之前他们的特征图大小应当是相同的**，所以DenseNet分为了好几个Dense Block，**每个Dense Block内部的feature map的大小相同**，而每个Dense Block之间使用一个Transition模块来进行下采样过渡连接，这个后文会介绍。\n\n## Growth rate\n假如输入特征图的channel为$K_0$，那么第 $l$ 层的channel数就为 $K_0+(l-1)K$，我们将其称之为网络的增长率（growth rate）。因为每一层都接受前面所有层的特征图，即特征传递方式是**直接将前面所有层的特征concat后传到下一层**，所以这个$K$不能很大，要注意这个K的实际含义就是这层新提取出的特征。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903111854807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n## Bottleneck\n\n在刚才Dense Block中的非线性组合函数是指**BN+ReLU+3x3 Conv**的组合，尽管每前进一层，只产生K张新特征图，但还是嫌多，于是**在进行3×3卷积之前先用一个 1×1卷积将输入的特征图个数降低到 4\\*k**，我们发现这个设计对于DenseNet来说特别有效。所以我们的非线性组合函数就变成了**BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv**的结构，由此形成的网络结构我们称之为**DenseNet-B**。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903164547925.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n**增加了1x1的卷积的Dense Block也称为Bottleneck结构**，实现细节如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903114842520.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n有以下几个细节需要注意：\n\n1. 每一个Bottleneck输出的特征通道数是相同的，例如这里的K=32。同时可以看到，经过concat操作后的通道数是按K的增长量增加的，因此这个K也被称为GrowthRate。\n2. 这里1×1卷积的作用是**固定输出通道数，达到降维的作用**，1×1卷积输出的通道数通常是GrowthRate的4倍。当几十个Bottleneck相连接时，concat后的通道数会增加到上千，如果不增加1×1的卷积来降维，后续3×3卷积所需的参数量会急剧增加。比如，输入通道数64，增长率K=32，经过15个Bottleneck，通道数输出为`64+15*32=544`，再经过第16个Bottleneck时，如果不使用1×1卷积，第16个Bottleneck层参数量是`3*3*544*32=156672`，如果使用1×1卷积，第16个Bottleneck层参数量是`1*1*544*128+3*3*128*32=106496`，可以看到参数量大大降低。\n3. Dense Block采用了激活函数在前、卷积层在后的顺序，即BN-ReLU-Conv的顺序，这种方式也被称为**pre-activation**。通常的模型relu等激活函数处于卷积conv、批归一化batchnorm之后，即Conv-BN-ReLU，也被称为post-activation。作者证明，如果采用post-activation设计，性能会变差。想要更清晰的了解pre-activition，可以参考我的博客[ResNet残差网络及变体详解](https://blog.csdn.net/qq_37555071/article/details/108258862)中的Pre Activation ResNet。\n\n\n\n\n\n\n\n## Transition layer\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903113233261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n两个相邻的Dense Block之间的部分被称为**Transition层**，具体包括BN、ReLU、1×1卷积、2×2平均池化操作。**通过1×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。**\n\n## 压缩因子\n为进一步提高网络的紧密度，我们可以在转换层(transition layers)减少feature-maps的数量。我们引入一个压缩因子$\\theta$，假定上一层得到的feature map的channel大小为$m$，那经过Transition层就可以产生 $\\theta m$ 个特征，其中$\\theta$在0和1之间。在**DenseNet-C**中，我们令$\\theta$=0.5。当模型结构即含瓶颈层，又含压缩层时，我们记模型为DenseNet-BC。\n\n## DenseNet网络结构\nDenseNet网络构成如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903151310711.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n上图中，增长率K=32，采用pre-activation，即BN-ReLU-Conv的顺序。\n\n以DenseNet-121为例，看下其网络构成：\n1. DenseNet-121由121层权重层组成，其中4个Dense block，共计2×(6+12+24+16) = 116层权重，加上初始输入的1卷积层+3过渡层+最后输出的全连接层，共计121层；\n2. 训练时采用了DenseNet-BC结构，压缩因子0.5，增长率k = 32；\n3. 初始卷积层有2k个通道数，经过7×7卷积将224×224的输入图片缩减至112×112；Denseblock块由layer堆叠而成，layer的尺寸都相同：1×1+3×3的两层conv（每层conv = BN+ReLU+Conv）；Denseblock间由过渡层构成，过渡层通过1×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽。最后经过全局平均池化 + 全连接层的1000路softmax得到输出。\n\n\n## DenseNet优缺点\nDenseNet的优点主要有3个：\n\n1. **更强的梯度流动**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903151844516.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\nDenseNet可以说是一种隐式的**强监督模式**，因为每一层都建立起了与前面层的连接，误差信号可以很容易地传播到较早的层，所以较早的层可以从最终分类层获得直接监管（监督）。\n2. **能够减少参数总量**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903152017839.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n3.**保存了低维度的特征**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903152107501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n在标准的卷积网络中，最终输出只会利用提取最高层次的特征。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903152120395.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n而**在DenseNet中，它使用了不同层次的特征，倾向于给出更平滑的决策边界**。这也解释了为什么训练数据不足时DenseNet表现依旧良好。\n\n\n\n**DenseNet的不足在于由于需要进行多次Concatnate操作，数据需要被复制多次，显存容易增加得很快，需要一定的显存优化技术。另外，DenseNet是一种更为特殊的网络，ResNet则相对一般化一些，因此ResNet的应用范围更广泛。**\n\n\n## 实验效果\n这里给出DenseNet在CIFAR-100和ImageNet数据集上与ResNet的对比结果，首先来看下**DenseNet与ResNet在CIFAR-100数据集上实验结果**，如下图所示，可以看出，只有0.8M大小的DenseNet-100性能已经超越ResNet-1001，并且后者参数大小为10.2M。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200903181215171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n下面是**DenseNet与ResNet在ImageNet数据集上的比较**，可以看出，同等参数大小时，DenseNet也优于ResNet网络。其它实验结果见原论文。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090318171124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n## Pytorch实现DenseNet\n首先实现DenseBlock中的内部结构，这里是BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv结构，最后也加入dropout层以用于训练过程。\n\n```python\nclass _DenseLayer(nn.Sequential):\n    \"\"\"Basic unit of DenseBlock (using bottleneck layer) \"\"\"\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\"norm1\", nn.BatchNorm2d(num_input_features))\n        self.add_module(\"relu1\", nn.ReLU(inplace=True))\n        self.add_module(\"conv1\", nn.Conv2d(num_input_features, bn_size*growth_rate,\n                                           kernel_size=1, stride=1, bias=False))\n        self.add_module(\"norm2\", nn.BatchNorm2d(bn_size*growth_rate))\n        self.add_module(\"relu2\", nn.ReLU(inplace=True))\n        self.add_module(\"conv2\", nn.Conv2d(bn_size*growth_rate, growth_rate,\n                                           kernel_size=3, stride=1, padding=1, bias=False))\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate)\n        # 在通道维上将输入和输出连结\n        return torch.cat([x, new_features], 1)\n```\n据此，实现DenseBlock模块，内部是密集连接方式（输入特征数线性增长）：\n\n```python\nclass _DenseBlock(nn.Sequential):\n    \"\"\"DenseBlock\"\"\"\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features+i*growth_rate, growth_rate, bn_size,\n                                drop_rate)\n            self.add_module(\"denselayer%d\" % (i+1), layer)\n```\n此外，实现Transition层，它主要是一个卷积层和一个池化层：\n\n```python\nclass _Transition(nn.Sequential):\n    \"\"\"Transition layer between two adjacent DenseBlock\"\"\"\n    def __init__(self, num_input_feature, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\"norm\", nn.BatchNorm2d(num_input_feature))\n        self.add_module(\"relu\", nn.ReLU(inplace=True))\n        self.add_module(\"conv\", nn.Conv2d(num_input_feature, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\"pool\", nn.AvgPool2d(2, stride=2))\n```\n最后我们实现DenseNet网络：\n\n```python\nclass DenseNet(nn.Module):\n    \"DenseNet-BC model\"\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64,\n                 bn_size=4, compression_rate=0.5, drop_rate=0, num_classes=1000):\n        \"\"\"        \n        :param growth_rate: 增长率，即K=32\n        :param block_config: 每一个DenseBlock的layers数量，这里实现的是DenseNet-121\n        :param num_init_features: 第一个卷积的通道数一般为2*K=64\n        :param bn_size: bottleneck中1*1conv的factor=4，1*1conv输出的通道数一般为factor*K=128\n        :param compression_rate: 压缩因子\n        :param drop_rate: dropout层将神经元置0的概率，为0时表示不使用dropout层\n        :param num_classes: 分类数\n        \"\"\"\n        super(DenseNet, self).__init__()\n        # first Conv2d\n        self.features = nn.Sequential(OrderedDict([\n            (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\"norm0\", nn.BatchNorm2d(num_init_features)),\n            (\"relu0\", nn.ReLU(inplace=True)),\n            (\"pool0\", nn.MaxPool2d(3, stride=2, padding=1))\n        ]))\n\n        # DenseBlock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers, num_features, bn_size, growth_rate, drop_rate)\n            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n            num_features += num_layers*growth_rate\n            if i != len(block_config) - 1:\n                transition = _Transition(num_features, int(num_features*compression_rate))\n                self.features.add_module(\"transition%d\" % (i + 1), transition)\n                num_features = int(num_features * compression_rate)\n\n        # final bn+ReLU\n        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n        self.features.add_module(\"relu5\", nn.ReLU(inplace=True))\n\n        # classification layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # params initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.avg_pool2d(features, 7, stride=1).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n```\n\n【参考文档】\n[深入解析DenseNet(含大量可视化及计算)](https://mp.weixin.qq.com/s?__biz=MzUzNzk2ODUyNQ==&mid=2247484110&idx=1&sn=0f415bac7335342fd151cffcd1dc502d&chksm=fadfa82ccda8213aedac938b58dca11ec0e8e0d3a9205f4efca844f77c92f12225920bef741d&mpshare=1&scene=1&srcid=0903APUhFluNU7752uehJ7pM&sharer_sharetime=1599098864114&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=af1f462991b3ae2f73d8edd600c4378b767aff3e954a4656a4eeb460f16939d828393da9cba16ed045c655c8fb207f7dbd13c2de88325f562ff92a6fcd2d95cf6ecbc5a4bccc2a8c973ffd995c75134146c311897ad6086c17c304e1fa2484da63fb4df1767131ed3f1605e2a6917c79b142a52b6737422fdebf716e9939e540&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AW14wywO1LP2LC9IpvKu9xI=&pass_ticket=mWKUrpjSxXDNNq6Wxeq9z%2bpdzjoRwb%2bsOY1LPtVaBwUd%2bbTos55%2bq/sa%2b%2bkvdHSF&wx_header=0)\n[来聊聊DenseNet及其变体PeleeNet、VoVNet](https://mp.weixin.qq.com/s?__biz=MzI0NDYxODM5NA==&mid=2247484785&idx=1&sn=fcfd18eff932853b4d3f7e7cec22d3d6&chksm=e95a4084de2dc992791d204a1232d237800c5e9f8166d000a16d54360bb60aba0fb6f9645f50&mpshare=1&scene=1&srcid=0829OeTUIOQO2urTTN8ArxN1&sharer_sharetime=1598666372095&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=029d4162e347a3ad8256a33675540963e9543dda8391a5f64379beeee4b8e75a04d1c796a2cac2066bb9a0384e2dc86527c3af81fcf09176d74d582e0a638e3d1dd6e86cb094342d867957c69fc56d0eb8f99f62d3df3f0491f44d396211630f7eda98e0645c1a23ee20847187d31d745b19eac23f4bd9d92a2fe5295788d935&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AQTz6sAlVMxA9IAp9ltR15Q=&pass_ticket=VOgSQ7cGMld09vU35rB0cjJS6QZUBmypkhw5kxq/X%2bFElM8Ehh63InCpsqo8aHzC)\n[稠密连接网络（DenseNet）](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.12_densenet)\n[深度学习网络篇——DenseNet](https://blog.csdn.net/weixin_43624538/article/details/85227041?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159866029119724825707695%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=159866029119724825707695&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v1~rank_blog_v1-1-85227041.pc_v1_rank_blog_v1&utm_term=densenet&spm=1018.2118.3001.4187)\n[论文笔记DenseNet](https://blog.csdn.net/ChuiGeDaQiQiu/article/details/79512257)\n[DenseNet：比ResNet更优的CNN模型](https://zhuanlan.zhihu.com/p/37189203)\nDensely Connected Convolutional Networks\n","tags":["DenseNet"],"categories":["神经网络"]},{"title":"细粒度分析与Bilinear CNN model（附代码实现）","url":"/2020/11/24/224009/","content":"\n## 前言\n\n\n有时，我们逛街时看到不同的狗，却不知道其具体品种，看到路边开满鲜花，却傻傻分不清具体是什么花。实际上，类似的问题在实际生活中屡见不鲜，人类尚且如此，更别说人工智能了。为了解决这一问题，研究者们提出了**细粒度分析**（fine-grained image analysis）这一专门研究物体精细差别的方向。\n\n<!-- more -->\n\n\n\n## 细粒度分析\n\n**细粒度分析**任务相较于**通用图像**（general/generic images）任务的区别和难点在于**其图像所属类别的粒度更为精细**。下图为例，通用图像分类其任务诉求是将“袋鼠”和“狗”这两个物体大类（蓝色框和红色框中物体）分开，可见无论从样貌、形态等方面，二者还是能很容易被区分；而细粒度图像分类任务则要求对“狗”类别下细粒度的子类，即分别对“哈士奇”和“爱斯基摩犬”的图像分辨出来。正因同类别物种的不同子类往往仅在耳朵形状、毛色等细微处存在差异，可谓“差之毫厘，谬以千里”。不止对计算机，对普通人来说，细粒度图像任务的难度和挑战无疑也更为巨大。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902104417284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n近年来，随着AI的发展，深度学习方面的细粒度图像分析任务可分为**细粒度图像分类**（fine-grained image classification）和 **细粒度图像检索**（fine-grained image retrieval）两大经典图像研究方向。\n\n\n**细粒度图像分类**\n\n由于细粒度物体的差异仅体现在细微之处，**如何有效地对图像进行分析检测，并从中发现重要的局部区域信息，成为了细粒度图像分类算法要解决的关键问题**。对细粒度分类模型，可以按照其使用的监督信息的多少，分为“基于强监督信息的分类模型”和“基于弱监督信息的分类模型”两大类。\n\n\n**基于强监督信息的细粒度图像分类模型**\n\n所谓“强监督细粒度图像分类模型”是指，在模型训练时，为了获得更好的分类精度，除了图像的类别标签（label）外，还使用了物体标注框（object bounding box）和 部位标注点（part annotation）等额外的人工标注信息，这点与目标检测（Detection）与图像分割（Segmentation）的含义是相同的，如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902105325386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n常见的强监督信息细粒度分类的经典模型有Part-based R-CNN、Pose Normalized CNN、Mask-CNN，这里不再详细赘述。\n\n\n**基于弱监督信息的细粒度图像分类模型**\n\n基于强监督信息的分类模型虽然取得了较满意的分类精度，但由于标注信息的获取代价十分昂贵，在一定程度上也局限了这类算法的实际应用。因此，目前细粒度图像分类的一个明显趋势是，希望在模型训练时仅使用**图像级别标注信息**（即图片的label），而不再使用额外的object bounding box和part annotation信息，也能取得与强监督分类模型可比的分类精度，这便是“基于弱监督信息的细粒度分类模型”。思路同强监督分类模型类似，基于弱监督信息的细粒度分类模型也需要借助**全局和局部信息来做细粒度级别的分类**。而区别在于，弱监督细粒度分类希望在不借助object bounding box和part annotation的情况下，**也可以做到较好的局部信息的捕捉**。当然，在分类精度方面，目前最好的弱监督分类模型仍与最好的强监督分类模型存在差距（分类准确度相差约1～2%）。常见的基于弱监督信息的细粒度图像分类模型有Two Level Attention Model、Constellations、Bilinear CNN model。\n\n\n**细粒度图像检索**\n\n图像分析中除监督环境下的分类任务，还有另一大类经典任务——无监督环境下的图像检索。图像检索（image retrieval）按检索信息的形式，分为“以文搜图”（text-based）和“以图搜图”（image-based），这里具体就不介绍了。\n\n\n\n## Bilinear CNN model\n\n**双线性模型**（Bilinear CNN model）是基于弱监督信息的细粒度图像分类模型，在2015与Bilinear CNN Models for Fine-grained Visual Recognition》被提出来用于fine-grained分类。\n\n我们知道，深度学习成功的一个重要精髓，就是将原本分散的处理过程，如特征提取，模型训练与决策等，整合进了一个完整的系统，进行端到端的整体优化训练，不需要人来干预了。并且，对于图像的不同特征，我们常用的方法是进行**连接**（特征图尺寸不变，通道数增加），或者进行**加和**（特征图同一位置像素值相加，通道数不变），或者max-pooling（通道数不变，特征图尺寸变小）。\n\n\n研究者们通过研究人类的大脑发现，人类的视觉处理主要有两条pathway（通路），一条是the ventral stream，对物体进行识别，另一条是the dorsal stream，为了发现物体的位置。作者基于这样的思想，希望能够将两个不同特征进行融合来共同发挥作用，提高细粒度图像的分类效果，即希望两个特征能分别表示图像的位置和对目标进行识别，模型框架如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902112409503.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n> 一种对Bilinear CNN模型的解释是，**网络A的作用是对物体／部件进行定位，即完成物体与局部区域检测工作，而网络B则是用来对网络A检测到的物体位置进行特征提取**。两个网络相互协调作用，完成了细粒度图像分类过程中两个最重要的任务：**物体、局部区域的检测与特征提取**。\n\n两个不同的stream代表着通过CNN得到的不同特征，然后将两个特征进行bilinear操作。一个 bilinear CNN model 由四元组构成，$\\beta=(f_A,f_B,P,C)$，其中$f_A$ 和 $f_B$ 代表特征提取函数，即网络中的A、B，$P$ 是一个池化函数（pooling function），C表示分类器。图像 $I$ 的特征的每一个位置 $l$，进行如下计算：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902160557998.png#pic_center)\n**具体来讲，过程如下：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902113008719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n就是先把在特征图同一位置上的特征进行矩阵相乘，得到矩阵 $b$ ，对所有位置的 $b$ 进行sum pooling（也可以是 max pooling，但一般采用 sum pooling 以方便进行矩阵运算）得到矩阵 $\\xi$。比如，对于一个CNN来讲，输入的特征图有c个通道数，那么在位置 $I$ 上的特征就是1\\*c 的大小，然后与同一位置上，不用CNN得到的 1\\*c 的矩阵进行乘积，得到c*c矩阵，然后将所有位置上的 c\\*c 的矩阵进行求和（就得到了 $\\xi$ ），再转换成向量的形式就可以得到Bilinear vector，即上图中的$x$  。对 $x$ 进行 $y=sign(x)\\sqrt{|x|}$ 操作，再对得到的 $y$ 进行 L2归一化操作 $z=y/||y||_2$，然后我们就可以把特征 $z$ 用于**细粒度图像分类**（fine-grained image classification）了。\n\n>L2归一化操作具体参考[L2范数归一化](https://blog.csdn.net/geekmanong/article/details/51344732)。\n\n\n上面的解释可能看的很懵，举一个例子来说明吧。如使用VGG Conv5_3 输出特征图维度为12\\*12\\*512（特征图大小12\\*12，有512个通道），则特征图共有12\\*12=144个位置，每个位置的特征维度为1\\*512，将两个特征图同一位置的512\\*1与1\\*512的矩阵相乘，得到该位置特征向量维度为512\\*512。文章中使用**所有位置特征向量之和**对其进行池化，故将144个512\\*512的特征向量相加，最终得到512\\*512的双线性特征。 该过程可以使用矩阵乘法实现，将特征图变形为144\\*512的特征矩阵，之后其转置与其相乘，得到512*512的双线性特征向量。\n\n\n```python\n# 实现方式：pytorch \nx = torch.randn(1,512,12,12)\nbatch_size = x.size(0)\nfeature_size = x.size(2)*x.size(3)\nx = x.view(batch_size , 512, feature_size)\nx = (torch.bmm(x, torch.transpose(x, 1, 2)) / feature_size).view(batch_size, -1)\nx = torch.nn.functional.normalize(torch.sign(x) * torch.sqrt(torch.abs(x) + 1e-10))\n```\n\n>关于torch.bmm参考[torch.bmm()函数解读](https://blog.csdn.net/qq_40178291/article/details/100302375)\n\n\nBilinear CNN model的形式简单，便于梯度反向传播，进而实现端到端的训练。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200902154836140.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n另外，值得一提的是，bilinear模型由于其优异的泛化性能，不仅在细粒度图像分类上取得了优异效果，还被用于其他图像分类任务，如行人重检测（person Re-ID）。\n\n\n## 代码实现\n\n通过引用resnet18的特征提取部分，实现Bilinear CNN model，如下所示：\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net,self).__init__()\n        self.features = nn.Sequential(resnet18().conv1,\n                                     resnet18().bn1,\n                                     resnet18().relu,\n                                     resnet18().maxpool,\n                                     resnet18().layer1,\n                                     resnet18().layer2,\n                                     resnet18().layer3,\n                                     resnet18().layer4)\n        self.classifiers = nn.Sequential(nn.Linear(512**2,14))\n        \n    def forward(self,x):\n        x=self.features(x)\n        batch_size = x.size(0)\n        feature_size = x.size(2)*x.size(3)\n        x = x.view(batch_size , 512, feature_size)\n        x = (torch.bmm(x, torch.transpose(x, 1, 2)) / feature_size).view(batch_size, -1)\n        x = torch.nn.functional.normalize(torch.sign(x)*torch.sqrt(torch.abs(x)+1e-10))\n        x = self.classifiers(x)\n        return x\n```\n\n\n【参考文档】\n1. [「见微知著」——细粒度图像分析进展综述](https://zhuanlan.zhihu.com/p/24738319)\n2. [双线性池化（Bilinear Pooling）详解、改进及应用](https://zhuanlan.zhihu.com/p/62532887)\n3. [bilinear model && bilinear pooling（一）](https://zhuanlan.zhihu.com/p/87650330)\n4. [学习笔记之——Bilinear CNN model](https://blog.csdn.net/gwplovekimi/article/details/91891439)\n5. [Bilinear CNN](https://blog.csdn.net/u013841196/article/details/102730183)\n6. BilinearCNNModelsforFine-grainedVisualRecognition","tags":["细粒度分析"],"categories":["神经网络"]},{"title":"RuntimeError CUDA out of memory（已解决）","url":"/2020/11/24/223828/","content":"\n今天用pytorch训练神经网络时，出现如下错误：\n\n**RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 2.00 GiB total capacity; 1.29 GiB already allocated; 79.00 MiB free; 1.30 GiB reserved in total by PyTorch)**\n\n明明 GPU 0 有2G容量，为什么只有 79M 可用？ 并且 1.30G已经被PyTorch占用了。**这就说明PyTorch占用的GPU空间没有释放，导致下次运行时，出现CUDA out of memory**。\n\n<!-- more -->\n\n\n\n解决方法如下：\n\n（1）新建一个终端\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200901171405531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（2）输入 `nvidia-smi`，会显示GPU的使用情况，以及占用GPU的应用程序\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200901171522884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（3）输入`taskkill -PID 进程号 -F ` 结束占用的进程，比如 `taskkill -PID 7392 -F`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200901171704466.png)\n（4）再次输入 `nvidia-smi` 查看GPU使用情况，会发现GPU被占用的空间大大降低，这样我们就可以愉快地使用GPU运行程序了\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200901171827184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n【参考文档】\n\n[CUDA out of memory.(已解决）](https://blog.csdn.net/weixin_43398590/article/details/105383173?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight)\n\n","tags":["pytorch"],"categories":["pytorch"]},{"title":"ResNet残差网络及变体详解（符代码实现）","url":"/2020/11/24/223553/","content":"\n本文通过分析深度网络模型的缺点引出ResNet残差网络，并介绍了几种变体，最后用代码实现ResNet18。\n\n<!-- more -->\n\n\n## 前言\n\nResNet（Residual Network， ResNet）是微软团队开发的网络，它的特征在于具有比以前的网络更深的结构，在2015年的ILSVRC大赛中获得分类任务的第1名。\n\n\n网络的深度对于学习表达能力更强的特征至关重要的。网络的层数越多，意味着能够提取的特征越丰富，表示能力就越强。（越深的网络提取的特征越抽象，越具有语义信息，特征的表示能力就越强）。\n\n\n但是，随着网络深度的增加，所带来的的问题也是显而易见的，主要有以下几个方面：\n1. 增加深度带来的首个问题就是**梯度爆炸/消散**的问题，这是由于随着层数的增多，在网络中反向传播的梯度会随着连乘变得不稳定，变得特别大或者特别小。这其中经常出现的是梯度消散的问题。\n2. 为了克服梯度消散也想出了许多的解决办法，如使用BatchNorm，将激活函数换为ReLu等，但是改善问题的能力有限。\n3. 增加深度的另一个问题就是网络的**degradation**（退化）问题，即随着深度的增加，网络的性能会越来越差。如下所示：![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTYwMzkyMC8yMDE5MDMvMTYwMzkyMC0yMDE5MDMyMjA5MTgwNjc4Ny02NDg1NTk0NDUuanBn?x-oss-process=image/format,png#pic_center)\n\n为了让更深的网络也能训练出好的效果，何凯明大神提出了一个新的网络结构——ResNet（Residual Network，残差网络）。通过使用残差网络结构，深层次的卷积神经网络模型不仅避免了出现模型性能退化的问题，并取得了更好的性能。\n\n需要注意的是，**Residual Network不是为了解决过拟合的问题，因为过拟合只是在测试集上错误率很高，而在训练集上错误率很低**，通过上图可以出，随着深度的加深而引起的 **model degradation**（模型退化）不仅在训练集上错误率高，在测试集上错误率也很高。所以说 Residual Network 主要是为了解决因网络加深而导致的模型退化问题（也有效避免了梯度消散问题，下面会讲）。\n\n\n## 模型退化\n\n通常，当我们堆叠一个模型时，会认为效果会越堆越好。因为，网络的层数越多，意味着能够提取到的特征越丰富，特征的表示能力就越强，假设一个比较浅的网络已经可以达到不错的效果，那么再进行叠加的网络如果什么也不做，效果不会变差。\n\n事实上，这是问题所在，**因为“什么都不做”是之前神经网络最难做到的事情之一**。这时因为由于非线性激活函数（Relu）的存在，每次输入到输出的过程都几乎是不可逆的（信息损失），所以很难从输出反推回完整的输入。所以随着深度的加深而引起的 **model degradation**（模型退化）仅通过普通网络模型是无法避免的。\n\nResidual Learning（残差学习）设计的本质，是让**模型的内部结构具有恒等映射的能力，至少让深度网络实现和浅层网络一样的性能，即让深度网络后面的层至少实现恒等映射的作用，这样在堆叠网络的过程中，不会因为继续堆叠而产生退化。**\n\n\n在mobileNetV2论文中，作者说明了使用ReLU的问题，即当使用ReLU等激活函数时，会导致信息丢失，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200827170000864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n低维（2维）的信息嵌入到n维的空间中（即Input的特征经过高维空间进行变换），并通过随机矩阵$T$对特征进行变换，之后再加上ReLU激活函数，之后在通过 $T^{−1}$ （T的逆矩阵）进行反变换。当n=2，3时，会导致比较严重的信息丢失，部分特征重叠到一起了；当n=15到30时，信息丢失程度降低。\n\n\n\n\n## 残差结构\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200827170620973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n在上图中，我们可以使用一个非线性变化函数来描述一个网络的输入输出，即深层的输入为X（X也为浅层的输出），深层的输出为F(x)+x，F通常包括了卷积，激活等操作。\n\n这里需要注意附加的恒等映射关系具有两种不同的使用情况：残差结构的输入数据若和输出结果的维度一致，则直接相加；若维度不一致，必须对x进行升维操作，让它俩的维度相同时才能计算。升维的方法有两种：\n- 直接通过zero padding 来增加维度（channel）；\n- 用1x1卷积实现，直接改变1x1卷积的filters数目，这种会增加参数。\n\n\n\n\n令H(x)=F(x)+x，即H(x)为深层的输出，则 F(x)=H(x)−x。此时残差结构如下图所示，虚线框中的部分就是F(x)，即H(x)−x。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200827172706533.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n\n\n当浅层的x代表的特征已经足够成熟（即浅层网络输出的特征x已经达到了最优），再经过任何深层的变换改变特征x都会让loss变大的话，**F(x)会自动趋向于学习成为0**，x则从恒等映射的路径继续传递。这样就在不增加计算成本的情况下实现了目的：**在前向过程中，当浅层的输出已经足够成熟，让深层网络后面的层能够实现恒等映射的作用（即让后面的层从恒等映射的路径继续传递）**，这样就解决了由于网络过深而导致的模型退化的问题。\n\n从另一方面讲，**残差结构可以让网络反向传播时信号可以更好的地传递**，以一个例子来解释。\n\n\n假设非残差网络输出为G(x)，残差网络输出为H(x)，其中H=F(x)+x，输入的样本特征 **x=1**。（注意：这里G和H中的F是一样的，为了区分，用不同的符号）\n\n（1）在某一时刻：\n\n非残差网络G(1)=1.1， 把G简化为线性运算$G(x)=W_g*x$，可以明显看出$W_g=1.1$。\n残差网络H(1)=1.1， H(1)=F(1)+1, F(1)=0.1，把F简化为线性运算$F(x)=W_f*x$，$W_f=0.1$。\n\n（2）经过一次反向传播并更新G和F中的$W_g$与$W_f$后（输入的样本特征x不变，仍为1）：\n\n非残差网络G’(1)=1.2， 此时$W_g=1.2$\n残差网络H’(1)=1.2, H’(1)=F’(1)+1, F’(1)=0.2，$W_f=0.2$\n\n可以明显的看出，F的参数$W_f$就从0.1更新到0.2，而G的参数$W_g$就从1.1更新到1.2，这一点变化对F的影响远远大于G，**说明引入残差后的映射对输出的变化更敏感，对权重的调整作用更大，所以效果更好**。\n\n从另外一方面来说，对于残差网络H=F(x)+x，每一个导数就加上了一个恒等项1，dh/dx=d(f+x)/dx=1+df/dx，此时就算原来的导数df/dx很小，这时候误差仍然能够有效的反向传播，有效避免了非残差网络链式求导连乘而引发的**梯度消散**。\n\n>这里可能要有人问，反向传播不应该是对权重求偏导吗，这里怎么是对x求偏导？\n反向传播的目的是为了更新权重，但是反向传播的过程是用链式法则实现，在这个过程中，网络中间层的x和w都会参与回传。乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，加法节点的反向传播将上游的值原封不动地输出到下游。\n\n\n\n\n\n\n\n因此，从上面的分析可以看出，残差模块最重要的作用就是改变了前向和后向信息传递的方式从而很大程度上促进了网络的优化。\n- 前向：当浅层的输出已经足够成熟，让深层网络后面的层能够实现恒等映射的作用（即让后面的层从恒等映射的路径继续传递），解决了由于网络过深而导致的模型退化的问题。\n- 后向：引入残差后的映射对输出的变化更敏感，对权重的调整作用更大，效果更好。\n\n至于为何 shortcut（即附加的恒等映射关系）的输入时X，而不是X/2或是其他形式。kaiming大神的另一篇文章 Identity Mappings in Deep Residual Networks 中探讨了这个问题，对以下6种结构的残差结构进行实验比较，shortcut 是X/2的就是第二种，结果发现还是第一种效果好。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828200641837.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n\n\nResNet的研究者还提出了**能够让网络结构更深的残差模块**，如下图所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828203656416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n将原来的building block(残差结构)改为bottleneck（瓶颈结构），很好地减少了参数数量，即先用第一个1x1的卷积把256维channel降到64维，第三个1x1卷积又升到256维，总共用参数：1x1x256x64+3x3x64x64+1x1x64x256=69632，如果不使用 bottleneck，参数将是 3x3x256x256x2=1179648，差了16.94倍。\n\n\n总的来说，由于将原来的building block(残差结构)改为bottleneck（瓶颈结构）减少了模型训练的参数量，同时减少整个模型的计算量，并且网络深度得以增加，这使得拓展更深的模型结构成为可能，于是出现了拥有50层、101层、152层的ResNet模型，这不仅没有出现模型性能退化的问题，而且错误率和计算复杂度都保持在很低的程度。\n\n作者最后在Cifar-10上尝试了1202层的网络，结果在训练误差上与一个较浅的110层的相近，但是测试误差要比110层大1.5%。作者认为是采用了太深的网络，发生了过拟合。所以现在的残差结构最多到100多层，不能再深了，后面会讲到如何把残差网络扩展到1000层。\n\n## ResNet网络结构\n我们以VGG作对比介绍ResNet网络。看下图：\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTYwMzkyMC8yMDE5MDMvMTYwMzkyMC0yMDE5MDMyMjExNTY1MjA2Ni0xMTE1OTA2MzA0LnBuZw?x-oss-process=image/format,png#pic_center)\n左边为基本的VGGNet，中间为基于VGG作出的扩增至34层的普通网络，右边为34层的残差网络，不同的是每隔两层就会有一个residual模块。对于残差网络（右图），维度匹配的shortcut连接为实线（输入和输出有相同的通道数），反之为虚线。维度不匹配时，同等映射有两种可选方案：全0填充和1x1卷积。\n\n\n\n常用的ResNet有5种常用深度：18，34，50，101，152层。网络分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x。如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200827205445989.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n根据上图，ResNet101的层数为3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，再加上第一层的卷积conv1，以及最后的fc层（用于分类），一共是99+1+1=101层。\n\n\n以往模型大多在ImageNet上作测试，所以这里只给出在ImageNet上的成绩，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20181217193440759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70#pic_center)\n可以看到，由于使用1×1的卷积层来减少模型训练的参数量，同时减少整个模型的计算量，增加了网络的深度，152层的ResNet相比于其他网络有提高了一些精度。\n\n\n\n## Pre Activation ResNet\n由于ResNet引入了残差模块，很好的解决了网络模型degradation的问题，从而提高了网络深度。由于将原来的building block（残差结构）改为bottleneck（瓶颈结构）减少了模型训练的参数量，同时减少整个模型的计算量，这使得拓展更深的模型结构成为可能，于是出现了拥有50层、101层、152层的ResNet模型，那么，**我们还能不能加深一些呢？100层可以，1000层呢？**\n\n答案是不可以，至少目前的残差模型是不行的，**因为目前的残差块在加和之后会经过一个relu，由于这个激活函数Relu的位置带来的影响**，使得增加的操作虽然在100层中不会有很大的影响，但是在1000层的超深网络里面还是会阻碍整个网络的前向反向传播（具体原因接着往下看），我们需要接着改进。\n\n\n\n当前卷积神经网络的基本模块通常为卷积+归一化+激活函数（conv+bn+relu）的顺序，对于普通的按照这些模块进行顺序堆叠的网络来说，各个模块的顺序没有什么影响，但是对于残差网络来说却不一样。\n\npre activation 和 post activation ，也就是**激活函数Relu的位置，是放在卷积前还是卷积后**。对于一般的网络来说，这是没什么区别的，因为网络是各个模块进行叠加，这个卷积块的激活函数的输出就是另一卷积块的激活函数的输入，总体来看无所谓先后。\n\n但是，残差网络不一样，它的残差分支包含着完整的卷积，归一化，激活函数层，而后这一分支要和原始信号分支进行相加，因此就有了多种方案，如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828155213685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n我们最常见的是图a的形式，即原始信号和残差信号相加之和再经过Relu输出到下一个block，但实际上还有b、c、d、e等形式，它们的性能如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828155347392.png#pic_center)\n\n\n\n图b，将BN拿出来，放到加和函数之后，结果最差，分析原因可能是不应该把原始信号和残差信号一起加和后归一化，改变了原始信号的分布。\n\n图c，将ReLU提到残差分支，结果也不行，因为这样一来，残差分支的信号就是非负的，当然会影响表达能力，结果也就变得更差了。\n\n图a是原始的ResNet模块，我们可以看到原始信号和残差信号相加后需要进入Relu做一个非线性激活，这样一来，相加后的结果永远是非负的，这就约束了模型的表达能力（和图c原理类似），因此需要做一个调整。图d和图e都是讲ReLU提到了卷积之前，但是BN的顺序有所不同。图d在临近输出放了BN，然后再和原始信号相加，**本来BN就是为了给数据一个固定的分布，一旦经过别的操作就会改变数据的分布，会削减BN的作用，在原版本的resnet中就是这么使用的BN，所以，图d效果与原始的ResNet（图a）性能大致相当**。图e在临近输入放了BN，效果大大提升，这应该是来自于BN层的功劳，本来BN层就应该放置在卷积层之前提升网络泛化能力。\n\n我们来看下这两种结构在CIFAR-10和CIFAR-100上的效果吧：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828162419741.png#pic_center)\n原始的ResNet结构增加到1000层错误率反而提高，但是用上Pre Activation unit后把网络增加到1000层，错误率显著降低，值的注意的是**这里所有精度提升都是出自于深度的增加**。\n\n1000层的残差网络和100层的网络之间的计算复杂度基本是线性的，因此从时间和算力的角度而言还是100层的网络更加实用，但是改进后的残差模块证明了1000层的网络的可实现性，实际上现在各大厂每次开会都要拿超深的网络出来吓人，原理就是这个模块。\n\n\n**那么，我们有没有什么其他不是靠深度的办法来增加特征的表征能力呢？如果有的话结合上ResNet的深度，会不会产生很好的效果呢？**\n\n## 其它的ResNet变体\n### Wide ResNet\n\nWide ResNet，就是比普通的残差网络更宽（也就是通道数量更大）的结构，那么它与ResNet有什么不同呢？\n\n首先，看一下几个不同的残差模块的对比，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828164655354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n(a)是普通的残差结构，(b)是使用1*1卷积进行升维和降维的结构，(c) 是直接增加网络的宽度，图中方块的宽度就表示它的残差块的通道数更大。(d)是文章中提出，可以看到相比于基础模块在中间加上了dropout层，这是因为增加了宽度后参数的数量会显著增加，为了防止过拟合使用了卷积层中的dropout，并取得了相比于不用dropout更好的效果。\n\n作者们实验结果表明：16层的改进残差网络就达到了1000层残差网络的性能，而且计算代价更低。\n\n\n\n\nWide ResNet网络结构如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20181217193732589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70#pic_center)\n\n作者通过实验发现每个残差内部由两个3\\*3卷积层组成可以有最好的效果，上图是改进后模型的基本架构，与ResNet唯一不同的是多了一个k，代表了原始模块中卷积核数量的k倍（也就是通道数量更大），B(3,3) 代表每一个残差模块内由两个3\\*3的卷积层组成。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828170839561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n上图是164层原始残差网络和28层十倍宽度残差网络在CIFAR-10和CIFAR-100数据集上的比较，实线是在测试集上的错误率，虚线是在训练集上的错误率，可以看到，改进后的宽网络无论在测试集上还是在训练集上都有更低的错误率。\n\n\n\n下图是不同宽度的模型之间纵向比较，同深度下，随着宽度增加，准确率增加。深度为22层，宽度为原始8倍的模型比深度为40层的同宽的模型准确率还要高\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20181217193750658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70#pic_center)\n我们可以得到如下结论：\n\n1. 在同深度情况下，增大宽度可以增加各种深度的残差模型的性能\n2. 宽度和深度的增加就可以使性能提升。\n3. 深度的增加对于模型的提升是有限的，在一定范围内，增加深度可以使模型性能提升，但是一定程度以后，在增加模型的深度，反而有更高的错误率\n4. 从某种意义上来说，宽度比深度增加对于模型的提升更重要。\n\n### Inception v4\n这里推荐看一下我的博客[深入解读GoogLeNet网络结构](https://blog.csdn.net/qq_37555071/article/details/108214680)，可以更好的理解Inception模块。作者在Inceptionv4论文中共提出了3个新的网络：Inceptionv4、Inception-ResNetv1、Inception-ResNetv2，并拿这三个网络加上Inceptionv3一起进行比较。\n\n作者认为，**对于训练非常深的卷积模型，残差连接本质上是必需的**。但似乎发现并**不支持这种观点**，至少对于图像识别来说是的。但是，它可能需要更多的测试数据和更深的模型**来了解残差连接提供的有益方面的真实程度**。在实验部分，作者证明了**在不利用残差连接的情况下训练非常深的网络并不是很困难**。然而，**使用残差连接似乎大大提高了训练速度**，这仅仅是它们使用的一个很好的论据。也就是说**Residual connection并不是必要条件，只是使用了Residual connection会加快训练速度。**\n\n\n\n我们直接来看Inception-ResNet的网络结构吧（下图）。值得注意的是，Inception-ResNetv1计算代价跟Inception-v3大致相同，Inception-ResNetv2的计算代价跟Inception-v4网络基本相同。\n\n![在这里插入图片描述](https://img-blog.csdn.net/20180613075024302?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70#pic_center)\n这里其他模块不介绍了，现在只重点关注Inception的ResNet模块部分。\n\nInception-ResNet-v1和Inception-ResNet-v2对应的Inception-resnet-A模块为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828173646805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n如上面的图片所示，改进主要有两点。1. 将residual模块加入inception，2. 将各分支的输出通过聚合后通过同一个1*1的卷积层来进行通道的扩增。\n\nInceptionv4比Inceptionv3层次更深、结构更复杂，并且IncpetionV4对于Inception块的每个网格大小进行了统一。Inception-ResNet在Inception块上加了残差连接加快训练速度。Inception-ResNet-v2的整体框架和Inception-ResNet-v1是一致的，只不过v2的计算量更加expensive些（输出的channel数量更多）。它们训练精度如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828174215491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n从上图可以看出，加了Residual模块的模型训练速度明显比正常的Inception模型快一些，而且也有稍微高一些的准确率。最后，Inception-ResNet-v2的Top-5准确率达到了3.1%，如下图所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828175612572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n想要更详细的了解Inceptionv4，可以参考这两篇博客[Inceptionv4论文详解](https://blog.csdn.net/qq_38807688/article/details/84590291) 和 [卷积神经网络的网络结构——Inception V4](https://blog.csdn.net/u013841196/article/details/80673688)\n\n\n\n### ResNext\nResNeXt基于wide residual和inception，提出了将残差模块中的所有通道分组进行汇合操作会有更好的效果。同时也给inception提出了一个简化的表示方式。\n\n简化的inception如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828191338471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n与原始的Inception相比，简化的Inception将不同尺寸的卷积核和池化等归一化为3\\*3的卷积核，并且每个分支都用 1\\*1 的卷积核去扩展通道后相加就得到了上面的结构，再这个基础上加上shortcut就得到了ResNext模块：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828191427486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\nResNext包含了32个分支的基模块，每个分支一模一样。每一个框中的参数分别表示输入维度，卷积核大小，输出维度，如256,1x1,64表示当前网络层的输入为256个通道，使用1x1的卷积，输出为64个通道。ResNext通过1x1的网络层，控制通道数先进行降维，再进行升维，然后保证和ResNet模块一样，输入输出的通道数都是256。因此，可以总结如下：\n\n1.  相对于Inception-Resnet，ResNext的每个分支都是相同的结构，相对于Inception，网络架构更加简洁。\n2. Inception-Resnet中的先各分支输出并concat，然后用1 \\* 1卷积变换深度的方式被先变换通道数后单位加的形式替代。\n3. 因为每个分支的相同结构，**可以提出除了深度和宽度之外的第三维度cardinality，即分支的数量**。\n\n\nResNext结构对比普通的ResNet结构，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020082818095390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n从另外一个角度，ResNext是也可以看做一个增加了分组的残差模块，对于上图的ResNet结构的第一个卷积模块，输入为256维，输出为64维。右侧包含了32个同样的支路，每一个支路的输入为256维，输出为4维。不过两者的参数量是相当的，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828191955481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n**类比Wide Residual的配置图，二者一个是改变了卷积核的倍数，一个增加了分组，但都是在残差模块做工作。** 不同深度、不同宽度、不同分组的网络对比如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200828192913658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n可以看到，相对于100层的残差网络，用深度，宽度，和cardinality三种方式增大了两倍的复杂度，相同复杂度下，**分组更多即C更大的模型性能更好，这说明cardinality是一个比深度和宽度更加有效的维度**。而且，ResNext的计算速度更快，因为ResNext的结构本来就非常适合硬件并行处理。\n\n\n\n## ResNet18的实现\n\n残差块的实现如下。它可以设定输出通道数、是否使用额外的 1×1 卷积层来修改通道数以及卷积层的步幅。\n\n```python\nimport time\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\n\nclass Residual(nn.Module):  \n    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n        super(Residual, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        # 1x1conv来升维\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        # 1x1conv对浅层输入的X升维\n        if self.conv3:\n            X = self.conv3(X)\n        return F.relu(Y + X)\n```\n下面我们来查看输入和输出形状一致的情况：\n\n```python\nblk = Residual(3, 3)\nX = torch.rand((4, 3, 6, 6))\nblk(X).shape # torch.Size([4, 3, 6, 6])\n```\n我们也可以在增加输出通道数的同时减半输出的高和宽。\n\n```python\nX = torch.rand((4, 3, 6, 6))\nblk = Residual(3, 6, use_1x1conv=True, stride=2)\nblk(X).shape # torch.Size([4, 6, 3, 3])\n```\n\nResNet的**前两层**跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。\n\n```python\nresnet_18 = nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n        nn.BatchNorm2d(64), \n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n```\n\nGoogLeNet在后面接了4个由Inception块组成的模块。**ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致**。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在**第一个残差块**里将上一个模块的通道数翻倍，并将高和宽减半。\n\n下面我们来实现这个模块。注意，这里对第一个模块做了特别处理。\n\n```python\ndef resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n    if first_block:\n        assert in_channels == out_channels # 第一个模块的通道数同输入通道数一致\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n        else:\n            blk.append(Residual(out_channels, out_channels))\n    # 解包迭代器，从而传入多个模块\n    return nn.Sequential(*blk)\n```\n接着我们为ResNet加入所有残差块。这里每个模块使用两个残差块。\n\n```python\nresnet_18.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\nresnet_18.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\nresnet_18.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\nresnet_18.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n```\n最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。\n\n```python\nclass GlobalAvgPool2d(nn.Module):\n    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n    def forward(self, x):\n        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n    \nclass FlattenLayer(torch.nn.Module):\n    def __init__(self):\n        super(FlattenLayer, self).__init__()\n    def forward(self, x): # x shape: (batch, *, *, ...)\n        return x.view(x.shape[0], -1)\n        \nresnet_18.add_module(\"global_avg_pool\", GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, 512, 1, 1)\nresnet_18.add_module(\"fc\", nn.Sequential(FlattenLayer(), nn.Linear(512, 10))) \n```\n这里每个模块里有4个卷积层（不计算1×1卷积层），加上最开始的卷积层和最后的全连接层，共计18层。这个模型通常也被称为ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。虽然ResNet的主体架构跟GoogLeNet的类似，但ResNet结构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。\n\n我们来观察一下输入形状在ResNet不同模块之间的变化。\n\n```python\nX = torch.rand((1, 1, 224, 224))\nfor name, layer in resnet_18.named_children():\n    X = layer(X)\n    print(name, ' output shape:\\t', X.shape)\n\"\"\"\n# 前面四层是 7x7conv、BN、nn.ReLU、MaxPool2d\n# 输出\n0  output shape:\t torch.Size([1, 64, 112, 112])\n1  output shape:\t torch.Size([1, 64, 112, 112])\n2  output shape:\t torch.Size([1, 64, 112, 112])\n3  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block1  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block2  output shape:\t torch.Size([1, 128, 28, 28])\nresnet_block3  output shape:\t torch.Size([1, 256, 14, 14])\nresnet_block4  output shape:\t torch.Size([1, 512, 7, 7])\nglobal_avg_pool  output shape:\t torch.Size([1, 512, 1, 1])\nfc  output shape:\t torch.Size([1, 10])\n\"\"\"\n```\n\n另外，本文用到的论文我上传到百度云上了，有需要的请自提，链接：https://pan.baidu.com/s/1cParM5EEOz3QOQgjAZt9jA 提取码：ngvb 。包含如下三个论文：\n- Identity Mappings in Deep Residual Networks\n- Wide Residual Networks\n- AggregatedResidualTransformationsforDeepNeuralNetworks\n\n\n【参考文档】\n[深度学习网络篇——ResNet](https://blog.csdn.net/weixin_43624538/article/details/85049699)\n[resnet中的残差连接，你确定真的看懂了？](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029645&idx=1&sn=75b494ec181fee3e8756bb0fa119e7ce&chksm=87134270b064cb66aea66e73b4a6dc283d5750cfa9d331015424f075ba117e38f857d2f25d07&mpshare=1&scene=1&srcid=0826sj1X99iidGol2vOYXoyL&sharer_sharetime=1598412745177&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a4858933403615d2b1a972f5118c4a09c232aef7f191fe77f564ecd8fcee532326e67d272b0451ccad5d44c0586df1184bd3170c632e98c86f33bc664bca4603775cab14cca0fe4739d5170a3f0b4fa71418da0597ca52322b9008de5cda1b2746ea23f0d96b8a0720c48fc50cf26ae7c16982bfd6f2aad9addf8&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AS/my8QJbD1iVlhaLq6mPl4=&pass_ticket=YcCsKQUoka0nj/P%2b0pwrfYVeXAi9wdtnOBln8h11m8ftsr1WLiJ5a6KMsypN6fsD)\n[残差网络（ResNet）](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.11_resnet)\n[残差网络ResNet笔记](https://www.cnblogs.com/alanma/p/6877166.html)\n[CNN 经典网络之-ResNet](https://www.cnblogs.com/yanshw/p/10576354.html)\n深度学习之Pytorch实战计算机视觉-唐进民著\n\n","tags":["ResNet"],"categories":["神经网络"]},{"title":"深入解读GoogLeNet网络结构（附代码实现）","url":"/2020/11/24/223407/","content":"\n\n\n## 前言\n\n> 七夕了，看着你们秀恩爱，单身狗的我还是做俺该做的事吧！\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vd3hsZXIvYmxvZ2ltZy9yYXcvbWFzdGVyL2ltZ3MvMjAyMDA4MDMxMDQ1MzcuZ2lm)\n\n\n在上一篇文章中介绍了[VGG网络结构](https://blog.csdn.net/qq_37555071/article/details/108199352)，VGG在2014年ImageNet 中获得了定位任务第1名和分类任务第2名的好成绩，而同年分类任务的第一名则是**GoogleNet** 。GoogleNet是Google研发的深度网络结构，之所以叫“GoogLeNet”，是为了向“LeNet”致敬，有兴趣的同学可以看下原文[Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842.pdf)。\n\n\n\n<!-- more -->\n\n\n\n\n与VGGNet模型相比较，GoogleNet模型的网络深度已经达到了22层（ 如果只计算有参数的层，GoogleNet网络有22层深 ，算上池化层有27层），而且在网络架构中引入了Inception单元，从而进一步提升模型整体的性能。虽然深度达到了22层，但大小却比AlexNet和VGG小很多，GoogleNet参数为500万个（ 5M ），VGG16参数是138M，是GoogleNet的27倍多，而VGG16参数量则是AlexNet的两倍多。\n\n## Inception单元结构\n\n我们先来看一下模型中的Inception单元结构，然后在此基础上详细分析GoogleNet网络结构，这里推荐看一下我的这篇博客[从Inception到Xception，卷积方式的成长之路](https://blog.csdn.net/qq_37555071/article/details/107835402)，可以对下面的内容有更好的理解。\n\nInception 最初提出的版本主要思想是**利用不同大小的卷积核实现不同尺度的感知**，网络结构图如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806113406524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nInception Module基本组成结构有四个成分。1\\*1卷积，3\\*3卷积，5\\*5卷积，3\\*3最大池化。最后对四个成分运算结果进行通道上组合，这就是Naive Inception的核心思想：利用不同大小的卷积核实现不同尺度的感知，最后进行融合，可以得到图像更好的表征。\n\n下面通过一个具体的实例来看看整个Naive Inception单元的详细工作过程，假设在上图中Naive  Inception单元的前一层输入的数据是一个32×32×256的特征图，**该特征图先被复制成4份并分别被传至接下来的4个部分**。我们假设这4个部分对应的滑动窗口的步长均为1，其中，1×1卷积层的Padding为0，滑动窗口维度为1×1×256，要求输出的特征图深度为128；3×3卷积层的Padding为1，滑动窗口维度为3×3×256，要求输出的特征图深度为192；5×5卷积层的Padding为2，滑动窗口维度为5×5×256，要求输出的特征图深度为96；3×3最大池化层的  Padding为1，滑动窗口维度为3×3×256。**这里对每个卷积层要求输出的特征图深度没有特殊意义，仅仅举例用**，之后通过计算，分别得到这4部分输出的特征图为32×32×128、32×32×192、32×32×96  和  32×32×256，最后在合并层进行合并，得到32×32×672的特征图，合并的方法是将各个部分输出的特征图相加，最后这个Naive  Inception单元输出的特征图维度是32×32×672，总的参数量就是`1*1*256*128+3*3*256*192+5*5*256*96=1089536`。\n\n但是Naive  Inception有两个非常严重的问题：**首先，所有卷积层直接和前一层输入的数据对接，所以卷积层中的计算量会很大；其次，在这个单元中使用的最大池化层保留了输入数据的特征图的深度，所以在最后进行合并时，总的输出的特征图的深度只会增加，这样增加了该单元之后的网络结构的计算量**。于是人们就要想办法减少参数量来减少计算量，在受到了模型 “Network in Network”的启发，开发出了在GoogleNet模型中使用的Inception单元（Inception V1），这种方法可以看做是一个额外的1\\*1卷积层再加上一个ReLU层。如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806113747780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n这里使用1x1 卷积核主要目的是进行**压缩降维，减少参数量**，从而让网络更深、更宽，更好的提取特征，这种思想也称为**Pointwise Conv**，简称PW。\n\n\n举个例子来论证下吧。假设新增加的  1×1 的卷积的输出深度为64，步长为1，Padding为0，其他卷积和池化的输出深度、步长都和之前在Naive  Inception单元中定义的一样（即上面例子中定义的一样），前一层输入的数据仍然使用同之前一样的维度为32×32×256的特征图，通过计算，分别得到这  4  部分输出的特征图维度为32×32×128、32×32×192、32×32×96  和32×32×64，将其合并后得到维度为32×32×480的特征图，将这4部分输出的特征图进行相加，最后Inception单元输出的特征图维度是32×32×480。新增加的3个  1×1 的卷积参数量是`3*1*1*256*64=49152`，原来的卷积核参数量是`1*1*256*128+3*3*64*192+5*5*64*96=296960`，总的参数量就是`49152+296960=346112`。\n\n在输出的结果中，32×32×128、32×32×192、32×32×96  和之前的Naive  Inception  单元是一样的，**但其实这三部分因为1×1卷积层的加入，总的卷积参数数量已经大大低于之前的Naive  Inception单元**，**而且因为在最大池化层之前也加入了1×1的卷积层，所以最终输出的特征图的深度也降低了，这样也降低了该单元之后的网络结构的计算量**。\n\n## GoogLeNet模型解读\n\nGoogleNet网络结构（Inception V1）的网络结构如下：\n\n\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X2pwZy94bkJGSGNSNDE4cUM4TE9zN1R0dnJOTDRLRllpYkRIMmtpYzluQjhweXowbXFIdU0yUVk4WjZtNXNJQzlwZDRJRmJ4MlZLbWttYm1wVlg0N2lidE1VaWMwdXcvNjQw?x-oss-process=image/format,png)\n\nGoogLeNet网络有22层深（包括pool层，有27层深），在分类器之前，采用Network in Network中用Averagepool（平均池化）来代替全连接层的思想，而在avg pool之后，还是添加了一个全连接层，是为了大家做finetune（微调）。而无论是VGG还是LeNet、AlexNet，在输出层方面均是采用连续三个全连接层，全连接层的输入是前面卷积层的输出经过reshape得到。**据发现，GoogLeNet将fully-connected layer用avg pooling layer代替后，top-1 accuracy 提高了大约0.6%；然而即使在去除了fully-connected layer后，依然必须dropout。**\n\n由于全连接网络参数多，计算量大，容易过拟合，所以GoogLeNet没有采用VGG、LeNet、AlexNet三层全连接结构，直接在Inception模块之后使用Average Pool和Dropout方法，不仅起到降维作用，还在一定程度上防止过拟合。\n\n**在Dropout层之前添加了一个7×7的Average Pool，一方面是降维，另一方面也是对低层特征的组合。我们希望网络在高层可以抽象出图像全局的特征，那么应该在网络的高层增加卷积核的大小或者增加池化区域的大小，GoogLeNet将这种操作放到了最后的池化过程，前面的Inception模块中卷积核大小都是固定的，而且比较小，主要是为了卷积时的计算方便。**\n\n\n\nGoogLeNet在网络模型方面与AlexNet、VGG还是有一些相通之处的，它们的主要相通之处就体现在卷积部分，\n\n- AlexNet采用5个卷积层\n- VGG把5个卷积层替换成5个卷积块\n- GoogLeNet采用5个不同的模块组成主体卷积部分\n\n用表格的形式表示GoogLeNet的网络结构如下所示：\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9zYnphQnhDRXJManBpYzBtaWNaOVFkZ3FTcjhXZ01WdEV2NGhvaFh2bWpxdHBxZlVKWGxIVWliSDZXaWFxWjNicW85NEJrazFCanBGaWNTd28yNkd6dDRLRlhBLzY0MA?x-oss-process=image/format,png#pic_center)\n上述就是GoogLeNet的结构，可以看出，和AlexNet统一使用5个卷积层、VGG统一使用5个卷积块不同，GoogLeNet在主体卷积部分是**卷积层与Inception块混合使用**。另外，需要注意一下，在输出层GoogleNet采用**全局平均池化**，得到的是高和宽均为1的卷积层，而不是通过reshape得到的全连接层。\n\n需要注意的是，上图中 “＃3×3reduce” 和 “＃5×5reduce” 表示在3×3和5×5卷积之前，使用的**降维层**中的1×1滤波器的数量。pool proj代表max-pooling后的投影数量（即先max-pooling，再PW降维），所有的reductions（降维）和projections（投影）也都使用激活函数ReLU。\n\n\n下面就来详细介绍一下GoogLeNet的模型结构。\n\n**输入**\n\n原始输入图像为224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）\n\n**第一模块**\n\n第一模块采用的是一个单纯的卷积层紧跟一个最大池化层。\n\n卷积层：卷积核大小7*7，步长为2，padding为3，输出通道数64，输出特征图尺寸为`(224-7+3*2)/2+1=112.5(向下取整)=112`，输出特征图维度为112x112x64，卷积后进行ReLU操作。\n\n池化层：窗口大小3*3，步长为2，输出特征图尺寸为`((112 -3)/2)+1=55.5(向上取整)=56`，输出特征图维度为56x56x64。\n\n关于卷积和池化中的特征图大小计算方式，可以参考我的博客[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)\n\n**第二模块**\n\n第二模块采用**2个卷积层**，后面跟一个最大池化层。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825150804453.png#pic_center)\n\n**卷积层：**\n1. 先用64个1x1的卷积核（3x3卷积核之前的降维）将输入的特征图（56x56x64）变为56x56x64，然后进行ReLU操作。参数量是`1*1*64*64=4096`\n2. 再用卷积核大小3*3，步长为1，padding为1，输出通道数192，进行卷积运算，输出特征图尺寸为`(56-3+1*2)/1+1=56`，输出特征图维度为56x56x192，然后进行ReLU操作。参数量是`3*3*64*192=110592`\n\n第二模块卷积运算总的参数量是`110592+4096=114688`，即`114688/1024=112K`。\n\n\n**池化层：** 窗口大小3*3，步长为2，输出通道数192，输出为`((56 - 3)/2)+1=27.5(向上取整)=28`，输出特征图维度为28x28x192。\n\n**第三模块(Inception 3a层)**\n\nInception 3a层，分为四个分支，采用不同尺度，图示如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/202008251231252.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n再看下表格结构，来分析和计算吧：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825151846422.png#pic_center)\n\n1. 使用64个1x1的卷积核，运算后特征图输出为28x28x64，然后RuLU操作。参数量`1*1*192*64=12288`\n2. 96个1x1的卷积核（3x3卷积核之前的降维）运算后特征图输出为28x28x96，进行ReLU计算，再进行128个3x3的卷积，输出28x28x128。参数量`1*1*192*96+3*3*96*128=129024`\n3. 16个1x1的卷积核（5x5卷积核之前的降维）将特征图变成28x28x16，进行ReLU计算，再进行32个5x5的卷积，输出28x28x32。参数量`1*1*192*16+5*5*16*32=15872`\n4. pool层，使用3x3的核，输出28x28x192，然后进行32个1x1的卷积，输出28x28x32.。总参数量`1*1*192*32=6144`\n\n将四个结果进行连接，对这四部分输出结果的第三维并联，即64+128+32+32=256，最终输出28x28x256。总的参数量是`12288+129024+15872+6144=163328`，即`163328/1024=159.5K`，约等于159K。\n\n**第三模块(Inception 3b层)**\n\nInception 3b层，分为四个分支，采用不同尺度。\n\n1. 128个1x1的卷积核，然后RuLU，输出28x28x128\n2. 128个1x1的卷积核（3x3卷积核之前的降维）变成28x28x128，进行ReLU，再进行192个3x3的卷积，输出28x28x192\n3. 32个1x1的卷积核（5x5卷积核之前的降维）变成28x28x32，进行ReLU，再进行96个5x5的卷积，输出28x28x96\n4. pool层，使用3x3的核，输出28x28x256，然后进行64个1x1的卷积，输出28x28x64\n\n将四个结果进行连接，对这四部分输出结果的第三维并联，即128+192+96+64=480，最终输出输出为28x28x480。\n\n**第四模块(Inception 4a、4b、4c、4e)**\n\n与Inception3a，3b类似\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825153012306.png#pic_center)\n**第五模块(Inception 5a、5b)**\n\n与Inception3a，3b类似\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825153237754.png#pic_center)\n\n\n**输出层**\n\n前面已经多次提到，在输出层GoogLeNet与AlexNet、VGG采用3个连续的全连接层不同，GoogLeNet采用的是全局平均池化层，得到的是高和宽均为1的卷积层，然后添加丢弃概率为40%的Dropout，输出层激活函数采用的是softmax。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825153225724.png#pic_center)\n\n**激活函数**\n\nGoogLeNet每层使用的激活函数为ReLU激活函数。\n\n**辅助分类器**\n\n根据实验数据，发现**神经网络的中间层也具有很强的识别能力，为了利用中间层抽象的特征，在某些中间层中添加含有多层的分类器**。如下图所示，红色边框内部代表添加的辅助分类器。**GoogLeNet中共增加了两个辅助的softmax分支，作用有两点，一是为了避免梯度消失，用于向前传导梯度。反向传播时如果有一层求导为0，链式求导结果则为0。二是将中间某一层输出用作分类，起到模型融合作用**。最后的loss=loss_2 + 0.3 \\* loss_1 + 0.3 \\* loss_0。实际测试时，这两个辅助softmax分支会被去掉。\n\n\n\n\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9QbjRTbTBSc0F1aWEwRUlYWE5zb1h1SHZqN241YmVTNHpWMmNLSXVZTWlibmliTHB2bWYxY3ZodkJuOUppYm51YnBjc04xeUZ2cDJuVThaRjY1WWljN0NBN3FBLzY0MA?x-oss-process=image/format,png#pic_center)\n\n## GoogLeNet其他版本\n上面介绍的GoogLeNet模型是Inception v1版本，还有Inception v2，v3，v4版本\n\n**Inception V2**\n\n1. 学习VGGNet的特点，用两个3*3卷积代替5*5卷积，可以降低参数量。\n2. 提出BN算法。BN算法是一个正则化方法，可以提高大网络的收敛速度。就是每一batch的输入分布标准化处理，使得规范化为N(0,1)的高斯分布，收敛速度大大提高。详情可以参考我的博客[Batch Normalization：批量归一化详解](https://blog.csdn.net/qq_37555071/article/details/107549047)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806130828579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n**Inception V3**\n\n学习Factorization into small convolutions的思想，在Inception V2的基础上，将一个二维卷积拆分成两个较小卷积，例如将7\\*7卷积拆成1\\*7卷积和7\\*1卷积，这样做的好处是降低参数量。该paper中指出，通过这种非对称的卷积拆分比对称的拆分为几个相同的小卷积效果更好，可以处理更多，更丰富的空间特征，这就是Inception V3网络结构。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080613330515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n**Inception V4**\n\n借鉴了微软的ResNet网络结构思想，后面写到Resnet再介绍吧。\n\n\n## GoogLeNet测试样本处理\n1. 对于一个测试样本，将图像的短边缩放成4种尺寸，分别为256，288，320，352。\n2. 从每种尺寸的图像的左边，中间，右边（或者上面，中间，下面）分别截取一个方形区域，共三块方形区域。\n3. 对于每一个方形区域，我们取其四角和中心，裁切出5个区域，再将方形区域缩小到224×224，共6快区域，加上它们的镜像版本（将图像水平翻转），一共得到4×3×6×2=144张图像。这样的方法在实际应用中是不必要的，可能存在更合理的修剪方法。下图展示了不同修剪方法和不同模型数量的组合结果：\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825155701819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n*上表中，通过改变模型数量以及切分数量，展示几种测试策略对于图片进行预测的效果，所有数据报告基于验证数据集,以避免测试集上的过拟合。*\n\n\n使用多个模型时，每个模型的Softmax分类器在多个修剪图片作为输入时都得到多个输出值，然后再对所有分类器的softmax概率值求平均。\n\n\n**效果如下所示：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200825155830735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n可以看到，GoogLeNet在验证集和测试集上top-5的错误率都降到了6.67%，在当年参赛者中排名第一。\n\n\n  \t\n\n\n\n\n## GoogleNet代码实现\n\n**Inception实现**\n\n```python\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nclass Inception(nn.Module):\n    # c1 - c4为每条线路里的层的输出通道数\n    def __init__(self, in_c, c1, c2, c3, c4):\n        super(Inception, self).__init__()\n        # 线路1，单1 x 1卷积层\n        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)\n        # 线路2，1 x 1卷积层后接3 x 3卷积层\n        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # 线路3，1 x 1卷积层后接5 x 5卷积层\n        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)\n \n    def forward(self, x):\n        p1 = F.relu(self.p1_1(x))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n        p4 = F.relu(self.p4_2(self.p4_1(x)))\n        return torch.cat((p1, p2, p3, p4), dim=1)  # 在通道维上连结输出\n```\n\n**GlobalAvgPool2d与FlattenLayer**\n\n```python\nclass GlobalAvgPool2d(nn.Module):\n    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n    def forward(self, x):\n        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n    \nclass FlattenLayer(torch.nn.Module):\n    def __init__(self):\n        super(FlattenLayer, self).__init__()\n    def forward(self, x): # x shape: (batch, *, *, ...)\n        return x.view(x.shape[0], -1)\n```\n\n**GoogLeNet实现**\n\n```python\nclass GoogLeNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(GoogLeNet, self).__init__()\n        \n        self.b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n                           nn.ReLU(),\n                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        self.b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n                           nn.Conv2d(64, 192, kernel_size=3, padding=1),\n                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        self.b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n                           Inception(256, 128, (128, 192), (32, 96), 64),\n                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        self.b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n                           Inception(512, 160, (112, 224), (24, 64), 64),\n                           Inception(512, 128, (128, 256), (24, 64), 64),\n                           Inception(512, 112, (144, 288), (32, 64), 64),\n                           Inception(528, 256, (160, 320), (32, 128), 128),\n                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        self.b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n                           Inception(832, 384, (192, 384), (48, 128), 128),\n                           GlobalAvgPool2d())\n        self.output=nn.Sequential(FlattenLayer(),\n                                  nn.Dropout(p=0.4),\n                                  nn.Linear(1024, 1000))\n        \n        def forward(self, x):\n            x=b1(x)\n            x=b2(x)\n            x=b3(x)\n            x=b4(x)\n            x=b5(x)\n            x=output(x)\n            return x\n```\n**测试输出**\n\n```python\nnet = GoogLeNet()\nX = torch.rand(1, 3, 224, 224)\n# 可以对照表格看一下各层输出的尺寸\nfor blk in net.children(): \n    X = blk(X)\n    print('output shape: ', X.shape)\n\"\"\"\n# 输出：\noutput shape:  torch.Size([1, 64, 56, 56])\noutput shape:  torch.Size([1, 192, 28, 28])\noutput shape:  torch.Size([1, 480, 14, 14])\noutput shape:  torch.Size([1, 832, 7, 7])\noutput shape:  torch.Size([1, 1024, 1, 1])\noutput shape:  torch.Size([1, 1000])\n\"\"\"\n```\n\n\n\n【参考文档】\n1. [GoogLeNet中的inception结构，你看懂了吗](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029565&idx=1&sn=330e398a4007b7b24fdf5203a5bf5d91&chksm=871345c0b064ccd6dd7d954c90d63f1f3b883c7d487844cbe3424bec3c9abb66625f1837edbd&scene=21#wechat_redirect)\n2. [GoogleNet 论文解析及代码实现](https://mp.weixin.qq.com/s?__biz=MzI1ODEzMDQ3OQ==&mid=2247484578&idx=1&sn=4e41b5304664aa3f8020faec9454747a&chksm=ea0d93e2dd7a1af4556b3852a00961615d0df3605d98dcd8c23acb37b22626455d11e6a90adf&mpshare=1&scene=1&srcid=08253WfHcIkpm3iCO1ICf58x&sharer_sharetime=1598319287371&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=19019e2f1359b2de084edfb643f43e91654d68635120cbf885413ba7afc22639d7757ea0d526f323ce679a5040343d9eb6f254294fee1ea2f991310ee5e25bbe884660b311ce10d0b531a684dd82ac14da5d51f7d6d5a73fc4ca5ec88189890b2fa1c86131d2bd1ec69398222799856c8b6c9890d64acdf0fe0410292c1a2d83&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AVxFjJhoCwO8UI1HylAlShI=&pass_ticket=vFUwXThz6nwuqDXBu8RFZMPKZjXMC0vxTeL29D9KIAj11S9RDOH70u14fPZeWLU0)\n3. [带你快速学会 GoogLeNet 神经网络](https://mp.weixin.qq.com/s?__biz=MjM5MjAwODM4MA==&mid=2650725770&idx=4&sn=34c148f1c5dd80115fea9c38215973cb&chksm=bea6ac5989d1254f8c8015ec27e0647f03099d5989d296fecf0fa5de78f205d3e3170f77e9ef&mpshare=1&scene=1&srcid=0825UnHhIih8vnQgNvChrWBQ&sharer_sharetime=1598319794600&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=b6de4a213a64b7292bca947a43a1bd9735df46270820810bf2af4d69d20c297f9f436989b6c839ca238cf07eb05b11f019b360e19c055a4b568a103b9d929516059f43875a73dd44f83eef760bb9e85c6573653e1a2a81124fcf925747d30ff106c8bdde5229391fea5028c415b8c87debb59134f8fce4f621ea9467cd5419dc&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=Afibu8%2bcZCDn182XsPwKq8g=&pass_ticket=vFUwXThz6nwuqDXBu8RFZMPKZjXMC0vxTeL29D9KIAj11S9RDOH70u14fPZeWLU0)\n4. [卷积神经网络之GoogLeNet](https://mp.weixin.qq.com/s?__biz=MzI0NTM1MzA2Mw==&mid=2247484704&idx=1&sn=6b75e56ee317922b536b9eca119dc06d&chksm=e94e9a28de39133ed340f38e332f064a3c0fe42d81842d93d7bec309222bc3270f49482b3513&mpshare=1&scene=1&srcid=082568MV7VsEDOmvSikHDyM6&sharer_sharetime=1598319942128&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a48589b9f7f2db79c9d66ee5708a8ce35f046f5ea2617868dda3dd5719d48310f524f62e09bd966f8f620da4a78c6b2d2a77c933ce7ea10befa64250913ac4bb5c84423d846f996f3c11eb5e10ca6830c919daaaca0e77e2060d642d0bb739722a44aa9e27ea068df75fb26003f530bda9c9eadfd72210c0fca94&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=ASSsmXagqLOuBFd%2byPqfM6s=&pass_ticket=vFUwXThz6nwuqDXBu8RFZMPKZjXMC0vxTeL29D9KIAj11S9RDOH70u14fPZeWLU0)\n5. [GoogLeNet学习笔记](https://zhuanlan.zhihu.com/p/27124535)\n6. [GoogLeNet模型](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.9_googlenet?id=_592-googlenet%E6%A8%A1%E5%9E%8B)\n7. 深度学习之pytorch计算机视觉-唐进民著\n8. Going Deeper with Convolutions, CVPR 2014\n\n我把GoogLeNet的论文上传到百度云上了，有需要请自提，链接：https://pan.baidu.com/s/1Tcg6-s1pHCE2WZ9TBaQ90A  提取码：ivft","tags":["GoogLeNet"],"categories":["神经网络"]},{"title":"深入解读VGG网络结构（附代码实现）","url":"/2020/11/24/223242/","content":"\nVGGNet由牛津大学的视觉几何组（Visual  Geometry  Group）提出，并在2014年举办的ILSVRC（ImageNet 2014比赛）中获得了定位任务第1名和分类任务第2名的好成绩，（GoogleNet 是2014 年的分类任务第1 名）。虽然VGGNet在性能上不及GoogleNet，但因为VGG结构简单，应用性强，所以很多技术人员都喜欢使用基于VGG 的网络。VGG论文[Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556.pdf)，有兴趣的同学可以看下。\n\n<!-- more -->\n\n\n\n\n**VGG 最大的特点就是通过比较彻底地采用 3x3 尺寸的卷积核来堆叠神经网络，这样也加深整个神经网络的深度。这两个重要的改变对于人们重新定义卷积神经网络模型架构也有不小的帮助，至少证明使用更小的卷积核并且增加卷积神经网络的深度，可以更有效地提升模型的性能。**\n\n\n\nVGG 选择的是在 AlexNet 的基础上加深它的层数，但是它有个很显著的特征就是持续性的添加 3x3 的卷积核。而AlexNet 有 5 层卷积层，从下面的网络结构图我们可以看出来，VGG 就是针对这 5 层卷积层进行改造，共进行了 6 种配置。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824151316119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n这六种配置的参数量：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152012157.png#pic_center)\n\n这六种配置的效果展示图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020082415194940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n从上面的效果图中，我们发现VGG19是最好的。但是，VGG-19 的参数比 VGG-16 的参数多了好多。由于VGG-19需要消耗更大的资源，因此实际中VGG-16使用得更多。而且VGG-16网络结构十分简单，并且很适合迁移学习，因此至今VGG-16仍在广泛使用，下面我们主要来讨论一下VGG16的网络结构，也就是图中的类型D。（VGG19即上面的E类型）\n\nVGG16相比AlexNet的一个改进是采用连续的3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5），如下图中所示，共有13个卷积层，3个全连接层。其**全部采用3\\*3卷积核，步长统一为1，Padding统一为1，和2\\*2最大池化核，步长为2，Padding统一为0**。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152348459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n那么，就让我们一层一层来分析吧！\n\n（1）INPUT层：VGG16卷积神经网络默认的输入数据必须是维度为224×224×3的图像，和  AlexNet一样，其输入图像的高度和宽度均为224，而且拥有的色彩通道是R、G、B这三个。\n（2）**CONV3-64**：使用的卷积核为`(3*3*3)*64`（卷积核大小为3\\*3，输入通道为3，输出通道为64），步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为224，即$224=\\frac{224-3+2}{1}+1$   ，最后输出的特征图的维度为224×224×64。卷积通用公式参考我的博客[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中卷积中的特征图大小计算方式。\n（3）**CONV3-64**：使用的卷积核为`(3*3*64)*64`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为224，即$224=\\frac{224-3+2}{1}+1$   ，最后输出的特征图的维度为224×224×64。\n（4）Max pool：池化核大小为2×2，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为112，即 $112=\\frac{224-2}{2}+1$   ，最后得到的输出的特征图的维度为112×112×64。\n（5）**CONV3-128**：使用的卷积核为`(3*3*64)*128`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为112，即$112=\\frac{112-3+2}{1}+1$   ，最后输出的特征图的维度为112×112×128。\n（6）**CONV3-128**：使用的卷积核为`(3*3*128)*128`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为112，即$112=\\frac{112-3+2}{1}+1$   ，最后输出的特征图的维度为112×112×128。\n（7）Max pool：池化核大小为2×2，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为56，即 $56=\\frac{112-2}{2}+1$   ，最后得到的输出的特征图的维度为56×56×128。\n（8）**CONV3-256**：使用的卷积核为`(3*3*128)*256`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为56，即$56=\\frac{56-3+2}{1}+1$   ，最后输出的特征图的维度为56×56×256。\n（9）**CONV3-256**：使用的卷积核为`(3*3*256)*256`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为56，即$256=\\frac{256-3+2}{1}+1$   ，最后输出的特征图的维度为56×56×256。\n（10）**CONV3-256**：经过`(3*3*256)*256`卷积核，生成featuremap为`56*56*256`。\n（11）Max pool：  经过`（2*2）`maxpool，生成featuremap为`28*28*256`。\n（12）**CONV3-512**：经过`（3*3*256）*512`卷积核，生成featuremap为`28*28*512`。\n（13）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`28*28*512`。\n（14）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`28*28*512`。\n（15）Max pool：经过`（2*2）`maxpool,生成featuremap为`14*14*512`。\n（16）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`14*14*512`。\n（17）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`14*14*512`。\n（18）**CONV3-512**：经过`（3*3*512）*512`卷积核，生成featuremap为`14*14*512`。\n（19）Max pool：经过`2*2`卷积，生成featuremap为`7*7*512`。\n（20）**FC-4096**：输入为`7*7*512`，和AlexNet模型一样，都需要对输入特征图进行扁平化处理以得到1×25088的数据，输出数据的维度要求是1×4096，所以需要一个维度为25088×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×4096。\n（21）**FC-4096**：输入数据的维度为1×4096，输出数据的维度要求是1×4096，所以需要一个维度为4096×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×4096。\n（22）**FC-1000**：输入数据的维度为1×4096，输出数据的维度要求是1×1000，所以需要一个维度为4096×1000的矩阵完成输入数据和输出数据的全连接，最后得到输入数据的维度为1×1000。\n\n上面加粗的层即带有可训练参数的层，共16 weight layers。\n\nVGG16的参数一共是多少呢，现在来计算吧！\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824164009624.png#pic_center)\nVGG16（即上图D）总参数量是138M，具体如下：\n第1层：`1792 = 3*3*3*64+64`\n第2层：`36928 = 3*3*64*64+64`\n第3层：`73856 = 3*3*64*128+128`\n第4层：`147584 = 3*3*128*128+128`\n第5层：`295168 = 3*3*128*256+256`\n第6层：`590080 = 3*3*256*256+256`\n第7层：`590080 = 3*3*256*256+256`\n第8层：`1180160 = 3*3*256*512+512`\n第9层：`2359808 = 3*3*512*512+512`\n第10层：`2359808 = 3*3*512*512+512`\n第11层：`2359808 = 3*3*512*512+512`\n第12层：`2359808 = 3*3*512*512+512`\n第13层：`2359808 = 3*3*512*512+512`\n第14层：`102764544 = 7*7*512*4096+4096`\n第15层：`16781312 = 4096*4096+4096`\n第16层：`4097000 = 4096*1000+1000`\n\n总计：138357544个  （138M）\n\n\n\n\n**总结：**\n\n1. VGG16相比AlexNet的一个改进是采用连续的3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）\n2. 加深结构都使用ReLU激活函数：提升非线性变化的能力\n3. VGG16 **全部采用3\\*3卷积核，步长统一为1，Padding统一为1，和2\\*2最大池化核，步长为2，Padding统一为0**\n4. VGG19比VGG16的区别在于多了3个卷积层，其它完全一样\n5. VGG16基本是AlexNet（AlexNet是8层，包括5个卷积层和3个全连接层）的加强版，深度上是其2倍，参数量大小也是两倍多。\n\n我们已经知道，VGG16相比AlexNet的一个改进是采用连续的3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5），现在，来思考几个问题吧。\n\n\n**Thinking1：使用3x3卷积核替代7x7卷积核的好处？**\n- 2 个 3x3 的卷积核叠加，它们的感受野等同于 1 个 5x5 的卷积核，3 个叠加后，它们的感受野等同于 1 个 7x7 的效果。用2个3x3的卷积核代替原来的 5x5卷积核如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200824152851501.png#pic_center)\n- 由于感受野相同，3个3x3的卷积，使用了3个非线性激活函数，增加了非线性表达能力，从而可以提供更复杂的模式学习。\n- 使用3x3卷积核可以减少参数，假设现在有 3 层 3x3 卷积核堆叠的卷积层，输出和输出通道数都是C，那么它的参数总数是 3x(3x3xCxC)=27xCxC 。同样和它感受野大小一样的一个卷积层，卷积核是 7x7 的尺寸，假如输出和输出通道数都是C，那么它的参数总数就是 7x7xCxC=49xCxC。而且通过上述方法网络层数还加深了。三层3x3的卷积核堆叠参数量比一层7x7的卷积核参数链还要少。\n- 总的来说，使用3x3卷积核堆叠的形式，既增加了网络层数又减少了参数量。\n\n\n**Thinking2：多少个3x3的卷积核可以替代原来11x11的卷积核？**\n\n(11-1)/2=5，故5个3x3的卷积核可以替代原来11x11的卷积核，即n-11+1=n+(-3+1)\\*5\n\n\n**Thinking3：VGG的C网络结构使用了1x1卷积核，1x1卷积核的主要好处?**\n\n- 使用多个1x1卷积核，在保持feature map 尺寸不变（即不损失分辨率）的前提下，可以大幅增加非线性表达能力，把网络做得很deep。\n- 进行卷积核通道数的降维和升维。\n- 1x1卷积相当于线性变换，非线性激活函数起到非线性作用。\n- 总结就是：1x1 卷积核的好处是不改变感受野的情况下，进行升维和降维，同时也加深了网络的深度。\n\n\n\nVGG16和VGG19都在pytorch封装好了，如下所示：\n\n```python\ntorchvision.models.vgg16(pretrained=False)\ntorchvision.models.vgg19(pretrained=False)\n```\n\n\n代码实现：\n\n```python\nimport torch\nimport torch.nn as nn\n# 带BN层的vgg16\nclass VGG16_bn(torch.nn.Module):\n\n    def __init__(self, num_classes):\n        super(VGG16_bn, self).__init__()\n        \n        self.block_1 = nn.Sequential(\n                nn.Conv2d(in_channels=3,\n                          out_channels=64,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=64,\n                          out_channels=64,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n            \n                nn.MaxPool2d(kernel_size=2,\n                             stride=2)\n        )\n        \n        self.block_2 = nn.Sequential(\n                nn.Conv2d(in_channels=64,\n                          out_channels=128,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(128),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=128,\n                          out_channels=128,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(128),\n                nn.ReLU(inplace=True),\n            \n                nn.MaxPool2d(kernel_size=2,\n                             stride=2)\n        )\n        \n        self.block_3 = nn.Sequential(\n                nn.Conv2d(in_channels=128,\n                          out_channels=256,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=256,\n                          out_channels=256,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=256,\n                          out_channels=256,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(inplace=True),\n            \n                nn.MaxPool2d(kernel_size=2,\n                             stride=2)\n        )\n        \n          \n        self.block_4 = nn.Sequential(\n                nn.Conv2d(in_channels=256,\n                          out_channels=512,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=3,\n                          stride=1,\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n\n                nn.MaxPool2d(kernel_size=2,\n                             stride=2)\n        )\n        \n        self.block_5 = nn.Sequential(\n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=(3, 3),\n                          stride=(1, 1),\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=(3, 3),\n                          stride=(1, 1),\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n            \n                nn.Conv2d(in_channels=512,\n                          out_channels=512,\n                          kernel_size=(3, 3),\n                          stride=(1, 1),\n                          padding=1),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True),\n\n                nn.MaxPool2d(kernel_size=(2, 2),\n                             stride=(2, 2))\n        )\n        \n        #自适应平均池化，见https://www.zhihu.com/question/282046628\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        \n        self.classifier = nn.Sequential(\n                nn.Linear(512 * 7 * 7, 4096),\n                nn.ReLU(True),\n                nn.Dropout(),\n                nn.Linear(4096, 4096),\n                nn.ReLU(True),\n                nn.Dropout(),\n                nn.Linear(4096, num_classes),\n        )\n        \n         \n        # 初始化权重\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # VGG采用了Kaiming initialization\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)            \n    \n        \n    def forward(self, x):\n\n        x = self.block_1(x)\n        x = self.block_2(x)\n        x = self.block_3(x)\n        x = self.block_4(x)\n        x = self.block_5(x) \n        x = self.avgpool(x)\n        # 拉平     \n        x = torch.flatten(x, 1)      \n        x = self.classifier(x)\n        # 一般不用softmax\n        # x = F.softmax(x, dim=1)\n        return x\n```\n\n```python\n# 拿一个数据测试下吧！\nif __name__ == \"__main__\":\n    a=torch.randn(3,224,224)\n    a=a.unsqueeze(0)\n    print(a.size())\n    net = VGG16_bn(10)\n    x=net(a)\n    print(x.size())\n\"\"\"\n# 输出\ntorch.Size([1, 3, 224, 224])\ntorch.Size([1, 10])\n\"\"\"\n```\n\n【参考文档】\n1. [【卷积神经网络结构专题】经典网络结构之VGG(附代码实现)](https://mp.weixin.qq.com/s?__biz=MzU2NDExMzE5Nw==&mid=2247487630&idx=2&sn=77d0bcef8452741823081ac8a0e4ed44&chksm=fc4eaacccb3923da09e61e26f4950a860881b07fb619f8f9d3b0ee61f17a3a76c2cba7aa75e9&scene=158#rd)\n2. [【模型解读】从LeNet到VGG，看卷积+池化串联的网络结构](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029512&idx=1&sn=a46fc10de7daba25694bda75a916aa91&chksm=871345f5b064cce3c16ab3b7c671f9e93c838836e20d0aa91bc83f7879915d0c8318bcd9d187&mpshare=1&scene=1&srcid=0804y5ewJsTSJKBSVaMNmvrm&sharer_sharetime=1596532567452&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a4858b2c5b0a38954321b0f8c3dc80d539d1fc2a0778dc107c45ce2aca8614afc433452f4fd83aba02ddb6a1be7e244027038c09196c4dc62cca07dafbdb87d527756f0a49d5105425e85&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AZ6oMa1fen0omFBd4MdcdrU=&pass_ticket=bfbIoM7yRmV2MGkwcfISFD0R1Uc2EfrGFv2CzbEqH1kNmM/wiobhHOk806C/dvoE)\n3.    [深度学习之基础模型-VGG](https://blog.csdn.net/whz1861/article/details/78111606)\n4. 【深度学习之pytorch计算机视觉】-唐进民著\n\n\n","tags":["VGG"],"categories":["神经网络"]},{"title":"array，list，tensor，Dataframe，Series之间互相转换总结","url":"/2020/11/24/223046/","content":"\n\n本文转载自：[【串讲总结】array, list, tensor，Dataframe，Series之间互相转换总结](https://blog.csdn.net/qq_33431368/article/details/107581604?utm_medium=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight)\n\n## 一、前言\n对于在Deep Learning的学习中总会有几个数据类型的转换，这次想把这些常用的转换做一个总结，方便以后看。\n\n<!-- more -->\n\n\n\n\n\n这些主要包括：`Dataframe、Series(pandas), array(numpy), list, tensor(torch)`\n\n## 二、定义\n\n### 2.1 Dataframe和Series\n\n这里简单介绍一下这两个结构。Dataframe创建的方式有很多种，这里不赘述了。以下举个例子，因为我们这里要讲的是和array等的转换，这里全都用数字型的元素。\n\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9GSXpPRWliOFZRVXBPczdPeU9INkxtSjRrcTdIcksxVVdtV3lRenpleUFQQzRXSDBkc2xDQjdOcDdobldxbllpYk50aWJmYzBSajNMb1pWR0N6cXNEekY1QS82NDA?x-oss-process=image/format,png)\n对于dataframe来说，我们打印出来，结构类似于一个二维矩阵格式，只是每一列和每一个行都有个index，这并且这些结构之间有很多方便的操作，在读入结构化数据的时候尤为方便，所以平时做偏结构化数据的时候， 比如excel、pickle等等，pandas的使用是绕不开的。\n\n而其中的series相当于dataframe的一个元素，如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081320193186.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nSeries只有row index，**有点类似于一个一维向量**。\n\n\n而DataFrame既有行索引也有列索引，它也可以被看做由Series组成的字典（共同用一个索引）\n\n\n### 2.2 array\n\n数组结构是由不同维度的list转换来的，用array的原因主要在于有更多的矩阵操作，数据使用起来更方便，比如转置、矩阵相乘、reshape等等。\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813204916515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n### 2.3 tensor\n张量是在深度学习框架中的一个数据结构，把数据喂进模型中需要把数据转换为tensor结构，等我们再取出来做框架以外的操作，比如保存成文件，用plot画图，都需要重新转换为array或list结构。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813204952507.png)\n## 三、互相转换\n\n先用一个例子直观举例下\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081320501214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813205228508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n## 四、详细总结\n### 4.1 Dataframe到Series\n挑一列的index取出\n\n```python\nSeries = Dataframe['column']\n```\n\n### 4.2 Series到list\n\n```python\nlist = Series.to_list()\n```\n\n### 4.3 list 转 array\n\n```python\narray = np.array(list)\n```\n\n### 4.4 array 转 torch.Tensor\n\n```python\ntensor = torch.from_numpy(array)\n```\n\n### 4.5 torch.Tensor 转 array\n\n```python\narray = tensor.numpy()\n# gpu情况下需要如下的操作\narray = tensor.cpu().numpy()\n```\n\n### 4.6 torch.Tensor 转 list\n\n```python\n# 先转numpy，后转list\nlist = tensor.numpy().tolist()\n```\n\n### 4.7 array 转 list\n\n```python\nlist = array.tolist()\n```\n\n### 4.8 list 转 torch.Tensor\n\n```python\ntensor=torch.Tensor(list)\n```\n\n### 4.9 array或者list转Series\n\n```python\nseries = pd.Series({'a': array})\nseries2 = pd.Series({'a': list})\n```\n\n之后这里的操作就多了，看你具体需求了，也可以多个series拼成一个dataframe, 如下， 其他操作不一一赘述了\n\n```python\ndf = pd.DataFrame({'aa': series, 'bb': series2})\n```\n\n原文链接：[https://blog.csdn.net/qq_33431368/article/details/107581604?utm_medium=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight](https://blog.csdn.net/qq_33431368/article/details/107581604?utm_medium=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-OPENSEARCH-1.edu_weight)\n","tags":["python"],"categories":["python"]},{"title":"【超详细】对比10种优化函数BGD、SGD、mini-batch GD、Momentum、NAG、Adagrad、RMSProp、Adadelta、Adam、AMSgrad","url":"/2020/11/24/222922/","content":"\n在实践中常用到一阶优化函数，典型的一阶优化函数包括 BGD、SGD、mini-batch GD、Momentum、Adagrad、RMSProp、Adadelta、Adam 等等，**一阶优化函数在优化过程中求解的是参数的一阶导数**，这些一阶导数的值就是模型中参数的微调值。另外，近年来二阶优化函数也开始慢慢被研究起来，二阶方法因为计算量的问题，现在还没有被广泛地使用。\n\n<!-- more -->\n\n\n\n深度学习模型的优化是一个**非凸函数优化**问题，这是与**凸函数优化**问题对应的。对于凸函数优化，任何局部最优解即为全局最优解。几乎所有用梯度下降的优化方法都能收敛到全局最优解，损失曲面如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811103153430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n而非凸函数优化问题则可能存在无数个局部最优点，损失曲面如下，可以看出有非常多的极值点，有极大值也有极小值。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811103348878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n本文将从原理、公式、代码、loss曲线图、优缺点等方面详解论述：\n@[TOC]( )\n\n\n## 一、BGD/SGD/mini-batch GD\n梯度下降算法主要有BGD、SGD、mini-batch GD，后面还有梯度下降算法的改进，即Momentum、Adagrad 等方法\n### 1.1 BGD\n\n**BGD**（Batch gradient descent，批量梯度下降），是拿所有样本的loss计算梯度来更新参数的，更新公式如下：\n$$\\theta=\\theta-\\eta· \\nabla_\\theta J(\\theta)$$\n\n> 在有的文献中，称GD是拿所有样本的loss计算梯度来更新参数，也就是全局梯度下降，和这里的BGD是一个意思\n\n其中，$\\theta$为要更新的参数，即weight、bias；$\\eta$ 为学习率；$J$为损失函数，即 loss function ；$\\nabla_\\theta J(\\theta)$ 是指对 loss function 的 $\\theta$ 求梯度。\n\n令$\\Delta\\theta_t=-\\eta· \\nabla_\\theta J(\\theta)$，则$\\theta_{t+1}=\\theta_{t}+\\Delta\\theta_t$，$\\theta_{1}$ 即初始化的weight、bias。\n\n写成伪代码如下：\n\n```python\n# all_input和all_target是所有样本的特征向量和label\nfor i in range(epochs):\n    optimizer.zero_grad()\n    output = model(all_input)\n    loss = loss_fn(output,all_target)\n    loss.backward()\n    optimizer.step()\n```\n\n\n\n在loss的等值线图中，随着 weight 的变化loss降低的曲线走向（红线）如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811100747627.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n其中 x1是纵轴，x2是横轴 ；$weight=[x1,x2]$ ，即两个坐标轴对应的点；$X_i=[x1_i,x2_i]$，即weight不同时刻的取值；$X_0=[x1_0,x2_0]$是weight的初始化值。\n\n从上图中可以看出，**BGD的loss曲线走向相对平滑，每一次优化都是朝着最优点走**。\n\n\n由于BGD在每次计算损失值时都是针对整个参与训练的样本而言的，所以会出现**内存装不下，速度也很慢的情况**。能不能一次取一个样本呢？于是就有了随机梯度下降（Stochastic gradient descent），简称 sgd。\n\n### 1.2 SGD\n\n**SGD**（Stochastic gradient descent，随机梯度下降）是**一次拿一个样本的loss计算梯度来更新参数**，其更新公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811105800214.png)\n其中，$x^{(i)}$ 是第一个样本的特征向量，$y^{(i)}$ 是第i个样本的真实值。\n\n也可以写成如下形式：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811110014943.png)\n其中，$g_{t,i}$ 是第i个样本的梯度。\n\n写成伪代码如下：\n\n```python\nfor i in range(epochs):\n    # batch=1,每次从dataset取出一个样本\n    for input_i,target_i in dataset:\n        optimizer.zero_grad()\n        output = model(all_input)\n        loss = loss_fn(output,all_target)\n        loss.backward()\n        optimizer.step()\n```\n\n在loss的等值线图中，随着 weight 的变化loss降低的曲线走向如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200811111504701.jpg)\n从上图中可以看出，**SGD的loss曲线走向是破浪式的，相对于BGD的方式，波动大，在非凸函数优化问题中，SGD可能使梯度下降到更好的另一个局部最优解，但从另一方面来讲，SGD的更新可能导致梯度一直在局部最优解附近波动**。\n\n> SGD的不确定性较大，可能跳出一个局部最优解到另一个更好的局部最优解，也可能跳不出局部最优解，一直在局部最优解附近波动\n\n由于同一类别样本的特征是相似的，因此某一个样本的特征能在一定程度代表该类样本，所以SGD最终也能够达到一个不错的结果，但是，SGD的更新方式的波动大，更新方向前后有抵消，存在浪费算力的情况。于是，就有了后来大家常用的小批量梯度下降算法（Mini-batch gradient descent）。\n\n### 1.3 Mini-batch GD\nMini-batch GD（Mini-batch gradient descent，小批量梯度下降）是**一次拿一个batch的样本的loss计算梯度来更新参数**，其更新公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812152612762.png)\n其中，batch_size=n。\n\n写成伪代码如下：\n\n```python\nfor i in range(epochs):\n    # batch_size=n,每次从dataset取n个样本\n    for input,target in dataset:\n        optimizer.zero_grad()\n        output = model(all_input)\n        loss = loss_fn(output,all_target)\n        loss.backward()\n        optimizer.step()\n```\n在loss的等值线图中，随着 weight 的变化loss降低的曲线走向如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812153352680.png)\n\n从上图可以看出，mini-batch GD 的loss走向曲线在BGD和SGD之间，mini-batch GD 既解决了SGD更新方式波动大的问题，又可以尽量去计算多个样本的loss，提高参数的更新效率。\n\n### 1.4 GD法的缺点\n梯度下降算法虽然取得了一定的效果，但是仍然有以下缺点：\n- 学习率大小比较难缺难确定，需要反复去试\n- 学习率不够智能，对每个参数的各个维度一视同仁\n- mini-batch GD算法中，虽然一定程度上提高参数的更新效率，并没有完全解决SGD中的问题，即更新方向仍然前后有抵消，仍然有浪费算力的情况\n- SGD和mini-batch GD由于每次参数训练的样本是随机选取的，模型会受到随机训练样本中噪声数据的影响，又因为有随机的因素，所以也容易导致模型最终得到局部最优解。\n\n\n\n## 二、Momentum/NAG\n知道了GD法的缺点，动量法通过之前积累梯度来替代真正的梯度从而避免GD法浪费算力的缺点，加快更新速度，我们现在来看**动量法**（momentum）和其改进方法NAG吧。\n### 2.1 Momentum\n在使用梯度下降算法的时，刚开始的时候梯度不稳定，波动性大，导致做了很多无用的迭代，浪费了算力，**动量法**（momentum）解决SGD/mini-batch GD中参数更新震荡的问题，**通过之前积累梯度来替代真正的梯度，从而加快更新速度**，其更新公式如下：\n$$\\begin{array}{l}\n\\upsilon_t=\\gamma\\upsilon_{t-1}+\\eta· \\nabla_\\theta J(\\theta)\\\\        \n\\theta=\\theta-\\upsilon_t\n\\end{array}$$\n其中，$\\theta$是要更新的参数即weight，$\\nabla_\\theta J(\\theta)$是损失函数关于weight的梯度，$\\gamma$为动量因子，通常设为0.9；$\\eta$ 为学习率。这里新出现了一个变量 $\\upsilon$，对应物理上的速度。\n也可以写成下面的形式（不常用）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812161157101.png)\n其中，$\\rho$为动量因子，通常设为0.9；$\\alpha$为学习率。\n\n只看公式可能不好理解，我们来代入计算下吧。\n\n假设 $\\eta$ 学习率为0.1，用 g 表示损失函数关于weight的梯度，则可计算如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812164126220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n**动量法**（momentum）更新示意图如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812161702116.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n这样， 每个参数的实际更新差值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内的梯度方向不一致时， 其真实的参数更新幅度变小，增加稳定性； 相反， 当在最近一段时间内的梯度方向都一致时， 其真实的参数更新幅度变大， 起到加速作用。\n\n一般而言， 在迭代初期， 梯度方向都比较一致， 动量法会起到加速作用， 可以更快地到达最优点。**在迭代后期， 梯度方向会不一致， 在收敛值附近振荡， 动量法会起到减速作用， 增加稳定性**。动量法也能解决稀疏梯度和噪声问题，这个到Adam那里会有详细解释。\n\n\n\n### 2.2 NAG\n**NAG**（Nesterov Accelerated Gradient，Nesterov加速梯度）是一种对动量法的改进方法， 也称为 Nesterov 动量法（Nesterov Momentum）。其公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812162552828.png)\n其中，$\\nabla_\\theta J(\\theta-\\gamma\\upsilon_{t-1})$是损失函数关于下一次（提前点）weight的梯度。\n\n也可以写为（不常用）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812164820951.png)\n\n**NAG**更新示意图如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812165251355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n由于momentum刚开始时梯度方向都比较一致，收敛较快，但是到后期，由于momentum惯性的存在，很可能导致在loss极值点的附近来回震荡，而**NAG向前计算了一次梯度，当梯度方向快要改变的时候，它提前获得了该信息，从而减弱了这个过程，再次减少了无用的迭代，并保证能顺利更新到loss的极小值点**。\n\n\n\n\n\n\n## 三、Adagrad/RMSProp/Adadelta\n在神经网络的学习中，学习率$\\eta$的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。\n\n在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”学，然后逐渐“少”学的方法，**在神经网络的学习中经常被使用。逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低**。而AdaGrad进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值，现在我们来看Adagrad/RMSProp/Adadelta吧。\n\n\n\n### 3.1 Adagrad\nAdaGrad（Adaptive Grad，自适应梯度）为参数的每个参数自适应地调整学习率，让不同的参数具有不同的学习率，其公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812173130536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n其中，W表示要更新的权重参数，$\\frac{ {\\partial L} }{ {\\partial W} }$表示损失函数关于W的梯度，$\\eta$表示学习率，这里新出现了变量$h$，它保存了以前的所有梯度值的平方和，$\\odot$表示对应矩阵元素的乘法。然后，在更新参数时，通过乘以 $\\frac{1}{\\sqrt h}$，就可以调整学习的尺度。这意味着，**参数的元素中变动较大（被大幅更新）的元素的学习率将变小。也就是说，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小**。\n\nAdagrad公式也可以写为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812192420580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n其中， $g_\\tau \\in R$ 是第$\\tau$次迭代时的梯度，$\\alpha$为初始的学习率，$\\varepsilon$是为了保持数值稳定性而设置的非常小的常数， 一般取值 $e^{−7}$ 到 $e^{−10}$，此外， 这里的开平方、 除、 加运算都是按元素进行的操作。\n\n由于Adagrad学习率衰减用了所有的梯度，如果在经过一定次数的迭代依然没有找到最优点时，累加的梯度幅值是越来越大的，导致学习率越来越小， 很难再继续找到最优点，为了改善这个问题，从而提出RMSProp算法。\n\n\n### 3.2 RMSProp\n与AdaGrad不同，**RMSProp（Root Mean Square Propagation，均方根传播） 方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度**。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812192541768.png)\n\n\n其中，$\\beta$ 为衰减率， 一般取值为 0.9，$\\alpha$为初始的学习率， 比如 0.001。\n\n从上式可以看出， RMSProp 算法和 AdaGrad 算法的区别在于 $G_t$ 的计算由累积方式变成了指数衰减移动平均。在迭代过程中， 并且，**每个参数的学习率并不是呈衰减趋势， 既可以变小也可以变大**（把$\\beta$设的更小些，每个参数的学习率就呈变大趋势）。\n\n> 这里不得不提一下RProp，Rprop可以看做 RMSProp的简单版，它是依据符号来改变学习率的大小：当最后两次梯度符号一样，增大学习率，当最后两次梯度符号不同，减小学习率。\n\n\n\n### 3.3 AdaDelta \nAdaDelta 与 RMSprop 算法类似， AdaDelta 算法也是通过**梯度平方的指数衰减移动平均来调整学习率**。此外， **AdaDelta 算法还引入了每次参数更新差值Δ𝜃 的平方的指数衰减权移动平均**。\n\n第 t 次迭代时， 参数更新差值 Δ𝜃 的平方的指数衰减权移动平均为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081218193921.png)\nAdaDelta 算法的参数更新公式为：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812192717500.png)\n\n\n其中 $G_t$ 的计算方式和 RMSprop 算法一样 ， $\\Delta X_{t - 1}^2$ 为参数更新差值 Δ𝜃 的指数衰减权移动平均。\n\n从上式可以看出， **AdaDelta 算法将 RMSprop 算法中的初始学习率 𝛽 改为动态计算的 $\\sqrt {\\Delta X_{t - 1}^2}$ ，在一定程度上平抑了学习率的波动**。除此之外，AdaDelta连初始的学习率都不要设置了，提升了参数变化量的自适应能力。\n\n除此之外，AdaDelta公式还有一个常用的表示方法：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812193957164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n## 四、Adam\\AMSgrad\n现在来介绍比较好用的方法Adam和其改进方法AMSgrad。\n### 4.1 Adam\n**Adam** 算法 （Adaptive Moment Estimation Algorithm）可以看作动量法和 RMSprop 算法的结合， **不但使用动量作为参数更新方向， 而且可以自适应调整学习率**。\n\n\nAdam 算法一方面计算梯度平方 $g^2_t$ 的指数衰减移动平均（和 RMSprop 算法类似）, 另一方面计算梯度 $g_t$ 的指数衰减移动平均 （和动量法类似），如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812194548127.png)\n\n\n其中 $\\beta_1$ 和 $\\beta_2$ 分别为两个移动平均的衰减率， 通常取值为 $\\beta_1$ = 0.9，$\\beta_2$ = 0.999。 **我们可以把 $M_t$ 和 $G_t$ 分别看作梯度的均值（一阶矩估计）和未减去均值的方差（二阶矩估计）**。其中，$M_t$ 来自momentum，用来稳定梯度，$G_t$ 来自RMSProp，用来是梯度自适应化。\n\n对$M_t$ 和 $G_t$偏差进行修正：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812210354974.png)\n\n\nAdam 算法的参数更新公式为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081220485436.png)\n\n\n\n其中，其中学习率 $\\alpha $ 通常设为 0.001， 并且也可以进行衰减， 比如 $\\alpha_t=\\frac{\\alpha_{0}}{\\sqrt t}$ 。\n\n\nAdam， 结合了 动量法 和 RMSProp 算法最优的性能，它还是能提供解决**稀疏梯度和噪声问题**的优化方法，在深度学习中使用较多。这里解释一下为什么Adam能够解决**稀疏梯度和噪声问题**：稀疏梯度是指梯度较多为0的情况，由于adam引入了动量法，在梯度是0的时候，还有之前更新时的梯度存在（道理跟momentum一样），还能继续更新；噪声问题是对于梯度来说有一个小波折（类似于下山时路不平有个小坑）即多个小极值点，可以跨过去，不至于陷在里面。\n\n\nAdam 算法是 RMSProp 算法与动量法的结合， 因此一种自然的 Adam 算法的改进方法是引入 Nesterov 加速梯度， 称为**Nadam** 算法\n\n**这里思考一个问题：为什么对Adam偏差进行修正？**\n\n网上没找到好的解释，那来看一下原文吧！\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812211001933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n大致意思是说：刚开始，我们任意初始化了一个 $m_0$（注意：一般$m_0$初始化为0，在Momentum算法中也是），并且根据公式$m_t=\\beta m_{t-1}+(1-\\beta)g_t$更新公式，得到 $m_1$，可以明显的看到，第一步更新严重依赖初始化的$m_0$，这样可能会造成严重的偏差。\n\n为了纠正这个，我们需要移除这个初始化$m_0$（偏置）的影响，例如，可以把$m_1=\\beta m_{0}+(1-\\beta)g_1$中的$\\beta m_{0}$从$m_1$移除（即$m_1-\\beta m_0$），并且除以（$1-\\beta$），这样公式就变为了 $\\hat{m}=\\frac{m_1-\\beta m_0}{1-\\beta}$，当$m_0$=0时，$\\hat{m_t}=\\frac{m_t}{1-\\beta^t}$。同理，对于$G_t$也是如此。\n\n总的来说，在迭代初期，$M_t$ 和 $G_t$的更新严重依赖于初始化的$M_0=0$、$G0=0$，当初始化$M_t$ 和 $G_t$都为0时，$M_t$ 和 $G_t$都会接近于0，这个估计是有问题的，可能会造成严重的偏差（即可能使学习率和方向与真正需要优化的学习率和方向严重偏离），所以，我需要移除这个初始化的$M_0$ 和 $G0$（偏置）的影响， 故需要对偏差进行修正。\n\n\n**上面说了这么多理论，分析起来头头是道，各种改进版本似乎各个碾压 SGD 算法。但是否真的如呢？此外，Adam看起来都这么厉害了，以后的优化函数都要使用Adam吗？**\n\n来看一下下面的实验：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813163800385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n所有方法都采用作者们（原文）的默认配置，并且进行了参数调优，不好的结果就不拿出来了。\n- nesterov 方法，与 sgd 算法同样的配置。\n- adam 算法，m1=0.9，m2=0.999，lr=0.001。\n- rms 算法，rms_decay=0.9，lr=0.001。\n- adagrad，adadelta 学习率不敏感。\n\n\n看起来好像都不如 SGD 算法，实际上这是一个很普遍的现象，各类开源项目和论文都能够印证这个结论。总体上来说，改进方法降低了调参工作量，只要能够达到与**精细调参的 SGD** 相当的性能，就很有意义了，这也是 Adam 流行的原因。但是，**改进策略带来的学习率和步长的不稳定还是有可能影响算法的性能**，因此这也是一个研究的方向，不然哪来这么多Adam 的变种呢。\n\n这里引用一位清华博士举的例子：很多年以前，摄影离普罗大众非常遥远。十年前，傻瓜相机开始风靡，游客几乎人手一个。智能手机出现以后，摄影更是走进千家万户，手机随手一拍，前后两千万，照亮你的美（咦，这是什么乱七八糟的）。但是专业摄影师还是喜欢用单反，孜孜不倦地调光圈、快门、ISO、白平衡……一堆自拍党从不care的名词。技术的进步，使得傻瓜式操作就可以得到不错的效果，但是在特定的场景下，要拍出最好的效果，依然需要深入地理解光线、理解结构、理解器材。优化算法大抵也如此，大家都是殊途同归，只是相当于在SGD基础上增加了各类学习率的主动控制。如果不想做精细的调优，那么Adam显然最便于直接拿来上手。\n\n原文在[Adam那么棒，为什么还对SGD念念不忘 (2)—— Adam的两宗罪](https://zhuanlan.zhihu.com/p/32262540)\n\nAdam那么棒，也不是没有缺点，继续往下面看！\n\n### 4.2 AMSgrad\nICLR 2018 最佳论文提出了 AMSgrad 方法，研究人员观察到 Adam 类的方法之所以会不能收敛到很好的结果，是因为在优化算法中广泛使用的指数衰减方法会使得梯度的记忆时间太短。\n\n在深度学习中，每一个 mini-batch 对结果的优化贡献是不一样的，有的产生的梯度特别有效，但是也一视同仁地被时间所遗忘。\n\n具体的做法是使用过去平方梯度的最大值来更新参数，而不是指数平均。其公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813161546652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n## 五、不同优化函数比较\n\n（1）不同优化函数的loss下降曲线如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812201908590.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（2）不同优化函数在 MNIST 数据集上收敛性的比较（学习率为0.001， 批量大小为 128）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812220337887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（3）不同优化函数配对比较\n![在这里插入图片描述](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS84MC92Mi0zOWM1ZGYxNjczZDk4MzFlOWJjMTBjOWU5YzRmMTJhOF8xNDQwdy5qcGc?x-oss-process=image/format,png)\n横纵坐标表示降维后的特征空间，区域颜色则表示目标函数值的变化，红色是高原，蓝色是洼地。他们做的是配对儿实验，让两个算法从同一个初始化位置开始出发，然后对比优化的结果。可以看到，几乎任何两个算法都走到了不同的洼地，他们中间往往隔了一个很高的高原。这就说明，不同算法在高原的时候，选择了不同的下降方向。\n\n这里参考 [Adam那么棒，为什么还对SGD念念不忘 (3)—— 优化算法的选择与使用策略述](https://zhuanlan.zhihu.com/p/32338983)\n\n## 六、pytorch不同优化函数的定义\n\n这里介绍几种常用优化函数的参数定义和解释\n\n（1）**torch.optim.SGD**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813093551803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n重要参数解释：\n- lr 学习率，用于控制梯度更新的快慢，如果学习速率过快，参数的更新跨步就会变大，极易出现震荡；如果学习速率过慢，梯度更新的迭代次数就会增加，参数更新、优化的时间也会变长，所以选择一个合理的学习速率是非常关键的。\n- momentum 动量因子，介于[0,1]之间，默认为0，一般设为0.9，当某个参数在最近一段时间内的梯度方向（即与动量$\\upsilon$方向）不一致时， 其真实的参数更新幅度变小，增加稳定性； 相反， 当在最近一段时间内的梯度方向都一致时， 其真实的参数更新幅度变大， 起到加速作用。\n- weight_decay 权重衰减（L2惩罚），默认为0，来看下原文吧：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813101331216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n大致意思是说：为了避免过拟合，在这里增加了一个L2惩罚项，推到如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813101959248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n其中，$\\eta$为学习率，而$\\eta\\ \\lambda$ 就是weight_decay，不过pytorch让用户可以自由设置，有了更大的自由度\n- nesterov，布尔类型，默认为False，设为True，可使用NAG动量\n\n**使用方式：**\n```python\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\nfor i in range(epochs):\n    # batch_size=n,每次从dataset取n个样本\n    for input,target in dataset:\n        optimizer.zero_grad()\n        output = model(all_input)\n        loss = loss_fn(output,all_target)\n        loss.backward()\n        optimizer.step()\n```\n（2）**torch.optim.Adagrad**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081310322668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n主要参数详解：\n- lr-decay 学习率衰减，学习率衰减公式如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813103611835.png)\n其中，$lr_i$为第i次迭代时的学习率，$lr_start$为原始学习率，decay为一个介于[0.0, 1.0]的小数。可以看到，decay越小，学习率衰减地越慢，当decay = 0时，学习率保持不变。decay越大，学习率衰减地越快，当decay = 1时，学习率衰减最快。\n- eps，相当于$\\varepsilon$，是为了保持数值稳定性而设置的非常小的常数， 一般取值 $e^{−7}$ 到 $e^{−10}$\n\n（3）**torch.optim.Adam**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813160557807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n- betas，默认值（0.9,0.999），即Adam里的$\\beta_1$、$\\beta_2$\n- amsgrad，是否启用Adam的改进方法AMSgrad，默认为False\n\n这里只介绍了需要注意的几个优化函数，更多函数定义，请查阅[pytorch官方文档](https://pytorch.org/docs/stable/index.html)\n\n\n【参考文档】\n神经网络与深度学习-邱锡鹏著\n有三AI-深度学习视觉算法工程师指导手册\n深度学习入门：基于pytorch的理论与实现-陆宇杰译\n深度学习之pytorch实战计算机视觉-唐进民著\n[optim.sgd学习参数](https://blog.csdn.net/apsvvfb/article/details/72536495?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.edu_weight)\n\n","tags":["优化函数"],"categories":["神经网络"]},{"title":"在Windows10中使用 Jupyter Notebook 运行C++","url":"/2020/11/24/222744/","content":"\n在Windows10中使用 Jupyter Notebook 运行C++ ！\n\n<!-- more -->\n\n\n\n## 一、安装Linux子系统\n（1）**开启Subsystem-Linux服务**\n\n鼠标右键开始，选中Windows PowerShell(管理员)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080719424473.png)\n输入 `Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux` 开启服务，**需要重启电脑**\n\n（2）**安装Linux 发行版本**\n鼠标右键开始，选中设置->系统->关于，查看当前Windows10版本，需要 16215 之后的版本才能安装Linux子系统，\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807194800357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**打开 Microsoft Store 搜索 Linux，选中Ubuntu**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080719495371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**点击获取：**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807195043302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**大约432M：**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807195111220.png)\n\n（3）**开始初始化Linux**\n\n下载完成后，点击启动，首次启动需要几分钟\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807195315132.png)\n输入账号和密码\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807195530167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n你也可以右键开始，点击运行，输入 `bash` 运行Linux：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807200039210.png)\n**两种方法都可以进入Linux，但是进入的根目录不一样，输入 `pwd` 查看当前的目录**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080720022272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n`/homw/wxl` 是Linux 系统的用户路径，`/mnt/c/Users/wang1` 是Linux 系统挂载的 Windows 盘符，即C盘。\n\n（4）**更新和升级发行版的软件包**\nLinux默认源是国外的站，所以访问速度可能比较慢，换成阿里的源，步骤如下：\n\n1. 备份之前的源 `sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup`\n2. 打开源文件 `sudo vim /etc/apt/sources.list`\n3. 复制中科大源或阿里云源：\n\n**中科大源：**\n```bash\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\ndeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n```\n**阿里云源：**\n```bash\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n注意：vim，`i` 是编辑模式，可以添加和删除内容，`Esc键` 命令模式（刚进入Vim就是命令模式，命令模式下才可以输入各种命令），`:wq` 是保存并退出。`%d` 删除文件中所有内容的方法：(要切换到命令模式输入)\n\n\n**如下所示**：![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080720164568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n4.输入如下命令更新软件包\n```bash\nsudo apt update\nsudo apt upgrade\n```\n到这里Linux子系统就安装完成了\n\n## 二、安装 Miniconda\n在Linux命令行输入如下命令安装 Miniconda：\n\n```bash\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n```\n也可以复制`https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`到浏览器里下载，除非你能科学上网，否则这里下载特别慢，我把下载好的Miniconda3-latest-Linux-x86_64.sh上传到百度云上了，请自行提取，链接：https://pan.baidu.com/s/16US_Jt-UjYC-ox4Siql5vw 提取码：cc0b\n\n先将下载好的Miniconda3-latest-Linux-x86_64.sh文件手动拷贝到windows  `C:\\Users\\wang1` 目录下（wang1是你的用户名），执行`cp /mnt/c/Users/wang1/Miniconda3-latest-Linux-x86_64.sh  /homw/wxl`就拷贝到了Linux 系统的用户路径下，然后执行 `bash Miniconda3-latest-Linux-x86_64.sh`，就可以安装了，中间需要输入两个`yes`，成功界面如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200807212151891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n推荐用Ubuntu启动的方式安装，我用 `bash` 启动的方式安装失败了，然后用这种方式就成功了。\n\n**到这一步，关闭 Ubuntu 重新打开，会发现原来的用户名前面有一个（base），说明安装成功。**\n\n## 三、安装 Jupyter notebook\n输入如下命令安装Jupyter notebook\n\n```bash\npip install jupyter -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com\n```\n注意：我用 `conda install jupyter` 安装之后不能用，我也搞不清楚为什么，大家要留意这个坑。\n\n安装之后，输入 `jupyter notebook` 就可以启动notebook，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808132036301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n复制划线的地址即可访问，可以看到，报错了，但是不影响访问，如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808132158195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n点击右上角new，只有python核，按`Ctrl + Z或Ctrl + C`关闭notebook，现在我们就安装C++内核吧。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808132305885.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n## 四、安装C++内核\n安装c++d的内核插件 `xeus-cling`，这里要用conda安装，pip无法安装。\n\n先设置conda国内源\n\n```bash\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\nconda config --set show_channel_urls yes\n```\n**下面的步骤一定要注意（此处很重要），我就因为这里的坑浪费了一天的时间，没错，就是一天的时间**\n\n\n**本人安装了很多次xeus-cling都因为下载速度太慢而以失败告终，后来发现是因为安装xeus-cling时，我们想通过指定channel加快访问速度时，conda反而会优先访问默认源而非镜像。**\n\n可以通过 `conda config --show` 看到 默认情况下的 `channel_alias` 值是 `https://conda.anaconda.org/` ，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080821212859.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**说明，在conda还是优先访问默认源而非镜像，输入如下命令修改默认源：**\n\n```bash\nconda config --set channel_alias https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n```\n可以看到，修改之后默认源就变了\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212254993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n然后输入下面命令愉快的安装c++内核插件吧，中间需要输入y确认\n\n```bash\nconda install -c conda-forge xeus-cling\n```\n看到这里就安装成功啦\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212454653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n## 五、使用notebook写C++程序\n\n安装c++内核插件后，再次输入`jupyter notebook` 命令，可以看到右上角多了`C++11、C++14、C++17`，这是不同版本的C++。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212624883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n新建一个 C++14 notebook，输入一些 C++ 代码，Shift + Enter 可以得到运行结果，没有报错就大功告成了！\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212749224.png)\n也可以输入`?cout 或 ?std::cout` 查看cout函数的帮助文档，其他的一些用法可以在 [xeus-cling 的文档](https://xeus-cling.readthedocs.io/en/latest/) 中查看\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808212933561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n**如果我们需要在某个文件夹中保存下 notebook，在要保存的文件夹下按住 shift 单击右键，选择“在此处打开 Linux shell”，这样打开的 Jupyter notebook 的目录就是该目录，新建的 notebook 也自动保存在了当前目录，而不会在 Linux 系统里。**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808213342832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n按`Ctrl+z`关闭notebook，为notebook安装左侧导航插件吧：\n\n```bash\npip install jupyter_nbextensions_configurator jupyter_contrib_nbextensions -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com \njupyter contrib nbextension install --user\njupyter nbextensions_configurator enable –user\n```\n这样左侧就出现导航啦\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808222228505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n可以再安装 Jupyter lab，比 Jupyter notebook 好看，文件管理也方便，直接安装就可以和 Jupyter notebook 一样使用：\n\n```bash\nconda install -c conda-forge jupyterlab\n\n安装完成后...\n\njupyter lab\n```\n打开 jupyter lab 界面如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808225255441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n**到这里就都大功告成了，留下一个问题，这里为什么报错（如下所示），我还没解决，虽然不影响使用，但是作为强迫症，总想把它去掉，拜托请留言，谢谢了**：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200808235538521.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n## 六、Linux常用命令\n\n在linux下，必然要经常和linux打交道，一些简单的linxu命令\n\n1、vim 命令：\n`vim 文件名` 用vim打开文件。vim中，`i` 是编辑模式，可以添加和删除内容，`Esc键` 命令模式（刚进入Vim就是命令模式，命令模式下才可以输入各种命令），`:wq` 是保存并退出。`%d` 删除文件中所有内容的方法：(要切换到命令模式输入)\n\n2、删除文件：\n- `rm 文件名`  \n\n3、删除整个文件夹及文件夹里的文件：\n`rm -rf /var/log/httpd/access` 将会删除/var/log/httpd/access目录以及其下所有文件、文件夹\n说明：\n-r 就是向下递归，不管有多少级目录，一并删除\n-f 就是直接强行删除，不作任何提示的意思\n\n4、拷贝文件：\n\n（1）`cp -rv A B` 拷贝A文件夹到B目录\n（2）如果你正在B目录下,可以这样:  `cp -rv A ./`\n（3）`cp -v A/A1 ./` 拷贝A文件下的A1文件\n\n\n5、赋予权限\n`sudo chmod -R 777` 文件夹\n参数-R是递归的意思\n777表示开放所有权限\n\n`chmod +x 某文件`\n\n如果给所有人添加可执行权限：chmod a+x 文件名；\n如果给文件所有者添加可执行权限：chmod u+x 文件名；\n如果给所在组添加可执行权限：chmod g+x 文件名；\n如果给所在组以外的人添加可执行权限：chmod o+x 文件名；\n\n6、`cd ~` 进入用户根目录\n\n7、查看历史命令\n`history`\n\n8、卸载软件\n`sudo apt-get remove nodejs` 卸载nodejs\n\n9、安装nodejs\n参考 https://blog.csdn.net/qq_41204927/article/details/83537207\n（1）去官网下载最新版nodejs https://nodejs.org/en/download/current/\n（2）卸载已安装的Node和npm，这一点很重要，要不你装好了 node -v 还是原来的版本\n```bash\nsudo apt remove npm  //卸载npm\nsudo apt remove node //卸载node\n\ncd /usr/local/bin   //进入该目录中，若有node或者npm文件，将他删除删除\n```\n\n（3）下载完成后通过XFtp 或者其他类似软件传到服务器上，然后解压到opt目录下\n`tar -xJf /mnt/c/Users/wang1/node-v14.7.0-linux-x64.tar.xz  -C /opt`\n（4）建立链接到 /usr/local/bin/ 目录（但是我试了/usr/bin/成功）\n`sudo ln -s /opt/node-v8.5.0-linux-x64/bin/node /usr/local/bin/node`\n输入`node -v`成功\n`sudo ln -s /opt/node-v14.7.0-linux-x64/bin/node /usr/local/bin/node`或`sudo ln -s /opt/node-v14.7.0-linux-x64/bin/node /usr/bin/node`\n不知道为什么，输入`npm -v`均不成功\n（5）设置淘宝镜像\n```bash\nsudo npm config set registry https://registry.npm.taobao.org   //设置淘宝镜像\nsource ~/.bashrc       //使修改立即生效\n```\n\n【参考文档】\n[在 Win10 中使用 Jupyter notebook 运行 C++ 详细教程](https://blog.csdn.net/qq_20084101/article/details/89494474)\n[Windows 10 安装 Linux 子系统（Windows Subsystem for Linux）](https://blog.csdn.net/qq_20084101/article/details/82316263)\n[windows下使用 Jupyter notebook 运行 C++](https://zhuanlan.zhihu.com/p/84753836)","categories":["工具"]},{"title":"从Inception到Xception，卷积方式的成长之路！","url":"/2020/11/24/222612/","content":"\n2014年Google提出了多尺度、更宽的**Inception**网络结构，不仅比同期的VGG更新小，而且速度更快。Xception则将Inception的思想发挥到了极致，解开了**分组卷积**和大规模应用的序幕。\n\n本文将详细讲述\n- Inception v1的多尺度卷积和Pointwise Conv\n- Inception v2的小卷积核替代大卷积核方法\n- Inception v3的卷积核非对称拆分\n- Bottleneck卷积结构\n- Xception的Depthwise Separable Conv深度可分离卷积\n\n<!-- more -->\n\n\n\n\n\n## 多尺度卷积\nInception 最初提出的版本主要思想是**利用不同大小的卷积核实现不同尺度的感知**，网络结构图如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806113406524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nInception Module基本组成结构有四个成分。1\\*1卷积，3\\*3卷积，5\\*5卷积，3\\*3最大池化。最后对四个成分运算结果进行通道上组合，这就是Inception Module的核心思想：利用不同大小的卷积核实现不同尺度的感知，最后进行融合，可以得到图像更好的表征。\n\n\n使用了多尺度卷积后，我们的网络更宽了，同时也提高了对于不同尺度的适应程度。\n\n## Pointwise Conv\n使用了多尺度卷积后，我们的网络更宽了，虽然提高了对于不同尺度的适应程度，但是计算量也变大了，所以我们就要想办法减少参数量来减少计算量，于是在 **Inception v1** 中的**最终版本**加上了 1x1 卷积核，网络结构图如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806113747780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*图a与图b的区别就是是否使用了 1x1 卷积进行压缩降维*\n\n使用1x1 卷积核主要目的是进行**压缩降维，减少参数量**，这就是**Pointwise Conv**，简称PW。\n\n\n\n举个例子，假如输入的维度是 96 维，要求输出的维度是 32 维，二种计算方式：\n- 第一种：用3x3的卷积核计算，参数量是`3*3*96*32=27648`（为了方便计算，这里忽略偏置bias，后面的计算均如此）\n- 第二种：先用1x1卷积核将输出通道降维到32，参数量是`1*1*96*32=3072`，再用3x3卷积计算输出，参数量是`3*3*32*32=9216`，总的参数量是`3072+9216=12288`\n\n从结果`12288/27648=0.44`可以看到，第二种方式的参数量是第一种方式的0.44倍，大大减少了参数量，加快训练速度。\n\n\n由Inception Module组成的GoogLeNet（Inception V1）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806125450803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n> Inception V1 的主要思想是利用不同大小的卷积核实现了不同尺度的感知，再加上1x1 卷积的大量运用，模型比较精简，比VGG更深但是却更小\n\n也有用**Pointwise Conv**做升维的，在 MobileNet v2 中就使用 Pointwise Conv 将 3 个特征图变成 6 个特征图，丰富输入数据的特征。\n\n## 卷积核替换\n\n就算有了Pointwise Conv，**由于 5x5 卷积核直接计算参数量还是非常大，训练时间还是比较长**，于是Google学习VGGNet的特点，提出了**使用多个小卷积核替代大卷积核的方法**，这就是 **Inception V2**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806130828579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n在Inception V2中，使用两个 3x3 卷积核来代替 5x5 卷积，不仅使参数量少了，深度也变深了，提升了神经网络的效果，可谓一举多得。\n> 为什么提升了网络深度，可以提升神经网络效果？\n> 因为多层非线性层(每一层都加了relu)可以提供更复杂的模式学习，而且参数量更少 => 采用堆积的小卷积核优于采用大卷积核（相同感受野的情况下）\n\n现在来计算一下参数量感受下吧！\n\n假设输入 256 维，输出 512 维，计算参数量：\n- 使用 **5x5 卷积核**，参数量为`5*5*256*512=3276800`\n- 使用**两个 3x3 卷积核**，参数量为`3*3*256*256+3*3*256*512=1769472`\n\n\n从结果`1769472/3276800=0.54`可以看到，第二种方式的参数量是第一种方式0.54倍，大大的减少了参数量，加快训练速度。\n\n## 卷积核拆分\n\n在使用多个小卷积核替代大卷积核的方法后，参数量还是比较大，于是Google学习Factorization into small convolutions的思想，在Inception V2的基础上，将一个二维卷积拆分成两个较小卷积，例如将7\\*7卷积拆成1\\*7卷积和7\\*1卷积，这样做的好处是降低参数量。该paper中指出，**通过这种非对称的卷积拆分比对称的拆分为几个相同的小卷积效果更好**，可以处理更多，更丰富的空间特征。这就是**Inception V3**网络结构：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080613330515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n让我们计算下参数量感受下吧！\n\n假设输入 256 维，输出 512 维，计算参数量：\n- 使用 **5x5 卷积核**，参数量为`5*5*256*512=3276800`\n- 先使用**两个 1x5和5x1 卷积核**，参数`1*5*256*256+5*1*256*512=983040`\n\n从结果`983040/3276800=0.3`可以看到，第二种方式的参数量是第一种方式0.3倍，比使用多个小卷积核替代大卷积核的方法减少还多。\n\nInception V4考虑到借鉴了微软的ResNet网络结构思想，等以后再做详细介绍。\n\n## Bottleneck\n我们发现使用上面的结构和方法，参数量还是较大，于是人们提出了 **Bottleneck** 的结构降低参数量。\n\n Bottleneck结构分三步走，首先用Pointwise Conv进行降维，再用常规卷积核进行卷积，最后使用Pointwise Conv进行进行升维，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806135210339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n来吧，又到了计算参数量的时刻！\n\n假设输入 256 维，输出 512 维，计算参数量：\n- 使用 **3x3 卷积核**，参数量为`3*3*256*256=589824`\n- 使用 **Bottleneck** 的方式，先使用**1x1卷积核**将输入的256维讲到64维，再使用**3x3卷积核**进行卷积，最后用**1x1卷积核**将64升到256维，参数量为`1*1*256*64+3*3*64*64+1*1*64*256=69632`\n\n从结果`69632/3276800=0.12`可以看到，第二种方式的参数量是第一种方式0.12倍，参数量降得令人惊叹！\n\n\n## Depthwise Separable Conv\n人们发现上面的方法参数量还是不少啊，于是又提出了**Depthwise Separable Conv**（深度可分离卷积），这就是大名鼎鼎的**Xception**的网络结构。\n\n\n\nDepthwise Separable Conv的核心思想是**首先经过1\\*1卷积，即Pointwise Convolution（逐点卷积），然后对每一个通道分别进行卷积，即Depthwise Conv（深度卷积）**，这就是**Xception**，即**Extreme Inception**。\n\n\n\n\n我们来回顾一下从Inception到Xception的过程：\n\n（1）典型的Inception结构（Inception V2）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806211303167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（2）简单的Inception结构：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806211439767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（3）对简单Inception结构进行严格等价变形的Inception结构：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806211608765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（4）极端的Inception结构（Extreme Inception），即Xception（Depthwise Separable Conv）：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806211840236.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\nXception Net的主题结构是以Separable Conv+relu为**基本模块**，再加上1x1卷积作为跳层连接，结构如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806151901810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n现在我们来对比一下，计算参数量吧！\n\n一般的卷积如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200806142140252.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n上图，输入通道2，输出通道3，卷积核大小3x3，参数量为`3*3*2*3=54`\n\nDepthwise Separable Conv如下：\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200904164732940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70#pic_center)\n\n\n输入通道2，先进行经过1\\*1卷积，输出通道为3，参数量：`1*1*2*3=6`，再对这三个通道分别进行卷积，即进行Depthwise Conv（深度卷积），参数量：`3*3*3=27`，总的参数量为`6+27=33`\n\n\n\n从结果`33/54=0.61`可以看到，第二种方式的参数量是第一种方式0.61倍，如果有更多卷积核对不同通道进行卷积，则参数量降低的效果更明显。\n\n> 需要注意的是，Xception里面的Depthwise Separable Convolution是先PW，后DW。而MobileNet里面的Depthwise Separable Convolution是先DW，后PW，这个在我后面的博客MobileNet里面会有详细介绍，并计算量这两种方式的参数量和性能。\n\n## Suummary\n- Inception v1的多尺度卷积利用不同大小的卷积核实现不同尺度的感知，可以得到图像更好的表征。\n- Inception v1的Pointwise Conv利用1x1卷积核进行压缩降维，减少参数量，使模型更加精简。\n- Inception v2使用多个小卷积核替代大卷积核的方法，不仅使参数量少了，深度也变深了，提升了神经网络的效果。\n- Inception v3的卷积核非对称拆分不仅可以降低参数量，而且可以处理更多，更丰富的空间特征。\n- Bottleneck卷积结构分三步走，参数量降得令人惊叹！\n- Xception的Depthwise Separable Conv首先经过PW，然后DW，再度减少参数量，使分组卷积这样的思想被广泛用于设计性能高效的网络。\n\n基于Xception的网络结构MobileNets构建了轻量级的28层神经网络，成为了移动端上的高性能优秀基准模型；Resnet的残差连接直接skip connect，解决了深层网络的训练问题；可变形卷积 deformable convolution network 通过可变的感受野提升了CNN对具有不同几何形变物体识别能力的模型；DenseNet密集连接网络，把残差做到了极致，提高了特征的利用率；非局部神经网络转换一种思维，采用Non-Local连接，让神经网络具有更大的感受视野；多输入网络可以输入多张图片来完成一些任务；3D卷积虽然带来了暴涨的计算量，但是可以用于视频分类和分割；RNN和LSTM用于处理非固定长度或者大小的视频，语音等，更加适合用来处理这些时序信号；非生成对抗网络GAN已从刚开始的一个生成器一个判别器发展到了多个生成器多个判别器等各种各样的结构......人类的探索历程永无止境，未来必然有更加优秀的卷积方式和CNN架构出现，加油吧，后浪！\n\n\n【参考文档】\n[1] [GoogLeNet中的inception结构，你看懂了吗](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029565&idx=1&sn=330e398a4007b7b24fdf5203a5bf5d91&chksm=871345c0b064ccd6dd7d954c90d63f1f3b883c7d487844cbe3424bec3c9abb66625f1837edbd&scene=21#wechat_redirect)\n[2] [总结12大CNN主流模型架构设计思想](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649031450&idx=1&sn=3f7f159458e5f1621531ee107be11a98&chksm=8712bd67b0653471c052e32b9d18a26f4d6a07852f56b50f6589b3baa7a46e1e44f302204a4e&mpshare=1&scene=1&srcid=0804G8KDpyT32uItY7tg5ELC&sharer_sharetime=1596532639265&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a4858a01a73460ab9243614f9e6d48f8d78ffc0a9c2f26b5f667bf5a4c95bda3770b77eac3a83af0df071fc66fe56a156ae26d89306dd2ae948ff43fc09a2489c9bc7df445f1a3adb3bbe&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AaY04ggBFXJJbqFKz4uc9W0=&pass_ticket=bfbIoM7yRmV2MGkwcfISFD0R1Uc2EfrGFv2CzbEqH1kNmM/wiobhHOk806C/dvoE)\n[3] [对于xception非常好的理解](https://www.jianshu.com/p/4708a09c4352)\n[4] Chollet F.  Xception: DeepLearningwithDepthwiseSeparableConvolutions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Xception"],"categories":["神经网络"]},{"title":"Alexnet网络结构逐层详细分析+代码实现","url":"/2020/11/24/222439/","content":"\n在2012年Imagenet比赛冠军—Alexnet （以第一作者Alex命名）直接刷新了ImageNet的识别率，奠定了深度学习在图像识别领域的优势地位。网络结构如下图：\n\n<!-- more -->\n\n\n\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080609545260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n下面分别对每层进行介绍：\n（1）**Input** 层：为输入层，AlexNet卷积神经网络默认的输入数据必须是维度为224×224×3的图像，即输入图像的高度和宽度均为224，\n色彩通道是R、G、B三个。\n（2）**Conv1** 层：为AlexNet的第1个卷积层，使用的卷积核为`(11*11*3)*96`（卷积核大小为11\\*11，输入通道为3，输出通道为96），步长为4，Padding为2。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为55，即$55=\\frac{224-11+4}{4}+1$   ，最后输出的特征图的维度为55×55×96。卷积通用公式参考我的博客[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中卷积中的特征图大小计算方式。\n（3）**MaxPool1** 层  ：为AlexNet的第1个最大池化层，池化核大小为3×3，步长为2。通过套用池化通用公式，可以得到最后输出的特征图的高度和宽度均为27，即 $27=\\frac{55-3}{2}+1$   ，最后得到的输出的特征图的维度为27×27×96。\n（4）**Norm1** 层：为AlexNet的第1个归一化层，即**LRN1**层，local_size=5（相邻卷积核个数设为5），输出的特征图的维度为27*27*96。\n（5）**Conv2** 层  ：为AlexNet的第 2个卷积层，使用的卷积核为`(5*5*96)*256`，步长为1，Padding为2。通过套用卷积通用公式，可以得\n到最后输出的特征图的高度和宽度均为27，即 $27=\\frac{27-5+4}{1}+1$   ，最后得到输出的特征图的维度为27×27×256。\n（6）**MaxPool2** 层  ：为AlexNet的第2个最大池化层，池化核大小为为3×3，步长为2。通过套用池化通用公式，可以得到最后\n输出的特征图的高度和宽度均为13，即  $13=\\frac{27-3}{2}+1$ ，最后得到输出的特征图的维度为13×13×256。\n（7）**Norm2** 层：为AlexNet的第2个归一化层，即**LRN2**层，local_size=5（相邻卷积核个数设为5），输出的特征图的维度为13×13×256。\n（8）**Conv3** 层  ：为AlexNet的第3个卷积层，使用的卷积核`(3*3*256)*384`，步长为1，Padding为1。通过套用卷积通用公式，可以得到最后输出的特征图的高度和宽度均为13，即   $13=\\frac{13-3+2}{1}+1$，最后得到特征图的维度为13×13×384。\n（9）**Conv4** 层  ：为AlexNet的第4个卷积层，使用的卷积核为`(3*3*384)*384`，步长为1，Padding为1。通过套用卷积通用公式，可以得\n到最后输出的特征图的高度和宽度均为13，即 $13=\\frac{13-3+2}{1}+1$   ，最后得到输出的特征图的维度为13×13×384。\n（10）**Conv5** 层  ：为AlexNet的第5个卷积层，使用的卷积核为`(3*3*384)*256`，步长为1，Padding为1。通过套用卷积通用公式，可以得\n到最后输出的特征图的高度和宽度均为13，即 $13=\\frac{13-3+2}{1}+1$，最后得到输出的特征图的维度为13×13×256。\n（11）**MaxPool3** 层  ：为AlexNet的第3个最大池化层，池化核大小为为3×3，步长为2。通过套用池化通用公式，可以得到最后\n输出的特征图的高度和宽度均6，即 $6=\\frac{13-3}{2}+1$，最后得到输出的特征图的维度为6×6×256。\n（12）**FC6** 层  ：为AlexNet的第1个全连接层，输入的特征图的维度为6×6×256，首先要对输入的特征图进行扁平化处理，将其变成维度为1×9216的输入特征图，因为本层要求输出数据的维度是1×4096，所以需要一个维度为9216×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×4096。\n（13）**Dropout6** 层：在训练的时候以1/2概率使得隐藏层的某些神经元的输出为0，这样就丢掉了一半节点的输出，反向传播的时候也不更新这些节点，输出的特征图的维度为1×4096。\n（14）**FC7** 层  ：为AlexNet的第2个全连接层，输入数据的维度为1×4096，输出数据的维度仍然是1×4096，所以需要一个维度为4096×4096的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度依旧为1×4096。\n（15）**Dropout7** 层：在训练的时候以1/2概率使得隐藏层的某些神经元的输出为0，这样就丢掉了一半节点的输出，反向传播的时候也不更新这些节点，输出的特征图的维度为1×4096(这些神经元还存在，只是置为0了，因此输出维度不变)。\n（16）**FC8** 层  ：为AlexNet的第3个全连接层，输入数据的维度为1×4096，输出数据的维度要求是1×1000，所以需要一个维度为4096×1000的矩阵完成输入数据和输出数据的全连接，最后得到输出数据的维度为1×1000。\n\n**总结**：\n1. 网络比LeNet更深，包括5个卷积层和3个全连接层。\n2. 使用relu激活函数，收敛很快，解决了Sigmoid在网络较深时容易出现梯度消失(或梯度弥散)的问题。\n3. 加入了dropout层，防止过拟合。\n4. 使用了LRN归一化层，通过在相邻卷积核生成的feature map之间引入竞争，从而有些本来在feature map中显著的特征在A中更显著，而在相邻的其他feature map中被抑制，这样让不同卷积核产生的feature map之间的相关性变小，从而增强了模型的泛化能力。\n5. 使用裁剪翻转等操作做数据增强，增强了模型的泛化能力。预测时使用提取图片四个角加中间五个位置并进行左右翻转一共十幅图片的方法求取平均值，这也是后面刷比赛的基本使用技巧。\n6. .分块训练，当年的GPU没有这么强大，Alexnet创新地将图像分为上下两块分别训练，然后在全连接层合并在一起。\n7. 总体的数据参数大概为240M。\n\n\n**代码实现：**\n\n```python\nclass AlexNet(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n```\n\n参考文档\n深度学习之Pytorch实战计算机视觉[唐进民著]\n[从LeNet到VGG，看卷积+池化串联的网络结构](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029512&idx=1&sn=a46fc10de7daba25694bda75a916aa91&chksm=871345f5b064cce3c16ab3b7c671f9e93c838836e20d0aa91bc83f7879915d0c8318bcd9d187&mpshare=1&scene=1&srcid=0804y5ewJsTSJKBSVaMNmvrm&sharer_sharetime=1596532567452&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=92bd8fadcb6a4858b2c5b0a38954321b0f8c3dc80d539d1fc2a0778dc107c45ce2aca8614afc433452f4fd83aba02ddb6a1be7e244027038c09196c4dc62cca07dafbdb87d527756f0a49d5105425e85&ascene=1&uin=MjA2Nzc1NzU0Mg==&devicetype=Windows%2010%20x64&version=6209007b&lang=zh_CN&exportkey=AZ6oMa1fen0omFBd4MdcdrU=&pass_ticket=bfbIoM7yRmV2MGkwcfISFD0R1Uc2EfrGFv2CzbEqH1kNmM/wiobhHOk806C/dvoE)","tags":["Alexnet"],"categories":["神经网络"]},{"title":"深入理解ReLU、Leaky ReLU、 PReLU、ELU、Softplus","url":"/2020/11/24/222236/","content":"\n## ReLU\n**ReLU**（Rectified Linear Unit，修正线性单元），也叫Rectifier 函数，它的定义如下：\n\n<!-- more -->\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805112618602.png)\n\nRelu可以实现**单侧抑制**（即把一部分神经元置0），能够稀疏模型， Sigmoid 型活tanh激活函数会导致一个非稀疏的神经网络，而Relu大约 50% 的神经元会处于激活状态，具有很好的稀疏性。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805144836975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\nRelu函数右侧线性部分梯度始终为1，具有 **宽兴奋边界的特性** （即兴奋程度可以非常高），不会发生神经网络的梯度消失问题， 能够加速梯度下降的收敛速度。而tanh和sigmoid在离0点近的时候梯度大，在远离0点的时候梯度小，容易出现梯度消失。\n\n>  在生物神经网络中， 同时处于兴奋状态的神经元非常稀疏． 人脑中在同一时刻大概只有 1% ∼ 4% 的神经元处于活跃状态\n\n**Relu的缺点**：ReLU 函数不是在0周围， 相当于给后一层的神经网络引入**偏置偏移**，会影响梯度下降的效率。另外，在训练时， 如果参数在一次不恰当的更新后， 某个 ReLU 神经元输出为0，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活， 这种现象称为**死亡 ReLU 问题** （Dying ReLU Problem）\n\n>ReLU 神经元指采用 ReLU 作为激活函数的神经元。\n\n\n\n## Leaky ReLU\n**Leaky ReLU**（带泄露的 ReLU ）在输入 $x < 0$ 时， 保持一个很小的梯度 $\\gamma$． 这样当**神经元输出值为负数**也能有一个非零的梯度可以更新参数， 避免永远不能被激活，Leaky ReLU定义如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805151056568.png)\n其中 $\\gamma$ 是一个很小的常数， 比如 0.01． 当 $\\gamma$ < 1 时， Leaky ReLU 也可以写为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805151239226.png)\n$max(0.1x,x)$的图像如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805152201645.png)\n\n##  PReLU\n**PReLU**（Parametric ReLU， PReLU，即带参数的 ReLU）引入一个可学习的参数， **不同神经元可以有不同的参数**。 对于第 𝑖 个神经元，ReLU 的定义为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805151431635.png)\n$\\gamma_i$是$x \\le 0$时的梯度， 如果$\\gamma_i$ = 0， 那么PReLU 就退化为 ReLU． 如果 $\\gamma_i$ 为一个很小的常数， 则 PReLU 可以看作Leaky ReLU。 PReLU 可以允许不同神经元具有不同的参数， 也可以一组神经元共享一个参数。\n\n## ELU \n**ELU**（Exponential Linear Unit， 指数线性单元）定义为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805152305703.png)\n其中 $\\gamma$ ≥ 0 是一个超参数，图像如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805152447816.png)\n## Softplus \n**Softplus** 函数 可以看作 Relu 函数的平滑版本，其定义为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805152555701.png)\n**Softplus 函数其导数刚好是 Logistic 函数。 Softplus 函数虽然也具有单侧抑制、 宽兴奋边界的特性， 却没有稀疏激活性，不会稀疏模型。** 图像如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805154409790.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n这几个函数的图像如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805154737405.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n","tags":["激活函数"],"categories":["神经网络"]},{"title":"深入理解dropout(原理+手动实现+实战)","url":"/2020/11/24/222059/","content":"\n在这篇博客中你可以学到\n\n- 什么是dropout\n- dropout为什么有用\n- dropout中的多模型原理\n- 手动实现dropout\n- 在pytorch中使用dropout\n\n\n<!-- more -->\n\n\n\n\n\n当训练一个深度神经网络时， 我们可以随机丢弃一部分神经元（同时丢弃其对应的连接边）来避免过拟合，这种方法就称为**dropout**(丢弃法)。\n\n每次选择丢弃的神经元是随机的．最简单的方法是设置一个固定的概率 p．对每一个神经元都以概率 p 随机丢弃（即置0），其原理如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804220653389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*上图，Bernoulli是概率p的伯努利分布，取值为0或1，即该分布中的每一个元素取值为0的概率是p，$y_{}^{\\left( l \\right)}$表示第 $l$ 个全连接层（或卷积层），x是特征向量*\n\n\n\n在没有使用dropout的情况下，第 $l$ 层的神经元的值经过线性（或卷积）运算后，通过激活函数输出。\n\n如果使用了dropout，第 $l$ 层的神经元的值乘上概率为p的Bernoulli分布，假如第 $l$ 层有10个神经元，那么产生的Bernoulli分布可能是$[0,1,1,0,0,1,0,0,0,1]^T$（相当于以概率p=0.6随机将$l$ 层的神经元置0），然后第 $l$ 层神经元的输出在第 $l+1$ 层经过线性（或卷积）运算后，再通过激活函数输出\n\n\n\n在train阶段，**dropout的本质通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。比如，以概率 p=0.6 随机将神经元置0，就相当于在10个神经元选4个神经元输出(4个神经元在工作，另外6神经元置0)，这时我们就相当于训练了 $C_{10}^4$ 个模型，只是每个模型的参数量更少了**（这也就是集成学习的思想）。使用了dropout的神经网络如下图所示：\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804225113464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*左图是一般的神经网络，右图是应用了Dropout的网络，Dropout通过随机选择并删除神经元，停止向前传递信号*\n\n\n>机器学习中经常使用集成学习。所谓集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值。用神经网络的语境来说，比如，准备5个结构相同（或者类似）的网络，分别进行学习，测试时，以这 5 个网络的输出的平均值作为答案。实验告诉们，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，这个集成学习与Dropout 有密切的关系。这是因为可以将Dropout理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例（比如，0.5 等），可以取得模型的平均值。\n\n\n**由于在测试时， 所有的神经元都是可以激活的， 这会造成训练和测试时网络的输出不一致，那么，测试的时候该怎么办呢？**\n\n答案是**测试的时候让每个模型投票来得到结果**。\n\n比如，训练的时候，10个神经元置中6个置为0(p=0.6，相当于训练时随机删除了6个神经元，**只有4个神经元在工作**)，**测试**的时候是用10个神经元来投票，那么每个神经元的权重是0.4（$1- p = 0.4$），操作的方法是将dropout层这10个神经元的值加起来乘以0.4，即每个神经元的值都乘以0.4。\n\n\n\n\n注意，是**所有神经元输出的值**乘以0.4，比如 **10个神经元每次只选一个神经元工作(以概率p=0.9将神经元置0)，就相当于训练了10个模型**，最后这10个神经元的结果都要输出，做法是把这10个神经元的值加起来乘以0.1（测试时），即相当于投票得出了结果。但是输出的个数不能少，该输出几个数还是几个数。\n\n> 有的教材设定保留神经元的概率为p（保留率），即神经元有效的概率是p，那么，测试的时候将dropout层的神经元乘以 p 输出，而这里神经元无效的概率是p（废弃率），因此，测试的时候将dropout层的神经元乘以 1-p 输出\n\n总的来说，对于一个神经网络层  $y = f\\left( {W \\cdot X + b} \\right)$，我们可以引入一个**掩蔽函数mask**使得 $y = f\\left( {W \\cdot ma{\\rm{s}}k(X) + b} \\right)$ ，设神经元**废弃率**为 p ，**掩蔽函数mask**可表示为：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804231847285.png)\n\n\n\n\n\n\n 一般来讲， 对于隐藏层的神经元， 其**废弃率** p = 0.5 时效果最好， 这对大部分的网络和任务都比较有效． 当 p = 0.5 时， 在训练时有一半的神经元被丢弃， 只剩余一半的神经元是可以激活的， 随机生成的网络结构**最具多样性**，比如10个神经元随机删除5个，则可训练的模型有$C_{10}^5$，相当于取了最大值。 对于输入层的神经元， 其保留率通常设为更接近 1 的数， 使得输入变化不会太大。 **对输入层神经元进行丢弃时， 相当于给数据增加噪声， 以此来提高网络的鲁棒性**。\n\n下面我们就来实现dropout吧\n\n**手动实现dropout：**\n\n```python\nimport numpy as np\nclass Dropout:\n    def __init__(self, dropout_ratio=0.5):\n        self.dropout_ratio = dropout_ratio\n        self.mask = None\n    def forward(self, x, train_flg=True):\n        if train_flg:\n            # *为序列解包\n            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n            return x * self.mask\n        else:\n            return x * (1.0 - self.dropout_ratio)\n    def backward(self, dout):\n        return dout * self.mask\n```\n\n\n\n这里的要点是，每次正向传播时，**掩蔽函数**`self.mask`中都会以False的形式保存要删除的神经元(相当于概率p的伯努利分布)。`self.mask`会随机生成和x形状相同的数组，并将值比dropout_ratio大的元素设为True。反向传播时的行为和ReLU相同。也就是说，**正向传播时传递了信号的神经元，反向传播时按原样传递信号；正向传播时没有传递信号的神经元，反向传播时信号将停在那里**。\n\n\n在pytorch中使用dropout，只需要一行`torch.nn.Dropout(p=0.5)`即可，如下所示\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200805094501511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)","tags":["dropout"],"categories":["神经网络"]},{"title":"Pytorch离线下载并使用torchvision.models预训练模型","url":"/2020/11/24/221921/","content":"\nPytorch离线下载并使用torchvision.models预训练模型\n\n<!-- more -->\n\n原本直接在IDE中执行`models.alexnet(pretrained=True)`就行了，**但是一直报错，搞得我好不难受**！\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804162456348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n不用说，肯定是**由于网络不好导致模型下载失败**，能不能离线下载完之后在本地直接使用？答案是肯定的\n\n其实步骤很简单(但是对于新手要命)，就和我们下载软件一样，详细步骤如下：\n\n1. 复制需要下载的模型地址，粘贴到浏览器地址栏中下载，各种模型的下载地址如下：\n\n```python\n1. Resnet:\n  model_urls = {\n      'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n      'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n      'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n      'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n      'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n  }\n\n2. inception:\n model_urls = {\n      Inception v3 ported from TensorFlow\n     'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n }\n\n3. Densenet: \n model_urls = {\n     'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n     'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n     'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n     'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n }\n\n4. Alexnet:\n model_urls = {\n     'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n}\n\n5. vggnet:\n model_urls = {\n     'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n     'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n     'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n     'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n     'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n     'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n     'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n     'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n }\n```\n2. 这里以`Alexnet`为例，复制`https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth`浏览器地址栏中下载\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080416345458.png)\n\n大约233M，慢慢下吧，友情提示一下，**在手机上下载每秒几M，在电脑下载每秒几kb，推荐在手机上下载好之后，再发送到电脑上**\n\n\n\n3. 将下载好的文件剪切到**torch缓存文件夹下即可**，windos和linux的torch缓存文件夹分别如下：\n- windows：**C:\\Users\\wang1\\\\.cache\\torch\\checkpoints**\t(wang1是你的电脑用户名)\n- linux：**/root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth**\n\n如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804163926693.png)\n4. 然后执行`models.alexnet(pretrained=True)`，发现OK，大功告成！\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200804164034307.png)","tags":["pytorch"],"categories":["pytorch"]},{"title":"python中超级实用的30个内置函数","url":"/2020/11/24/221744/","content":"\n>**python的内置函数是指不需要任何导入，就可以使用的函数**\n\n<!-- more -->\n\n\n\n\n\n\n\n\n\n# max(iterable, *[, default=obj, key=func])\n内置函数max的作用是找到最大值，**对所有容器型数据都可以使用**\n\n- 找到容器的最大值\n\n```python\nIn [311]: a = [1,2,3,3,3,4,2,3]\nIn [312]: max(a)\nOut[312]: 4\n\n# 对于集合，默认是找出键的最大值\nIn [320]: di = {'a':3,'b1':1,'c':4}\nIn [321]: max(di)\nOut[321]: 'c'\n\nIn [316]: max('63447')\nOut[316]: '7'\n# 这里要注意，对于字符串，max是按照第一个字符的Ascii码比较的\nIn [318]: max({'3','10','7'})\nOut[318]: '7'\nIn [319]: max({'3','9','7'})\nOut[319]: '9'\n```\n\n- 找到列表最多重复的元素\n\n```python\nIn [313]: max(a,key=lambda x: a.count(x))\nOut[313]: 3\n```\n\n- 如果容器为0，设置默认值为0\n\n```python\nIn [315]: max([],default=0)\nOut[315]: 0\n```\n\n\n\n\n- 当内置函数max使用`lambda`时，lambda里的x就是容器里面的一个个元素\n\n\n\n```python\n# 找到年龄最大的人\nIn [323]: a = [{'name':'a','age':18,'gender':'male'},{'name':'b','age':20,'gender':'female'}]\n\nIn [324]: max(a,key=lambda x: x['age'])\nOut[324]: {'name': 'b', 'age': 20, 'gender': 'female'}\n```\n\n- 找到每一个列表中第二个元素最大的列表\n\n```python\nIn [325]: lst = [[1,3],[0,6],[4,5]]\nIn [326]: max(lst,key=lambda x: x[1])\nOut[326]: [0, 6]\n```\n\n\n\n# sum(iterable, start=0, /)\n\n- 列表(所有容器型数据均可)求和\n\n\n```python\nIn [327]: a = [1,3,2,1,4,2]\n\nIn [328]: sum(a)\nOut[328]: 13\n\nIn [329]: sum(a,2) # start=2 表示求和的初始值为 2\nOut[329]: 15\n```\n\n\n\n\n\n# sorted(iterable, /, *, key=None, reverse=False)\n\n- 列表排序\n\n```python\nIn [330]: a = [1,2,3,3,4,2,3]\n# 默认从小到大排序\nIn [331]: sorted(a)\nOut[331]: [1, 2, 2, 3, 3, 3, 4]\n# 从大到小排序\nIn [332]: sorted(a,reverse=True)\nOut[332]: [4, 3, 3, 3, 2, 2, 1]\n```\n\n- 按照每一个列表的第二个元素排序\n\n```python\nIn [333]: lst = [[1,4],[3,2]]\nIn [334]: sorted(lst,key = lambda x:x[1])\nOut[334]: [[3, 2], [1, 4]]\n```\n\n\n- 字典排序\n\n```python\n# 对字典直接使用sorted是对键排序\nIn [335]: d = {'a':5,'c':7,'d':6}\nIn [336]: sorted(d)\nOut[336]: ['a', 'c', 'd']\n# 按照字典的值排序，注意是d.items()\nIn [337]: sorted(d.items(),key = lambda x:x[1])\nOut[337]: [('a', 5), ('d', 6), ('c', 7)]\n```\n\n\n\n\n# len(obj, /)\n- 求容器中元素的个数\n\n```python\nIn [340]: dic = {'a':1,'b':3}\nIn [341]: len(dic)\nOut[341]: 2\n\nIn [342]: s = 'abc'\nIn [343]: len(s)\nOut[343]: 3\n```\n\n\n\n\n\n\n\n# pow(x, y, z=None, /)\n\n- x 为底的 y 次幂，如果 z 给出，取余\n\n\n```python\nIn [344]: pow(3, 2)\nOut[344]: 9\n\nIn [345]: pow(3, 2, 4)\nOut[345]: 1\n```\n\n\n\n\n# round(number, ndigits=None)\n\n- 四舍五入，ndigits 代表小数点后保留几位\n\n\n```python\nIn [346]: round(10.0222222, 3)\nOut[346]: 10.022\n\nIn [347]: round(10.02252222, 3)\nOut[347]: 10.023\n\nIn [349]: round(10.0222222)\nOut[349]: 10\n# 默认ndigits为0，返回int\nIn [348]: type(round(10.0222222))\nOut[348]: int\n```\n\n\n\n\n# abs(x, /)\n\n\n```python\nIn [350]: abs(-6)\nOut[350]: 6\n\nIn [351]: abs(-10.223555)\nOut[351]: 10.223555\n```\n\n\n# all(iterable, /)\n- 接受一个迭代器，如果迭代器的所有元素都为真，返回 True，否则返回 False\n\n\n```python\nIn [352]: all([1,0,3,6])\nOut[352]: False\n\nIn [353]: all([1,2,3])\nOut[353]: True\n```\n\n\n#  any(iterable, /)\n\n- 接受一个迭代器，如果迭代器里有一个元素为真，返回 True，否则返回 False\n\n\n```python\nIn [354]: any([0,0,0,[]])\nOut[354]: False\n\nIn [355]: any([0,0,1])\nOut[355]: True\n```\n\n\n\n# bin(number, /)\n\n- 将十进制转换为二进制，返回二进制的字符串\n\n\n```python\nIn [356]: bin(10)\nOut[356]: '0b1010'\n# 0b代表二进制\nIn [357]: 0b1010\nOut[357]: 10\n```\n\n\n\n\n# oct(number, /)\n\n- 将十进制转换为八进制，返回八进制的字符串\n\n\n```python\nIn [358]: oct(12)\nOut[358]: '0o14'\n# 0o代表八进制\nIn [359]: 0o14\nOut[359]: 12\n```\n\n\n\n# hex(number, /)\n\n- 将十进制转换为十六进制，返回十六进制的字符串\n\n\n```python\nIn [360]: hex(15)\nOut[360]: '0xf'\n# 0x代表16进制\nIn [361]: 0xf\nOut[361]: 15\n```\n\n\n# bool(x)\n\n- 如果是一个值，如果为0返回False，如果为1返回True，如果是一个容器，容器为空返回Flase，容器不为空返回True\n\n\n```python\nIn [362]: bool(0)\nOut[362]: False\n\nIn [363]: bool(1)\nOut[363]: True\n\nIn [364]: bool([1,0,0,0])\nOut[364]: True\n\nIn [365]: bool([0])\nOut[365]: True\n\nIn [366]: bool([])\nOut[366]: False\n# 容器中如果是空元素，也返回True\nIn [367]: bool([[],[]])\nOut[367]: True\n\nIn [368]: bool([[]])\nOut[368]: True\n```\n\n\n\n# str(object='')\n\n- 将字符类型、数值类型等转换为字符串类型\n\n\n```python\n# 默认为空\nIn [383]: str()\nOut[383]: ''\nIn [369]: i =123\nIn [370]: str(i)\nOut[370]: '123'\nIn [371]: str(10.235)\nOut[371]: '10.235'\n```\n\n\n\n# ord(c, /)\n\n- 查看某个字符对应的ASCII码，必须传入单个字符串\n\n\n```python\nIn [372]: ord('A')\nOut[372]: 65\n\nIn [374]: ord('0')\nOut[374]: 48\n```\n\n\n\n# dict()、dict(mapping)、dict(iterable)\n\n- 创建数据字典\n- `dict()`，通过构造函数常见字典\n- `dict(mapping)`，通过映射创建字典\n- `dict(iterable)`，通过迭代器创建字典\n\n\n```python\nIn [92]: dict()\nOut[92]: {}\n\nIn [93]: dict(a='a',b='b')\nOut[93]: {'a': 'a', 'b': 'b'}\n\nIn [94]: dict(zip(['a','b'],[1,2]))\nOut[94]: {'a': 1, 'b': 2}\n\nIn [95]: dict([('a',1),('b',2)])\nOut[95]: {'a': 1, 'b': 2}\n```\n\n# object()\n\n- 返回一个根对象，它是所有类的基类\n\n\n```python\nIn [137]: o = object()\nIn [138]: type(o)\nOut[138]: object\n```\n\n- 使用python内置函数dir，返回object的属性、方法列表\n\n\n```python\nIn [379]: dir(o)\nOut[379]: \n['__class__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__']\n```\n\n# int(x, base=10)\n\n`int(x, base =10)`，x 可能为字符串或数值，将 x 转换为一个整数，base是基数\n\n\n\n```python\nIn [380]: int('12')\nOut[380]: 12\n# int没有四舍五入的功能\nIn [382]: int(13.56)\nOut[382]: 13\n```\n\n- `int`的基数base有很大的作用，**可以将任何一个其它进制的数据转化为十进制**\n\n\n```python\n# 将八进制转化为十进制\n# 1*8+2\nIn [384]: int('12',8)\nOut[384]: 10\n# 将16进制转化为10进制\n# 1*16+2\nIn [386]: int('12',16)\nOut[386]: 18\n# 二进制转十进制\nint('0100',2)\n```\n\n\n- 将其它进制的数据转化为十进制的时候，传入的x必须是字符串，否则报错\n\n```python\n# x必须是字符串\nIn [387]: int(12,16)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-387-2153b26001c8> in <module>\n----> 1 int(12,16)\n\nTypeError: int() can't convert non-string with explicit base\n```\n\n\n\n# float(x=0, /)\n\n- 将一个字符串或整数转换为浮点数\n\n\n```python\nIn [390]: float()\nOut[390]: 0.0\n\nIn [388]: float('30.01')\nOut[388]: 30.01\n\nIn [389]: float(30)\nOut[389]: 30.0\n```\n\n\n\n# list(iterable=(), /)\n\n- 将一个容器转化为列表类型\n\n\n```python\nIn [391]: a = {1,2,3}\nIn [392]: list(a)\nOut[392]: [1, 2, 3]\n\nIn [393]: d = {'a':1,'b':2,'c':3}\nIn [394]: list(d)\nOut[394]: ['a', 'b', 'c']\n\nIn [395]: list(d.items())\nOut[395]: [('a', 1), ('b', 2), ('c', 3)]\n```\n\n\n\n# set()、set(iterable)\n\n- `set()`创建一个空的集合对象\n- `set(iterable)`返回一个集合对象，并允许创建后再增加、删除元素。\n\n**集合的一大优点，容器里不允许有重复元素，因此可对列表内的元素去重。**\n\n\n```python\nIn [397]: a=set()\nIn [399]: a.add('a')\nIn [400]: a\nOut[400]: {'a'}\n\nIn [401]: a = [1,2,1,3,4]\nIn [402]: set(a)\nOut[402]: {1, 2, 3, 4}\n```\n\n\n\n\n# slice(stop)、slice(start, stop[, step])\n\n返回一个由 `range(start, stop, step)` 所指定索引集的 slice 对象\n\n\n```python\nIn [403]: a = [1,2,1,3,4,6,7]\n\nIn [404]: a[slice(0,7,2)] #等价于a[0:7:2]\nOut[404]: [1, 1, 4, 7]\nIn [405]: a[0:7:2]\nOut[405]: [1, 1, 4, 7]\n```\n\n\n\n\n# tuple(iterable=(), /)\n- 将一个迭代器转化为元祖\n\n```python\nIn [409]: a = [1,2,3]\nIn [410]: tuple(a)\nOut[410]: (1, 2, 3)\n\nIn [411]: t = tuple(range(1,10,2))\nIn [412]: t\nOut[412]: (1, 3, 5, 7, 9)\n```\n\n\n# type(object)\n\n- 查看对象的类型\n\n\n```python\nIn [413]: type([1,2,3])\nOut[413]: list\n\nIn [414]: type((1,2,3))\nOut[414]: tuple\n\nIn [415]: type({1,2,3})\nOut[415]: set\n\nIn [416]: type({'a':1,'b':2})\nOut[416]: dict\n```\n\n\n\n# zip(*iterables)\n\n- 创建一个迭代器，聚合每个可迭代对象的元素的`元祖`。\n\n- 参数前带 *，意味着是可变序列参数，可传入 1 个，2 个或多个参数。\n\n\n```python\na = [1,2,3]\nfor i in zip(a):\n    print(i)\n\"\"\"\n    (1,)\n    (2,)\n    (3,)\n\"\"\"\nb = ['a','b','c']\nfor m,n in zip(a,b):\n    print(m,n)\n\"\"\"\n    1 a\n    2 b\n    3 c\n\"\"\"\n```\n\n\n# dir([object])　\n\n- 不带参数时，返回当前范围内的变量、方法和定义的类型列表；带参数时返回参数的属性、方法列表\n\n\n```python\nIn [417]: lst = [1,2,3]\nIn [418]: dir(lst)\nOut[418]: \n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n```\n\n\n\n# isinstance(object, classinfo)\n- 判断 object 是否为类 classinfo 的实例，若是，返回 true\n\n\n```python\nclass Node():\n    def __init__(self,value):\n        self.value = value\nnode = Node('node')\nisinstance(node,Node)\n# 输出 True\n```\n\n\n# map(func, *iterables)\n内置函数`map()` 会根据提供的函数对指定序列做映射。\n\n第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表\n\n\n```python\n# 计算平方\ndef square(x) :            \n    return x ** 2\n \nIn [426]: list(map(square, [1,2,3,4,5])) # 计算列表各个元素的平方\nOut[426]: [1, 4, 9, 16, 25]\n\nIn [427]: list(map(lambda x: x ** 2, [1, 2, 3, 4, 5]))  # 使用 lambda 匿名函数\nOut[427]: [1, 4, 9, 16, 25]\n# 提供了两个列表，对相同位置的列表数据进行相加\nIn [428]: list(map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10]))\nOut[428]: [3, 7, 11, 15, 19]\n```\n\n\n# reversed(sequence, /)\n- 逆置序列，返回一个迭代器\n\n```python\nIn [421]: ''.join(reversed('123'))\nOut[421]: '321'\nIn [422]: list(reversed([1,2,3]))\nOut[422]: [3, 2, 1]\n```\n\n\n","tags":["python"],"categories":["python"]},{"title":"python中dict和set的30种操作方法","url":"/2020/11/24/221600/","content":"\n# 字典dict\n\n字典（dict），一种映射对象（mapping）类型，键值对的容器\n\n<!-- more -->\n\n\n\n\n## 创建字典(五种方法)\n\n### 手动创建\n\n\n```python\ndic1 = {} # 创建空字典\ndic2 = {'a':1,'c':3,'e':5}\n```\n\n### 使用 dict() 构造函数\n\n\n```python\ndict() #创建空字典\ndict(a=1,b=2,c=3) # {'a': 1, 'b': 2, 'c': 3}\ndict({'a':1,'b':2},c=3,d=4) \n# {'a': 1, 'b': 2, 'c': 3, 'd': 4}\n```\n\n### 使用可迭代对象\n\n```python\ndict([('a',1),('b',2)],c=3) #  {'a': 1, 'b': 2, 'c': 3}\ndict([('c',3),('d',4),('e',5)]) # {'c': 3, 'd': 4, 'e': 5}\ndict([['c',3],['d',4],['e',5]]) # {'c': 3, 'd': 4, 'e': 5}\ndict((('c',3),('d',4),('e',5))) #  {'c': 3, 'd': 4, 'e': 5}\n```\n\n\n\n\n### 使用fromkeys() 方法\n\n已知键集合（keys），values 为初始值\n\n\n```python\n{}.fromkeys(['k1','k2','k3'],[1,2,3])\n# {'k1': [1, 2, 3], 'k2': [1, 2, 3], 'k3': [1, 2, 3]}\n{'a':1,'b':2}.fromkeys(['c','d'],[1,2])\n# {'c': [1, 2], 'd': [1, 2]}\n```\n\n\n### 使用内置函数zip\n\n\n```python\nlist1 = ['a','b','c','d','e']\nlist2 = [1,3,2,5,6]\ndict(zip(list1,list2))\n# {'a': 1, 'b': 3, 'c': 2, 'd': 5, 'e': 6}\n```\n\n\n\n## 遍历字典\n- d.items()将字典转换为可遍历的列表\n\n```python\nd = {'a':1,'b':2,'c':3}\nfor key, val in d.items():\n    print(key,val)\n\"\"\"\n输出：\na 1\nb 2\nc 3\n\"\"\"\nd.items()\n# dict_items([('a', 1), ('b', 2), ('c', 3)])\n```\n\n下面的写法会报错\n\n```python\nIn [180]: d = {'a':1,'b':2,'c':3}\n\nIn [181]: for key, val in d:\n     ...:     print(key,val)\n     ...: \n---------------------------------------------------------------------------\nValueError: not enough values to unpack (expected 2, got 1)\n```\n\n可以这样写，直接遍历所有的键\n\n```python\nIn [180]: d = {'a':1,'b':2,'c':3}\nIn [183]: for key in d:\n     ...:     print(key)\n     ...: \na\nb\nc\n```\n\n\n## 获取字典所有键集合\n\n\n```python\nd = {'a':1,'b':2,'c':3,'d':4}\n# 方法1\nset(d.keys())\n# 方法2\nset(d)\n# {'a', 'b', 'c', 'd'}\n```\n\n\n## 获取字典所有值集合\n\n\n```python\nlist(d.values())\n# [1, 2, 3, 4]\n```\n\n\n\n## 判断键是否在字典中\n\n```python\nIn [184]: d = {'a':1,'b':2,'c':3,'d':4}\nIn [185]: 'c' in d\nOut[185]: True\n\nIn [186]: 'e' in d\nOut[186]: False\n\nIn [187]: 'e' not in d\nOut[187]: True\n```\n\n\n## 获取某键对应的值\n\n\n```python\nd = {'a':1,'b':2,'c':3,'d':4}\nd.get('c') # 3\n```\n\n\n## 添加或修改一个键值对\n\n\n```python\nIn [189]: d = {'a':1,'b':2,'c':3,'d':4}\n# 添加键值对\nIn [190]: d['e'] = 4\nIn [191]: d\nOut[191]: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 4}\n# 修改键值对\nIn [192]: d['e'] = 'aa'\nIn [193]: d\nOut[193]: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 'aa'}\n```\n\n\n\n## 删除一个键值对\n\n\n```python\nd = {'a':1,'b':2,'c':3,'d':4}\ndel d['d']\nd # {'a': 1, 'b': 2, 'c': 3}\n```\n\n\n## 获取字典视图\n\n字典自带的三个方法 `d.items()、d.keys()、d.values()`，分别返回如下对象：\n\n\n```python\nIn [194]: d = {'a': 1, 'b': 2, 'c': 3}\nIn [195]: d.keys()\nOut[195]: dict_keys(['a', 'b', 'c'])\n\nIn [196]: d.values()\nOut[196]: dict_values([1, 2, 3])\n\nIn [197]: d.items()\nOut[197]: dict_items([('a', 1), ('b', 2), ('c', 3)])\n```\n\n它们都是原字典的视图，修改原字典对象，视图对象的值也会发生改变。\n\n## 键必须是可哈希的对象\n\n**可哈希的对象才能作为字典的键，不可哈希的对象不能作为字典的键，字典的哈希表实现使用从键值计算的哈希值来查找键。**\n\n简要的说可哈希的数据类型，即不可变的数据结构(数字类型（int，float，bool）字符串str、元组tuple、自定义类的对象)。如果一个对象是可哈希的,那么在它的生存期内必须不可变(而且该对象需要一个哈希函数)\n\n**对于不可变类型而言，不同的值意味着不同的内存，相同的值存储在相同的内存**。详情可参考[详解Python中的可哈希对象与不可哈希对象](https://blog.csdn.net/qq_27825451/article/details/102822506)\n\n*同理，不可哈希的数据类型，即可变的数据结构 (字典dict，列表list，集合set)*\n\n\n\n\n\n\n**dict中键必须是可哈希的对象**\n\n\n- list是不可哈希对象\n\n```python\n# list是不可哈希对象\nIn [198]: lst = [1,2]\nIn [199]: d = {lst:'ok'}\n---------------------------------------------------------------------------\nTypeError: unhashable type: 'list'\n```\n\n\n- 元祖是可哈希对象\n\n\n```python\n# 元祖是可哈希对象\nIn [200]: tup = (1,2)\nIn [201]: d = {tup:'ok'}\nIn [202]: d[tup]\nOut[202]: 'ok'\n```\n\n- 自定义类的对象也是可哈希的\n\n```python\nclass Animal:\n    def __init__(self, name):\n        self.name=name\n    def eat(self):\n        print(\"i love eat !\")\n\nan = Animal('123')\nd = {an:'ok'}   \nd[an]\n'''\n'ok'\n'''\n```\n\n\n\n\n## 批量插入键值对\n\n-  使用字典的update方法批量插入键值对\n\n```python\nIn [203]: d = {'a': 1, 'b': 2}\nIn [204]: d.update({'c':3,'d':4,'e':5})\nIn [205]: d\nOut[205]: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n```\n\n\n\n## 字典键值对存在不插入，不存在则插入\n\n如果仅当字典中不存在某个键值对时，才插入到字典中；如果存在，不必插入（也就不会修改键值对），这种情况下，使用字典自带方法 setdefault\n\n```python\nIn [206]: d = {'a':1,'b':2}\n# setdefault返回插入的value值\nIn [207]: r = d.setdefault('c',3) # r: 3\nIn [208]: r\nOut[208]: 3\nIn [209]: d\nOut[209]: {'a': 1, 'b': 2, 'c': 3}\n# 已经存在 'c':3 的键值对，所以 setdefault 时 d 无改变\nIn [210]: r = d.setdefault('c',33)\nIn [211]: r\nOut[211]: 3\nIn [212]: d\nOut[212]: {'a': 1, 'b': 2, 'c': 3}\n```\n\n\n\n## 将两个字典合并\n\n\n```python\nIn [213]: d1 = {'a':1,'b':2}\n\nIn [214]: {*d1}\nOut[214]: {'a', 'b'}\n\nIn [215]: {**d1}\nOut[215]: {'a': 1, 'b': 2}\n\nIn [216]: d2 = {'c':3,'a':2}\n# {**d1,**d2} 实现合并 d1 和 d2，返回一个新字典\nIn [217]: {**d1,**d2} # 后面的key会覆盖前面的\nOut[217]: {'a': 2, 'b': 2, 'c': 3}\n# {*d1,*d2} 只能合并 d1 和 d2 的键\nIn [218]: {*d1,*d2}\nOut[218]: {'a', 'b', 'c'}\n```\n\n\n\n\n## 求两个字典的差集\n\n\n```python\nd1 = {'a':1,'b':2,'c':3}\nd2 = {'b':2}\nd3 = dict([(k,v) for k,v in d1.items() if k not in d2])\nd3\n#  {'a': 1, 'c': 3}\n```\n\n\n\n## 按字典的key排序\n\n如果直接使用python内置函数sorted，则只是返回key排序后的集合，可以使用`lambda`按字典的key排序\n\n```python\nIn [220]: d = {'a':5,'d':7,'c':6}\n\nIn [221]: sorted(d)\nOut[221]: ['a', 'c', 'd']\n# 注意是d.items()\nIn [222]: sorted(d.items(),key = lambda x:x[0])\nOut[222]: [('a', 5), ('c', 6), ('d', 7)]\n```\n\n\n\n## 按字典的value排序\n\n\n```python\nIn [220]: d = {'a':5,'d':7,'c':6}\nIn [224]: sorted(d.items(),key = lambda x:x[1])\nOut[224]: [('a', 5), ('c', 6), ('d', 7)]\n```\n\n\n## 获取字典最大key的value\n\n\n```python\nIn [225]: d = {'a':3,'c':1,'b':2}\nIn [226]: max_key = max(d.keys())\nIn [227]: max_key\nOut[227]: 'c'\n\nIn [228]: d[max_key]\nOut[228]: 1\n\nIn [229]: (max_key,d[max_key])\nOut[229]: ('c', 1)\n```\n\n\n\n## 获取字典最大value的key\n\n- 获取字典最大value的key可能有很多\n\n```python\n# 获取字典最大value的key可能有很多\nIn [230]: d = {'a':3,'c':3,'b':2}\nIn [231]: max_val = max(d.values())\nIn [232]: max_vals = [(key,val) for key,val in d.items() if d[key]==max_val ]\n\nIn [233]: max_vals\nOut[233]: [('a', 3), ('c', 3)]\n```\n\n\n\n# 集合\n**集合（set）是一个无序的不重复元素序列，不能用列表直接索引的方式获取集合中的元素**\n\n## 创建集合\n可以使用set() 函数创建集合，注意：**创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典**\n\n```python\nIn [234]: s_1 = set()\nIn [235]: s_1\nOut[235]: set()\n\nIn [236]: s_2 = { 'orange',  'pear', 'orange', 'banana'}\nIn [237]: s_2\nOut[237]: {'banana', 'orange', 'pear'}\n```\n\n\n\n## 集合并集\n**集合并集就是将两个集合中的元素合并**\n\n- 方法1，集合转化为列表，使用列表的特性\n\n```python\nset1 = {1,3,5,7} \nset2 = {4,5,6}\nset(list(set1)+list(set2))\n#  {1, 3, 4, 5, 6, 7}\n```\n\n- 方法2，使用集合extend方法\n\n```python\n# list添加多个元素的操作是extend\nIn [279]: s_1.update([1,2,3]) #添加多个元素\nIn [280]: s_1.update({3,4}) # 添加集合\n\nIn [281]: s_1\nOut[281]: {1, 2, 3, 4, 'a'}\n```\n- 方法3，使用`|`运算符\n\n```python\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [268]: a|b\nOut[268]: {'a', 'b', 'c', 'd'}\n```\n\n\n## 集合交集\n\n\n```python\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [267]: a & b\nOut[267]: {'c'}\n```\n\n## 集合差集\n\n```python\n# 集合a与b的差集：在集合a存在，不在集合b存在的元素\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [269]: a-b\nOut[269]: {'a', 'b'}\n```\n\n\n## 集合异或\n\n```python\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [270]: a^b\nOut[270]: {'a', 'b', 'd'}\n```\n\n集合a与b的异或,可以理解为先求出只存在于a的元素集合，在求出只存在于b的元素集合，然后取并集\n\n异或是指相同为1，不同为0\n\n\n\n## 添加元素\n集合添加元素的方法是add，list添加元素的方法是append,insert，**由于集合中的元素是无序的，因此不支持在指定位置添加元素，故没有insert方法**\n\n```python\nIn [274]: s_1={'banana', 'orange', 'pear'}\nIn [275]: s_1.add('apple') #添加一个元素\n\n\nIn [277]: s_1\nOut[277]: {'apple', 'banana', 'orange', 'pear'}\n```\n\n## 移除元素\n- 集合的remove和discard方法都可以移除元素\n\n```python\nIn [282]: s_1 = {'apple', 'banana', 'orange', 'pear'}\nIn [283]: s_1.remove('apple')\n\nInIn [284]: \nIn [284]: s_1\nOut[284]: {'banana', 'orange', 'pear'}\n\nIn [285]: s_1.discard('pear')\nIn [286]: s_1\nOut[286]: {'banana', 'orange'}\n\n```\n注意：***区别*** 来了\n\n\n```python\nIn [287]: 'apple' in s_1\nOut[287]: False\nIn [288]: s_1.remove('apple')\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-288-e8c4186ba334> in <module>\n----> 1 s_1.remove('apple')\n\nKeyError: 'apple'\n\nIn [289]: s_1.discard('apple')\n```\n当删除的元素不存在时\n- remove会报错\n- discard不会报错\n\n**集合的pop方法也可以移除元素，由于set是无序的，故set.pop()是随机删除元素**,也正因此，set没有提供排序函数\n\n所以pop的索引是不能用的，即pop里面不能有参数\n\n```python\nIn [290]: print(s_1)\n{'orange', 'banana'}\n\nIn [291]: s_pop = s_1.pop()\nIn [292]: s_pop\nOut[292]: 'orange'\nIn [293]: s_1\nOut[293]: {'banana'}\n```\n\n**set的pop()是随机删除元素**\n\n\n\n\n\n## 集合元素限制\n\n可哈希的数据类型，即不可变的数据结构(数字类型（int，float，bool）字符串str、元组tuple、自定义类的对象)。如果一个对象是可哈希的,那么在它的生存期内必须不可变(而且该对象需要一个哈希函数)\n\n**对于不可变类型而言，不同的值意味着不同的内存，相同的值存储在相同的内存**。详情可参考[详解Python中的可哈希对象与不可哈希对象](https://blog.csdn.net/qq_27825451/article/details/102822506)\n\n同理，不可哈希的数据类型，即可变的数据结构 (字典dict，列表list，集合set)\n\n**集合中的元素必须是可哈希的数据类型**\n\n- 列表是不可哈希的，将不可哈希的列表放入集合中就会报错，如下：\n\n\n```python\nIn [294]: {[1,2]}\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-294-4a5722e7ef02> in <module>\n----> 1 {[1,2]}\n\nTypeError: unhashable type: 'list'\n```\n\n- 将可哈希的元祖放入列表中，则不会报错：\n\n\n```python\nIn [295]: {(1,2),(1,2,3),(1,2)}\nOut[295]: {(1, 2), (1, 2, 3)}\n```\n\n- bool类型也是可哈希的数据类型\n\n```python\nIn [296]: {True,False,True}\nOut[296]: {False, True}\n```\n\n- 字符串也是可哈希的数据类型\n\n```python\nIn [297]: set1 = {'1','2','1'}\nIn [298]: set1\nOut[298]: {'1', '2'}\n```\n\n- 自定义类的对象也是可哈希的\n\n```python\nclass Animal:\n    def __init__(self, name):\n        self.name=name\n    def eat(self):\n        print(\"i love eat !\")\n        \nIn [306]: an = Animal('狗')\nIn [307]: set1 = {an}\nIn [308]: an.name='猫'\n\nIn [309]: a = set1.pop()\nIn [310]: a.name\nOut[310]: '猫'\n```\n\n\n**set集合也可以用python内置函数list()将其转化为列表，从而使用list的所有操作方法**\n\n\n","tags":["python"],"categories":["python"]},{"title":"Python 中 （&，|）和（and，or）之间的区别","url":"/2020/11/24/221418/","content":"\n`（&，|）` 和 `（and，or）`是两组比较相似的运算符。他们的用法如下：\n\n<!-- more -->\n\n\n\n```text\na & b\na | b\na and b\na or b\n```\n（1）如果a，b是数值变量， 则`&`、`|`表示位运算，`and`、`or`则依据是否非0来决定输出，\n\n- **`&`、`|`**\n\n` 1 & 2`，2在二进制里面是`10`，1在进制中是`01`，那么`01`与运算`10`得到是0\n\n```python\nIn [239]: 1 & 2\nOut[239]: 0\n```\n\n- **`and`、`or`**\n\n`and`运算时，`and`中含0，则返回0，均为非0时，则返回后一个值\n\n```python\nIn [240]: 2 and 0\nOut[240]: 0\n\nIn [241]: 2 and 1\nOut[241]: 1\n\nIn [242]: 1 and 2\nOut[242]: 2\n```\n`or`运算时，至少一个非0时，则返回第一个非0\n\n\n```python\nIn [243]: 2 or 0\nOut[243]: 2\n\nIn [244]: 2 or 1\nOut[244]: 2\n\nIn [245]: 0 or 1\nOut[245]: 1\n```\n\n（2）如何a, b是逻辑变量， 则两类的用法基本一致\n\n```python\nIn [247]: True | False\nOut[247]: True\n\nIn [248]: True & False\nOut[248]: False\n\nIn [249]: True and False\nOut[249]: False\n\nIn [250]: True or False\nOut[250]: True\n```\n\n（3）`&、|` 支持set集合运算\n\n如果a与b是两个set集合，则可以做如下运算：\n\n- a与b的交集\n\n```python\nIn [265]: a = {'a','b','c'}\nIn [266]: b = {'c','d'}\n\nIn [267]: a & b\nOut[267]: {'c'}\n```\n\n- a与b的并集\n\n```python\nIn [268]: a|b\nOut[268]: {'a', 'b', 'c', 'd'}\n```\n\n\n除了 `&`、`|` 之外，set集合也支持 `-`、`^` 运算\n\n- a与b的差集：在集合a存在，不在集合b存在的元素\n \n\n```python\nIn [269]: a-b\nOut[269]: {'a', 'b'}\n```\n\n\n- a与b的异或\n\n```python\nIn [270]: a^b\nOut[270]: {'a', 'b', 'd'}\n```\n\na与b的异或,可以理解为先求出只存在于a的元素集合，在求出只存在于b的元素集合，然后取并集\n\n异或是指相同为1，不同为0\n\n\n\n\n【参考文档】\nhttps://www.cnblogs.com/danjiu/p/11332278.html\n","tags":["python"],"categories":["python"]},{"title":"Python中字符串与正则表达式的25个常用操作","url":"/2020/11/24/221218/","content":"\nPython 中没有像 C++ 表示的字符类型（char），所有的字符或串都被统一为 `str` 对象。如单个字符 `c` 的类型也为 `str`，因此在python中**字符也就是字符串**\n\nstr 类型会被经常使用，一些高频用法如下\n\n<!-- more -->\n\n\n\n\n\n# 字符串去空格\n\n- 使用字符串的strip方法去除字符串**开头的结尾**的空格，也可以用字符串的replace方法替换掉所有的空格\n\n```python\nIn [57]: '  I love python  '.strip()\nOut[57]: 'I love python'\nIn [58]: '  I love python  '.replace(' ','')\nOut[58]: 'Ilovepython'\n```\n\n\n\n# 字符串替换\n\n- 使用字符串的replce进行字符替换\n```python\n# 替换所有的字符\nIn [59]: 'i love python'.replace(' ','_')\nOut[59]: 'i_love_python'\n```\n\n# 字符串串联\n- 使用字符串的join方法将**一个容器型中的字符串**联为一个字符串\n\n```python\nIn [60]: '_'.join(['book', 'store','count'])\nOut[60]: 'book_store_count'\nIn [66]: '_'.join(('1','2','3'))\nOut[66]: '1_2_3'\n```\n\n\n# 查找子串位置\n- 使用字符串的find方法返回匹配字符串的**起始位置索引**\n\n```python\nIn [96]: 'i love python'.find('python')\nOut[96]: 7\n```\n\n\n\n# 反转字符串\n- 方法1，使用字符串的join方法和python内置函数将字符串反转\n\n```python\n\nIn [99]: s = \"python\"\nIn [100]: rs = ''.join(reversed(s))\nIn [101]: rs\nOut[101]: 'nohtyp'\n```\n\n- 方法2，利用字符串的切片，只要列表可以使用的切片功能，字符串都可以用\n\n\n\n\n\n```python\nIn [104]: s = \"python\"\nIn [105]: s[::-1]\nOut[105]: 'nohtyp'\n```\n\n\n\n# 字符串切片\n\n- 只要列表可以使用的切片功能，字符串都能用\n\n\n```python\nIn [110]: s = '123456'\nIn [111]: s[1:5]\nOut[111]: '2345'\n\nIn [112]: s[1:5:2]\nOut[112]: '24'\n\nIn [113]: s[::-2]\nOut[113]: '642'\n```\n\n\n\n# 分割字符串\n\n根据指定字符或字符串，分割一个字符串时，使用方法 split。\n\n**join是字符串串联， 和可以把join和split 可看做一对互逆操作**\n\n```python\nIn [114]: 'I_love_python'.split('_')\nOut[114]: ['I', 'love', 'python']\n```\n\n\n# 子串判断\n\n判断 a 串是否为 b 串的子串。\n\n- 方法1，使用成员运算符`in`\n\n```python\nIn [115]: a = 'abc'\nIn [116]: b = 'mnabcdf'\nIn [117]: a in b\nOut[117]: True\n```\n\n- 方法2，使用方法 find，返回字符串 b 中匹配子串 a 的最小索引\n\n  注意：str的find方法与list的index方法用途一样\n\n\n```python\nIn [118]: b.find(a)\nOut[118]: 2\n```\n\n## 去除重复元素\n\n```python\nIn [257]: s = 'abcadcba'\nIn [258]: s = set(s)\nIn [259]: s\nOut[259]: {'a', 'b', 'c', 'd'}\n\nIn [260]: ''.join(s)\nOut[260]: 'bdac'\n```\n\n**得到的结果是乱序的，如果要求和原来的循序一样，勿用此方法**\n\n\n# 正则表达式\n\n字符串封装的方法，处理一般的字符串操作，还能应付。但是，稍微复杂点的字符串处理任务，需要靠正则表达式，简洁且强大。\n\n首先，导入所需要的模块 re\n\n\n```python\nimport re\n```\n\n认识常用的**元字符**：\n\n- `.` 匹配除 \"\\n\" 和 \"\\r\" 之外的任何单个字符。\n- `^` 匹配字符串开始位置\n- `$` 匹配字符串中结束的位置\n- `*` 前面的原子重复 0 次、1 次、多次\n- `?` 前面的原子重复 0 次或者 1 次\n- `+` 前面的原子重复 1 次或多次\n- `{n}` 前面的原子出现了 n 次\n- `{n,}` 前面的原子至少出现 n 次\n- `{n,m}` 前面的原子出现次数介于 n-m 之间\n- `( )` 分组，输出需要的部分\n\n再认识常用的**通用字符**：\n\n- `\\s` 匹配空白字符\n- `\\w` 匹配任意字母/数字/下划线\n- `\\W` 和小写 w 相反，匹配任意字母/数字/下划线以外的字符\n- `\\d` 匹配十进制数字\n- `\\D` 匹配除了十进制数以外的值\n- `[0-9]` 匹配一个 0~9 之间的数字\n- `[a-z]` 匹配小写英文字母\n- `[A-Z]` 匹配大写英文字母\n\n正则表达式，常会涉及到以上这些元字符或通用字符，下面是一些使用方式和小技巧\n\n## search 第一个匹配串\n\n\n```python\nimport re\ns = 'I am a good student'\npat = 'good'\nr = re.search(pat,s)\n# 返回子串的起始位置和终止位置\nr.span()\n# 输出  (7, 11)\n```\n\n\n## match匹配开始位置\n\n正则模块中，match、search 方法匹配字符串不同\n\n- match 在原字符串的开始位置匹配\n- search 在字符串的任意位置匹配\n\n使用match方法的时候，如果子串不是在开始位置则报错\n\n\n```python\nimport re\ns = 'helloworld'\npat = 'wor'\nr = re.match(pat,s)\nr.span()\n# 报错  AttributeError: 'NoneType' object has no attribute 'span'\n```\n把子串`wor`放在最前面，再使用match匹配就不会报错了\n\n\n```python\ns = 'world'\npat = 'wor'\nr = re.match(pat,s)\nr.span()\n# 输出(0, 3)\n```\nmatch场景用的不多，如果只是匹配一个子串的位置，大多数情况下，使用search匹配\n\n\n## finditer匹配迭代器\n\n使用正则模块，finditer 方法，**返回所有子串匹配位置的迭代器**\n\n通过返回的对象 `re.Match`，使用它的方法 span 找出匹配位置\n\n\n```python\nIn [119]: s = 'I am a good student'\nIn [120]: pat = 'a'\nIn [121]: r = re.finditer(pat,s)\nIn [124]: for i in r:\n     ...:     print(i)\n     ...: \n<re.Match object; span=(2, 3), match='a'>\n<re.Match object; span=(5, 6), match='a'>\n```\n\n\n## findall 所有匹配\n\n`findall` 方法能查找出子串的所有匹配。\n\n注意：\n - `findall`是返回所有的匹配\n - `finditer`是返回所有匹配的位置\n\n\n\n```python\ns = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\n```\n\n目标查找出所有所有数字：通用字符 `\\d` 匹配一位数字 [0-9]，`+` 表示匹配数字前面的一个字符 1 次或多次。\n\n不带元字符`+`情况下，输出的是匹配到的一个个数字，这肯定不符合要求\n\n\n```python\nIn [125]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [126]: pat = r'\\d'\nIn [127]: r = re.findall(pat,s)\nIn [128]: print(r)\n['8', '3', '7', '1', '9', '5', '6', '3']\n```\n\n带上元字符`+`\n\n\n```python\nIn [125]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [129]: pat = r'\\d+'\nIn [130]: r = re.findall(pat,s)\nIn [131]: print(r)\n['8371', '9', '56', '3']\n```\n\n`re.findall`返回一个列表，里面包含四个数字，可以看到里面没有小数点，如果我们要找到`9.56`，应该怎么做呢？\n\n## 匹配浮点数和整数\n\n- 元字符`?` 表示前一个字符匹配 0 或 1 次\n- 元字符`.?` 表示匹配小数点（`.`）0 次或 1 次。\n\n匹配浮点数和整数，我们先来看正则表达式：`r'\\d+\\.?\\d+'`，该正则表达式可以理解为：先匹配1个或多个数字，再匹配小数点（`.`）0 次或 1 次，最后匹配1个或多个数字，分解演示如下：\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731172504960.jpg)\n\n```python\ns = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [132]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [133]: pat = r'\\d+\\.?\\d+'\nIn [134]: r = re.findall(pat,s)\nIn [135]: r\nOut[135]: ['8371', '9.56']\n```\n\n\n可以看到，**没有匹配到3**，哪里出错了呢？\n\n出现问题原因：`r'\\d+\\.?\\d+'`， 前面的`\\d+` 表示至少有一位数字，后面的`\\d+`也表示至少有一位数字，因此，整个表达式至少会匹配两位数。\n\n现在将最后的 `+` 后修改为 `*`，表示匹配前面字符 0 次、1 次或多次。如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731172652929.jpg)\n\n\n```python\nIn [132]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [136]: pat = r'\\d+\\.?\\d*'\nIn [137]: r = re.findall(pat,s)\nIn [138]: r\nOut[138]: ['8371', '9.56', '3']\n```\n\n到这里就大功完成了，到这里我们再思考一下，如果把`.?`前面的`\\`去掉以后会怎么样？如下：\n\n\n```python\nIn [132]: s = '呼叫战狼,我是8371,请您在9.56秒后回到3号机场'\nIn [139]: pat = r'\\d+.?\\d*'\nIn [140]: r = re.findall(pat,s)\nIn [141]: r\nOut[141]: ['8371,', '9.56', '3号']\n```\n\n可以看到，匹配到了一个汉字，为什么会出现这种情况呢？因为这里`.?`的含义就变了，之前是把`\\.?`作为一个整体，现在`.?`的含义是附加到`\\d.?`上。\n\n\n`\\d+.?\\d`中`.?`含义如下：\n- `.` 匹配除 `\\n` 和 `\\r` 之外的任何单个字符。\n- 那`.?`就表示匹配`\\n` 和 `\\r` 之外的任何单个字符0 次或 1 次。\n\n我们看下演示图就知道了，`\\d+.?\\d`的演示图如下\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020073117291416.jpg)\n\n\n## 匹配正整数\n\n案例：**写出匹配所有正整数的正则表达式**\n\n我们先来看几个正则表达式即它们的含义\n\n\n前面讲过\n\n- `\\d+` 表示数字出现1次或多次\n- `\\d*` 表示数字出现0次或多次\n\n1. `\\d*` 表示数字出现0次或多次，也就是会匹配所有的字符\n\n\n```python\nIn [142]: s = [2,'我是1','1是我','123',0, 3,'已经9.56秒了',10,-1]\nIn [143]: pat = r'\\d*'\nIn [144]: [i for i in s if re.match(pat,str(i))]\nOut[144]: [2, '我是1', '1是我', '123', 0, 3, '已经9.56秒了', 10, -1]\n```\n\n2. `^\\d*$`,在数字出现0次或多次前加上了开始位置，再后加上了结束位置。即必须是以数字的开头，以数字结尾，才能匹配到\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731173222954.jpg)\n\n```python\nIn [142]: s = [2,'我是1','1是我','123',0, 3,'已经9.56秒了',10,-1]\nIn [145]: pat =r'^\\d*$'\nIn [146]: [i for i in s if re.match(pat,str(i))]\nOut[146]: [2, '123', 0, 3, 10]\n```\n\n\n可以看到，匹配到了0，我们的目的是匹配所有正整数，所以不正确\n\n\n3. `^[1-9]*$` 表示匹配一个 0~9 之间的数字\n\n\n```python\nIn [147]: s = [2,'我是1','1是我','123',0, 3,'已经9.56秒了',10,-1]\nIn [148]: pat =r'^[1-9]*$'\nIn [149]: [i for i in s if re.match(pat,str(i))]\nOut[149]: [2, '123', 3]\n```\n\n可以看到，不能匹配10，因此这个也不正确\n\n4. `^[1-9]\\d*$` 表示匹配一个 0~9 之间的数字并且匹配的数字出现0次或多次\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731173331745.jpg)\n\n\n```python\nIn [150]: s = [2,'我是1','1是我','123',0, 3,'已经9.56秒了',10,-1]\nIn [151]: pat =r'^[1-9]\\d*$'\nIn [152]: [i for i in s if re.match(pat,str(i))]\nOut[152]: [2, '123', 3, 10]\n```\n\n\n\n## re.I 忽略大小写\n\n找出字符串中所有字符 t 或 T 的位置，不区分大小写。\n\n\n```python\ns = 'HELLO，World'\npat = 'L'\nr = re.finditer(pat,s,re.I)\nfor i in r:\n    print(i.span())\n\"\"\"\n输出：\n    (2, 3)\n    (3, 4)\n    (9, 10)\n\"\"\"\n```\n\n​    \n\n\n```python\ns = 'HELLO，World'\npat = 'L'\nr = re.findall(pat,s,re.I)\nr\n# 输出 ['L', 'L', 'l']\n```\n\n\n## re.split分割单词\n\n**正则模块中 split 函数强大，能够处理复杂的字符串分割任务**\n\n对于简单的分割，直接用分隔符\n\n\n```python\nIn [153]: s = 'I-am-a-good-student'\nIn [154]: s.split('-')\nOut[154]: ['I', 'am', 'a', 'good', 'student']\n```\n\n\n但是，对于分隔符复杂的字符串，split 函数就无能为力\n\n如下字符串，可能的分隔符有`, ; - . |`和空格\n\n\n```python\nIn [155]: s = 'I,,,am |  ;a - good.. student'\nIn [156]: s.split('[,;\\-.|]')\nOut[156]: ['I,,,am |  ;a - good.. student']\n```\n\n可以看到，直接使用是区分不开的，所有可以用**正则模块中的split**\n\n`\\s` 匹配空白字符\n\n\n```python\nIn [157]: s = 'I,,,am |  ;a - good.. student'\nIn [158]: w = re.split(r'[,.\\s;\\-|]+',s)\nIn [159]: w\nOut[159]: ['I', 'am', 'a', 'good', 'student']\n```\n\n注意：**re.split默认匹配所有的候选字符**，因此不需要元字符`+`了\n\n但是，如果在字符串前面和后面加多个空格，`\\s`就区分不开了，如下：\n\n```python\nIn [163]: In [157]: s = '  I,,,am |  ;a -     good.. student    '\nIn [164]: w = re.split(r'[,.\\s;\\-|]+',s)\nIn [165]: w\nOut[165]: ['', 'I', 'am', 'a', 'good', 'student', '']\n```\n\n这是，可以用字符串strip方法去除字符串的前后空格\n\n```python\nIn [163]: In [157]: s = '  I,,,am |  ;a -     good.. student    '\nIn [172]: s=s.strip()\nIn [173]: w = re.split(r'[,.\\s+;\\-|]+',s)\nIn [174]: w\nOut[174]: ['I', 'am', 'a', 'good', 'student']\n```\n\n\n## sub 替换匹配串\n\n正则模块，sub 方法，替换匹配到的子串\n\n\n```python\ns = '你好，我是12306'\npat = r'\\d+'\nw = re.sub(pat,'hello',s)\nw\n# 输出 '你好，我是hello'\n```\n\n\n## compile 预编译\n\n如果要用同一匹配模式，做很多次匹配，可以使用 compile 预先编译串\n\n如果我们要：从一系列字符串中，挑选出所有正浮点数\n\n正则表达式为：`^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*$`，字符 `a|b `表示 `a` 串匹配失败后，才执行 `b` 串\n\n\n```python\ns = [-1,10,0,7.21,0.5,'123','你好','3.25',11.0,9.]\nrec = re.compile(r'^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*$')\n[ i for i in s if rec.match(str(i))]\n# 输出 [7.21, 0.5, '3.25', 11.0, 9.0]\n```\n\n\n","tags":["python"],"categories":["python"]},{"title":"python中list和tuple的22种操作方法","url":"/2020/11/24/221035/","content":"\n# 列表list\n\n列表（list）作为 Python 中最常用的数据类型之一，是一个可增加、删除元素的可变（mutable）容器。\n\n<!-- more -->\n\n\n\n\n## 创建列表\n\n\n```python\nlst = [] #创建空列表\nlst1 = [1,'xiaoming',29.5,'17312662388']\nlst2 = [['hello','world'],[12,'abc'],['冰箱','空调']]\n```\n\n## 查看列表长度\n\n\n```python\nlen(lst) # 0\nlen(lst1) # 4\nlen(lst2) # 3\n```\n\n\n## 遍历列表\n\n\n```python\nIn [16]: for t in lst1:\n    ...:     print(t)\n    ...: \n1\nxiaoming\n29.5\n17312662388\n```\n\n\n```python\nIn [17]: for t1,t2 in lst2:\n    ...:     print(t1,t2)\n    ...: \nhello world\n12 abc\n冰箱 空调\n```\n\n​    \n\n## 元素在列表中的位置\n\n\n```python\nIn [18]: ls_3=[1, 3, 12.3, 'apple', 0.001, 'apple', 'orange', 1, 2, 3, 4]\n# 从列表ls_3 中找出\"apple\" 第一个匹配项的索引位置\nIn [19]: ls_3.index(\"apple\")\nOut[19]: 3\n\nIn [20]: ls_3.index(3)\nOut[20]: 1\n```\n\n\n\n\n\n## 列表中增加元素\n- 方法1 ，使用列表的append方法在列表尾部添加元素\n\n```python\nIn [21]: lst = ['abc','123']\nIn [22]: lst.append('efg')\nIn [23]: lst\nOut[23]: ['abc', '123', 'efg']\n```\n- 方法2，使用列表的insert方法插入元素\n```python\nIn [24]: lst = ['abc','123']\nIn [25]: lst.insert(1,'hello')\nIn [26]: lst\nOut[26]: ['abc', 'hello', '123']\n```\n\n\n## 合并列表\n- 方法1，使用列表的extend方法添加另一列表\n\n```python\nls_3 = ['abc','123']\nls_3.extend([1, 2, 3]) \nls_3.extend([4,5])\nprint (ls_3)\n# ['abc', '123', 1, 2, 3, 4, 5]\n```\n- 方法2，直接使用运算符+合并列表\n\n```python\nIn [27]: ls_3 = ['abc','123']\nIn [28]: ls_4 = [4,5]\nIn [29]: ls_3+ls_4\nOut[29]: ['abc', '123', 4, 5]\n```\n\n- 方法3，使用`*`合并列表\n\n```python\nIn [27]: ls_3 = ['abc','123']\nIn [28]: ls_4 = [4,5]\nIn [31]: [*ls_3]\nOut[31]: ['abc', '123']\nIn [32]: [ls_3]\nOut[32]: [['abc', '123']]\n\nIn [33]: [*ls_3,*ls_4]\nOut[33]: ['abc', '123', 4, 5]\n```\n\n## 移除列表元素\n- 方法1，使用列表的pop方法移除元素\n\n列表的pop默认移除最后一个元素，一个移除指定元素\n\n```python\nIn [35]: lst = ['abc', '123', 'efg','苹果','香蕉']\n# 列表的pop方法返回移除的元素\nIn [36]: a = lst.pop()\nIn [37]: a\nOut[37]: '香蕉'\nIn [38]: lst\nOut[38]: ['abc', '123', 'efg', '苹果']\nIn [39]: b = lst.pop(1)\nIn [40]: b\nOut[40]: '123'\nIn [41]: lst\nOut[41]: ['abc', 'efg', '苹果']\n```\n\n- 方法2，使用列表的remove方法移除元素\n\n```python\nlst = ['abc', '123', 'efg']\n# 列表的remove方法没有返回值\na = lst.remove('efg')\nprint(a) # None\nprint(lst) # ['abc', '123']\n```\n\n- 方法3，使用关键字del删除列表中的元素\n\n```python\na = ['苹果','香蕉','橘子','香蕉']\ndel a[2]\na # ['苹果', '香蕉', '香蕉']\n```\n\n\n\n\n## 列表的深浅拷贝\n\n\n```python\nlst = ['你好',['hello','world'],[12,'abc']]\n# 列表的copy方法只能实现浅copy\ncopy1 = lst.copy()\ncopy1[1][0] = 3\nprint(lst) # ['你好', [3, 'world'], [12, 'abc']]\nprint(copy1) # ['你好', [3, 'world'], [12, 'abc']]\n```\n\n\n要想实现深度拷贝，需要使用 `copy` 模块的 `deepcopy` 函数\n\n\n```python\nfrom copy import deepcopy\nlst = ['你好',['hello','world'],[12,'abc']]\n# 列表的copy方法只能实现浅copy\ncopy1 = deepcopy(lst)\ncopy1[1][0] = 3\nprint(lst) # ['你好', ['hello', 'world'], [12, 'abc']]\nprint(copy1) # ['你好', [3, 'world'], [12, 'abc']]\n```\n\n\n## 判断元素是否在列表中\n\n列表所有数据类型的元素，都可以用该方法判断\n\n\n```python\na = [1,2,3]\n1 in a # True\n5 in a # False\nb = ['你','好','啊']\n'你' in b #True\nc = [(1,2),(3,4)]\n(1,2) in c # True\nc = [([1,2],2),(3,4)]\n([1,2],2) in c #True\nd = [[1,2],[3,4]]\n[1,2] in d # True\n[1,3] in d # False\n'ab' in 'abc' # True\n```\n\n\n## 列表的切片\n\n\n```python\na = list(range(1,20,3))\nprint(a) # [1, 4, 7, 10, 13, 16, 19]\n```\n\n切片的返回结果也是一个列表\n\n- 使用 `a[:3]` 获取列表 a 的前三个元素\n- 使用 `a[-1]` 获取 a 的最后一个元素，返回 int 型，值为 19\n- 使用 `a[:-1]` 获取除最后一个元素的切片 `[1, 4, 7, 10, 13, 16]`\n- 使用 `a[1:5]` 生成索引为 `[1,5)`（不包括索引 5）的切片 `[4, 7, 10, 13]`\n- 使用 `a[1:5:2]` 生成索引 `[1,5)` 但步长为 2 的切片 `[4,10]`\n- 使用 `a[::3]` 生成索引 `[0,len(a))` 步长为 3 的切片 `[1,10,19]`\n- 使用 `a[::-1]` 生成逆向索引 `[len(a),0)` 的切片 `[19, 16, 13, 10, 7, 4, 1]`\n- 使用 `a[::-3]` 生成逆向索引 `[len(a),0)` 步长为 3 的切片 `[19,10,1]`\n\n需要注意的是，`a[1:5:-1]`执行会返回空列表，不是生成`[1,5)`的逆向索引，如果要生成`[1,5)`的逆向索引，可以用如下方式：\n\n\n```python\na[1:5:-1]  # []\nb = a[1:5]\nb[::-1] # [13, 10, 7, 4]\n```\n\n\n## 列表的操作符\n\n列表的操作符只支持`+、*`\n\n```python\nIn [44]: [1, 2,]+[2, 3] #列表合并\nOut[44]: [1, 2, 2, 3]\n\nIn [42]: [1]*3  #列表元素重复\nOut[42]: [1, 1, 1]\n\nIn [43]: [1,2]*3\nOut[43]: [1, 2, 1, 2, 1, 2]\n\n```\n\n\n\n## 列表中某个元素的数量\n\n\n```python\na = ['苹果','香蕉','橘子','香蕉']\nprint(a.count('苹果')) # 1\nprint(a.count('香蕉')) # 2\n```\n\n\n## 列表转化为集合\n\n\n```python\na = ['苹果','香蕉','橘子','香蕉']\nset(a) # {'橘子', '苹果', '香蕉'}\n\n# 集合里面必须是不可哈希对象(不可变对象)\na = [[1,2],[1,2],[1,2,3]]\nset(a) # 报错 unhashable type: 'list'\n```\n\n\n## 判断中有无重复元素\n\n\n```python\n# 方法1\ndef judge_duplicated(lst):\n    for x in lst:\n        if lst.count(x) > 1: # 判断 x 元素在 lst 中的出现次数\n            return True # 重复返回ture\n    return False  # 无重复返回false\n# 方法2\ndef judge_duplicated(lst):\n    return len(lst) != len(set(lst)) # 重复返回ture，不重复返回False\n```\n\n## 列表使用python内置函数\n\n**python内置函数无需import**\n\n\n```python\nlen([1,2,3,4,5]) # 列表长度 5\nmax([1, 5, 65, 5]) #返回列表最大值 65\nmin([1, -1, 89]) #返回列表最小值 -1\nsorted(['10','3','2']) # 从小到大 ['10', '2', '3']\n# 注意：max，min，sorted只能用在全数值型列表中\n```\n\n\n```python\n# 也可以判断字符串大小，判断字符串大小是根据字符串首字符ASCII编码判断的\nmax(['10','3','2']) # 返回`3`\n# 判断字符串直接用运算符\n# '10'>'3' False\n# '10'<'3' True\n```\n\n\n\n\n\n## 列表数据重洗\n\n\n```python\nfrom random import shuffle\na = [1,3,2,5,6]\nshuffle(a) # 重洗数据\na # [3, 2, 6, 1, 5]\n```\n\n## 列表逆置\n\n- 方法1，使用列表的reverse方法对列表元素逆置\n\n```python\n# 方法一\nIn [52]: a = [1,3,2,5,6]\nIn [53]: a.reverse()\nIn [54]: a\nOut[54]: [6, 5, 2, 3, 1]\n\n```\n\n- 方法2，使用python内置函数reversed方法对列表元素逆置\n\n```python\n# 方法2\nIn [52]: a = [1,3,2,5,6]\nIn [56]: list(reversed(a))\nOut[56]: [6, 5, 2, 3, 1]\n```\n- 方法3，利用列表的切片\n\n```python\nIn [102]: a = [1,3,2,5,6]\nIn [103]: a[::-1]\nOut[103]: [6, 5, 2, 3, 1]\n```\n\n## 列表排序规则\n\n- 一般的列表元素排序\n\n```python\n# sorted(a) a要求必须是数值型\na = [1,3,2,5,6]\nsorted(a)\n```\n\n- 按照列表的某个位置排序\n\n如下所示，按照每一个列表的第二个位置排序：\n\n```python\n# 方法一\nIn [45]: a=[[1,2],[4,1],[9,10],[13,-3]]\nIn [46]: a.sort(key=lambda x: x[1])\nIn [47]: a\nOut[47]: [[13, -3], [4, 1], [1, 2], [9, 10]]\n# 方法二\nIn [48]: a=[[1,2],[4,1],[9,10],[13,-3]]\nIn [49]: sorted(a,key = lambda x: x[1] )\nOut[49]: [[13, -3], [4, 1], [1, 2], [9, 10]]\n```\n\n\n## 列表重复最多的元素\n- 找到列表重复最多的元素\n\n```python\nIn [50]: a = [1,2,3,3,4,2,3]\nIn [51]: max(a,key=lambda x: a.count(x), default=1)\nOut[51]: 3\n```\n\n\n\n\n\n# 元组tuple\n\n元祖与列表很相似，只不过元组是不可变（immutable）对象(即不可哈希对象)，元组中的元素**不能修改**，没有增加、删除元素的方法。**元组一旦创建后，长度就被唯一确定**\n\n**创建元素**\n\n```python\ntup_0 = tuple() #创建空元组\ntup_1 = () #创建空元组\ntup_2 = (1, ) #创建只包含一个元素的元组，注意要有逗号,否则会被认为元素本身\ntup_3 = (1, 3, \"apple\", 66) #创建多元素元组\n```\n\n\n**元组除了没有增加、删除元素的方法，不能修改，其他用法与list完全一样，如果我们想要高效的操作元祖，大可用python内置函数list将元祖转化为列表**\n\n**上述列表中的操作，除了增加、删除元素的操作之外，其他的列表操作方法完全可以用在元组上，这里就不一一介绍了**\n\n这里仅说明一些需要注意的点\n\n对于元组，里面的元素可以是任意数据类型，这就意味着也可以是列表、字典等可哈希类型，如下所示：\n\n```python\nIn [82]: a = ([1,2],)\nIn [83]: a[0][0]=3\nIn [84]: a\nOut[84]: ([3, 2],)\nIn [79]: type(a)\nOut[79]: tuple\n```\n**刚刚明明说好元组中的元素不能改变，为什么这里变了？**\n\n这里元组中存的列表的地址，换句话说列表的地址没有改变，只是地址对应的内容变了，如下\n\n```python\nIn [85]: lst = [1,2]\nIn [91]: id(lst) # id获取地址\nOut[91]: 2763791008392\n\nIn [89]: tup=(lst,)\nIn [90]: tup\nOut[90]: ([1, 2],)\n\nIn [92]: lst[0]=3\nIn [93]: lst\nOut[93]: [3, 2]\nIn [94]: tup\nOut[94]: ([3, 2],)\n\nIn [95]: id(lst)\nOut[95]: 2763791008392\n```\n可以看到，列表的地址并没有改变\n\n","tags":["python"],"categories":["python"]},{"title":"Python入门之运算符与基本数据类型详解","url":"/2020/11/24/220821/","content":"\n# Python 的运算符\n\n## 算数运算符\n\n算数运算符 `+ - * / // % **`\n\n`//` 用于两个数值相除且向下取整\n\n<!-- more -->\n\n\n\n\n\n\n```python\nIn [1]: 5//2\nOut[1]: 2\n\nIn [2]: 5//3.0\nOut[2]: 1.0\n```\n\n`**` 用于幂运算\n\n```python\nIn [3]: 2**3\nOut[3]: 8\n```\n\n\n## 比较运算符\n\nPython比较运算符 `== != > < >= <=`，返回值：True/False\n\nPython 比较运算符还支持链式比较，应用起来更加方便\n\n\n```python\ni = 3\nprint(1 < i < 3) # False\nprint(1 < i <= 3) # True\n```\n\n\n关于`==`的详细的用法见[Python中is、in、==之间的区别](https://blog.csdn.net/qq_37555071/article/details/107664916)\n\n## 赋值运算符\n\nPython赋值运算符 `= 、+=、 -=、 \\*=、 /=、 //=、 %=、 **=、 :=`\n\n\n```python\n# 幂赋值运算符 c **= a 等效于 c = c ** a\n#2的5次方\nIn [5]: a = 2\n   ...: a**=5\n   ...: a\nOut[5]: 32\n```\n\n\n## 逻辑运算符\n\nPython逻辑运算符 `and or not`,是指 `与`  `或`  `非` 的意思\n\n\n```python\nIn [6]: True and True\nOut[6]: True\n\nIn [7]: True and False\nOut[7]: False\n\nIn [8]: True or False\nOut[8]: True\n# not优先级最高，使用时一定要注意\nIn [9]: not True and False\nOut[9]: False\n\nIn [10]: not (True and False)\nOut[10]: True\n```\n\n\n\n## 成员运算符\n\nPython成员运算符 `in 、not in `\n\n- 如果元素 i 是 s 的成员，则 `i in s` 为 True\n- 若不是 s 的成员，则返回 False，也就是`i not in s` 为 True\n\n关于`in 、not in `的详细的用法见[Python中is、in、==之间的区别](https://blog.csdn.net/qq_37555071/article/details/107664916)\n\n## 身份运算符\n\nPython身份运算符 `is 、is not`\n\n`is`是Python身份运算符，用于判断两个对象的标识符是否相等(python中万物皆对象)，实质是用于**比较两个对象是否指向同一存储单元**\n\n关于`is 、is not`的详细的用法见[Python中is、in、==之间的区别](https://blog.csdn.net/qq_37555071/article/details/107664916)\n\n# Python 四大数据类型总结\n\n## 数值型\n\n`int` 整型对象、`float` 双精度浮点型、`bool` 逻辑对象，它们都是单个元素，称为数据型\n\n前缀加 `0x`，创建一个十六进制的整数\n\n\n```python\nIn [11]: 0xa5 # 等于十进制的 165\nOut[11]: 165\n```\n\n\n使用 `e` 创建科学计数法表示的浮点数：\n\n\n```python\nIn [12]: 1.05e3 # 1050.0\nOut[12]: 1050.0\n\nIn [13]: 1e3 # 1000.0\nOut[13]: 1000.0\n```\n\n\n## 容器型\n\n可容纳多个元素的容器对象，常用的比如：list 列表对象、 tuple 元组对象、dict 字典对象、set 集合对象\n\n使用一对中括号 `[]`，创建一个 list 型变量\n\n\n```python\nlst = [1,3,5] # list 变量\n```\n\n使用一对括号 `()`，创建一个 `tuple` 型对象\n\n\n```python\ntup = (1,3,5) # tuple 变量\n```\n\n但需要注意，含单个元素的元组后面必须保留一个逗号，才被解释为元组\n\n\n```python\ntup = (1,) # 必须保留逗号\n```\n\n否则会被认为元素本身\n\n\n```python\ntup=(1)\nprint(type(tup))\n# <class 'int'>\n```\n\n仅使用一对花括号 `{}`，创建一个 set 对象\n\n```python\ns = {1,3,5} # 集合变量\n```\n\n**字符串**\n\nPython 中没有像 C++ 表示的字符类型（char），所有的字符或串都被统一为 `str` 对象。如单个字符 `c` 的类型也为 `str`\n\nstr 类型会被经常使用，一些高频用法如下\n\nstrip 用于去除字符串前后的空格\n\n\n```python\n'  I love python  '.strip()\n# 'I love python'\n```\n\n\nreplace 用于字符串的替换\n\n\n```python\n'i love python'.replace(' ','_')\n# 'i_love_python'\n```\n\n\njoin 用于合并字符\n\n\n```python\n'_'.join(['book', 'store','count'])\n# 'book_store_count'\n```\n\nfind 用于返回匹配字符串的起始位置索引\n\n```python\n'i love python'.find('python')\n# 7\n```\n\n\n","tags":["python"],"categories":["python"]},{"title":"Python中is、in、==之间的区别","url":"/2020/11/24/220637/","content":"\nPython 中，对象相等性比较相关关键字包括` is、in`，比较运算符有 `==`\n\n- `is`判断两个对象的标识符是否相等\n- `in`用于成员检测\n- `==`用于判断值或内容是否相等，默认是基于两个对象的标识号比较\n\n也就是说，如果 `a is b` 为 True 且如果按照默认行为，意味着 `a==b` 也为 True\n\n<!-- more -->\n\nPython 中，对象相等性比较相关关键字包括` is、in`，比较运算符有 `==`\n\n- `is`判断两个对象的标识符是否相等\n- `in`用于成员检测\n- `==`用于判断值或内容是否相等，默认是基于两个对象的标识号比较\n\n也就是说，如果 `a is b` 为 True 且如果按照默认行为，意味着 `a==b` 也为 True\n\n## is 判断标识号是否相等\n\n`is`是Python身份运算符，用于判断两个对象的标识符是否相等(python中万物皆对象)，实质是**用于比较两个对象是否指向同一存储单元**，以下有几点`is`的使用方法：\n\n(1) Python 中使用 `id()` 函数获取对象的标识号，可以理解为内存地址\n\n```python\nIn [49]: a = 1\nIn [50]: b = 1\nIn [51]: id(a)\nOut[51]: 140708412432784\nIn [52]: id(b)\nOut[52]: 140708412432784\nIn [53]: a is b\nOut[53]: True\n\nIn [54]: s1 = 'abc'\nIn [55]: s2 = 'abc'\nIn [56]: id(s1)\nOut[56]: 1554773534064\nIn [57]: id(s2)\nOut[57]: 1554773534064\nIn [58]: s1 is s2\nOut[58]: True\n```\n\n\n(2) 由于创建的两个列表实例位于不同的内存地址，所以它们的标识号不等，即便对于两个空列表实例也一样\n\n```python\nIn [77]: a = [1,2]\nIn [78]: b = [1,2]\nIn [79]: id(a)\nOut[79]: 1552668915144\nIn [80]: id(b)\nOut[80]: 1552668915208\nIn [81]: a is b\nOut[81]: False\nIn [83]: a is not b\nOut[83]: True\nIn [84]: not  a is b\nOut[84]: True\n\nIn [85]: a = []\nIn [86]: b = []\nIn [87]: id(a)\nOut[87]: 1552668699208\nIn [88]: id(b)\nOut[88]: 1552669048456\nIn [89]: a is b\nOut[89]: False\n```\n(3) 对于序列型、字典型、集合型对象，一个对象实例指向另一个对象实例，is 比较才返回真值。\n\n```python\nIn [95]: a = [1,2]\nIn [96]: b = a\nIn [97]: a is b\nOut[97]: True\n```\n\n\n(4) 需要注意的是，Python 解释器，对位于区间 `[-5,256]` 内的小整数，会进行缓存，不在该范围内的不会缓存，因此会出现如下现象：\n\n```python\nIn [90]: a = 12345\nIn [91]: b = 12345\nIn [92]: id(a)\nOut[92]: 1552668913040\nIn [93]: id(b)\nOut[93]: 1552668911312\nIn [94]: a is b\nOut[94]: False\n```\n\n(5) Python 中 None 对象是一个单例类的实例，具有唯一的标识号，在判断某个对象是否为 None 时，最便捷的做法：`variable is None`，如下：\n\n```python\nIn [98]: id(None)\nOut[98]: 140708411956448\nIn [99]: a = None\nIn [100]: a is None\nOut[100]: True\nIn [101]: id(a)\nOut[101]: 140708411956448\n```\n\n## in 用于成员检测\n- 如果元素 i 是 s 的成员，则 `i in s` 为 True；\n- 若不是 s 的成员，则返回 False，也就是` i not in s` 为 True\n\nin 是判断成员是否属于某个序列，in 后即可以跟字符串，也可以跟列表,实际上各种集合类型都可以。\n\n（1） 对于字符串类型，`i in s` 为 True，意味着 i 是 s 的子串。\n\n```python\nIn [104]: '234' in '12345'\nOut[104]: True\n# 当然。也可以用字符串的find()方法判断\nIn [105]: '12345'.find('234')\nOut[105]: 1\n```\n（2）列表中任何数据类型的元素都可以用in判断，同理，元祖和集合也可以这样做\n\n```python\nIn [106]: [1,2] in [[1,2],3,'abc']\nOut[106]: True\nIn [107]: lst = [[1,2],3,'abc']\nIn [109]: [1,2] in lst\nOut[109]: True\nIn [110]: 'abc' in lst\nOut[110]: True\n```\n\n（3）对于字典类型，in 操作判断 i 是否是字典的键，但不能直接判断值，如果要判断一个值是否在字典中，可以用字典的values()方法\n\n```python\nIn [111]: dct = {'a':12,'b':35,'key':'abc'}\nIn [112]: 'a' in dct\nOut[112]: True\nIn [113]: 'key' in dct\nOut[113]: True\n\nIn [114]: 12 in dct\nOut[114]: False\nIn [115]: 12 in dct.values()\nOut[115]: True\nIn [116]: 'abc' in dct.values()\nOut[116]: True\n```\n\n（4）对于自定义类型，判断是否位于序列类型中，需要重写序列类型的 魔法方法 __contains__。\n\n```python\nclass Node():\n    def __init__(self,value):\n        self.value = value\n        \nclass Nodes(list):\n        def __contains__(self,node):\n            for s in self:\n                if s.value == node.value:\n                    return True\n            return False    \n\nn1 = Node('linear')\nn2 = Node('sigmoid')\na = Nodes()\na.extend([n1,n2])\nn3 = Node('linear')\nprint(n3 in a) # True\nn4 = Node('tanh')\nprint(n4 in a) # False\n```\n\n## == 判断值是否相等\n（1） **对于数值型、字符串、列表、字典、集合，默认只要元素值相等，== 比较结果是 True**\n\n```python\nIn [117]: str1 = 'abc'\nIn [118]: str2 = 'abc'\nIn [119]: str1 == str2\nOut[119]: True\n\nIn [120]: a = [1,2]\nIn [123]: b = [1,2]\nIn [124]: a == b\nOut[124]: True\n\nIn [125]: a = [[1,2],3]\nIn [126]: b = [[1,2],3]\nIn [127]: a == b\nOut[127]: True\n\nIn [128]: a = {'a':1,'b':2}\nIn [129]: b = {'a':1,'b':2}\nIn [130]: a == b\nOut[130]: True\n\nIn [131]: a = {1,2}\nIn [132]: b = {1,2}\nIn [133]: a == b\nOut[133]: True\n```\n\n（2）对于自定义类型，当所有属性取值完全相同的两个实例，判断 == 时，返回 False。如果需要用`==`判断两个对象是否相等，需要重写方法 `__eq__`，使用` __dict__ `获取实例的所有属性。\n\n```python\nclass Node():\n    def __init__(self,value):\n        self.value = value\n    def __eq__(self,node):\n        return self.__dict__ == node.__dict__\n    \nn1 = Node('linear')\nn2 = Node('sigmoid')\nn3 = Node('linear')\n\nprint(n1.__dict__) # {'value': 'linear'}\nprint(n1 == n2) # False\nprint(n1 == n3) # True\n```\n","tags":["python"],"categories":["python"]},{"title":"python中print函数的所有输出形式","url":"/2020/11/24/220508/","content":"\npython中print函数的所有输出形式\n\n<!-- more -->\n\n\n## 普通输出\n\n```python\nIn [11]: print('b')\nb\n# end为不换行输出\nIn [14]: for i in range(10):\n    ...:     print(i,end='')\n0123456789\n```\n\n## format输出\n\n```python\nIn [16]: a, b = 1, 2\nIn [17]: print('{},{}'.format(a,b))\n1,2\nIn [18]: print('{1},{0}'.format(a,b))\n2,1\n```\n\n## f输出\n\n```python\nIn [16]: a, b = 1, 2\nIn [19]: print(f'{a},{b}')\n1,2\n```\n## %输出\n- %s --- str 字符串\n- %x --- hex 十六进制\n- %d --- dec 十进制\n- %o --- oct 八进制\n\n```python\nIn [16]: a, b = 1, 2\nIn [20]: print('%s,%s'%(a,b))\n1,2\nIn [29]: nHex = 0x20\nIn [30]: print(\"nHex = %x,nDec = %d,nOct = %o\" %(nHex,nHex,nHex))\nnHex = 20,nDec = 32,nOct = 40\n```\n\n## float格式化输出\n- %f --- float 浮点数\n\n\n```python\nIn [32]: import math\nIn [33]: print(\"PI = %f\"%(math.pi))\nPI = 3.141593\n# 保留三维有效数字\nIn [35]: print(\"PI = %.3f\" % math.pi)\nPI = 3.142\n# width = 10,precise = 3,align = right\nIn [36]: print(\"PI = %10.3f\" % math.pi)\nPI =      3.142\n# width = 10,precise = 3,align = left\nIn [37]: print(\"PI = %-10.3f\" % math.pi)\nPI = 3.142\n# 前面填充字符\nIn [38]: print(\"PI = %06d\" % int(math.pi))\nPI = 000003\n```\n\n","tags":["python"],"categories":["python"]},{"title":"卷积神经网络CNN与LeNet5详解(可训练参数量、计算量、连接数的计算+项目实战)","url":"/2020/11/24/220330/","content":"\n## 神经网络\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728094437615.png)\n神经网络可以看成一个端到端的黑盒，中间是隐藏层(可以很深)，两边是输入与输出层，完整的神经网络学习过程如下：\n1. 定义网络结构（指定输入层、隐藏层、输出层的大小）\n2. 初始化模型参数\n3. 循环操作：\n\t\t3.1.  执行前向传播（输入参数，计算一个个结点的值得到y’，即预测值）\n\t\t3.2.  计算损失函数（拿y’-y计算Loss）\n\t\t3.3.  执行后向传播（求梯度，为了更新参数）\n\t\t3.4.  权值更新\n\n<!-- more -->\n\n\n\n## CNN卷积神经网络\n### CNN的由来\n卷积神经网络（CNN）是人工神经网络的一种，是多层感知机（MLP）的一个变种模型，它是从生物学概念中演化而来的。\n\n> Hubel和Wiesel早期对猫的视觉皮层的研究中得知在视觉皮层存在一种细胞的复杂分布，这些细胞对于外界的输入局部是很敏感的，它们被称为“感受野”（细胞），它们以某种方法来覆盖整个视觉域。这些细胞就像一些滤波器一样，够更好地挖掘出自然图像中的目标的空间关系信息。\n视觉皮层存在两类相关的细胞，S细胞（Simple Cell）和C（Complex Cell）细胞。S细胞在自身的感受野内最大限度地对图像中类似边缘模式的刺激做出响应，而C细胞具有更大的感受野，它可以对图像中产生刺激的模式的空间位置进行精准地定位。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728095934616.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70 =500x300)\n卷积神经网络已成为语音和图像识别的研究热点，80年代末，Yann LeCun就作为贝尔实验室的研究员提出了卷积网络技术，并展示如何使用它来大幅度提高手写识别能力。在图像识别领域，CNN已经成为一种高效的识别方法\n\nCNN的应用广泛，包括图像分类，目标检测，目标识别，目标跟踪，文本检测和识别以及位置估计等。\n\n\nCNN的基本概念：\n- 局部感受野（local receptive fields）\n- 共享权重（shared weights）\n- 池化（pooling）\n\n### 局部感受野\n**局部感受野（local receptive fields）**：图像的空间联系是局部的，就像人通过**局部的感受野**去感受外界图像一样，每个神经元只感受局部的图像区域，然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728104951198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**CNN中相邻层之间是部分连接，即某个神经单元的感知区域来自于上层的部分神经单元。这个与MLP多层感知机不同，MLP是全连接，某个神经单元的感知区域来自于上层的所有神经单元。**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728105134988.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n### 共享权重\n**共享权重（shared weights）**：共享权重即参数共享，隐藏层的参数个数和隐藏层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关。也就是说，对于一个特征图，它的每一部分的卷积核的参数都是一样的。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728105613313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*左图是没有参数共享的情况，右图进行了参数共享*\n\n如果要提取不同的特征，就需要多个滤波器。每种滤波器的参数不一样，表示它提出输入图像的不同特征。**这样每种滤波器进行卷积图像就得到对图像的不同特征的反映，我们称之为Feature Map**；100种卷积核就有100个Feature Map，这100个Feature Map就组成了一层神经元。\n\n### 池化\n池化（pooling）原理：根据图像局部相关的原理，图像某个邻域内只需要一个像素点就能表达整个区域的信息， 池化也称为混合、下采样，目的是减少参数量。分为最大池化，最小池化，平均池化。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072811065160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n### CNN的结构\n\n\nCNN的网络结构如下：\n- 输入层，Input，输入可以是灰度图像或RGB彩色图像（三通道）。对于输入的图像像素分量为 [0, 255]，为了计算方便一般需要归一化（如果使用sigmoid激活函数，会归一化到[0, 1]，如果使用tanh激活函数，则归一化到[-1, 1]）\n- 卷积层，C*，特征提取层，得到特征图，目的是使原信号特征增强，并且降低噪音；\n- 池化层，S*，特征映射层，将C*层多个像素变为一个像素，目的是在保留有用信息的同时，尽可能减少数据量\n- 光栅化：为了与传统的多层感知器MLP全连接，就是把池化层得到的特征图拉平\n- 多层感知器(MLP)：最后一层为分类器，多分类使用Softmax，二分类可以使用Logistic Regression\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072811090764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n**卷积过程包括**：用一个可训练的滤波器fx去卷积一个输入的图像（第一阶段是输入的图像，后面的阶段就是卷积特征图），然后加一个偏置bx，得到卷积层Cx\n**下采样(池化)过程包括**：每邻域四个像素求和变为一个像素，然后通过标量Wx+1加权，再增加偏置bx+1，然后通过一个sigmoid激活函数，产生一个缩小四倍的特征映射图Sx+1。(**下图很重要，下面可训练参数量的计算就可以从这里看出**)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728111236668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n卷积神经网络就是让权重在不同位置共享的神经网络，在下图中，局部区域圈起来的所有节点会被连接到下一层的一个节点上(卷积核，称为 kernel 或 filter 或 feature detector，filter的范围叫做filter size，比如 2x2的卷积核)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728112632366.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n上图的运算过程可用如下公式表示(**上图有重要作用，下面的连接数和神经元个数就可以从这里看出来**)：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728112703711.png)\nCNN学习可以帮我们进行特征提取，比如我们想要区分人脸和狗头，那么通过CNN学习，背景部位的激活度基本很少。\nCNN layer越多，学习到的特征越高阶，如下图所示：\n- layer 1、layer 2学习到的特征基本上是颜色、边缘等低层特征\n- layer 3开始稍微变得复杂，学习到的是纹理特征，比如网格纹理\n- layer 4学习到的是比较有区别性的特征，比如狗头\n- layer 5学习到的则是完整的，具有辨别性关键特征\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728113651972.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n### 光栅化\n**光栅化（Rasterization）**：为了与传统的MLP（多层感知机）全连接，把上一层的所有Feature Map的每个像素依次展开，排成一列。\n图像经过下采样后，得到的是一系列的特征图，而多层感知器接受的输入是一个向量，所以需要将这些特征图中的像素依次取出，排列成一个向量。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072811181585.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n## LeNet5详解\n>1990年，LeCun发表了一篇奠定现在CNN结构的重要文章，他们构建了一个叫做LeNet-5的多层前馈神经网络，并将其用于手写体识别。就像其他前馈神经网络，它也可以使用反向传播算法来训练。它之所以有效，是因为它能从原始图像学习到有效的特征，几乎不用对图像进行预处理。\n\nLeNet5共有7层（不包含输入层）:\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728114959612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n### LeNet5-C1层\nLeNet5的第一层是卷积层\n- 输入图片大小：`32*32`                \n- 卷积窗大小：`5*5`(作者定义)                        \n- 卷积窗种类：`6`(作者定义)\n- 输出特征图数量：`6`  。\n\t由于卷积窗种类是6，故输出的输出特征图数量也是6\n- 输出特征图大小：`(32-5+1)*(32-5+1)=28*28`   \n\t输出特征图大小计算可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中的特征图大小计算方式\n- 可训练参数量：`(5*5+1)*6`或`(5*5)*6+6`\n\t参数量就是卷积核中元素的个数+偏置bias，总共6种卷积核\n- 计算量：`(5*5+1)*6*28*28`\n\t每一个像素的计算量是`(5*5+1)`，总共`6*28*28`个像素，所以总的计算量是`(5*5+1)*6*28*28`\n- 神经元数量：`(28*28)*6)`\n\t参考上面卷积的计算方式和图像就可以看出，计算出的一个结果(像素点)就是一个神经元\n- 连接数：`(5*5+1)*6*28*28`\n\t参考上面卷积的计算方式和图像就可以看出，输入特征图的每个像素是一个神经元，输出特征图的每个像素也是一个神经元，**只要参与了计算，两个神经元之间就算做一个连接**，**因此卷积中连接数与计算数是一样的**\n\t\n\n\n### LeNet5-S2层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)LeNet5的第二层是池化层（池化核大小`2*2`，步长为2）\n- 输入大小：`(28*28)*6 `                 \n- 采样区域(池化核大小)：`2*2` (作者定义)\n- 下采样数量：`6`\n\t这6个下采样的方式其实是一样的\n- 输出特征图大小：`14*14`\n\t输出特征图大小计算可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中的特征图大小计算方式，卷积和池化输出特征图大小的计算方式是通用的\n- 可训练参数量：`(1+1)*6`\n 池化层的可训练参数量不是池化核元素的个数，而是每一个池化都有一个权重w和偏置b，总共6个下采样，故可训练参数是`(1+1)*6`\n- 计算量 ：`(2*2+1)*14*14*6`\n\t对于池化层的计算量，可能不同地方的描述不一样，我的理解是，采样区域是`2*2` ，要找到最大的采样点，需要比较3次，也就是3次计算，才能找到最大值(如果采样区域是`3*3`，那么需要比较8次才能找到最大值)，然后这个最大值再乘以权重w，加上偏置b，因此得到一个像素点的计算量是`3+1+1=(2*2-1+1+1)=(2*2+1)`，总共`14*14*6`个像素点，可得总共的计算量为`(2*2+1)*14*14*6`\n- 神经元数量：`(14*14)*6`\n\t计算出的一个结果(像素点)就是一个神经元\n- 连接数：`(2*2+1)*[(14*14)*6]`\n\t每四个输入的像素点(输入四个神经元)，输出的像素点为1个(输出一个神经元)，再加上一个偏置的点(偏置神经元)，可得每计算一个像素点的连接数是`(2*2+1)`，总共`(14*14)*6`个像素点，故总的连接数为`(2*2+1)*[(14*14)*6]`，可以看到，**池化中计算量和连接数也是一样的**\n\n### LeNet5-C3层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nLeNet5的第3层是卷积层\n- 输入图片大小：`(14*14)*6`                \n- 卷积窗大小：`5*5`(作者定义)                        \n- 卷积窗种类：`16`(作者定义)              \n- 输出特征图数量：`16`  \n- 输出特征图大小：`(14-5+1)*(14-5+1)=10*10`   \n- 可训练参数量：`(6*5*5+1)*16`\n\t每张图片的输入通道是6，我们需要用对应维度的卷积核做卷积运算，也就是要用`6*5*5`的卷积做运算，输出通道是16，因此要有16个这样的卷积核，每个卷积核再加上一个偏置bias，所以参数量是`(6*5*5+1)*16`，可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)\n- 计算量：`(6*5*5+1)*16*10*10`\n\t一个像素点的计算量为`(6*5*5+1)`，总的输出像素个数为`16*10*10`，故总的计算量为`(6*5*5+1)*16*10*10`，这里要注意：**多维卷积计算后需要把每个卷积的计算结果相加，这个相加的结果才是一个像素点，这个计算就忽略不计了**\n- 神经元数量：`10*10*16`\n\t参考上面卷积的计算方式和图像就可以看出，计算出的一个结果(像素点)就是一个神经元\n- 连接数：`(6*5*5+1)*16*10*10`\n\n\t\n\n\n### LeNet5-S4层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\nLeNet5的第四层是池化层（池化核大小`2*2`，步长为2）\n- 输入大小：`(10*10)*16 `                 \n- 采样区域(池化核大小)：`2*2` (作者定义)\n- 下采样数量：`16`\n- 输出特征图大小：`5*5`\n- 可训练参数量：`(1+1)*16`\n- 计算量 ：`(2*2+1)*5*5*16`\n- 神经元数量：`5*5*16`\n- 连接数：`(2*2+1)*5*5*16`\n\t\n\n### LeNet5-C5层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nLeNet5的第5层是卷积层\n- 输入图片大小：`(5*5)*16`                \n- 卷积窗大小：`5*5`(作者定义)                        \n- 卷积窗种类：`120`(作者定义)              \n- 输出特征图数量：`120`  \n- 输出特征图大小：`(5-5+1)*(5-5+1)=1*1`   \n- 可训练参数量：`(16*5*5+1)*120`\n- 计算量：`(16*5*5+1)*1*1*120`\n- 神经元数量：`1*1*120`\n- 连接数：`(16*5*5+1)*1*1*120`\n\n\n### LeNet5-F6层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728154632901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\nLeNet5的第六层是全连接层\n- 输入图片大小：`(1*1)*120`                \n- 卷积窗大小：`1*1`(作者定义)                        \n- 卷积窗种类：`84`(作者定义)              \n- 输出特征图数量：`84`  \n- 输出特征图大小：`1*1`   \n- 可训练参数量：`(120+1)*84`(全连接层)\n\t这里相当于84个线性模型，即MLP多层感知机\n- 计算量：`(120+1)*84`\n- 神经元数量：`84`\n- 连接数：`(120+1)*84`(全连接层)\n\n### LeNet5-OUTPUT层\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072818211542.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\nLeNet5的第7层是输出层\n- 输入图片大小：1*84\n- 输出特征图数量：1*10\n输出1000000000，则表明是数字0的分类\n\n\n### 计算公式\n**卷积层：设卷积核大小为`k*k`，步长为1，输入特征图(输入图片)大小为`n*n`，输入通道是`a`，输出通道是`b`(输出通道就是卷积核的种类数)**\n- **输出**特征图大小：`(n-k+1)*(n-k+1)=m*m`\n- 可训练参数量：`(a*k*k+1)*b`\n- 计算量：`(a*k*k+1)*b*m*m`\n- 神经元数量：`m*m*b`\n- 连接数：`(a*k*k+1)*b*m*m`\n\n卷积步长为n的输出特征图大小计算方式可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)\n\n**池化层：设池化核大小为`k*k`，步长是stride，输入特征图(输入图片)大小为`n*n`，输入通道是`a`，输出通道是`a`(池化层输入通道和输出通道是样的)**\n- 输出特征图大小：$\\frac{ n-k}{stride} + 1=m$\n\t可参考[神经网络之多维卷积的那些事](https://blog.csdn.net/qq_37555071/article/details/107541194)中的特征图大小计算方式\n- 可训练参数量：`(1+1)*a`\n\t池化层可训练的参数量和池化核大小没有关系\n- 计算量：`(k*k+1)*(m*m*a)`\n- 神经元数量：`m*m*a`\n- 连接数：`(k*k+1)*(m*m*a)`\n\n\n\n## LeNet5实战\n### 定义网络模型\n\n```python\nimport torch\nfrom torch import nn,optim\nimport torchvision\nimport torch.nn.functional as F\n#没参数可以用nn，也可以用F，有参数的只能用nn\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #定义卷定义层卷积层,1个输入通道，6个输出通道，5*5的filter,28+2+2=32\n        #左右、上下填充padding\n        #MNIST图像大小28，LeNet大小是32\n        self.conv1 = nn.Conv2d(1,6,5,padding=2)\n        #定义第二层卷积层\n        self.conv2 = nn.Conv2d(6,16,5)\n        \n        #定义3个全连接层\n        self.fc1 = nn.Linear(16*5*5,120)\n        self.fc2 = nn.Linear(120,84)\n        self.fc3 = nn.Linear(84,10)\n        \n    #前向传播\n    def forward(self,x):\n        #先卷积，再调用relue激活函数，然后再最大化池化\n        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n        x=F.max_pool2d(F.relu(self.conv2(x)),(2,2))        \n        #num_flat_features=16*5*5\n        x=x.view(-1,self.num_flat_features(x))\n\n        #第一个全连接\n        x=F.relu(self.fc1(x))\n        x=F.relu(self.fc2(x))\n        x=self.fc3(x)\n        return x\n    def num_flat_features(self,x):\n        size=x.size()[1:]\n        num_features=1\n        for s in size:\n            num_features=num_features*s\n        return num_features \n```\n### 初始化模型参数\n\n```python\nimport torchvision.datasets as datasets\n#import torchvision.transforms as transforms \nfrom torchvision import transforms\n\nfrom torch.utils.data import DataLoader\n\n#超参数定义\n#人为定义的参数是超参数，训练的是参数\nEPOCH = 10               # 训练epoch次数\nBATCH_SIZE = 64     # 批训练的数量\nLR = 0.001                # 学习率\n\n#首次执行download=True，下载数据集\n#mnist存在的路径一定不要出现 -.？等非法字符\ntrain_data=datasets.MNIST(root='./dataset',train=True,transform=transforms.ToTensor(),download=False)\ntest_data=datasets.MNIST(root='./dataset',train=False,transform=transforms.ToTensor(),download=False)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint('训练集大小',train_data.train_data.size())\nprint('训练集标签个数',train_data.train_labels.size())\nplt.imshow(train_data.train_data[0].numpy(),cmap='gray')\nplt.show()\n```\n输出：\n\n\t\t训练集大小 torch.Size([60000, 28, 28])\n\t\t训练集标签个数 torch.Size([60000])\n\n\n​\t\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728182811711.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n​\t\n\n```python\n#如果有dataloader 的话一般都是在dataset里，dataloader和dataset 这俩一般一起用的\n#使用DataLoader进行分批\n#shuffle=True是设置随机数种子\ntrain_loader=DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\ntest_loader=DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=True)\n\n#创建model\nmodel=LeNet5()\n#定义损失函数\ncriterion=nn.CrossEntropyLoss()\n#定义优化器\noptimizer=optim.Adam(model.parameters(),lr=1e-3)\n\n#device  cuda:0是指使用第一个gpu\ndevice=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# device=torch.device('cpu')\nprint(device,type(device))\nmodel.to(device)\n```\n### 训练\n\n```python\n\n# 训练\nfor epoch in range(EPOCH):\n    for i,data in enumerate(train_loader):\n        inputs,labels=data\n        #可能会使用GPU，注意掉就不会在gpu里运行了\n        inputs,labels=inputs.to(device),labels.to(device)\n        # print(type(inputs),inputs.size(),'\\n',inputs)\n        #前向传播\n        outpus=model(inputs)\n        \n        #计算损失函数\n        loss=criterion(outpus,labels)\n        #清空上一轮梯度\n        optimizer.zero_grad()\n        #反向传播\n        loss.backward()\n        #参数更新\n        optimizer.step()\n \n    print('epoch {} loss:{:.4f}'.format(epoch+1,loss.item()))\n```\n输出：\n\n\tepoch 1 loss:0.1051\n\tepoch 2 loss:0.0102\n\tepoch 3 loss:0.1055\n\tepoch 4 loss:0.0070\n\tepoch 5 loss:0.2846\n\tepoch 6 loss:0.1184\n\tepoch 7 loss:0.0104\n\tepoch 8 loss:0.0014\n\tepoch 9 loss:0.0141\n\tepoch 10 loss:0.0126\n\n\n​\t\n### 测试准确率\n\n```python\n#保存训练模型\ntorch.save(model,'dataset/mnist_lenet.pt')\nmodel=torch.load('dataset/mnist_lenet.pt')\n\n#测试\n\"\"\"\n训练完train_datasets之后，model要来测试样本了。\n在model(test_datasets)之前，需要加上model.eval(). \n否则的话，有输入数据，即使不训练，它也会改变权值。\n\"\"\"\nmodel.eval()\ncorrect=0\ntotal=0\n\nfor data in test_loader:\n    images,labels=data\n    images,labels=images.to(device),labels.to(device)\n    #前向传播  model(images)  和 model.forward(x)一样的\n    out=model(images)\n    \"\"\"\n    首先得到每行最大值所在的索引（比图第7个分类是最大值，则索引是7）\n    然后，与真实结果比较（如果一个样本是图片7，那么该labels是7），如果相等，则加和\n    \"\"\"\n    _,predicted=torch.max(out.data,1)\n    total=total+labels.size(0)\n    correct=correct+(predicted==labels).sum().item()\n    \n#输出测试的准确率\nprint('10000张测试图像 准确率：{:.4f}%'.format(100*correct/total))\n```\n输出：\n\n\t\t10000张测试图像 准确率：98.9400%\n### 预测结果\n\n```python\n#记住，输入源一定要转化为torch.FloatTensor，否则无法预测，并且一定要加model.eval()，否则会更新权重\ndef predict_Result(img):\n    \"\"\"\n    预测结果，返回预测的值\n    img，numpy类型，二值图像\n    \"\"\"\n    model.eval()\n    img=torch.from_numpy(img).type(torch.FloatTensor).unsqueeze(0).unsqueeze(1)\n    img=img.to(device)\n    out=model(img)\n    _,predicted=torch.max(out.data,1)\n    return predicted.item()\n    \nimg2=train_data.train_data[167].numpy()\nplt.imshow(img2,cmap='gray')\nprint('预测结果：',predict_Result(img2))\n```\n\n输出：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200728183140187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n","tags":["LeNet5"],"categories":["神经网络"]},{"title":"Python函数参数之*与**用法详解","url":"/2020/11/24/220157/","content":"\n首先，我们来看一个函数定义：\n\n```python\ndef f(a,*b,**c):\n    print(f'a:{a},b:{b},c:{c}')\n```\n`*b与**c`都是可变参数\n- 出现带一个星号的参数 b，**这是可变位置参数**\n- 带两个星号的参数 c，**这是可变关键字参数**\n\n<!-- more -->\n\n\n\n首先，我们来看一个函数定义：\n\n```python\ndef f(a,*b,**c):\n    print(f'a:{a},b:{b},c:{c}')\n```\n`*b与**c`都是可变参数\n- 出现带一个星号的参数 b，**这是可变位置参数**\n- 带两个星号的参数 c，**这是可变关键字参数**\n\n现在执行如下代码：\n\n```python\nIn [2]: f(1,2,3,w=4,h=5)\na:1,b:(2, 3),c:{'w': 4, 'h': 5}\n```\n可以看到，参数 b 被传递 2 个值，参数 c 也被传递 2 个值。**可变位置参数 b 被解析为元组，可变关键字参数 c 被解析为字典**。\n\n下面有几个重要规则：\n\n（1）**可变位置参数不能传入关键字参数**\n\n```python\nIn [41]: def f(*a):\n    ...:   print(a)\nIn [42]: f(1)\n(1,)\nIn [43]: f(1,2,3)\n(1, 2, 3)\nIn [44]: f(a=1)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-44-6a0ab2c303a9> in <module>\n----> 1 f(a=1)\nTypeError: f() got an unexpected keyword argument 'a'\n```\n（2） **可变关键字参数不能传入位置参数**\n\n```python\nIn [45]: def f(**a):\n    ...:   print(a)\nIn [46]: f(a=1)\n{'a': 1}\nIn [47]: f(a=1,b=2,width=3)\n{'a': 1, 'b': 2, 'width': 3}\nIn [48]: f(1)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-48-281ab0a37d7d> in <module>\n----> 1 f(1)\nTypeError: f() takes 0 positional arguments but 1 was given\n```\n","tags":["python"],"categories":["python"]},{"title":"Anaconda与jupyter安装、操作及插件安装","url":"/2020/11/24/215911/","content":"# 一、anaconda简介\n\n1. 进入官网https://www.anaconda.com\n2. 点解download进入下载页面\n3. 按步骤安装(默认，下一步)\n4. 启动jupyterlab应用\n\n![\\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-pELNUSl3-1595855913557)(attachment:launch.jpg)\\]](https://img-blog.csdnimg.cn/20200727212201931.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n<!-- more -->\n\n\n\n# 二、jupyter lab 基本操作\n\n## 1. jupyter lab 简介\n\n- JupyterLab包含了Jupyter Notebook所有功能。\n- JupyterLab作为一种基于web的集成开发环境，你可以使用它编写notebook、操作终端、编辑markdown文本、打开交互模式、查看csv文件及图片等功能\n\n## 2. jupyter lab 基本操作\n\n### 2.1 操作模式\n\n两种操作模式 `command  mode`  和  `edit  mode`\n\n在一个`cell`中按下`enter`就进入edit  mode，按下`Esc`进入command  mode\n\n### 2.2 cell转换类型\n\n- code:代码环境\n- markdown: 带有latex公式输入的增强markdown\n- raw：纯文本\n\n**cell类型转换方式**\n\n- 鼠标操作\n- 快捷键\n\ncommand模式下：m:markdown r:raw text y:code\n\n### 2.3 运行cell\n\n- shift+enter:运行并跳转到下一个cell\n- control+enter：运行并停留在当前cell\n\n### 2.4 增加/删除cell\n\n- 鼠标操作：两种模式均可\n- 快捷键：a/d command模式\n\n\n1. A 在当前cell的上面添加cell\n2. B 在当前cell的下面添加cell\n3. 双击D  快速删除当前cell\n\n### 2.5 代码提示功能\n\n- tab 代码补全或缩进\n- Shift-Tab 提示\n- 更多快捷键参照 [Jupyter Notebook骚操作](https://mp.weixin.qq.com/s?__biz=MzU5MjI3NzIxMw==&mid=2247488276&idx=2&sn=696900f0744474486eece7dee706e31e&chksm=fe2368a6c954e1b01e7e5943b697c1aec840aba995446230d9c6dc385f8471750002539a5921&mpshare=1&scene=24&srcid=&sharer_sharetime=1591664539198&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&ascene=14&devicetype=android-29&version=27000f3d&nettype=WIFI&abtest_cookie=AAACAA%3D%3D&lang=zh_CN&exportkey=AX%2BZM%2BDxUN6bs5k3X3SQbQY%3D&pass_ticket=njbcV9rUgIhNR7K4XdDWqfGsAPAcyPs1nE8Eoab4vyjmTWneMPO%2FDwraZRwPmQxP&wx_header=1)\n\n### 2.6 注释多行代码\n- ctrl / 注释多行代码\n\n### 2.6 运行py文件\n```python\n% run path/filename \n```\n例如：\n```python\n%run C:\\Users\\wang1\\Desktop\\go.py\n```\n\n\n### 2.7 Markdown标题级别\n\n- 命令行模式下，1 2 3 4 5 6分别是1-6级标题\n\n### 2.8 cell加行号\n\n- 命令行模式下，shilt+L为所有cell加上行号，L不持支给单个cell加行号\n- 鼠标点击view显示行号\n\n### 2.9 cell合并分割\n\n- 编辑模式，`crtl shift - 在光标处分割cell`\n- 命令行模式 `shift 鼠标 选中多个cell`  \n`shift m 合并选中给的cell`\n\n# 三、插件安装\n- 更新插件：`jupyter labextensio update 插件名`\n- 更新所有插件：`jupyter labextension update --all`\n- 卸载插件：`jupyter labextensio uninstall 插件名`\n- 安装插件：`jupyter labextensio install 插件名`\n- 远程仓库安装插件：`jupyter labextension install 参考地址`\n- 安装制定版本插件：`jupyter labextensio install 插件名=版本号`\n- 查看已安装插件：`jupyter labextension list`\n\n安装目录插件，可以执行如下命令：\n```powershell\njupyter labextension install @jupyterlab/toc\n或者\njupyter labextension install https://github.com/jupyterlab/jupyterlab-toc.git\n```\n收藏了几个很实用的jupyter插件网站，如下：\n\n## 1.Jupyter Notebook目录插件\n\nhttps://www.jianshu.com/p/f314e9868cae\n\n## 2.Jupyter lab目录插件\n\nhttps://www.pinggu.com/post/details/5eccf61a7fba3d625f75771d\n\n## 3.jupyterlab_code_formatter–代码pep8\n\nhttps://www.brothereye.cn/python/362/\n\n**jupyterlab_code_formatter官网**\n- https://github.com/ryantam626/jupyterlab_code_formatter\n\n## 4.15个好用到爆炸的Jupyter Lab插件\n\nhttps://zhuanlan.zhihu.com/p/101070029\n\n## 5.工具篇-Jupyter Lab效率提升插件\n\nhttps://zhuanlan.zhihu.com/p/73374773\n\n## 6.解决build失败问题\n\n终止JupyterLab后，在命令行下输入`jupyter-lab build`\n\n","tags":["jupyter"],"categories":["工具"]},{"title":"万字详解决策树之ID3、CART、C4.5【原理+实例+代码实现】","url":"/2020/11/24/215647/","content":"\n## 决策树的含义\n首先我们先来了解下什么是决策树。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727111117277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n上图就是一颗带有决策的树，其中天气、温度等称为特征，后面问号的位置称为阈值，如温度=35°，则阈值为35。\n\n\n<!-- more -->\n\n\n\n\n- 决策树是一种基本的分类与回归方法。这里主要讨论决策树用于分类。\n- 决策树的结点和有向边分别表示\n\t1. 内部结点表示一个特征或者属性。\n\t2. 叶子结点表示一个分类。\n\t3. 有向边代表了一个划分规则。\n- 决策树从根结点到子结点的的有向边代表了一条路径。\n- 决策树的路径是互斥并且是完备的。\n- 用决策树分类时，是对样本的某个特征进行测试，根据测试结果将样本分配\n- 如果将样本分配到了树的子结点上，每个子结点对应该特征的一个取值。\n- 决策树的优点：可读性强，分类速度快。\n- 决策树学习通常包括3个步骤：\n\t1. 数据标注\n\t2. 特征选择。\n\t3. 决策树生成。\n\t4. 分类和识别。\n\n决策树模型可以认为是if-then规则的集合。决策树学习的算法通常是遍历选择最优特征和特征值，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类，这一过程对应着特征空间的划分，也对应着决策树的构建。\n\n## (信息熵+信息增益)&ID3\n\n**信息熵(Entropy)用来度量不确定性的，当熵越大，信息的不确定性越大，对于机器学习中的分类问题，那么，当前类别的熵越大，它的不确定性就越大**\n\nH(D)代表一个决策树的熵，熵的数学公式如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727112506781.png)\n*n是分类的数目，$p_i$是当前分类发生的概率。*\n\n细想一下，如果10个硬币，分类结果是10个正面，没有反面，那么信息熵为0，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727144929893.png)\n信息熵为0，就意味着信息确定，就意味着分类完成了。\n\n\n\n在原有树的熵 H(D) 增加了一个分裂节点，使得熵变成了H(D|A)，则**信息增益(Information Gain，IG)**为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727114424888.png)\n*A是选择作为分裂依据的特征，g(D,A)也称为条件熵*\n\n**即信息增益 = 分裂前的信息熵-分裂后的信息熵**\n\n也有更加准确的定义方法：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727154914395.png)\n*V表示根据特征a对样本集D划分(分裂)后，获得的总共类别数量(一般是二分类)；$D^v$表示每一个新类别中样本数量；Ent(D)和H(D)的含义相同，表示信息熵*\n\n\n我们以判断学生好坏的案例，计算信息熵和信息增益：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725210419125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n（1）在初始状态下，有10个学生，7个是好学生，3个不是好学生，计算树的信息熵(此时只有一个根节点) `H(D)=0.88`，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725202826602.png)\n\n（2）我们根据分数这个特征对树进行分裂，设置分数的阈值为70，计算分裂后树的信息熵`H(D|A1)=0.4344`，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727140142378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n（3）把(2)的特征有分数改为出勤率，设置出勤率阈值为75%，计算分裂后树的信息熵`H(D|A2)=0.79`，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727140432832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n从上面的计算中，我们得到了3个熵：\n- 在初始状态下，信息熵为0.88，\n- 设置分数阈值为70进行分裂，信息熵为0.4344\n- 设置出勤率阈值为75%进行分裂，信息熵为0.79\n\n只要增加分裂节点后的熵比之前的熵小，那么就可以认为本次分裂是有效的，现在(2)和(3)的分裂都有效，但是哪个更好呢？\n在设置分数阈值为70的熵比设置出勤率阈值为75%的熵要小，即前者分裂后数据比较纯一些，整个数据的确定性大了，因此设置分数阈值为70分裂方式效果更好，所以我们以该方式为基准进行分裂。\n\n\n现在来计算信息增益，**信息增益 = 分裂前的信息熵-分裂后的信息熵**，计算如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727141600553.png)\n*A1是以分数这个特征进行分裂，A2是以出勤率这个特征进行分裂*\n\n可以看到，以设置分数阈值为70进行分裂，信息增益更大。\n\n> 信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好，该特征具有更强的分类能力，如果一个特征的信息增益为0，则表示该特征没有什么分类能力。\n\n\n\n**ID3的原理：**\n\n**利用数据标注和信息增益以及遍历，可以完成一个决策树中的特征和阈值的选择(得到最大信息增益的特征和阈值)，利用这三个(标注、信息增益、遍历)就可以完成一颗树，决策的树，分类的树，这个算法就是ID3()算法的思想**\n\n**ID3(Iterative Dichotomisor 3) 迭代二分类三代，用信息增益准则选择特征，判定分类器的性能，从而构建决策树**\n\n经过上面的例子，可以得到如下结论：\n- 只要分类后的总熵为0，那么这课树就训练完了（也就是分类完了）\n- 熵在决策树中的作用：判断分类后，有没有达到我们的需求和目的，也就是数据是不是更加纯了，更加确定了。如果经过分类后，信息总熵的值更加小了，那么这次分类就是有效的。\n- 熵越大，不确定就越大，分类未完成时，熵不为0\n- 熵越小，分得越好，分类完成时，熵为0\n\n\n根据上述思想，可以完成如下一颗树：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725212940738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n\n## (纯度+基尼系数)&CART\n在上面的例子中，我们是用决策树做分类的，那么做回归的时候该怎么做呢？ 现在把上述的分类结果由好学生换成是否为好学生的概率，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725214016787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n分类的结果是概率，是个连续的变量，无法计算信息熵和信息增益了，这个时候怎么做呢？\n\n为了解决这个问题，引入了纯度的概念。\n\n**纯度的定义如下：**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727145525183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*𝑃_𝑙 和𝑃_r  是按照某一特征分裂后，树的两个分裂结点各自占的比例，Var是方差，就是计算左侧和右侧部分的方差，y_i是具体的值(这里是好学生的概率)*\n\n**纯度增益 = 分裂前的纯度 - 分裂后的纯度**\n\n可以看到，纯度和方差的定义大致一样(这里的纯度没有除以n，方差的定义需要除以n)，在有的文献中也称为偏差，其实含义都一样，都是**表示数据的离散程度**\n\n细想一下，如果上述10个学生的是好学生的概率都一样(比如都是0.9)，那么纯度(方差)是0，说明数据完全没有离散性，非常的确定。\n> 可以看到，纯度和信息熵所代表的含义是一样的，只不过熵表示分类信息的不确定性，纯度表示数据的离散程度，下面即将要介绍的基尼系数，表示的也是分类(CART的分类)信息的确定性\n\n（1）在初始状态下，计算树的纯度(此时只有一个根节点) `纯度=0.4076`，如下所示：\n\n```python\nimport numpy as np\na = [0.9,0.9,0.8,0.5,0.3,0.8,0.85,0.74,0.92,0.99]\na = np.array(a)\na_mean = np.mean(a)\n# 不用除以n\na_va = np.sum(np.square(a-a_mean))\na_va\n# 输出 0.4076000000000001\n```\n\n(2) 设置分数阈值为70进行分裂，计算`纯度=0.2703`，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725215757527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n(3) 设置出勤率阈值为75%进行分类，计算纯度：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072522060291.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n从上面的计算中，我们得到了3个纯度：\n- 在初始状态下，纯度为0.4076，\n- 设置分数阈值为70进行分裂，纯度为0.2703\n- 设置出勤率阈值为75%进行分裂，纯度0.1048\n\n从这三个纯度可以得知，这两种方式的分裂均有效，按照出勤率阈值=75%计算的纯度更小，说明分裂的效果更好，信息更确定了。\n\n也可以计算出纯度的增益：\n- 设置分数阈值为70进行分裂，纯度的增益为`0.4079 - 0.2703 = 0.1376`\n- 设置出勤率阈值为75%进行分裂，纯度的增益为`0.4079 - 0.1048 =0.3031`\n\n设置出勤率阈值为75%进行分裂得到纯度增益更大，说分裂效果更好(这里和信息熵及信息增益的原理是一样的)\n\n\n**思考：为什么决策树分类和回归的结果不同？**\n因为我们的标注不同，之前标注的是好学生与坏学生，现在重新标注了是好学生的概率，标注不同对我们模型训练的引导就不同，造成的结果就不同。\n\n\n**思考：将模型训练完之后怎么去得到具体回归的那个值呢？**\n如果最后只有一个结点，就是指这个结点的值，如果最后有多个结点，那应该是平均值\n\n**利用数据标注和纯度以及遍历，可以完成一个决策树中的特征和阈值的选择(得到最大信息增益的特征和阈值)，利用这三个(标注、纯度、遍历)就可以完成一颗树，决策的树，回归的树，这个算法就是CART的回归算法**\n\nCART做回归时用的是纯度，做分类时用的是基尼系数。\n\n\n**基尼系数**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727152553331.png)\n\n*n代表n分类，$p_i$表示不同类别的概率*\n\n按照某一特征分裂后的基尼系数为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727152816890.png)\n\n**基尼增益 = 分裂前的基尼 - 分裂后的基尼**\n\n我们以硬币来举例子解释基尼系数，假设10个硬币做二分类，10个是正面，没有反面，那么基尼系数为：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725231308198.png)\n如果10个硬币做二分类，5个是正面，5个是反面，那么基尼系数为\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725231336871.png)\n\n可以看到，当分类完成时，基尼系数为0，因此基尼系数与信息的含义类似，基尼系数越大，信息的不确定性就越大。\n\n**注意：基尼系数和信息熵都是用于分类的**\n\n然后，按照某一特征进行分裂，比如按照硬币的大小(或面值)进行分裂，计算分裂后的基尼系数，最后计算基尼增益，得到的基尼增益最大的特征和阈值就是我们要找的分裂方式。\n> 纯度增益、基尼增益、信息增益的原理完全一样\n\n\n**CART（classification and regression tree）分类和回归树，分类是使用基尼系数判定分类器的性能，回归时使用纯度判定分类器的性能**\n\n##  信息增益率&C4.5\n**信息增益准则对可取值数目较多的特征有所偏好**，为了减少这种偏好可能带来的不利影响，C4.5决策树算法使用了“增益率”：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072715533054.png)\n其中IV(a)称为特征a的“固有值”，称为特征a的分裂信息度量，其实就是特征a的信息熵(注意：这个和信息增益减去的按特征a分裂后的信息熵不一样，在下面具体例子中可看到)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727155553702.png)\n**需要注意的是，信息增益率对可取值数目较少的特征所有偏好，因此，C4.5算法并不是直接选择信息增益率最大的特征进行分裂，而是使用了一个启发式：先找出信息增益高于平均水平的特征，再从中选择增益率最高的。** 可以看出，使用信息增益率可以解决分裂后叶子结点过多的问题，从而解决过拟合\n\n我们用下面的例子计算信息增益率(抱歉，没找到更清晰的图片)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727160759867.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n（1）第一步：计算样本集D的信息熵\n\nEnt(D) = `-9/14*log2(9/14) – 5/14*log2(5/14) = 0.940`\n*Ent(D)表示熵，也可以H(D)表示*\n\n（2）第二步：依据每个特征划分样本集D，并计算每个特征（划分样本集D后）的信息熵\n\n- Ent(天气) = `5/14*[-2/5*log2(2/5)-3/5*log2(3/5)] + 4/14*[-4/4*log2(4/4)] + 5/14*[-3/5log2(3/5) – 2/5*log2(2/5)] = 0.694`\n- Ent(温度) = 0.911\n- Ent(湿度) = 0.789\n- Ent(风速) = 0.892\n\n（3）第三步：计算信息增益\n\n- Gain(天气) = Ent(D) - Ent(天气) = 0.246\n- Gain(温度) = Ent(D) - Ent(温度) = 0.029\n- Gain(湿度) = Ent(D) - Ent(湿度) = 0.150\n- Gain(风速) = Ent(D) - Ent(风速) = 0.048\n\n（4）第四步：计算特征(属性)分裂信息度量\n\n- IV(天气) = `-5/14*log2(5/14) – 4/14*log2(4/14) – 5/14*log2(5/14) = 1.577`\n- IV(温度) = 1.556\n- IV(湿度) = 1.000\n- IV(风速) = 0.985\n\n（5）第五步：计算信息增益率\n\n- Gain_ratio(天气) = 0.246 / 1.577 = 0.155\n- Gain_ratio(温度) = 0.0187\n- Gain_ratio(湿度) = 0.151\n- Gain_ratio(风速) = 0.048\n\n可以看到，天气的信息增益率最高，选择天气为分裂属性。发现分裂了之后，天气是“阴”的条件下，类别是”纯“的，所以把它定义为叶子节点，选择不“纯”的结点继续分裂。\n\n我们在ID3和CART中的例子特征值是离散的，即特征值是一个个数值，不是本例中的类别，如果特征是类别的样本，就没有阈值的选择，直接按该特征的类别进行分裂，比如本例中按天气把决策树分裂成晴、阴、雨三个结点。\n\n思考：为什么说**信息增益准则对可取值数目较多的特征有所偏好**？\n\n就如本例中的天气特征，它的可取值数据为3个：晴、阴、雨，如果天气特征的可取值数目增加到5个，那么按照天气分裂后的信息熵就会降低的更多，它的信息增益就越大，因为天气特征的可取值数目越多，分裂的就清晰。如果把天气特征的可取值数目增加到与样本数一样，那么分裂后的信息熵就是0了，所以说信息增益准则对可取值数目较多的特征有所偏好。\n\n思考：为什么说**信息增益率对可取值数目较少的特征所有偏好**？\n\n还拿本例中的天气特征来说，它的可取值数据为3个：晴、阴、雨，如果天气特征的可取值数目减少到一个，那么天气特征(属性)分裂信息度量就为0了，此时，信息增益率就是无穷大，因此信息增益率对可取值数目较少的特征所有偏好。\n\n## 各种决策树比较与总结\n\n**ID3、CART、C4.5比较**\n信息增益和信息增益率通常用于离散型的特征划分，ID3和C4.5通常情况下都是多叉树，也就是根据离散特征的取值会将数据分到多个子树中，当然采用信息增益和信息增益率的时候也可以对连续特征进行划分并计算最优点，如上面判断好坏学生的例子。CART树为二叉树，使用基尼指数作为划分准则，对于离散型特征和连续行特征都能很好的处理。\n**离散型的特征是指：特征是分类，如天气：晴、阴、雨，不是连续的值\n连续型的特征是指：特征是一个个值，如分数、身高等，不是分类**\n\n\n**决策树的参数和训练方法：**\n\n- 决策树的参数：选择的特征及其对应的阈值，还有它的拓扑结构\n- 训练方法：就是遍历的方法，用纯度、基尼系数、信息熵、信息增益或信息增益率来表示\n- 决策树既可以做分类，也可以做回归，既可以做二分类，也可以做多分类\n\n**决策树中的分类器**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725221843381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n**如何求解决策树？**\n\n求解决策树，实际上就是求解分类器(求解每个特征和阈值)，步骤如下：\n1. 设定一个评价分类结果的好坏的准则(信息增益、基尼系数、纯度、信息增益率)\n2. 用遍历的方法求解\n\n\n## 决策树编程实战\n\n### 调用sklearn库实现回归决策树\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import linear_model\n \n# Data set\nx = np.array(list(range(1, 11))).reshape(-1, 1)\ny = np.array([5.56, 5.70, 5.91, 6.40, 6.80, 7.05, 8.90, 8.70, 9.00, 9.05]).ravel()\n \n# Fit regression model\nmodel1 = DecisionTreeRegressor(max_depth=1)\nmodel2 = DecisionTreeRegressor(max_depth=3)\nmodel3 = linear_model.LinearRegression()\nmodel1.fit(x, y)\nmodel2.fit(x, y)\nmodel3.fit(x, y)\n \n# Predict\nX_test = np.arange(0.0, 10.0, 0.01)[:, np.newaxis]\ny_1 = model1.predict(X_test)\ny_2 = model2.predict(X_test)\ny_3 = model3.predict(X_test)\n \n# Plot the results\nplt.figure()\nplt.scatter(x, y, s=20, edgecolor=\"black\",\n            c=\"darkorange\", label=\"data\")\nplt.plot(X_test, y_1, color=\"cornflowerblue\",\n         label=\"max_depth=1\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=3\", linewidth=2)\nplt.plot(X_test, y_3, color='red', label='liner regression', linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n```\n输出\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727174125995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n### 手动实现ID3决策树\n\n```python\n#coding:utf-8\nimport torch\nimport pdb\n\n# 样本的特征，每个样本包含七个特征\nfeature_space=[[1., 3., 2., 2., 3., 0.,3.], \n[2., 0., 2., 5., 1., 2.,3.], \n[3., 2., 3., 3., 2., 3.,2.], \n[4., 0., 3., 3., 2., 0.,1.], \n[3., 1., 2., 2., 5., 1.,3.], \n[1., 4., 3., 3., 1., 5.,2.], \n[3., 3., 3., 3., 1., 0.,1.], \n[5., 1., 1., 4., 2., 2.,2.], \n[6., 2., 3., 3., 2., 3.,0.], \n[2., 2., 2., 2., 5., 1.,4.]]\n\ndef get_label(idx):\n    label= idx\n    return label\n\n#  计算以某个特征某个阈值进行分裂时的信息熵，即条件熵\ndef cut_by_node(d,value,feature_space,list_need_cut):\n    # 分别存放以某个特征某个阈值分裂后的左右侧样本点\n    right_list=[]\n    left_list=[]\n    for i in list_need_cut:\n\n        if feature_space[i][d]<=value:\n             right_list.append(i)\n        else:\n             left_list.append(i)\n\n    left_list_t = list2label(left_list,[0,0,0,0,0,0,0,0,0,0])\n    right_list_t = list2label(right_list,[0,0,0,0,0,0,0,0,0,0])\n    e1=get_emtropy(left_list_t) \n    e2=get_emtropy(right_list_t) \n    n1 = float(len(left_list))\n    n2 = float(len(right_list))\n    e = e1*n1/(n2+n1) + e2*n2/(n1+n2)\n\n    return e,right_list,left_list\n\n# 将分到样本点转为one-hot各式，方便计算信息熵\ndef list2label(list_need_label,list_label):\n     for i in list_need_label:\n         label=get_label(i)\n         list_label[label]+=1\n     return list_label\n\ndef get_emtropy(class_list):\n   E = 0\n   sumv = float(sum(class_list))\n   if sumv == 0:\n       sumv =0.000000000001\n   for cl in class_list:\n       if cl==0:\n           cl=0.00000000001\n       p = torch.tensor(float(cl/sumv))\n       # log以2为底\n       E += -1.0 * p*torch.log(p)/torch.log(torch.tensor(2.))\n   return E.item()\n```\n\n```python\ndef get_node(complate,d,list_need_cut):\n    # 初始时的信息熵，设为最大(根节点计算之前的信息熵)\n    e = 10000000\n    # node 代表以那个维度的那个特征值进行分裂\n    node=[]\n    # list_select:树分裂好的一个个结点序列，不包含已经分类好可以识别的结点\n    list_select=[]\n    # 存放可以识别的结点\n    complate_select=[]\n    # 0~8就是选择的阈值\n    for value in range(0,8):\n        complate_tmp=[]\n        etmp=0\n        list_select_tmp=[]\n        # 子序列总的长度\n        sumv=0.000000001\n        # 要进行分裂的所有结点序列\n        for lnc in list_need_cut:\n\n            # 计算条件熵，返回熵、左侧结点的样本点，右侧结点的样本点\n            etmptmp,r_list,l_list=cut_by_node(d,value,feature_space,lnc)\n            # 这行代码和etmp/sumv 就是求分裂后的总熵，就是那个pl*H1+pr*H2的公式\n            etmp+=etmptmp*len(lnc)\n            sumv+=float(len(lnc))\n            # 存放可以识别的结点\n            if len(r_list)>1:\n                list_select_tmp.append(r_list)\n            if len(l_list)>1:\n                list_select_tmp.append(l_list)\n            if len(r_list)==1:\n                complate_tmp.append(r_list)\n            if len(l_list)==1:\n                complate_tmp.append(l_list)\n            # print('子序列child:以每个样本的第{}个特征，特征值大小为{}划分{}子序列，得到的熵为{}'.format(d,value,lnc,etmptmp))    \n            \n        etmp = etmp/sumv\n        sumv=0\n        \n        # print('总序列All：以每个样本的第{}个特征，特征值大小为{}划分{}序列，得到的总熵为{}'.format(d,value,list_need_cut,etmp))    \n        \n        # 得到第n个特征以某一阈值分裂后的最小信息熵，及此时分裂后序列\n        if etmp<e:\n            e=etmp\n            node=[d,value]\n            list_select=list_select_tmp\n            complate_select=complate_tmp\n    for ll in complate_select:\n         complate.append(ll)\n    return node,list_select,complate\n```\n\n```python\nimport pdb\ndef get_tree():\n#     pdb.set_trace()\n    # 10个样本：0, 1, 2, 3, 4, 5, 6, 7, 8, 9，注意是这里是二维数组\n    ## 二维数组里刚开始只有一个序列，即初始时只有一个根节点\n    all_list=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] \n    # complate的含义是：比如树的某个结点只有一个样本，那么存放到complate中\n    ## 即complate存放的是已经分类好的可以识别的结点\n    complate=[]\n    # 遍历样本的7个特征，这个序列就是遍历的先后顺序\n    ## d 首先遍历每个样本的第3个特征，把分类的结果返回\n    for d in [3,4,2,5,0,1,6]:\n        # node 代表以那个维度的那个特征值进行分裂\n        # all_list:树分裂好的一个个结点序列，不包含已经分类好可以识别的结点\n        node,all_list,complate=get_node(complate,d,all_list)\n        print(\"node=%s,complate=%s,all_list=%s\"%(node,complate,all_list))\n\nif __name__==\"__main__\":\n    get_tree()\n```\n输出：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727185457146.png)\n\n### 使用sklearn库回归决策树预测boston房价\n[decision_sklearn_tree_regressor_boston](https://gitee.com/wxler/AIProjectTraining/blob/master/practice/%E5%86%B3%E7%AD%96%E6%A0%91/decision_sklearn_tree_regressor_boston.ipynb)\n### 使用sklearn库分类决策树对iris分类\n[decision_sklearn_tree_classify_iris](https://gitee.com/wxler/AIProjectTraining/blob/master/practice/%E5%86%B3%E7%AD%96%E6%A0%91/decision_sklearn_tree_classify_iris.ipynb)\n\n参考文档\n[信息增益、信息增益比、基尼指数的比较](https://www.cnblogs.com/rezero/p/13057584.html)\n[华校专的决策树](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/4_decision_tree.html)\n[机器学习（二）-信息熵，条件熵，信息增益，信息增益比，基尼系数](https://www.cnblogs.com/xiaofeiIDO/p/11947380.html)\n","tags":["决策树"],"categories":["机器学习"]},{"title":"k-means及k-means++原理【python代码实现】","url":"/2020/11/24/215155/","content":"\n\n\n## 前言\n\nk-means算法是无监督的聚类算法，实现起来较为简单，k-means++可以理解为k-means的增强版，在初始化中心点的方式上比k-means更友好。\n\n<!-- more -->\n\n\n\n## k-means原理\nk-means的实现步骤如下：\n1. 从样本中随机选取k个点作为聚类中心点\n2. 对于任意一个样本点，求其到k个聚类中心的距离，然后，将样本点归类到距离最小的聚类中心，直到归类完所有的样本点（聚成k类）\n3. 对每个聚类求平均值，然后将k个均值分别作为各自聚类新的中心点\n4. 重复2、3步，直到中心点位置不在变化或者中心点的位置变化小于阈值\n\n优点：\n- 原理简单，实现起来比较容易\n- 收敛速度较快，聚类效果较优\n\n缺点：\n- 初始中心点的选取具有随机性，可能会选取到不好的初始值。\n\n## k-means++原理\n\n**k-means++是k-means的增强版，它初始选取的聚类中心点尽可能的分散开来，这样可以有效减少迭代次数，加快运算速度**，实现步骤如下：\n1. 从样本中随机选取一个点作为聚类中心\n2. 计算每一个样本点到已选择的聚类中心的距离，用D(X)表示：D(X)越大，其被选取下一个聚类中心的概率就越大\n3. 利用**轮盘法**的方式选出下一个聚类中心(D(X)越大，被选取聚类中心的概率就越大)\n4. 重复步骤2，直到选出k个聚类中心\n5. 选出k个聚类中心后，使用标准的k-means算法聚类\n\n>这里不得不说明一点，有的文献中把**与已选择的聚类中心最大距离的点选作下一个中心点**，这个说法是不太准确的，准的说是**与已选择的聚类中心最大距离的点被选作下一个中心点的概率最大**，但不一定就是改点，因为总是取最大也不太好（遇到特殊数据，比如有一个点离某个聚类所有点都很远）。\n\n一般初始化部分，始终要给些随机。因为数据是随机的。\n\n尽管计算初始点时花费了额外的时间，但是在迭代过程中，k-mean 本身能快速收敛，因此算法实际上降低了计算时间。\n\n现在重点是利用**轮盘法**的方式选出下一个聚类中心，我们以一个例子说明K-means++是如何选取初始聚类中心的。\n\n假如数据集中有8个样本，分布分布以及对应序号如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200726212919102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n我们先用 k-means++的步骤1选择6号点作为第一个聚类中心，然后进行第二步，计算每个样本点到已选择的聚类中心的距离D(X)，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200726213127851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n- D(X)是每个样本点与所选取的聚类中心的距离(即第一个聚类中心)\n- P(X)每个样本被选为下一个聚类中心的概率\n- Sum是概率P(x)的累加和，用于轮盘法选择出第二个聚类中心。\n\n然后执行 k-means++的第三步：利用**轮盘法**的方式选出下一个聚类中心，**方法是随机产生出一个0~1之间的随机数，判断它属于哪个区间，那么该区间对应的序号就是被选择出来的第二个聚类中心了**。\n\n在上图1号点区间为[0,0.2)，2号点的区间为[0.2, 0.525)，4号点的区间为[0.65,0.9)\n\n从上表可以直观的看到，1号，2号，3号，4号总的概率之和为0.9，这4个点正好是离第一个初始聚类中心(即6号点)较远的四个点，因此选取的第二个聚类中心大概率会落在这4个点中的一个，其中2号点被选作为下一个聚类中心的概率最大。\n\n## k-means及k-means++代码实现\n\n这里选择的中心点是样本的特征(不是索引)，这样做是为了方便计算，选择的聚类点(中心点周围的点)是样本的索引。\n\n**k-means实现**\n\n```python\n# 定义欧式距离\nimport numpy as np\ndef get_distance(x1, x2):\n    return np.sqrt(np.sum(np.square(x1-x2)))\n```\n\n```python\nimport random\n# 定义中心初始化函数，中心点选择的是样本特征\ndef center_init(k, X):\n    n_samples, n_features = X.shape\n    centers = np.zeros((k, n_features))\n    selected_centers_index = []\n    for i in range(k):\n        # 每一次循环随机选择一个类别中心,判断不让centers重复\n        sel_index = random.choice(list(set(range(n_samples))-set(selected_centers_index)))\n        centers[i] = X[sel_index]\n        selected_centers_index.append(sel_index)\n    return centers\n```\n\n```python\n# 判断一个样本点离哪个中心点近， 返回的是该中心点的索引\n## 比如有三个中心点，返回的是0，1，2\ndef closest_center(sample, centers):\n    closest_i = 0\n    closest_dist = float('inf')\n    for i, c in enumerate(centers):\n        # 根据欧式距离判断，选择最小距离的中心点所属类别\n        distance = get_distance(sample, c)\n        if distance < closest_dist:\n            closest_i = i\n            closest_dist = distance\n    return closest_i\n```\n\n```python\n# 定义构建聚类的过程\n# 每一个聚类存的内容是样本的索引，即对样本索引进行聚类，方便操作\ndef create_clusters(centers, k, X):\n    clusters = [[] for _ in range(k)]\n    for sample_i, sample in enumerate(X):\n        # 将样本划分到最近的类别区域\n        center_i = closest_center(sample, centers)\n        # 存放样本的索引\n        clusters[center_i].append(sample_i)\n    return clusters\n```\n\n```python\n# 根据上一步聚类结果计算新的中心点\ndef calculate_new_centers(clusters, k, X):\n    n_samples, n_features = X.shape\n    centers = np.zeros((k, n_features))\n    # 以当前每个类样本的均值为新的中心点\n    for i, cluster in enumerate(clusters):  # cluster为分类后每一类的索引\n        new_center = np.mean(X[cluster], axis=0) # 按列求平均值\n        centers[i] = new_center\n    return centers\n```\n\n```python\n# 获取每个样本所属的聚类类别\ndef get_cluster_labels(clusters, X):\n    y_pred = np.zeros(np.shape(X)[0])\n    for cluster_i, cluster in enumerate(clusters):\n        for sample_i in cluster:\n            y_pred[sample_i] = cluster_i\n            #print('把样本{}归到{}类'.format(sample_i,cluster_i))\n    return y_pred\n```\n\n```python\n# 根据上述各流程定义kmeans算法流程\ndef Mykmeans(X, k, max_iterations,init):\n    # 1.初始化中心点\n    if init == 'kmeans':\n        centers = center_init(k, X)\n    else: centers = get_kmeansplus_centers(k, X)\n    # 遍历迭代求解\n    for _ in range(max_iterations):\n        # 2.根据当前中心点进行聚类\n        clusters = create_clusters(centers, k, X)\n        # 保存当前中心点\n        pre_centers = centers\n        # 3.根据聚类结果计算新的中心点\n        new_centers = calculate_new_centers(clusters, k, X)\n        # 4.设定收敛条件为中心点是否发生变化\n        diff = new_centers - pre_centers\n        # 说明中心点没有变化，停止更新\n        if diff.sum() == 0:\n            break\n    # 返回最终的聚类标签\n    return get_cluster_labels(clusters, X)\n```\n\n```python\n# 测试执行\nX = np.array([[0,2],[0,0],[1,0],[5,0],[5,2]])\n# 设定聚类类别为2个，最大迭代次数为10次\nlabels = Mykmeans(X, k = 2, max_iterations = 10,init = 'kmeans')\n# 打印每个样本所属的类别标签\nprint(\"最后分类结果\",labels)\n## 输出为  [1. 1. 1. 0. 0.]\n```\n\n```python\n# 使用sklearn验证\nfrom sklearn.cluster import KMeans\nX = np.array([[0,2],[0,0],[1,0],[5,0],[5,2]])\nkmeans = KMeans(n_clusters=2,init = 'random').fit(X)\n# 由于center的随机性，结果可能不一样\nprint(kmeans.labels_)\n```\n**k-means++实现**\n\n```python\n## 得到kmean++中心点\ndef get_kmeansplus_centers(k, X):\n    n_samples, n_features = X.shape\n    init_one_center_i = np.random.choice(range(n_samples))\n    centers = []\n    centers.append(X[init_one_center_i])\n    dists = [ 0 for _ in range(n_samples)]\n\n    # 执行\n    for _ in range(k-1):\n        total = 0\n        for sample_i,sample in enumerate(X):\n            # 得到最短距离\n            closet_i = closest_center(sample,centers)\n            d = get_distance(X[closet_i],sample)\n            dists[sample_i] = d\n            total += d\n        total = total * np.random.random()\n\n        for sample_i,d in enumerate(dists): # 轮盘法选出下一个聚类中心\n            total -= d\n            if total > 0:\n                continue\n            # 选取新的中心点\n            centers.append(X[sample_i])\n            break\n    return centers\n```\n\n```python\nX = np.array([[0,2],[0,0],[1,0],[5,0],[5,2]])\n# 设定聚类类别为2个，最大迭代次数为10次\nlabels = Mykmeans(X, k = 2, max_iterations = 10,init = 'kmeans++')\nprint(\"最后分类结果\",labels)\n## 输出为  [1. 1. 1. 0. 0.]\n```\n\n```python\n# 使用sklearn验证\nX = np.array([[0,2],[0,0],[1,0],[5,0],[5,2]])\nkmeans = KMeans(n_clusters=2,init='k-means++').fit(X)\nprint(kmeans.labels_)\n```\n\n参考文档\n[K-means与K-means++](https://www.cnblogs.com/wang2825/articles/8696830.html)\n[K-means原理、优化及应用](https://blog.csdn.net/weixin_42029738/article/details/81978038)","tags":["k-means"],"categories":["机器学习"]},{"title":"在jupyter中使用python pdb调试代码","url":"/2020/11/24/215133/","content":"\n> 目前在jupyter中还没有可视化调试界面，而python pdb是代码调试的一个不错的选择，它支持设置断点和单步调试，使用起来非常方便\n\n<!-- more -->\n\n\n\n\n## pdb常用命令\n\n| 参数 | 说明      |  实例\n|:--------:| :-----------|:-------------|\n|`h`  | help 帮助文档 | `h b`： 查看 b 命令的文档|\n|`b` | break 打断点|`b`：查看所有断点 <br> `b 5`： 给第5行打断点 <br>    `b function_name`：当前文件名为 function_name 的函数打断点<br> `b test1.A.add`：在 import test1 文件的 A 类的 add 方法打断点   <br> `b A.add`：在 A 类的 add 方法打断点 |\n|`tbreak` | 设置临时断点，运行完毕后会删除这个断点| 设置方法和 b 一样|\n|`w `| where 查看当前执行的位置| `w`|\n| `cl`| clear 清除断点| `cl`：清除所有断点  <br> `cl 2`：清除断点列表中编号为2断点 <br> `cl test.py:18`：清除 test.py 文件编号为18断点 <br>`cl test1:18`：清除 import test1 文件编号为18的断点 |\n|`condition ` | 给断点设置条件| `condition 1 i==4`：当断点列表中编号为1的断点中变量 i 等于 4 的时候执行断点|\n|`s` | step 执行下一条命令，遇到函数则进入\t| 参考下面执行效果|\n|`n` |next 执行下一条语句，遇到函数不进入 |参考下面执行效果 |\n| `c`|continue  继续执行，直到遇到下一条断点\t |参考下面执行效果 |\n|`r` | return  执行当前运行函数到结束| 参考下面执行效果  |\n|`args` | args  打印当前函数的所有参数及参数值 | 参考下面执行效果 |\n| `p`| print 打印出当前所在函数中的变量或表达式结果 | `p a`：打印变量a <br> `p dir(a)`：打印变量a所有属性|\n|`pp`| 格式化打印出来的结果| `pp a`：格式化打印变量a|\n`run`| 重新执行| |\n|`q` | quit 退出pdb调试| |\n\n## pdb进阶阶命令\n| 参数 | 说明      |  实例\n|:--------:| :-----------|:-------------|\n| `l`| list 列出当前或范围周围代码  |`l 5, 20`： 列出5到20行代码 <br>`l`： 查看当前位置的代码 |\n|`disable ` |  停用断点|  `disable`：清除所有断点  <br> `disable 2`：清除断点列表中编号为2断点 <br> `disable test.py:18`：清除 test.py 文件编号为18断点 <br>`disable test1:18`：清除 import test1 文件编号为18的断点|\n|`enable`  |启动断点\t | 用法和`disable`一样|\n| `ignore bpnumber`| 忽略某个断点几次| `ignore 1 3`：忽略断点列表中第1个断点3次，一般循环中用， |\n| `commands`| 给断点写一个脚本执行 | `commands 1`：给断点编号为1的的断点写脚本 |\n| `unt`| until 执行到下一行 | 参考下面 unt 执行效果 |\n| `j`| jump 跳转至指定程序行，如果是前行，则忽略中间行代码。<br>如果是后退，状态重设为回退行状态 | 注意：是跳转到不是执行 |\n|`alias` | 自定义一个函数，参数可由％1，％2来表示。<br>类似 Python 的 lambda| |\n|`unalias` | 删掉别名函数| `unalias name`|\n\n\n\n\n## 实例1\n代码如下：\n\n```python\nimport pdb\npdb.set_trace()\ndef mul(a, b,e = 88):\n    c = a * b\n    return c\n\nfor i in range(10):\n    a = i\n    b = i + 1\n    r = mul(a, b)\n    print(r)\n```\n\n\n使用`condition`给编号为6的断点设置条件为`i==3`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725175733203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n给第8行设置断点，然后输入n单步执行(遇到函数不进入)：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072518002253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n给第8行设置断点，但函数位置输入s遇到函数进入，然后输入r，直接执行到函数尾部：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725180402190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n输入`args`打印当前函数的所有参数及参数值，注意：只有在函数内部该命令才有效\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725181732246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n使用`commands 22`为编号为22的断点编写脚本\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725183606110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n使用`unt`命令执行到下一行\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725183902126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n\n## 示例2\n代码如下：\n\n```python\nimport pdb\npdb.set_trace()\nclass A():\n    def __init__(self,value):\n        self.value = value\n    def printParam(self):\n        print(self.value)\nv = 3\na = A(v)\na.printParam()\n```\n\n\n输入`p dir(a)` 打印a的所有属性：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725181606662.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n输入`l`列出当前位置的代码\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200725182355224.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n参考文档\n[python pdb 代码调试 - 最全最详细的使用说明](https://www.jianshu.com/p/8e5fb5fe0931)\n\n","tags":["jupyter"],"categories":["工具"]},{"title":"手推公式带你轻松理解L1/L2正则化","url":"/2020/11/24/214850/","content":"\n## 前言\n\n\n>L1/L2正则化的目的是为了解决过拟合，因此我们先要明白什么是过拟合、欠拟合。\n\n- 过拟合：训练出的模型在测试集上Loss很小，在训练集上Loss较大\n- 欠拟合：训练出的模型在测试集上Loss很大，在训练集上Loss也很大\n- 拟合：训练的刚刚好，在测试集上Loss很小，在训练集上Loss也很小\n\n现在，让我们开启L1/L2正则化正则化之旅吧！\n\n<!-- more -->\n\n\n\n## L1/L2正则化原理\nL1与L2正则是通过在损失函数中增加一项对网络参数的约束，使得参数渐渐变小，模型趋于简单，以防止过拟合。\n\n**损失函数Loss**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724182727639.png)\n*上述Loss，MSE均方误差的Loss*\n\n**L1正则化的损失函数**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724182742754.png)\n*W代表网络中的参数，超参数λ需要人为指定。需要注意的是，**L1使用绝对值来约束参数***\n\n**L2正则化的损失函数**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724182808385.png)\n*相比于L1正则化，L2正则化则使用了平方函数来约束网络参数*\n\n> 需要注意的是，在有的文献中，把L2正则项定义为权值向量w中各个元素的平方和然后再求平方根，其实，L2正则加不加平方根影响不大，原理都是一样的，但不加平方根更容易数学公式推导\n\n\n我们知道，当W的值比较大时(即W的值距离0很远，取几百甚至几千的值)，则拟合的曲线比较陡，x稍微一变化，y的影响就比较大，如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724184041339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n可以看到，你的模型复杂度越大，拟合的曲线就越陡，惩罚项W就越大，在这种情况容易出现过拟合，所以要避免W出现比较大的值，一个有效的方法是给loss加上一个与W本身有关的值，即L1正则项或L2正则项，这样，我们在使用梯度下降法让Loss趋近于0的时候，也必须让W越来越小，W值越小，模型拟合的曲线会越平缓，从而防止过拟合。也可以从奥卡姆剃刀原理的角度去解释，即在所有可以选择的模型中，能够很好拟合当前数据，同时又十分简单的模型才是最好的。\n\nL1与L2正则化让W变小的原理是不同的：\n\n- **L1能产生等于0的权值，即能够剔除某些特征在模型中的作用（特征选择），即产生稀疏的效果**。\n- **L2可以得迅速得到比较小的权值，但是难以收敛到0，所以产生的不是稀疏而是平滑的效果**。\n\n下面，从两个角度理解L1/L2正则化这两个结论\n\n##  从数学的角度理解L1/L2正则化\n我们来看看**L1正则化的损失函数的求导及梯度更新公式**：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724201746252.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*lr是学习率(更新速率)，上述求导是Loss或$Loss_l1$对$w_i$的偏导，为了方便书写，将$w_i$写成W*\n\n- 上面是加上L1正则化的损失函数后，W更新公式及loss对W求梯度的公式(准确的说是对wi求偏导)，|W|对Wi的导数是1或-1，这样W更新公式就变为加上或减去一个常量lr，\n- 就是说权值每次更新都固定减少一个特定的值(比如0.01)，那么经过若干次迭代之后，权值就有可能减少到0。\n\n**L1正则化的损失函数的求导及梯度更新公式(假设$\\lambda=1/4$)**：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724203126899.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n- 从上面的公式中可以看到，加上L2正则项后，W实际上每次变为原来的C倍(另一项忽略不计），假如C=0.5，那么，W每次缩小为原来的一半，虽然权值W不断变小，但是因为每次都等于上一次的一半，所以很快会收敛到较小的值但不为0。\n\n## 从几何的角度理解L1/L2正则化\n由于L1正则化项是$|w_1+w_2+...+w_n|$，为了便于理解，我们假设，L1正则项是$w_1+w_2$，将其画在二维坐标轴上如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724204849791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n- 横轴是w1,纵轴是w2，优化空间是一个等高线图，可以看优化曲线在圆圈上移动时，**只有直达W2轴的交点处，才能满足两个条件：让loss最小，让w1+w2最小，此时w2=0**，所以说，L1中两个权值倾向于一个较大另一个为0即产生稀疏的效果\n\n由于L2正则化使用了平方函数，而如果两个参数的平方和相同，呈现出的形状会是一个圆。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200724205519269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n- 可以看优化曲线在圆圈上移动时，**只有在两个圆圈的交点处，才能满足两个条件：让loss最小，让$w^1+w^2$最小，此时L2中两个权值都倾向于为非零的较小数**，所以说，L2产生平滑的效果。\n\n\n\n## L1/L2正则化使用情形\n\n- L1能产生等于0的权值，即能够剔除某些特征在模型中的作用（特征选择），即产生稀疏的效果，如果需要做模型的压缩，L1正则是一个不错的选择。\n\n- 如果不做模型的压缩，在实际中更倾向于L2正则化，因为在实际操作的过程中，模型该用多少层，模型的参数量，这个不好确定，我们也不知道解决这个实际问题要用多少层网络，用多少个参数，只能去试。在试的过程中，最快最经济的做法是：**在模型最开始的时候就加上正则化，不管是在欠拟合或过拟合都加上正则化，然后就不断的去训练，后面根据模型的拟合情况只要增加模型参数和模型结构就行了，不用考虑其他的，即把正则化作为默认的选项去试就可以了。**\n\n\n\n","tags":["normalize"],"categories":["normalize"]},{"title":"numpy和torch数据类型转化问题","url":"/2020/11/24/214655/","content":"\n>在实际计算过程中，float类型使用最多，因此这里重点介绍numpy和torch数据float类型转化遇到的问题，其他类型同理。\n\n<!-- more -->\n\n\n\n## numpy数据类型转化\n\n- numpy使用astype转化数据类型，float默认转化为64位，可以使用`np.float32`指定为32位\n\n\n```python\n#numpy转化float类型\na= np.array([1,2,3])\na = a.astype(np.float)\nprint(a)\nprint(a.dtype)\n```\n\n`[1. 2. 3.]`  \n`float64`\n    \n\n- 不要使用a.dtype指定数据类型，会使数据丢失\n\n\n```python\n#numpy转化float类型\nb= np.array([1,2,3])\nb.dtype= np.float32\nprint(b)\nprint(b.dtype)\n```\n\n`[1.e-45 3.e-45 4.e-45]`  \n`float32`\n    \n\n- 不要用float代替np.float，否则可能出现意想不到的错误\n- 不能从np.float64位转化np.float32，会报错\n- np.float64与np.float32相乘，结果为np.float64\n\n> 在实际使用过程中，可以指定为np.float，也可以指定具体的位数，如np.float，不过直接指定np.float更方便。\n\n## torch数据类型转化\n\n- torch使用`torch.float()`转化数据类型，float默认转化为32位，torch中没有`torch.float64()`这个方法\n\n\n```python\n# torch转化float类型\nb = torch.tensor([4,5,6])\nb = b.float()\nb.dtype\n```\n\n\n\n\n    torch.float32\n\n\n\n- `np.float64`使用`torch.from_numpy`转化为torch后也是64位的\n\n\n```python\nprint(a.dtype)\nc = torch.from_numpy(a)\nc.dtype\n```\n\n`float64`  \n`torch.float64`\n\n\n\n- 不要用float代替torch.float，否则可能出现意想不到的错误\n- torch.float32与torch.float64数据类型相乘会出错，因此相乘的时候注意指定或转化数据float具体类型\n\n> np和torch数据类型转化大体原理一样，只有相乘的时候，torch.float不一致不可相乘，np.float不一致可以相乘，并且转化为np.float64\n\n\n## numpy和tensor互转\n- tensor转化为numpy\n\n```python\nimport torch\nb = torch.tensor([4.0,6])\n# b = b.float()\nprint(b.dtype)\nc = b.numpy()\nprint(c.dtype)\n```\n\n`torch.int64`  \n`int64`\n\n- numpy转化为tensor\n\n```python\nimport torch\nimport numpy as np\nb= np.array([1,2,3])\n# b = b.astype(np.float)\nprint(b.dtype)\nc = torch.from_numpy(b)\nprint(c.dtype)\n```\n`int32`  \n`torch.int32`  \n\n**可以看到，torch默认int型是64位的，numpy默认int型是32位的**","tags":["pytorch","numpy"],"categories":["pytorch"]},{"title":"Batch Normalization：批量归一化详解","url":"/2020/11/24/214400/","content":"\n## 为什么要使用BN\n在深度学习中，层数很多，可能有几十层甚至上百层，每次训练激活的过程中，参数不断改变，导致后续每一层输入的分布也发生变化，而学习的过程就是要使每一层适应输入的分布，如果不做BN就会使模型学的很小心，也会使学习速率变慢，因此我们不得不降低学习率、小心地初始化。\n\n**那怎么才能让我们的模型学习的更高效呢？**\n原有的方式可能是歪着学的，学的效果不是很好，如下图B所示，现在我们需要让它激活到一个更适合学习的位置上，那就需要把它放到原点，这个位置更适合，这时候就需要BN了\n>原点周围更敏感，假定一开始你的数据不在原点周围，后面如果越来越偏，说不定会偏去哪，也就是指W一会大一会小，一会是正值，一会是负值，如下图B所示，也不利于更新。\n\n<!-- more -->\n\n\n\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723220500179.png)\n\n\n## BN的工作原理\n>批量归一化 （Batch Normalization， BN）方法是一种有效的逐层归一化方法，可以对神经网络中任意的中间层进行归一化操作。\n\n\nBatch Normalization，顾名思义，以进行学习时的batch为单位，按batch进行规范化。具体而言，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布，用数学式表示的话，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072322071327.png)\n\n*m代表batch的大小，$μ_B$为批处理数据的均值，$σ^2_B$为批处理数据的方差。*\n\n减去平均值是将数据放在原点周围，除以方差是因为让原来挤在一起的数据变得更均匀一些，如下图C->图D，原来分散的，也会让它们更加紧促一些，在这样一个数据分布里面，使得非线性变换函数的输入值落入对输入比较敏感的区域，从而避免梯度消失问题。这样输入的小变化就会导致损失函数较大的变化（使得梯度变大，避免梯度消失)。可以看出，提高参数更新的效率条件有：让数据的分布在原点附近，让数据分布不离散也不紧凑。\n\n\n将数据转化为均值为0、方差为1的数据分布后，接着，BN层会对正规化后的数据进行缩放和平移的变换，用数学式可以如下表示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723222606550.png)\n\n*这里，γ和β是参数。一开始γ=1，β=0，然后再通过学习调整到合适的值。*\n\n**思考：为什么BN要引入线性变化操作？**\n\nBN层相当于固定了每一层的输入分布，从而加速网络模型的收敛速到，但是这也限制了网络模型中数据的表达能力，浅层学到的参数信息会被BN的操作屏蔽掉，因此，BN层又增加了一个线性变换操作，让数据尽可能地恢复本身的表达能力\n\n\n## BN的优点\n**缓解梯度消失，加速网络收敛速度**。BN层可以让激活函数(非线性变化函数)的输入数据\n落入比较敏感的区域，缓解了梯度消失问题。\n\n**简化调参的负担，网络更稳定**。在调参时，学习率调得过大容易出现震荡与不收敛，BN层则抑制了参数微小变化随网络加深而被放大的问题，因此对于参数变化的适应能力更强，更容易调参。\n\n**防止过拟合**。BN层将每一个batch的均值与方差引入到网络中，由于每个batch的这两个值都不相同，可看做为训练过程增加了随机噪音，可以起到一定的正则效果，防止过拟合\n\n\n在测试时应该注意的问题：\n**在测试时，由于是对单个样本进行测试，没有batch的均值与方差，通常做法是在训练时将每一个batch的均值与方差都保留下来，在测试时使用所有训练样本均值与方差的平均值。**\n\n\n## PyTorch中使用BN层\n\n```python\n>>> import torch.nn as nn\n>>> import torch\n# 使用BN层需要传入一个参数为num_features，即特征的通道数\n>>> bn = nn.BatchNorm2d(64)\n# eps为公式中的є，momentum为均值方差的动量，affine为添加可学习参数\n>>> bn\nBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n>>> input = torch.randn(4, 64, 224, 224)\n>>> output = bn(input)\n>>> output.shape\n# BN层不改变输入、输出的特征大小\ntorch.Size([4, 64, 224, 224])\n>>>     \n```\n## BN的弊端\n管BN层取得了巨大的成功，但仍有一定的弊端，主要体现在以下两点：\n- 由于是在batch的维度进行归一化，BN层要求较大的batch才能有效地工作，而物体检测等任务由于占用内存较高，限制了batch的大小，这会限制BN层有效地发挥归一化功能。\n- 数据的batch大小在训练与测试时往往不一样。**在训练时一般采用滑动来计算平均值与方差(一个batch一个batch计算)，在测试时直接拿训练集的平均值与方差来使用**。这种方式会导致测试集依赖于训练集，然而有时训练集与测试集的数据分布并不一致。\n\n\n因此，我们能不能避开batch来进行归一化呢？答案是可以的，最新的GN（Group  Normalization）从通道方向计算均值与方差，使用更为灵活有效，避开了batch大小对归一化的影响。具体来讲，GN先将特征图的通道分为很多个组，对每一个组内的参数做归一化，而不是batch。GN之所以能够工作的原因，可以认为是在特征图中，不同的通道代表了不同的意义，例如形状、边缘和纹理等，这些不同的通道并不是完全独立地分布，而是可以放到一起进行归一化分析。\n\n\n参考文档\n\n深度学习之PyTorch物体检测实战[董洪义著]\n深度学习入门基于Python的理论与实现[斋藤康毅著，陆宇杰译]\n神经网络与深度学习[邱锡鹏著]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["normalize"],"categories":["normalize"]},{"title":"神经网络之多维卷积的那些事(一维、二维、三维)","url":"/2020/11/24/214142/","content":"\n## 前言\n一般来说，一维卷积用于文本数据，二维卷积用于图像数据，对宽度和高度都进行卷积，三维卷积用于视频及3D图像处理领域（检测动作及人物行为），对立方体的三个面进行卷积 。二维卷积的用处范围最广，在计算机视觉中广泛应用。\n<!-- more -->\n\n\n## 一维卷积Conv1d\n一维卷积最简单，实质是对一个词向量做卷积，如下所示：\n![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vd3hsZXIvYmxvZ2ltZy9yYXcvbWFzdGVyL2ltZ3MvMjAyMDA3MTAxNzM0NTEucG5n?x-oss-process=image/format,png)\n\n- 图中的输入的数据维度为8，过滤器的维度为5。卷积后输出的数据维度为8−5+1=4\n- 如果过滤器数量仍为1，输入数据的channel数量变为16，则输入数据维度为8×16\n- 一维卷积常用于序列模型，自然语言处理领域。\n\n\nPytorch中nn.Conv1d卷积运算要求输入源是3维，输入源的三个维度分别是：第一个维度代表每个序列的个数即样本数，第二个维度代表每一个序列的通道数，第三个维度代表这个词向量序列，如下所示：\n```python\nimport torch\nimport torch.nn as nn\n# 输入源：1个样本，16个通道，8个数据\na = torch.randn(1,16,8)\n# 卷积：输入通道为16，输出通道为1，卷积核大小 5*5\nconv = nn.Conv1d(16, 1, 5)\nc = conv(a)\nprint('a:', a.size())\nprint('c:', c.size())\n```\n\n**output**\n\n```\na: torch.Size([1, 16, 8])\nc: torch.Size([1, 1, 4])\n```\n\n## 二维卷积Conv2d\n二维卷积是最常见、用途最广泛的卷积。先假定卷积核(过滤器)数量为1，图片通道数为1，卷积操作如下：\n![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vd3hsZXIvYmxvZ2ltZy9yYXcvbWFzdGVyL2ltZ3MvMjAyMDA3MTAxNzM4NDUucG5n?x-oss-process=image/format,png)\n\n- 图中的输入的数据维度为`14×14`，卷积核数量为1，图片通道数为1\n- 二维卷积输出的数据尺寸为`8−5+1=4`，即`4×4`\n\n这是最简单的二维卷积的场景，现在重点来了，假定图片通道数为3，卷积核的数量为1，则卷积操作如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723171151348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n- 如上图所示，输入源是`6*6*3`（图片大小`6*6`,三个通道），卷积核大小`3*3`，filters(卷积核数量)=1,即权重矩阵是`3*3*3*1`，得到的结果`4*4*1`（图片大小`4*4`,一个通道）\n- 其实就是三个`3*3`的卷积核分别对图片的三个通道做卷积，然后把结果相加得到一个`4*4`的图片。所以，这里卷积核w的参数个数是`(3*3*3+1)*1`，(输入通道`3`，卷积核大小`3*3`，一个偏置，输出通道1)\n- 上图卷积 Pytorch中表示为：`nn.Conv2d(3,1,kernel_size=(3,3),stride=1)`\n\n**到这里可能有人会问，卷积核大小为3\\*3，为什么变成3\\*3\\*3了？**\n可以细想一下，图片的大小是`6*6*3`（图片大小6\\*6,三个通道），也就是三维的图片，二维的卷积是肯定不能对其操作的，所以卷积核的维度会随着图片的输入通道改变，如果图片的输入通道是3，那么卷积核的维度也是3，如果图片的输入通道是1，那么卷积核的维度也是1，这也就是为什么Pytorch中nn.Conv2d的输入通道要与图片的输入通道保持一致的原因，否则无法进行卷积操作。\n\n**现在，假定图片通道数为3，卷积核的数量为2，则卷积操作如下**：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723172020944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n- 如上图所示，输入源是`6*6*3`（图片大小`6*6`,三个通道），卷积核大小`3*3`，filters=2，即权重矩阵是`3*3*3*2`，得到的结果`4*4*2`\n- 上图每一个卷积核卷积参数的个数是`(3*3*3+1)*2`\n- 上图卷积 Pytorch中表示为：`nn.Conv2d(3,2,kernel_size=(3,3),stride=2)`\n\n计算图如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200731150746946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n二维卷积常用于计算机视觉、图像处理领域。\n\n\nPytorch中nn.Conv2d卷积运算要求输入源是4维，输入源的四个维度分别是：第一个维度代表图片的个数即样本数，第二个维度代表每一张图片的通道数，后面二个维度代表图片的像素矩阵，如下所示：\n\n```python\nimport torch\nimport torch.nn as nn\na = torch.Tensor([[[[1,2,3,4],\n                [5,6,7,8],\n                [9,10,11,12],\n                [13,14,15,16]]],\n               [[[1,2,3,4],\n                [5,6,7,8],\n                [9,10,11,12],\n                [13,14,15,16]]]])\nprint('a:',a.size())\n# 卷积核：输入通道为1，输出通道6，卷积核大小2*2\nconv = nn.Conv2d(1,6,2)\nc = conv(a)\nprint('c:',c.size())\n\nconv1 = nn.Conv2d(6,16,2)\nc1 = conv1(c)\nprint('c1:',c1.size())\n```\n\n**output**\n```\na: torch.Size([2, 1, 4, 4])\nc: torch.Size([2, 6, 3, 3])\nc1: torch.Size([2, 16, 2, 2])\n```\n\n## 三维卷积Conv3d\n\n![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vd3hsZXIvYmxvZ2ltZy9yYXcvbWFzdGVyL2ltZ3MvMjAyMDA3MTAxNzU3NTMucG5n?x-oss-process=image/format,png)\n\n三维卷积具体思想与一维卷积、二维卷积相同，假设卷积核大小为`f1*f2*f3`(类似于二维卷积，三维卷积实际计算的时候卷积核是四维的，另一个维度由输入源的通道数决定)\n\n- 假设输入数据的大小为`a1×a2×a3`\n- 基于上述情况，三维卷积最终的输出为`(a1−f1+1)×(a2−f2+1)×(a3−f3+1)`\n- 三维卷积常用于医学领域（CT影响），视频处理领域（检测动作及人物行为）。\n\nPytorch中nn.Conv3d要求输入源是5维的，输入源的5个维度分别表示为：第一个维度代表样本的个数，第二个维度代表每个样本的通道数，后面三个维度代表三维立体图形的像素矩阵，如下所示：\n\n```python\nimport torch\nimport torch.nn as nn\nx = torch.randn(1,2,6,1,1)\nconv = nn.Conv3d(in_channels=2,\n                 out_channels=6,\n                 kernel_size=(2,1,1))\nc = conv(x)\nprint('x:', x.size())\nprint('c:', c.size())\n```\n\n**output**\n```\nx: torch.Size([1, 2, 6, 1, 1])\nc: torch.Size([1, 6, 5, 1, 1])\n```\n*说明：通道数从输入的2转化为6，一个立体像素矩阵，输入前大小为6\\*1\\*1, 卷积核2\\*1\\*1，得到结果为(6-2+1)\\*(1-1+1)\\*(1-1+1)=5\\*1\\*1*\n\n\n## 卷积中的特征图大小计算方式\n在神经网络卷积操作主要是提取特征的，因此卷积的输出称为**特征图(FeatureMap)**，神经网络中有多层卷积，所以除了最开始输入的原始图像外，我们认为卷积输入的也是特征图\n\n当卷积操作步长为1的时候，进行卷积操作后特征图的尺寸比较容易计算，如果步长为2或者更大就不容易计算了。其实这里有一个通用的公式，如下所示：\n$$W_{out} = \\frac{ W_{in}+2*padding-K}{stride} + 1$$\n其中，$W_{out}$为输出特征图的大小，$W_{in}$为输入特征图的大小，K为卷积核大小，stride为卷积步长，padding为特征图填充的圈数。\n\n这里不得不提一下，**池化操作对上述公式也适用，我看有的文章里说，卷积除不尽的结果都向下取整，池化除不尽的结果都向上取整，这个说法是错误，我经过测试得出Pytorch和TensorFlow默认都是向下取整**。如下所示：\n\n**卷积操作向下取整**\n```python\n# 卷积操作向下取整\nimport torch\nimport torch.nn as nn\na = torch.rand(1,1,10,10)\n# (10+2*2-5)/2.0+1 = 5.5\nconv2d = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(5,5),padding=2,stride=2)\noutput = conv2d(a)\noutput.size()\n# 输出为：torch.Size([1, 1, 5, 5])\n```\n**池化默认也是向下取整**\n```python\n# 池化默认也是向下取整\nimport torch\nimport torch.nn as nn\na = torch.rand(1,1,10,10)\n# (10+2*2-5)/2.0+1 = 5.5\nmaxpool = nn.MaxPool2d(kernel_size=(5,5),stride=2,padding=2)\noutput = maxpool(a)\noutput.size()\n# 输出为：torch.Size([1, 1, 5, 5])\n```\n**但是，Pytorch中池化层有个参数ceil_node=True是向上取整的，它默认是False，TensorFlow里面没找到类似的参数**\n\n```python\n# 池化层ceil_mode=True向上取整\nimport torch\nimport torch.nn as nn\na = torch.rand(1,1,10,10)\n# (10+2*2-5)/2.0+1 = 5.5\nmaxpool = nn.MaxPool2d(kernel_size=(5,5),stride=2,padding=2,ceil_mode=True)\noutput = maxpool(a)\noutput.size()\n# 输出为：torch.Size([1, 1, 6, 6])\n```\n\n\n\n\n## 总结\n到这里我们学到了\n- 不同卷积的应用\n- 多维卷积的原理\n- 多维卷积核参数的个数\n- 卷积中的特征图计算方式\n\n是不是感觉收获满满！！！\n\n\n\n","tags":["卷积"],"categories":["神经网络"]},{"title":"PyTorch之torchvision.transforms详解[原理+代码实现]","url":"/2020/11/24/213959/","content":"## 前言\n\n我们知道，在计算机视觉中处理的数据集有很大一部分是图片类型的，如果获取的数据是格式或者大小不一的图片，则需要进行归一化和大小缩放等操作，这些是常用的数据预处理方法。如果参与模型训练中的图片数据非常有限，则需要通过对有限的图片数据进行各种变换，如缩小或者放大图片的大小、对图片进行水平或者垂直翻转等，这些都是数据增强的方法。庆幸的是，这些方法在torch.transforms中都能找到，在torch.transforms中有大量的数据变换类，有很大一部分可以用于实现**数据预处理（Data Preprocessing）和数据增广（Data Argumentation）**。\n<!-- more -->\n\n\n## torchvision.transforms常用变换类\n\n\n### transforms.Compose\ntransforms.Compose类看作一种容器，它能够同时对多种数据变换进行组合。传入的参数是一个列表，列表中的元素就是对载入的数据进行的各种变换操作。\n\n**首先使用PIL加载原始图片**\n\n```python\n#Pyton Image Library  PIL 一个python图片库\nfrom PIL import Image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimg = Image.open(\"./imgs/dianwei.jpg\")\nprint(img.size)\nplt.imshow(img)\n```\n输出：\n`(1102, 735)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103029941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n```python\ntransformer = transforms.Compose([                                \n    transforms.Resize(256),\n    transforms.transforms.RandomResizedCrop((224), scale = (0.5,1.0)),\n    transforms.RandomHorizontalFlip(),\n])\ntest_a = transformer(img)\nplt.imshow(test_a)\n```\n输出：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103150561.png)\n\n\n### transforms.Normalize(mean, std)\n\n这里使用的是标准正态分布变换，这种方法需要使用原始数据的均值（Mean）和标准差（Standard Deviation）来进行数据的标准化，在经过标准化变换之后，数据全部符合均值为0、标准差为1的标准正态分布。计算公式如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723113549916.png)\n一般来说，mean和std是实现从原始数据计算出来的，对于计算机视觉，更常用的方法是从样本中抽样算出来的或者是事先从相似的样本预估一个标准差和均值。如下代码，对三通道的图片进行标准化：\n\n```python\n# 标准化是把图片3个通道中的数据整理到规范区间 x = (x - mean(x))/stddev(x)\n# [0.485, 0.456, 0.406]这一组平均值是从imagenet训练集中抽样算出来的\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n```\n### transforms.Resize(size)\n对载入的图片数据按照我们的需要进行缩放，传递给这个类的size可以是一个整型数据，也可以是一个类似于 (h  ,w) 的序列。如果输入是个(h,w)的序列，h代表高度，w代表宽度，h和w都是int，则直接将输入图像resize到这个(h,w)尺寸，相当于force。如果使用的是一个整型数据，则将图像的短边resize到这个int数，长边则根据对应比例调整，图像的长宽比不变。\n```python\n# 等比缩放\ntest1 = transforms.Resize(224)(img)\nprint(test1.size)\nplt.imshow(test1)\n```\n输出：\n`(335, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103305748.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n### transforms.Scale(size)\n对载入的图片数据我们的需要进行缩放，用法和torchvision.transforms.Resize类似。。**传入的size只能是一个整型数据**，`size`是指缩放后图片最小边的边长。举个例子，如果原图的`height>width`,那么改变大小后的图片大小是`(size*height/width, size)`。\n```python\n# 等比缩放\ntest2 = transforms.Scale(224)(img)\nprint(test2.size)\nplt.imshow(test2)\n```\n输出：\n`(335, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103356414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n\n### transforms.CenterCrop(size)\n以输入图的中心点为中心点为参考点，按我们需要的大小进行裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于(h,w)的序列。**如果输入的是一个整型数据，那么裁剪的长和宽都是这个数值**\n```python\ntest3 = transforms.CenterCrop((500,500))(img)\nprint(test3.size)\nplt.imshow(test3)\n```\n输出：\n`(500, 500)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103440830.png)\n```python\ntest4 = transforms.CenterCrop(224)(img)\nprint(test4.size)\nplt.imshow(test4)\n```\n输出：\n`(224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103532667.png)\n\n\n\n### transforms.RandomCrop(size)\n用于对载入的图片按我们需要的大小进行随机裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于(h,w)的序列。**如果输入的是一个整型数据，那么裁剪的长和宽都是这个数值**\n```python\ntest5 = transforms.RandomCrop(224)(img)\nprint(test5.size)\nplt.imshow(test5)\n```\n输出：\n`(224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727103718102.png)\n```python\ntest6 = transforms.RandomCrop((300,300))(img)\nprint(test6.size)\nplt.imshow(test6)\n```\n输出：\n`(300, 300)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072710391757.png)\n\n\n\n### transforms.RandomResizedCrop(size,scale)\n\n先将给定图像随机裁剪为不同的大小和宽高比，然后缩放所裁剪得到的图像为size的大小。即先随机采集，然后对裁剪得到的图像安装要求缩放，默认scale=(0.08, 1.0)。scale是一个面积采样的范围，假如是一个100\\*100的图片，scale = (0.5,1.0)，采样面积最小是0.5\\*100\\*100=5000，最大面积就是原图大小100\\*100=10000。先按照scale将给定图像裁剪，然后再按照给定的输出大小进行缩放。\n```python\ntest9 = transforms.RandomResizedCrop(224)(img)\nprint(test9.size)\nplt.imshow(test9)\n```\n输出：\n`(224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104231793.png)\n```python\ntest9 = transforms.RandomResizedCrop(224,scale=(0.5,0.8))(img)\nprint(test9.size)\nplt.imshow(test9)\n```\n输出：\n`(224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104330982.png)\n\n\n\n\n### transforms.RandomHorizontalFlip\n用于对载入的图片按随机概率进行水平翻转。我们可以通过传递给这个类的参数自定义随机概率，如果没有定义，则使用默认的概率值0.5。\n```python\ntest7 = transforms.RandomHorizontalFlip()(img)\nprint(test7.size)\nplt.imshow(test7)\n```\n输出：\n`(1102, 735)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104041714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n### transforms.RandomVerticalFlip\n用于对载入的图片按随机概率进行垂直翻转。我们可以通过传递给这个类的参数自定义随机概率，如果没有定义，则使用默认的概率值0.5。\n```python\ntest8 = transforms.RandomVerticalFlip()(img)\nprint(test8.size)\nplt.imshow(test8)\n```\n输出：\n`(1102, 735)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104127721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n### transforms.RandomRotation\n```python\ntransforms.RandomRotation(\n    degrees,\n    resample=False,\n    expand=False,\n    center=None,\n    fill=None,\n)\n```\n- 功能：按照degree随机旋转一定角度\n- degree：加入degree是10，就是表示在（-10，10）之间随机旋转，如果是（30，60），就是30度到60度随机旋转\n- resample是重采样的方法\n- center表示中心旋转还是左上角旋转\n\n```python\ntest10 = transforms.RandomRotation((30,60))(img)\nprint(test10.size)\nplt.imshow(test10)\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/202009022028180.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n### transforms.ToTensor\n用于对载入的图片数据进行类型转换，将之前构成PIL图片的数据转换成Tensor数据类型的变量，让PyTorch能够对其进行计算和处理。\n\n### transforms.ToPILImage\n用于将Tensor变量的数据转换成PIL图片数据，主要是为了方便图片内容的显示。\n\n## torchvision.transforms编程实战\n```python\n# RandomResizedCrop 将给定图像随机裁剪为不同的大小和宽高比，然后缩放所裁剪得到的图像为制定的大小\nprint(\"原图大小：\",img.size)\n# Crop代表剪裁到某个尺寸\ndata1 = transforms.RandomResizedCrop(224)(img)\n# data1、data2、data3尺寸一样，长宽都是224*224  size也可以是一个Integer，在这种情况下，切出来的图片的形状是正方形\nprint(\"随机裁剪后的大小:\",data1.size)\ndata2 = transforms.RandomResizedCrop(224)(img)\ndata3 = transforms.RandomResizedCrop(224)(img)\n\n# 放四个格，布局为2*2\nplt.subplot(2,2,1),plt.imshow(img),plt.title(\"Original\")\nplt.subplot(2,2,2),plt.imshow(data1),plt.title(\"Transform 1\")\nplt.subplot(2,2,3),plt.imshow(data2),plt.title(\"Transform 2\")\nplt.subplot(2,2,4),plt.imshow(data3),plt.title(\"Transform 3\")\nplt.show()\n```\n输出：\n`原图大小： (1102, 735)`\n`随机裁剪后的大小: (224, 224)`\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072710460549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n```python\n# 以输入图的中心点为中心点做指定size的crop操作\nimg1 = transforms.CenterCrop(224)(img)\nimg2 = transforms.CenterCrop(224)(img)\nimg3 = transforms.CenterCrop(224)(img)\n# img1、img2、img3三个图是一样的\nplt.subplot(2,2,1),plt.imshow(img),plt.title(\"Original\")\nplt.subplot(2,2,2), plt.imshow(img1), plt.title(\"Transform 1\")\nplt.subplot(2,2,3), plt.imshow(img2), plt.title(\"Transform 2\")\nplt.subplot(2,2,4), plt.imshow(img3), plt.title(\"Transform 3\")\nplt.show()\n```\n输出：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104637657.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n```python\n# 以给定的概率随机水平旋转给定的PIL的图像，默认为0.5\nimg1 = transforms.RandomHorizontalFlip()(img)\nimg2 = transforms.RandomHorizontalFlip()(img)\nimg3 = transforms.RandomHorizontalFlip()(img)\n\nplt.subplot(2,2,1),plt.imshow(img),plt.title(\"Original\")\nplt.subplot(2,2,2), plt.imshow(img1), plt.title(\"Transform 1\")\nplt.subplot(2,2,3), plt.imshow(img2), plt.title(\"Transform 2\")\nplt.subplot(2,2,4), plt.imshow(img3), plt.title(\"Transform 3\")\nplt.show()\n```\n输出：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727104701915.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n\n源码在[PyTorch之torchvision.transforms实战](https://gitee.com/wxler/AIProjectTraining/blob/master/practice/PyTorch%E4%B9%8Btorchvision.transforms%E5%AE%9E%E6%88%98.ipynb)，请自提！\n\n\n参考文档\n深度学习pytoch实战计算机视觉(唐进民著)\n\n\n","categories":["数据预处理"]},{"title":"深入理解GAN对抗生成网络","url":"/2020/11/24/213812/","content":"\n##  什么是GAN\n>Generative Adversarial Networks，生成式对抗网络，Ian Goodfellow 在2014 年提出的一种生成式模型\n基本思想来自博弈论的二人零和博弈（纳什均衡）, 由一个生成器和一个判别器构成，通过对抗学习来训练\n\n- 生成器的目的是尽量去学习真实的数据分布\n- 判别器的目的是尽量正确判别输入数据是来自真实数据还是来自生成器\n- 生成器和判别器就是一个矛和盾互相PK的过程\n- 为了取得游戏胜利，这两个游戏参与者需要不断优化， 各自提高自己的生成能力和判别能力，这个学习优化过程就是寻找二者之间的一个纳什均衡\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722120217603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*G代表生成器，D代表判别器，Z是输入源，称为Noise source，就是一个随机编码。给出一个random code(即Z)由生成器G生成假数据X'，假数据X'和真实数据X喂给判别器D，由D判别出哪个是real，哪个是fake，这个就是gan的基本原理*\n\n<!-- more -->\n\n@[TOC]\n\n\n## 纳什均衡\n我们以囚徒困境的例子来解释纳什均衡的含义\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722120703173.png)\nA和B属于零和游戏，需要在A和B的决策中进行Trade Off(权衡)，由此看出，抵赖对两个人来说都是最优的结果，这个就是纳什均衡。\n\n>亚当·斯密的“看不见的手”，在市场经济中，每一个人都从利己的目的出发，不断调和与迭代，最终全社会达到利他的效果\n\n## GAN的学习过程\nGAN的学习过程其实就是把D和G达成一个均衡，这是我们的目标，因此不仅要训练G，也要训练D\n\n**为什么D和G是对抗的？**\nG是生成器，它生成的数据是虚假的，它目标是让生成的数据骗过D。D是判别器，它的目标是要把虚假的数据给找出来，因此D和G是对抗的\n\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072212094128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n对于 GAN 的学习过程 ，需要训练模型D来最大化判别数据来源于真实数据或者伪数据分布 ，同时，我们需要训练模型 G来最小化 loss。\n\n**我们该采用怎样的优化方法，对生成器G和判别器D进行优化呢？**\n\n- Step1，固定生成器 G=>优化判别器 D，让D的判别准确率最大化\n- Step2，固定判别器 D => 优化生成器 G，让D的判别准确率最小化\n\n训练 GAN 时，在同一轮参数更新中，通常对 D 的参数更新 k 次，再对 G的参数更新 1 次，这样做的目的是让D学的更快点，因为我们最终要的是G，为此需要把D这个教练先变得越来越好，由此才能训练出更好的G\n\n**Generator与Discriminator的工作原理**\n\n- Generator，在输入一个随机编码（random code）z之后，它将输出一幅由神经网络自动生成的、假的图片G(z)\n- Discriminator，接受G输出的图像作为输入，然后判断这幅图像的真假，真的输出1，假的输出0\n- G生成的图像会越来越逼真，D也越来越会判断图片的真假，最后我们就不要D了，直接用G来生成图像\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722121543593.png)\n我们就是要在最大化D的能力的前提下，最小化D对G的判断能力 ，所以称之为 最小最大值问题\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722121618497.png)\n*损失函实际上是一个交叉熵，判别器的目的是尽可能的令D(x)接近1(对于真图像x的处理评分要高)，令D(G(z))接近0(对于假图像G(z)的处理评分要尽量降低)，所以D主要是最大化上面的损失函数(让D的辨别能力更强)，G恰恰相反，他主要是最小化上述损失函数(让生成的假图像G(z)变得更真实)。*\n\n为了增强D的能力，我们分别考虑输入真的图像和假的图像的情况\n\n**D的目标是什么？G的目标是什么？**\n- D的目标是：D(G(z))处理的是假图像G(z) => 评分D(G(z))要尽量降低，对于真图像x的处理 => 评分要高，这样D的辨别能力才会更强\n- G的目标是：让生成的假图像G(z)变得更真实、更逼真，让D难以辨别\n\n\n\n## GAN的局限性\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722121848640.png)\n\n用户输入random code由生成器G参数 fake image即G(Z)，传统的GAN中会出现如下局限性：\n\n 1. 在传统的GAN里，由于没有用户控制能力，输入一个随机噪声，就会输出一幅随机图像（可能输出猫在左边或是猫在右边的图像）\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722121929235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n>这里user input 是指输入的random code，output是G生成的G(Z)，还要从现实世界中取一张或者画一张真实图像，与output一起输入判别器\n\n2. 低分辨率（Low resolution）和低质量（Low quality）问题\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722122102813.png)\n生成的图片看起来不错，但放大看，会发现细节相当模糊\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722122125898.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n## 如何改善GAN的局限性\n如改善GAN的局限性可以从以下两个方面入手：\n- 提高GAN的用户控制能力\n- 提高GAN生成图片的分辨率和质量\n\n从以上两个方面提出新的算法模型：\n\n1. pix2pix，有条件的使用用户输入，使用成对的数据（paired data）进行训练。比如，输入的是猫在左边，你就不能生成猫在右边的图。Pix2pix的缺点：在训练过程中，需要人为给它标出数据的对应关系，比如 现在的输入条件是猫在左边，人要给它画一张或找一张猫在左边的图像，才会让G更好的学习，以产生猫在右边的图像，这对数据源的要求会很高\n2. CycleGAN，使用不成对的数据（unpaired data）就能训练。  \n以马为例，马训练马是成对的数据，用马生成斑马，是不成对数据。由于现实生活中成对的样本比较少，对于没有成对样本的情况，使用CycleGAN，CycleGAN有两个生成器，马->斑马，斑马->马，我们最终想要的是马->斑马，利用理论上开始的马和马->斑马->马是一样的，同时优化马->斑马，斑马->马两个生成器，最终使用马->斑马。\nCycleGAN本质是优化生成器的一个思想，拿文本翻译来说，你把一段英文翻译成中文，再把中文翻译回英文，假如翻译回来的英文和一开始的英文天差地别，那么这个两次翻译的结果肯定是很差的；反之，如果能够让翻译回来的英文和原本的一样，就相当于是改进了两次翻译的效果，CycleGAN利用这种方式来优化生成器。\n\n\n3. pix2pixHD，生成高分辨率、高质量的图像\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722122528208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n未来还会持续更新：\n\n- Conditional GAN\n- pix2pix\n- CycleGAN\n- GauGAN\n\n谢谢支持！","tags":["GAN"],"categories":["神经网络"]},{"title":"Pycharm 2020 中导入Anaconda3创建的环境","url":"/2020/11/24/213628/","content":"\n## 在pycharm配置环境Anaconda环境\n之前用的Anaconda3中的jupyter Lab写python程序，后来根据需要用到pycharm，又不想重新安装python库，直接用到Anaconda3中下载好的库该有多好，现在尝试用pycharm2020配置Anaconda3创建的环境。\n<!-- more -->\n\n\n\n\n假设pycharm2020和Anaconda3安装好了，现在就开始配置流程吧，选择File->Setting->Project Interpreter，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707153748538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n点击右上角齿轮->add![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707153849133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n选择Conda Environment->Existing enviroment\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707153952159.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n到这里只要选择你的python虚拟环境所在的目录就行了，不知道conda安装的python虚拟环境在哪里？这好办，在开始菜单打开Anconda Powershell Prompt\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707154203781.png)\n然后执行`jupyter kernelspec list`,就可以显示Anaconda所有的python内核环境,如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/202007071544150.png)\n不过这里显示的可不是python环境真正的目录，以上图第一个虚拟环境所示，在文件管理器中打开`D:\\install\\anaconda3\\share\\jupyter\\kernels\\python3`，可以找到文件kernel.json，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707154711688.png)\n上面划线部分才是真正python虚拟环境所在的目录，将`D:/install/anaconda3\\\\python.exe`应用到Interpreter所在位置即可\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707154847886.png)\n*注意：Make available to all projects:应用到所有项目，推荐勾选*\n\n目录加载之后，会自动加载该python环境下的包，如下所示，点击apply即可\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707155226636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n## 用配置好的环境新建项目\n在pycharm新建有两种方式，分别是New enviroment using 和 Existing interpreter，如下所示：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707155611812.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n- New enviroment using：用新环境创建项目\n- Existing interpreter：用已存在的环境创建项目\n\n###  用Existing interpreter方式新建项目\n\n我们先用Existing interpreter创建项目，项目命名为test4，创建完成后，导入torch（在Anaconda安装了pytorch，并没有在pycahrm中安装），新建一个tset.py，可以看到，程序完美运行。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707160012207.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\ntest.py运行结果如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707160621212.png)\n### 用New enviroment using方式新建项目\n\n- New environment using 设置新的依赖环境。它是pycharm自带的virtualenv创建项目，可以在项目目录中新建一个venv（virtualenv）目录，用于存放虚拟的python环境，这里所有的类库依赖都可以直接脱离系统安装的python独立运行。\n- Location：填写新环境的文件目录\n- Base interpreter下拉框：选择基础解释器，默认是环境中配置的，可以修改。\n- Inherit global site-packages：可以使用base interpreter（基础解释器）中的第三方库，可能会花费时间进行复制；如果不勾选将和外界完全隔离，会在base interpreter的基础上创建一个新的虚拟解释器。\n- Make available to all projects：是否将此虚拟环境提供给其他项目使用。勾选之后，可以提供给其他项目，等再新建下一个项目的时候，可以修改Base interpreter，位置指向现在建立的虚拟环境。\n\n看到上面的解释，我们大致对New enviroment using方式新建项目有所了解了。现在，新建一个项目，命名为test5。该方式创建项目的过程会比较慢，因为它要项目加载所需要类库\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707161010286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n可以看到，以这种方式新建的项目目录下，会多出一个venv目录（用于存放该项目用到的类库），如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707162417938.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n下图可以看出，以这种方式新建的项目，不能直接使用Anaconda的python环境\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707162630586.png)\n如果想要使用Anaconda的python环境，只需要将其Project Interpreter改为我们刚刚配置的Conda环境即可。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707162849405.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n从上面可以看出，用New enviroment using方式新建项目，不仅无法使用conda中的环境，还会自己生成一个用于存放python解释器的目录，所以创建的项目会非常大，不推荐使用。\n\n\n参考文档\n\n[pycharm创建工程的两种方式](https://www.cnblogs.com/xiaohailuo/p/11083211.html)\n[PyCharm新建项目教程](https://blog.csdn.net/Aidiying/article/details/104889259)\n[Pycharm 2020 中导入Anaconda3创建的环境](https://blog.csdn.net/qq_37555071/article/details/107182623)\n","categories":["工具"]},{"title":"hexo设置permalink以比避免url中出现中文","url":"/2020/11/24/204707/","content":"\n当我把hexo的博客标题的时候，url中就会出现中文，很不雅观，这里我通过permalink设置博客链接！\n\n\n\n<!-- more -->\n\n\n\n## 第一步\n\n在_config.yml文件中修改permalink\n\n```bash\n# permalink: :year/:month/:day/:title/ 这是之前的设置\npermalink: :year/:month/:day/:id/\npermalink_defaults:\n```\n\n## 第二步\n\n第一步的`:id`是自己添加的，因此需要在`scaffolds/post.md`中添加id，如下:\n\n```bash\ntitle: {{ title }}\nid: \ndate: {{ date }}\ncategories: Life  #文章分类\ntags: [tag1,tag2]  #文章标签，多标签时使用英文逗号隔开\n```\n\n我一般是把id设置为时分秒，如现在是`20:47:07`，我将id设置为`204707`\n\n\n\n\n\n参考文档\n\n[hexo设置permalink-避免url中出现中文](https://blog.csdn.net/weixin_30394669/article/details/97839708)  ","tags":["hexo"],"categories":["工具"]},{"title":"Anaconda中离线升级jupyterlab并为jupyterlab安装插件","url":"/2020/11/24/194523/","content":"\n## Anaconda中升级jupyterlab\n\n我之前尝试了如下两种方法，升级失败：\n\n- `conda update -c conda-forge jupyterlab`\n- 在Anaconda Navigator 界面升级\n\n后来直接在[anaconda官网](https://anaconda.org/)下载jupyterlab的安装文件，然后执行`conda install 文件名`就安装成功了。\n<!-- more -->\n\n\n\n\n首先，在[anaconda官网](https://anaconda.org/)下载文件时，在搜索栏输入jupyterlab，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200706193847540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n然后点击文件名，进如下页面，再点击Files就可以下载文件\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200706194046685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n最后，打开Anaconda Powershell Prompt（如果配置了环境变量，直接打开cmd也可以），`cd 文件所在目录`，执行`conda install 文件名`就成功啦。比如我下载的是jupyterlab-2.1.5-py_0.tar.bz2，执行`conda install jupyterlab-2.1.5-py_0.tar.bz2`即可，**安装成功后，会默认覆盖Anaconda自带的jupyterlab，所以就意味着升级了jupyterlab**，安装jupyterlab之后，并不能在shell命令窗口直接输入jupyterlab直接启动，但是可以在Anaconda Navigator界面启动。如果想要在shell窗口启动，则需要配置conda环境变量。\n\n**这个安装方法其实算是一类离线安装方法，无法通过命令安装的anaconda插件，都可以在[anaconda官网](https://anaconda.org/)下载之后离线安装**\n\n## 升级jupyterlab插件出现问题\n升级之和，之前在低版本jupyterlab下安装的一部分插件就可能过时，在Anaconda Powershell Prompt执行`jupyter labextension list`就可以查看插件的情况，如下所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200706195338601.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n遇见过时的插件，可以执行`jupyter labextensio update 插件名`或`jupyter labextension update --all`来更新插件，但是我尝试了之后，没有一个插件能够更新成功，索性就执行`jupyter labextensio uninstall 插件名`把过时的插件都卸载了，然后执行`jupyter labextensio install 插件名`安装需要的插件即可，比如我要安装jupyter的目录插件，可以执行：\n\n```powershell\njupyter labextension install @jupyterlab/toc\n或者\njupyter labextension install https://github.com/jupyterlab/jupyterlab-toc.git\n```\n\n## conda&jupyterlab插件相关命令\n### conda命令\n删除一个名为 mytest 的环境或库。-n为该环境或库的名字，--all 说明删除 mytest 环境下的所有内容，也就是这个环境被删除了：`conda remove -n mytest --all`\n\n删除一个库`conda remove 库名`,卸载一个库`conda uninstall 库名 --force`，根据帮助中的描述,这两个命令是一样的。假如如果你安装了tensorflow和numpy,想把numpy降级到另外一个版本。使用conda uninstall numpy会把tensorflow、pytorch等其他依赖numpy的库一起删除.此时加上conda uninstall numpy --force就仅卸载numpy了.一定要看看conda 的帮助.然后在安装需要的numpy版本。\n\n在不指定的情况下，conda install命令默认从 conda 官网 https://conda.anaconda.org/ 上下载。比如下面的，conda-forge 是一个用户，他上传了一个 opencv 的 python 库。opencv=3.2.0 指定了版本，不指定的情况下，下载最新版本：\n```powershell\nconda install -c conda-forge opencv=3.2.0\n```\n当然，你也可以使用 -c 参数，指定一个远程仓库，从这个仓库中下载：\n```powershell\nconda install -c https://conda.anaconda.org/menpo opencv3\n```\n\n**可以通过`anaconda-navigator --reset`解决Anaconda启动慢的问题**\n\n\n\n### jupyterlab插件命令\n\n- 更新插件：`jupyter labextensio update 插件名`\n- 更新所有插件：`jupyter labextension update --all`\n- 卸载插件：`jupyter labextensio uninstall 插件名`\n- 安装插件：`jupyter labextensio install 插件名`\n- 远程仓库安装插件：`jupyter labextension install 参考地址`\n- 安装制定版本插件：`jupyter labextensio install 插件名=版本号`\n- 查看已安装插件：`jupyter labextension list`\n\n\n\n\n\n\n参考文档\n\n[附录C：conda相关命令](https://www.jianshu.com/p/b2b46dd6332b)","categories":["工具"]},{"title":"数据预处理：归一化/标准化详解","url":"/2020/11/24/192901/","content":"\n## 前言\n\n一般而言，样本的原始特征中的每一维特征由于来源以及度量单位不同，其特征取值的分布范围往往差异很大，比如身高、体重、血压等它们的度量和分布范围往往是不一样的。当我们计算不同样本之间的欧氏距离时，取值范围大的特征会起到主导作用。这样，**对于基于相似度比较的机器学习方法（比如最近邻分类器），必须先对样本进行预处理，将各个维度的特征归一化到同一个取值区间，并且消除不同特征之间的相关性，才能获得比较理想的结果**。虽然神经网络可以通过参数的调整来适应不同特征的取值范围，但是会导致训练效率比较低。\n\n<!-- more -->\n\n\n\n\n\n## 归一化的必要性及价值\n现在假设一个只有一层的网络：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723211349432.png)\n我们知道，tanh 函数的导数在区间 [−2, 2] 上是敏感的，其余的导数接近于 0，tanh图像如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723211508510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n\n因此，如果 $w1x1 + w2x2 + b$ 过大或过小，都会导致梯度过小，难以训练。**为了提高训练效率，我们需要使$w1x1 + w2x2 + b$在 [−2, 2] 区间，我们需要将w1 设得小一点，比如在 [−0.1, 0.1] 之间。可以想象，如果数据维数很多时，我们很难这样精心去选择每一个参数**。因此，如果每一个特征的取值范围都在相似的区间，比如 [0, 1] 或者 [−1, 1]，那该多好啊，我们就不太需要区别对待每一个参数，减少人工干预。\n\n>我们经常见到，归一化、标准化、规范化，其实他们的含义是一样的，都是消除数据量纲带来的差异，加快模型的训练效率\n\n除了参数初始化之外，不同输入特征的取值范围差异比较大时，梯度下降法的效率也会受到影响。下图给出了数据归一化对梯度的影响。其中，图a为未归一化数据的等高线图。**取值范围不同会造成在大多数位置上的梯度方向并不是最优的搜索方向。当使用梯度下降法寻求最优解时，会导致需要很多次迭代才能收敛**。如果我们把数据归一化为取值范围相同，如图b所示，大部分位置的梯度方向近似于最优搜索方向。这样，**在梯度下降求解时，每一步梯度的方向都基本指向最小值，训练效率会大大提高**。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723212620723.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n## 归一化的方式\n归一化的方法有很多种，最常用的是最小最大归一化和标准归一化\n\n**最小最大归一化**使结果落到[0,1]区间，转换函数如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723213531858.png)\n\n*其中 min(x) 和 max(x) 分别是特征 x 在所有样本上的最小值和最大值。*\n\n**标准归一化**也叫 z-score 归一化，将每一个维特征都处理为符合标准正态分布（均值为 0，标准差为 1）。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200723214316862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70)\n*这里 σ 不能为 0。如果标准差为 0，说明这一维特征没有任务区分性，可以直接删掉。在标准归一化之后，每一维特征都服从标准正态分布。*\n\n## 总结\n\n总的来说，归一化的好处是：帮助你去除数据的量纲和数据大小的差异，让数据每一个特征的取值范围都在相似的区间，可以让数据在同一个数量级下来做一个比较。这样做可以让模型更快的收敛，因为它不需要去考虑那些夸大的特征，把所有特征的尺度看的同等重要\n\n参考文档\n神经网络与深度学习[邱锡鹏著]","categories":["数据预处理"]},{"title":"深入理解model.eval()与torch.no_grad()","url":"/2020/11/24/192512/","content":"\n我们用pytorch搭建神经网络经常见到model.eval()与torch.no_grad()，它们有什么区别？是怎么工作的呢？现在就让我们来探究其中的奥秘\n<!-- more -->\n\n## model.eval()\n\n- 使用model.eval()切换到测试模式，不会更新模型的k，b参数\n- 通知dropout层和batchnorm层在train和val中间进行切换\n在train模式，**dropout层会按照设定的参数p设置保留激活单元的概率（保留概率=p，比如keep_prob=0.8），batchnorm层会继续计算数据的mean和var并进行更新**\n在val模式下，**dropout层会让所有的激活单元都通过，而batchnorm层会停止计算和更新mean和var，直接使用在训练阶段已经学出的mean和var值**\n- model.eval()不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反向传播(backprobagation)\n\n## torch.no_grad()\n使用方法：\n```python\nwith torch.no_grad()：\n\t# 代码块\n```\n\n\n- 用于停止autograd模块的工作，起到加速和节省显存的作用（具体行为就是停止gradient计算，从而节省了GPU算力和显存）\n- 不会影响dropout和batchnorm层的行为\n\n\n`model.eval()`与`torch.no_grad()`可以同时用，更加节省cpu的算力\n\n## 思考\n\n在val模式下，为什么让dropout层所有的激活单元都通过，因为train阶段的dropout层已经屏蔽掉了一些激活单元，在val模式下，让所有的激活单元都通过还能预测数据吗?\n**在val模式下，让所有的激活单元都通过当然能预测数据了，相当于学习时限定你每次只能选择一份资料学，考试时开卷所有资料你都带着。val模式下，虽然让所有的激活单元都通过，但是对于各个神经元的输出， 要乘上训练时的删除比例后再输出。**","tags":["pytorch"],"categories":["pytorch"]},{"title":"使用VGG迁移学习开启《猫狗大战挑战赛》","url":"/2020/11/24/181519/","content":" 使用VGG迁移学习开启《猫狗大战挑战赛》，内容如下：\n一、前言\n二、加载数据集\n三、数据预处理\n四、构建VGG模型\n五、训练VGG模型\n六、保存与测试模型\n七、总结\n\n \n\n<!-- more -->\n\n\n\n\n\n# 一、前言\n\n猫狗大战挑战由Kaggle于2013年举办的，目前比赛已经结束，不过仍然可以把[AI研习社猫狗大战赛平台](https://god.yanxishe.com/41)作为练习赛每天提交测试结果，该平台数据集包含猫狗图片共24000张，没有任何标注数据，选手需要训练模型正确识别猫狗图片，**1= dog，0 = cat**。这里使用在 ImageNet 上预训练的 VGG 网络模型进行测试，因为原网络的分类结果是1000类，所以要进行迁移学习，对原网络进行 fine-tune （即固定前面若干层，作为特征提取器，只重新训练最后两层），并把测试结果提交到该平台。那么，现在就让我们开始吧。\n\n\n\n\n\n# 二、加载数据集\n\n前期如何把**解压后的竞赛数据集**放到colab上着实耗费了我大量的时间，我认为非常有必要把这个单独作为一章讲一下。如果你本地有很强的GPU，不需要在colab上跑代码，这章节可以忽略，由于我的电脑跑不动这么多数据，GPU也不行，所以只能在colab上运行。在这个过程中许多问题本是可以避免的，由于对一些操作和指令不熟练，导致许多时间白白流失，即打消了初学者的自信心，也拖慢了实验的进度，究其原因，主要有以下几点：\n\n1. 在google drive上传和解压数据集时间特别慢，需要数十个小时\n\n2. colab运行时间有时限，长时间不操作（大概20分钟左右）会导致当前训练的数据被回收\n\n3. 猫狗大战数据集是没有标签的，需要自己定义Dataset类加载数据\n\n现在就来一个个解决上面的几个痛点吧！\n\n**（1）colab上传和解压大数据集**\n\n我们的目的是要在colab上读取竞赛数据集的图片，达到目的的方式有三个：\n\n- 方式一：把数据集压缩包上传到google drive，在drive上解压\n- 方式二：数据集解压后再上传到google drive\n- 方式三：把数据集压缩包上传到google drive，在colab连接的虚拟机上解压\n\n上面几种方式哪个好呢？我先不直接说结果，来实验下吧！\n\n首先，采用方式一，把数据集压缩包上传到google drive，在drive上解压，操作很简单，在google drive上右键上传[竞赛数据集cat_dog.rar](https://static.leiphone.com/cat_dog.rar)，文件大小521MB，上传时间二十多分钟，上传完毕后，再drive上解压，现在痛点来了，**时间竟然要十几个小时**，具体操作如下：\n\n- 打开colab，挂载google drive，方法可以参考我的博客[Google Colab挂载drive上的数据文件](https://blog.csdn.net/qq_37555071/article/details/107544680)。\n\n- 解压drive上的cat_dog.rar文件，命令为\n\n  ```bash\n  ! apt-get install rar\n  !unrar x \"/content/drive/Colab/人工智能课/cat_dog.rar\" \"/content/drive/Colab/人工智能课/\"\n  ```\n\n  解压过程如下：\n\n  ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120155811.png)\n\n\n\n\n我大致算了一下，每张图片解压时间5秒钟左右，24000张图片要大约33小时啊！！！所以，这种方式直接pass掉。\n\n再来看，方式二，把数据集解压后再上传到google drive，解压后的数据集文件夹大小虽然只有五百多兆，但上传速度特别慢，大概要5至7个小时，**并且一旦中间断网或是网络不稳定，极有可能导致数据损坏**。我就是花费了大半天时间把所有解压后的文件上传完了，由于中间网络不稳定，导致数据读取不正确，最终这种方式也放弃了，哎，说多了都是泪！\n\n最后，就只有方式三了，把数据集压缩包上传到google drive，在colab连接的虚拟机上解压文件，方法是：\n\n- 将google drive上数据集文件cat_dog.rar拷贝到colab连接的虚拟机上\n  ```bash\n  !cp -i /content/drive/Colab/人工智能课/cat_dog.rar /content/\n  ```\n- 在虚拟机上解压压缩文件：\n  ```bash\n  ! apt-get install rar\n  ! unrar x cat_dog.rar\n  ```\n  运行过程如下：![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120162337.png)\n\n\n\n这种方式速度非常快，如果操作正确，**解压时间仅有一分钟左右**，非常值得推荐！\n\n**（2）阻止Colab自动掉线**\n\n在colab上训练代码，页面隔一段时间无操作之后就会自动掉线，之前训练的数据都会丢失。现在你体会到我之前连续几个小时在google drive解压数据集文件的艰辛路程了吧。不过好在最后终于找到了一种可以让其自动保持不离线的方法，用一个js程序自动点击连接按钮。代码如下：\n\n```js\nfunction ClickConnect(){\n  console.log(\"Working\"); \n  document\n    .querySelector(\"#top-toolbar > colab-connect-button\")\n    .shadowRoot\n    .querySelector(\"#connect\")\n    .click()\n}\n \nsetInterval(ClickConnect,60000)\n```\n\n使用方式是：按快捷键`ctrl+shift+i`，并选择`Console`，然后复制粘贴上面的代码，并点击回车，该程序便可以运行了，如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120163050.png)\n\n**（3）猫狗大战数据集是没有标签的，需要自己定义Dataset类才能加载数据**\n\n猫狗大战数据集是没有标签的，但是从其训练集和验证集的图片名字可以获取标签，这就需要我们自己定义Dataset类了，由于这个部分篇幅较多，我们放在下一章讲吧。\n\n\n\n\n\n\n\n# 三、数据预处理\n\n传统的mnist数据集是集成到`torchvision.datasets`，我们使用`datasets.MNIST`就可以方便加载数据，不用做过多的其它处理，而猫狗大战竞赛数据集是如下图方式，并没有用标签对文件夹分类存放，所以我们需要通过图片名称获取标签，并自定义Dataset类加载图片。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120201612.png)\n\n我定义的Dataset类如下所示：\n\n```python\nfrom torch.utils.data import Dataset,DataLoader\n# 创建自己的类：MyDataset,继承 Dataset 类\nclass MyDataset(Dataset):\n    def __init__(self, txt, data_path=None, transform=None, target_transform=None, loader=default_loader):\n        super(MyDataset, self).__init__()\n        file_path = data_path + txt\n        file = open(file_path, 'r', encoding='utf8')\n        imgs = []\n        for line in file:\n            line = line.split()\n            imgs.append((line[0],line[1].rstrip('\\n')))\n\n        self.imgs = imgs\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n        self.data_path = data_path\n\n    # 可以通过索引进行条用，如data[1]\n    def __getitem__(self, index):\n        # 按照索引读取每个元素的具体内容\n        imgName, label = self.imgs[index]\n        # imgPath = self.data_path + imgName\n        imgPath = imgName\n        # 调用那张图片读哪张，最大限度发挥GPU显存\n        img = self.loader(imgPath)\n        if self.transform is not None:\n            img = self.transform(img)\n            label = torch.from_numpy(np.array(int(label)))\n        return img, label\n\n    def __len__(self):\n        # 数据集的图片数量\n        return len(self.imgs)\n    \n# 定义读取文件的各式\ndef default_loader(path):\n    return Image.open(path).convert('RGB')\n```\n\n具体要加载图片数据还要进行几个处理，即事先准备好train、val数据集的路径和标签，以及test数据集的路径，然后使用`MyDataset`加载图片路径文件，最后就可以通过`torch.utils.data.DataLoader`加载图片数据了。具体步骤如下：\n\n（1）首先，读取cat_dog文件夹下的图片路径\n\n```python\n#读一个文件夹下的所有文件名称\ndef read_file_name(file_dir):\n    filename = []\n    for root, dirs, files in os.walk(file_dir):\n        filename = files #当前路径下所有非目录子文件\n        break #这里只要图片文件，执行一次即可退出\n    return filename\n```\n\n（2）然后将文件名格式化为竞赛要求的类型，这里cat标签为0，dog为1\n\n```python\n# 将文件名格式化为要求的类型，这里cat标签为0，dog为1\ndef format_inputAndlabel(file_dir):\n    format_result = []\n    filename = read_file_name(file_dir)\n    for n in filename:#cat为0，dog为1\n        if \"cat\" in n:\n            format_result.append(n+\" 0\")\n        else:\n            format_result.append(n+\" 1\")\n    return format_result\n```\n\n（3）分别传入train、test、val路径读取数据\n\n```python\n# 格式化读取train、test、val\nformat_train_result = format_inputAndlabel(\"cat_dog/train\")\nformat_test_result = format_inputAndlabel(\"cat_dog/test\")\nformat_val_result = format_inputAndlabel(\"cat_dog/val\")\n```\n\n（4）由于自定义的DataSet必须知道文件路径，所以先将格式化的文件名写入文件里，再用自定义的MyDataset读取\n\n```python\ndef convert_format(content):\n  result = []\n  for t in content:\n    v = t.split('.')\n    result.append(int(v[0]))\n  return result\n# 写入train、val文件\ndef write_file(path,file_prefix,content):\n  with open(path, 'w', encoding='utf8') as f:\n      for line in content:\n          f.write(file_prefix+line+'\\n')\n# 写入test文件，由于读取时候文件名是乱序的，因此要先排序\ndef write_test_file(path,test_file_prefix,content):\n  content=convert_format(content)\n  content.sort() #排序\n  with open(path, 'w', encoding='utf8') as f:\n      for line in content: \n          f.write(test_file_prefix+str(line)+'.jpg 0'+'\\n')# test文件没有标签，默认用0填充就行\n\n# 因为自定义的DataSet必须知道文件路径，所以先将格式化的文件名写入文件里，再用自定义MyDataset读取\nwrite_file(path=\"cat_dog/train.txt\",file_prefix=\"cat_dog/train/\",content=format_train_result)\nwrite_file(path=\"cat_dog/val.txt\",file_prefix=\"cat_dog/val/\",content=format_val_result)\nwrite_test_file(path=\"cat_dog/test.txt\",test_file_prefix=\"cat_dog/test/\",content=format_test_result)\n```\n\n（5）对数据进行预处理变换\n\n```python\nfrom torch.utils.data import Dataset,DataLoader\nimport torchvision.transforms as transforms\n# 预处理设置\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\ntrain_transformer = transforms.Compose([\n    transforms.Resize(256),\n    transforms.transforms.RandomResizedCrop((224), scale = (0.5,1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    normalize])\n\n# val和test是类似的，训练的时候可以多一些增强，这里只做验证就可以\nval_transformer = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    normalize\n])\n```\n\n（6）使用`MyDataset`加载图片路径文件\n\n```python\n# 数据集加载方式设置\ncmd_path='cat_dog/'\ntrainset = MyDataset(txt='train.txt',data_path=cmd_path,transform=train_transformer)\nvalset = MyDataset(txt='val.txt',data_path=cmd_path,transform=val_transformer)\ntestset = MyDataset(txt='test.txt',data_path=cmd_path,transform=val_transformer)\nprint('训练集：',trainset.__len__())\nprint('验证集：',valset.__len__())\nprint('测试集：',testset.__len__())\n\"\"\"\n输出：\n训练集： 20000\n验证集： 2000\n测试集： 2000\n\"\"\"\n```\n\n（7）使用`torch.utils.data.DataLoader`加载图片数据，并将其放入`dataloaders_dict`\n\n```python\nbatchsize=128\n# 构建DataLoader\ntrain_loader = DataLoader(trainset, batch_size = batchsize, drop_last = False, shuffle = True)\n## val_loader和train_loader不做shuffle\nval_loader = DataLoader(valset, batch_size = batchsize, drop_last = False, shuffle = False)\ntest_loader = DataLoader(testset, batch_size = batchsize, drop_last = False, shuffle = False)\ndataloaders_dict = {'train':train_loader,'val':val_loader,'test':test_loader}\n```\n\n最终，数据集文件被放入`dataloaders_dict`，后面就可以通过该字典方便的传入相应的数据集了。\n\n\n\n# 四、构建VGG模型\n\nVGG 模型如下图所示，主体由三种元素组成：\n\n- 卷积层（CONV）是发现图像中局部的 pattern\n- 全连接层（FC）是在全局上建立特征的关联\n- 池化（Pool）是给图像降维以提高特征的 invariance(不变性)\n\n关于VGG模型的更详细介绍，可以参考我的博客[深入解读VGG网络结构](https://blog.csdn.net/qq_37555071/article/details/108199352)\n\n![VGG](http://fenggao-image.stor.sinaapp.com/20191006215625.jpg)\n\n\n\n默认情况下，当我们加载预训练的模型时，所有参数都具有`requires_grad = True`，如果我们从头开始或进行微调训练就不用更改。但是，如果我们要进行特征提取，并且只想为新初始化的图层计算梯度，那么我们希望所有其他参数都不需要梯度更新，需要用`set_parameter_requires_grad`函数将模型中参数的requires_grad属性设置为False，具体如下：\n\n```python\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n```\n\n这里我使用预训练好的VGG模型进行迁移学习，只想更新最后一层的参数，并且希望所有其他参数都不需要梯度更新，所以要用`set_parameter_requires_grad`函数将模型最后一层参数的requires_grad属性设置为False，由于猫狗大战数据集是二分类，需要把最后的`nn.Linear` 层由1000类，替换为2类。如下：\n\n```python\ndef initialize_model(num_classes, feature_extract, use_pretrained=True):\n    # 初始化模型变量\n    model_vgg = None\n    # 加载预训练模型\n    model_vgg = models.vgg16(pretrained=use_pretrained)\n    # 更改输出层\n    set_parameter_requires_grad(model_vgg, feature_extract)\n    model_vgg.classifier[6] = nn.Linear(4096, num_classes)\n    model_vgg.classifier.add_module('7',torch.nn.LogSoftmax(dim = 1))\n    return model_vgg\n\nmodel_vgg_new = initialize_model(num_classes=2,feature_extract = True,use_pretrained=True)\nprint(model_vgg_new.classifier)\n```\n\n输出`model_vgg_new`的`classifier`层，如下所示，可以看到最后一层全连接输出为2，并且使用`LogSoftmax`为output层。\n\n```tex\nSequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=2, bias=True)\n  (7): LogSoftmax(dim=1)\n)\n```\n\n\n\n# 五、训练VGG模型\n\n训练定义好的VGG模型，即训练最后一层全连接层，具体操作步骤如下：\n\n（1）创建损失函数和优化器\n\n损失函数 `NLLLoss()` 的输入是一个对数概率向量和一个目标标签，它不会为我们计算对数概率，适合最后一层是`log_softmax()`的网络。Adam优化器是目前性能比较好的优化器之一，因此这里采用Adam。\n\n```python\n'''\n第一步：创建损失函数和优化器\n'''\n# 损失函数\ncriterion = nn.NLLLoss()\n# 学习率\nlr = 0.001\n# 优化器\noptimizer_vgg = torch.optim.Adam(model_vgg_new.classifier[6].parameters(),lr = lr)\n```\n\n（2）判断是否存在GPU设备，并将model切换到相应的device\n\n```py\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('Using gpu: %s ' % torch.cuda.is_available())\nmodel_vgg_new.to(device)\n```\n\n（3）训练模型\n\n这里我定义了一个`train_model`训练的方法，并将验证集上结果最好的一次训练存储下来，为了减少训练时间，我把`epoch`设置为4\n\n```python\n'''\n第三步：训练模型\n'''\ndef train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n    val_acc_history = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # 每个epoch都进行训练和验证\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # 将模型设置为训练模式\n            else:\n                model.eval()   # 将模型设置为验证模式\n\n            running_loss = 0.0 # 记录训练时的loss下降过程\n            running_corrects = 0\n\n            # 遍历数据\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                # 梯度初始化\n                optimizer.zero_grad()\n                # 前向传播\n                outputs = model(inputs)\n                loss = criterion(outputs, labels.long())\n                # 得到预测结果\n                _, preds = torch.max(outputs, 1)\n                # 仅在训练时更新梯度，反向传播，backward + optimize\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # 将验证集上结果最好的一次训练存储下来\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history\n\n# 训练\nmodel_new_vgg, hist = train_model(model_vgg_new, dataloaders_dict, criterion, optimizer_vgg, num_epochs=4)\n\n```\n\n经过4次epoch，输出的记录如下，可以看到虽然训练次数不多，但是在验证集上效果还是很不错的\n\n```python\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nplt.title(u\"val acc plot\")\nplt.xlabel(u\"epoch\")\nplt.ylabel(u\"val acc\")\nacc= hist\nplt.xticks(range(len(acc)))\nplt.plot(acc)\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120204636.png)\n\n\n\n\n\n\n\n# 六、保存与测试模型\n\n（1）保存训练好的模型\n\npytorch保存和加载模型有两种方式，**不同的保存方式对应不同的读取方式**，两者各有利弊。\n\n方式一：直接保存整个模型\n\n```python\ntorch.save(model_new_vgg, 'model_new_vgg.pt')\nmodel_new_vgg = torch.load('model_new_vgg.pt')\n```\n\n方式二：只保存模型中的参数\n\n```python\nmodel = initialize_model(num_classes=2,feature_extract = True,use_pretrained=True)\nmodel.to(device)\nmodel.load_state_dict(torch.load(\"model_new_vgg.pt\"))\n```\n\n可以看到，用第一种方法能够直接保存模型，加载模型的时候直接把读取的模型给一个参数就行。而第二种方法则只是保存参数，在读取模型参数前**要先定义一个模型**（模型必须与原模型相同的构造），然后对这个模型导入参数。虽然麻烦，但是可以**同时保存多个模型**的参数，而第一种方法则不能，而且第一种方法**有时不能保证模型的相同性**（你读取的模型并不是你想要的）。所以，这里我采用第二种方式来保存并加载模型。\n\n（2）对模型进行测试\n\n接下来就要用test数据集对模型进行测试了，把测试结果保存到`pred_outputs`，具体如下：\n\n```python\ndef test_model(model, test_loader):\n    model.eval() #把训练好的参数冻结\n    total,correct = 0,0\n    pos = 0\n    pred_outputs= np.empty(len(test_loader.dataset),dtype=np.int)\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            pred_outputs[pos:pos+len(preds)]=preds.cpu().numpy()\n            pos += len(preds)\n    return pred_outputs\n\npred_outputs = test_model(model,dataloaders_dict['test'])\n```\n\n（3）将测试结果写入`cat_dog_result.csv`\n\n```python\nwith open(\"cat_dog_result.csv\", 'w') as f:\n    for i in range(len(test_loader.dataset)):\n        f.write(\"{},{}\\n\".format(i, pred_outputs[i]))\n```\n\n因为我是在colab环境上训练的，还要把`cat_dog_result.csv`拷贝到google drive才能下载，命令如下：\n\n```bash\n!cp -i /content/cat_dog_result.csv /content/drive/\n```\n\n（4）提交测试结果\n\n把`cat_dog_result.csv`提交到[AI研习社猫狗大战--经典图像分类题](https://god.yanxishe.com/41)，现在就让我们见证奇迹的时刻吧！\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201120181445.png)\n\n可以看到，只训练了4次epoch，测试就达到了98.9的准确率，把epoch设置得更大，结果应该会更好，由于时间原因，就不训练了。\n\n\n\n\n\n# 七、总结\n\n从加载猫狗大战竞赛数据集到colab上，到测试完模型并提交，我大概花费了几天的时间，并且主要时间不是用在定义模型和调参上，而是如何处理数据上。我认为这次的收获还是很大的，因为我知道了**如何以最快最有效的方式在colab上加载要训练的数据**，并定义了自己Dataset类，**以后对于任何类型、任何格式的训练数据，我应该都能定义相应Dataset类并且去处理它**。这次我用了近三天，下次可能一个小时不到就搞定了，这难道不是一个巨大的进步吗？此外，我通过预训练好的VGG模型进行迁移学习，训练了猫狗大战数据集，仅训练了4次epoch，测试数据就达到了98.9的准确率，说明预训练好的VGG模型是非常容易学习的，以后再遇到类似的识别分类任务，就不需要从头开始训练了，真的是非常快速又方便。\n\n最后，附上我的colab共享地址：https://drive.google.com/file/d/1t-DVQwo92dBuy3JgNhdYFD_CndwyBE3U/view?usp=sharing\n\n里面格式有点乱，但是内容一点都不少哦！","tags":["VGG"],"categories":["神经网络"]},{"title":"Hexo博客发布和配置的一些常用命令","url":"/2020/11/24/165533/","content":"\nHexo博客发布和配置的一些常用命令，在此记录！如果遇到新的且使用的命令，会不端完善！\n<!-- more -->\n\n\n\n## 基本命令\n\n`hexo init`  \n初始化站点，生成一个简单网站所需的各种文件。\n\n`hexo clean == hexo c`  \n清除缓存 网页正常情况下可以忽略此条命令\n\n`hexo generate == hexo g`  \n生效新增、修改、更新的文件\n\nHexo 能够监视文件变动并立即重新生成静态文件，在生成时会比对文件的 SHA1 checksum，只有变动的文件才会写入。`hexo generate --watch`\n\n\n\n`hexo server == hexo s`  \n启动本地网站，可在本地观察网站效果，同时也可以输入`http://localhost:4000/admin`管理文章\n\n`hexo s --draft`\n这个发布时可以预览草稿\n\n`hexo s --debug`  \n以调试模式启动本地网站，在此模式下，对文件的更改无需停止网站只需刷新即可看到效果，调试非常方便\n\n\n`hexo clean && hexo s`  \n一次执行两个命令\n\n`hexo deploy == hexo d`  \nhexo的一键部署功能，执行此命令即可将网站发布到配置中的仓库地址，执行此命令前需要配置站点配置文件_config.yml\n\n**一键本地启动**：`hexo clean && hexo g && hexo s`\n\n**一键部署**：`hexo clean && hexo g && hexo d`\n\n您可执行下列的其中一个命令，让 Hexo 在生成完毕后自动部署网站，两个命令的作用是相同的。\n\n```\n$ hexo generate --deploy\n$ hexo deploy --generate  或 hexo g -d or hexo d -g\n```\n\n## 创建和发布文章\n\n`hexo new [layout] <title>`\n新建一篇新文章，会自动按照模板里面的格式创建文章\n\n里面的布局（layout），默认为 post，布局共有三种：\n\n```tex\npost\tsource/_posts\npage\tsource\ndraft\tsource/_drafts\n```\n\n**发布草稿命令：**\n\n1. `hexo publish 文章文件名`\n2. 或者是手动将`_drafts`目录下的草稿移动到`_posts`目录下即可发布草稿为正式文章。\n\n\n\n## PicGO图床快捷键\n快捷键为：`ctrl+shift+p`\n\n\n\n## Hexo博客头部配置\n\n（1）文章置顶\n\n在文章的 Front-Matter 中，使用 `top: true` 来实现置顶。在文章的 Front-Matter 中，使用 `top: true` 来实现置顶。\n\n（2）自定义样式\n\n如果你想修改主题的样式，推荐将样式代码添加到 `source/css/_custom` 目录下的 `index.styl` 文件中。这样，当主题更新时，不会覆盖你已经修改了的样式代码。\n\n> 当然，你也可以进行模块化分类：在该目录下新建样式文件，然后通过 `@import xxx` 语句在同目录下的 `index.styl` 文件中引入你新建的样式文件。\n\n（3）文章左侧目录\n\n启用文章目录后，默认对所有文章页面生效。你可以在 Front-Matter 中，设置 `toc: false` 来指定某篇文章不启用该功能。\n\n（4）文章业内目录\n\n`@[TOC]( )`这个写到文章页面内任何一个地方即可\n\n更多详细设置，请参考[ hexo-theme-stun](https://theme-stun.github.io/docs/zh-CN/)\n\n","categories":["工具"]},{"title":"Typora需要注意的换行符","url":"/2020/11/24/104944/","content":"\n在Typora中一定要换行符，包括普通换行，以及整个段落的换行，这些和普通的markdown编辑器是不太一样的\n\n\n\n\n\n<!-- more -->\n\n（1）首先，Typora中如果只按`Enter`键，**则把其看做另起一个段落**，段落之间的间距是比较大的，从Typora的界面上就可以看出来，如下图所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124110809.png)\n\n（2）其次，如果要只换行，而不另起一个段落，则需要使用`<br/>`或`<br/>`或按下`Shift`+`Enter`键，这时候不会另起一段，两行仍然在一个段落里面，此时间距是比较小的，如下所示：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124111640.png)\n\n\n\n（3）另外，需要注意的是，对于**有序或无序排列**，如果要在一个排列里面写多行东西，不要只按`Enter`键，这样会另起一段，Typora会使所有的排序当做单独的一段，在源码模式下可以看到如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124111559.png)\n\n（4）可以采用`<br/>`或`<br/>`或按下`Shift`+`Enter`键的方式另起一行，这就所有的排序就在一段内了，如下：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124111952.png)\n\n\n\n（5）先按下两个空格，再按下`Shift`+`Enter`键，会出现如下符号，我认为这和只用`Shift`+`Enter`是一样的效果，目前还没发现什么问题，以后若发现区别会再补充。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124132953.png)\n\n\n\n（6）需要注意的是，一定要勾选，菜单栏中编辑->空格与换行->保留单独的换行符，如下图所示，这样才能使用`Shift`+`Enter`键的方式另起一行，如果不勾选，只能用`<br/>`或`<br/>`的方式另起一行。\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124112415.png)\n\n","categories":["工具"]},{"title":"使用pytorch的auto_grad实现线性模型对mnist数据集多分类","url":"/2020/07/18/181520/","content":"\n使用pytorch的auto_grad实现线性模型对mnist数据集多分类，选取mnist100张图片，前80张为测试集，后20张为训练集，eporch 500次\n\n\n## 知识储备\n\n使用多个线性模型进行多分类 原理：每一个线性模型做二分类  \n多个线性模型 = 感知机，实质就是每一个线性模型做二分类\n\n<!-- more -->\n\n## 数据加载&归一化\n\n\n```python\nimport torch\nfrom mnist import MNIST\nimport numpy as np\nimport pdb\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nmndata = MNIST('dataset/python-mnist')\nimage_data_all, image_label_all = mndata.load_training()\nimage_data = image_data_all[0:100]\nimage_data = np.array(image_data,dtype = np.float)/255\nimage_label = image_label_all[0:100]\nimage_label = np.array(image_label,dtype = np.int)\nprint(image_data.shape,image_label.shape)\n```\n\n    (100, 784) (100,)\n\n\n## 定义模型\n\n\n```python\ndef model(image_data_one,weights,bias):\n    \"\"\"\n    这里直接使用图片本身作特征，也可以提取features后传入模型中\n    \"\"\"\n    # image_data_one转化为二维\n    xt = torch.from_numpy(image_data_one.reshape(1,28*28))\n    y = xt.mm(weights)+bias\n    return y\n\ndef get_acc(image_data,image_label,weights,bias,start_i,end_i):\n    correct = 0\n    # 这里可以不加，因为loss计算于此无关\n    with torch.no_grad():\n        for i in range(start_i,end_i):\n            y = model(image_data[i],weights,bias)\n            # 获取第i张图片的label\n            gt = image_label[i]\n            # 获取与y最近接的label值\n            pred = torch.argmin(torch.from_numpy(np.array([torch.min((torch.abs(y-j))).item() for j in range(0,10)]))).item()\n            if gt == pred:\n                correct += 1\n    # 确保万一，除法分子或分母一个指定为float        \n    return float(correct/float(end_i-start_i))\n```\n\n\n```python\n#显示训练集和测试集精度变换\ndef show_acc(train_accs,test_accs):\n    plt.figure(figsize = (10,4))\n    plt.title('train_accs and test_accs')\n    plt.plot(np.arange(len(train_accs)), train_accs, color='green', label='train_accs')\n    plt.plot(np.arange(len(test_accs)), test_accs, color='red', label='test_accs')\n    plt.legend() # 显示图例\n    plt.xlabel('index')\n    plt.ylabel('accs')\n    plt.show()\n```\n\n\n```python\ndef train_model(image_data,image_label,weights,bias,lr):\n    loss_value_before=1000000000000000.\n    loss_value=10000000000000.\n    train_accs = []\n    test_accs = []\n    for epoch in range(0,500): \n        loss_value_before=loss_value\n        loss_value=0\n        for i in range(0,80):\n            y = model(image_data[i],weights,bias)\n            # 获取第i张图片的label\n            gt = image_label[i]\n            # 只关心一个值，更新的时候也只更新对应线性模型的weights和bias\n            loss = torch.sum((y[0,gt:gt+1]-gt).mul(y[0,gt:gt+1]-gt))\n            loss_value += loss.data.item()\n            loss.backward()\n            weights.data.sub_(weights.grad.data*lr)\n            weights.grad.data.zero_()\n            bias.data.sub_(bias.grad.data*lr)\n            bias.grad.data.zero_()            \n\n        train_acc = get_acc(image_data,image_label,weights,bias,0,80)\n        test_acc = get_acc(image_data,image_label,weights,bias,80,100)\n        train_accs.append(train_acc)\n        test_accs.append(test_acc)\n        #print(\"epoch=%s,loss=%s/%s,train/test_acc=%s/%s,\"%(epoch,loss_value,loss_value_before,train_acc,test_acc))\n    show_acc(train_accs,test_accs)\n```\n\n## 训练\n\n\n```python\nweights = torch.randn(28*28,10,dtype = torch.float64,requires_grad = True)\nbias = torch.zeros(10,dtype = torch.float64,requires_grad = True)\nlr = 1e-3\n# 对模型进行训练：\ntrain_model(image_data,image_label,weights,bias,lr)    \n```\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200718181909.png)\n\n> 由于样本少，导致程序过拟合，结果是训练集精度高，测试集精度低。\n","tags":["pytorch","MLP"],"categories":["project实战"]},{"title":"numpy和torch数据类型转化","url":"/2020/07/18/164540/","content":"\n在实际计算过程中，float类型使用最多，因此这里只介绍numpy和torch数据float类型转化，其他类型同理。\n<!-- more -->\n\n## numpy数据类型转化\n\n- numpy使用astype转化数据类型，float默认转化为64位，可以使用`np.float32`指定为32位\n\n\n```python\n#numpy转化float类型\na= np.array([1,2,3])\na = a.astype(np.float)\nprint(a)\nprint(a.dtype)\n```\n\n`[1. 2. 3.]`  \n`float64`\n    \n\n- 不要使用a.dtype指定数据类型，会使数据丢失\n\n\n```python\n#numpy转化float类型\nb= np.array([1,2,3])\nb.dtype= np.float32\nprint(b)\nprint(b.dtype)\n```\n\n`[1.e-45 3.e-45 4.e-45]`  \n`float32`\n    \n\n- 不要用float代替np.float，否则可能出现意想不到的错误\n- 不能从np.float64位转化np.float32，会报错\n- np.float64与np.float32相乘，结果为np.float64\n\n> 在实际使用过程中，可以指定为np.float，也可以指定具体的位数，如np.float，不过直接指定np.float更方便。\n\n## torch数据类型转化\n\n- torch使用`torch.float()`转化数据类型，float默认转化为32位，torch中没有`torch.float64()`这个方法\n\n\n```python\n# torch转化float类型\nb = torch.tensor([4,5,6])\nb = b.float()\nb.dtype\n```\n\n\n\n\n    torch.float32\n\n\n\n- `np.float64`使用`torch.from_numpy`转化为torch后也是64位的\n\n\n```python\nprint(a.dtype)\nc = torch.from_numpy(a)\nc.dtype\n```\n\n`float64`  \n`torch.float64`\n\n\n\n- 不要用float代替torch.float，否则可能出现意想不到的错误\n- torch.float32与torch.float64数据类型相乘会出错，因此相乘的时候注意指定或转化数据float具体类型\n\n> np和torch数据类型转化大体原理一样，只有相乘的时候，torch.float不一致不可相乘，np.float不一致可以相乘，并且转化为np.float64\n","tags":["pytorch","numpy"]},{"title":"Google Colab挂载drive上的数据文件","url":"/2020/07/18/103558/","content":"\nGoogle Colab是完全云端的，所以，每次如果想让他访问谷歌云盘的内容，必须要先进性授权操作，直接在colab的jupyter中进行绑定授权操作\n\n**每次在Google Colab中打开notebook文件时，都必须重新执行命令获得授权。**\n<!-- more -->\n\n## 获取授权脚本代码\n```bash\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n!apt-get update -qq 2>&1 > /dev/null\n!apt-get -y install -qq google-drive-ocamlfuse fuse\nfrom google.colab import auth\nauth.authenticate_user()\nfrom oauth2client.client import GoogleCredentials\ncreds = GoogleCredentials.get_application_default()\nimport getpass\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\nvcode = getpass.getpass()\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n```\n**期间会输入两次授权码，点击相应链接复制即可**\n\n## 挂载到drive上\n```bash\n!mkdir -p drive\n!google-drive-ocamlfuse drive\n```\n\n## 切换到工作文件夹\n\n```python\n# 指定当前的工作文件夹\nimport os\n# google drive中的文件路径为/content/drive\nos.chdir(\"/content/drive/Colab\") \n```\n也可以使用`%cd`切换工作路径，推荐使用\n\n\n## 几个常用命令\n\n- `%cd`切换工作目录\n- `!ls`查看当前目录下的文件\n- `!pwd`查看当前的工作路径\n\n\n\n参考文档\n\n[谷歌云盘Colaboratory如何载入文件](https://blog.csdn.net/Einstellung/article/details/81006408)  \n","tags":["colab"],"categories":["工具"]},{"title":"使用numpy实现逻辑回归对IRIS数据集二分类","url":"/2020/07/16/175900/","content":"使用numpy实现逻辑回归对IRIS数据集二分类，使用对数似然损失(Log-likelihood Loss)，并显示训练后loss变化曲线。\n\n知识储备如下：\n- 逻辑回归Logistic Regression\n- 对数似然损失\n- IRIS数据集介绍\n- np.concatenate使用\n<!-- more -->\n\n\n## 知识储备\n\n### 逻辑回归Logistic Regression\n\n$$y = \\frac{1}{1+e^{-(wx+b)}}$$\n名字虽然叫回归，但是一般处理的是分类问题，尤其是二分类，比如垃圾邮件的识别，推荐系统，医疗判断等，因为其逻辑与实现简单，在工业界有着广泛的应用。\n\n__优点__：\n\n* 实现简单，计算代价不高，易于理解和实现, 广泛的应用于工业问题上；\n* 分类时计算量非常小，速度很快，存储资源低；\n\n__缺点__：\n\n* 容易欠拟合，当特征空间很大时，逻辑回归的性能不是很好；\n* 不能很好地处理大量多类特征或变量；\n\n### 对数似然损失\n对数损失, 即对数似然损失(Log-likelihood Loss), 也称逻辑斯特回归损失(Logistic Loss)或交叉熵损失(cross-entropy Loss), 是在概率估计上定义的。它常用于(multi-nominal, 多项)逻辑斯特回归和神经网络,以及一些期望极大算法的变体,可用于评估分类器的概率输出。可参考[对数损失函数(Logarithmic Loss Function)的原理和 Python 实现](https://www.cnblogs.com/klchang/p/9217551.html)了解详情\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716170728.png)\n\n损失函数:\n\n\n$$L=\\frac{1}{m}*\\sum_i^m -y_ilog(f(x_i))-(1-y_i)log(1-f(x_i))$$\n梯度计算：\n$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m}X^T*(f(x)-y)$$\n\n权重更新：\n$$w = w -\\alpha\\frac{\\partial L}{\\partial w}$$\n\n### IRIS数据集介绍\n\n该数据集包含4个特征变量，1个类别变量。iris每个样本都包含了4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，以及1个类别变量（label）。详情见[加载数据](#加载数据)\n\n### np.concatenate使用\n\n\n```python\na = np.array([[1, 2],[3, 4]])\nb = np.array([[5, 6]])\nnp.concatenate((a, b), axis = 0)\n```\n\n\n\n\n    array([[1, 2],\n           [3, 4],\n           [5, 6]])\n\n\n\n\n```python\nnp.concatenate((a, b.T), axis = 1)\n```\n\n\n\n\n    array([[1, 2, 5],\n           [3, 4, 6]])\n\n\n\n## 加载数据\n\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_iris\n%matplotlib inline\n```\n\n\n```python\ndataset = load_iris()\ninputs = dataset['data']\ntarget = dataset['target']\nprint('inputs.shape:', inputs.shape)\nprint('target.shape:', target.shape)\n# 三个类别\nprint('labels:', set(target))\n```\n\n    inputs.shape: (150, 4)\n    target.shape: (150,)\n    labels: {0, 1, 2}\n\n\n\n```python\ntarget\n```\n\n\n\n\n    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\n\n```python\nvalues = [np.sum(target == 0), np.sum(target == 1), np.sum(target == 2)]\nplt.pie(values,labels=[0, 1, 2], autopct = '%.1f%%')\nplt.show()\n```\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716171020.png)\n\n\n关于参数train_test_split的`random_state`的解释：  \n>Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.   \nrandom_state即随机数种子，**目的是为了保证程序每次运行都分割一样的训练集和测试集。否则，同样的算法模型在不同的训练集和测试集上的效果不一样。**\n\n\n```python\nfrom sklearn.model_selection import train_test_split\n# 只取前两类， 做二分类\ntwo_class_input = inputs[:100]\ntwo_class_target = target[:100]\nx_train, x_test, y_train, y_test = train_test_split(\n                    two_class_input,two_class_target,\n                    test_size = 0.3,\n                    random_state = 0)\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n```\n\n    (70, 4) (30, 4) (70, 1) (30, 1)\n\n\n\n```python\n# add one feature to x\nx_train = np.concatenate([x_train, np.ones((x_train.shape[0], 1))], axis = 1)\nx_test = np.concatenate([x_test, np.ones((x_test.shape[0], 1))], axis = 1)\nprint(x_train.shape, x_test.shape)\n```\n\n    (70, 5) (30, 5)\n\n\n## 定义模型\n\n\n```python\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\nx = np.arange(-10, 10, step = 0.1)\nfig, ax = plt.subplots(figsize = (8, 4))\nax.plot(x, sigmoid(x), c = 'green')\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x28f77053348>]\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716171106.png)\n\n\n\n```python\ncompute_loss = lambda pred_y, y: np.mean(-y * np.log(pred_y)-(1-y) * np.log(1-pred_y))\n# weight and bias init\nw = np.random.randn(5, 1)\n# 上一个loss\nlosses = []\nlast_loss = 10000\npred_y =sigmoid(np.dot(x_train, w))\n# 当前loss\nnow_loss = compute_loss(pred_y, y_train)\ni = 0\nwhile abs(now_loss - last_loss)>1e-4:\n    last_loss = now_loss\n    i = i + 1\n    # 计算梯度\n    grad = x_train.T.dot((pred_y - y_train)) / len(y_train)\n    # 更新梯度\n    w = w - 0.001 * grad\n    \n    # 前导计算\n    pred_y = sigmoid(np.dot(x_train, w))\n    now_loss = compute_loss(pred_y, y_train)\n    losses.append(now_loss)\nfig, ax = plt.subplots(figsize = (10, 4))\nax.plot(np.arange(len(losses)), losses, c = 'r')\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x28f77053508>]\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716171124.png)\n\n\n## 测试样例\n\n\n```python\n# 测试\ntest_pred = sigmoid(np.dot(x_test, w))\npre_test_y = np.array(test_pred > 0.5, dtype = np.float32)\nacc = np.sum(pre_test_y == y_test) / len(y_test)\nprint(\"the accary of model is {}\".format(acc*100))\n```\n\n    the accary of model is 100.0\n\n\n\n```python\nprint(pre_test_y.reshape(1,-1))\nprint(y_test.reshape(1,-1))\n```\n\n`[[0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n      0. 0. 0. 1. 1. 1.]]`   \n`[[0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1]]`\n    \n","tags":["numpy","逻辑回归"],"categories":["project实战"]},{"title":"使用numpy实现线性模型预测boston房价","url":"/2020/07/16/115900/","content":"使用numpy实现线性模型预测boston房价，激活函数为Relu，使用MSE_loss，手动求导，并显示训练后loss变化曲线。\n\n知识储备如下：\n- Scikit-learn\n- boston房价数据解读\n- 标准差公式\n- Linear及MSE_loss求导公式\n<!-- more -->\n\n## 知识储备\n\n### Scikit-learn\n\nScikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。其优点为：\n- 简单高效的数据挖掘和数据分析工具\n- 让每个人能够在复杂环境中重复使用\n- 建立NumPy、Scipy、MatPlotLib之上\n\n安装方法 pip install scikit-learn\n\n### boston房价数据解读\n\n使用sklearn.datasets.load_boston即可加载相关数据。该数据集是一个回归问题。每个类的观察值数量是均等的，共有506个观察，13个输入变量和1个输出变量。每条数据包含房屋以及房屋周围的详细信息。其中包含城镇犯罪率，一氧化氮浓度，住宅平均房间数，到中心区域的加权距离以及自住房平均房价等等，具体如下：\n- CRIM：城镇人均犯罪率。\n- ZN：住宅用地超过 25000 sq.ft. 的比例。\n- INDUS：城镇非零售商用土地的比例。\n- CHAS：查理斯河空变量（如果边界是河流，则为1；否则为0）。\n- NOX：一氧化氮浓度。\n- RM：住宅平均房间数。\n- AGE：1940 年之前建成的自用房屋比例。\n- DIS：到波士顿五个中心区域的加权距离。\n- RAD：辐射性公路的接近指数。\n- TAX：每 10000 美元的全值财产税率。\n- PTRATIO：城镇师生比例。\n- B：1000（Bk-0.63）^ 2，其中 Bk 指代城镇中黑人的比例。\n- LSTAT：人口中地位低下者的比例。\n- MEDV：自住房的平均房价，以千美元计。\n- 预测平均值的基准性能的均方根误差（RMSE）是约 9.21 千美元。\n\n### 标准差公式\n\n如x1,x2,x3...xn的平均数为M，则方差可表示为：\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125539.png)\n\n样本标准差=方差的算术平方根=s=sqrt(((x1-x)^2 +(x2-x)^2 +......(xn-x)^2)/(n-1) )  \n总体标准差=σ=sqrt(((x1-x)^2 +(x2-x)^2 +......(xn-x)^2)/n )  \n如是总体，标准差公式根号内除以n  \n如是样本，标准差公式根号内除以（n-1)。  \n因为我们大量接触的是样本，所以普遍使用根号内除以（n-1)。  \n\n\n```python\na=np.array([[1,2,3],[4,5,6]])\nnp.mean(a,axis=1)\n```\n\n\n\n\n    array([2., 5.])\n\n\n\n\n```python\n# axis = 1表示行，ddof = 1是除以n-1\nnp.std(a, axis = 1,ddof = 1) \n```\n\n\n\n\n    array([1., 1.])\n\n\n\n\n```python\n# ddof默认为0,是除以n\nnp.std(a, axis = 1) \n```\n\n\n\n\n    array([0.81649658, 0.81649658])\n\n\n\n\n```python\n# 求所有数平均值\nnp.mean(a)\n```\n\n\n\n\n    3.5\n\n\n\n### Linear及MSE_loss求导公式\n\n损失函数\n$L = \\frac{1}{2N}\\sum_{i=1}^{N}(z^{i} - y^{i})^{2}$\n\n线性函数\n$z^i = \\sum_{j=0}^{N}x_j^{(i)}w^{(j)} + b^{(j)}$\n\n对 $w$ 偏导，得到$w$ 更新梯度\n$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{N}\\sum_{i}^{N}(z^{(i)} - y^{(i)})x_j^{(i)}$\n\n对 $b$ 偏导，得到$b$ 更新梯度\n$\\frac{\\partial L}{\\partial b} = \\frac{1}{N}\\sum_{i}^{N}(z^{(i)} - y^{(i)})$\n\n## 数据加载\n\n\n```python\nfrom sklearn.datasets import load_boston\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n\n```python\ndata = load_boston()\nX_ = data['data']\ny = data['target']\nprint(type(data), type(X_), type(y))\nprint('data keys:', data.keys())\nprint('X_.shape:', X_.shape)\nprint('y.shape:', y.shape)\n```\n\n\n`<class 'sklearn.utils.Bunch'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> ` \n`data keys: dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])  `\n`X_.shape: (506, 13)  `\n `y.shape: (506,)  `\n\n\n\n\n\n## 数据规范化\n\n\n```python\n# 转化为标准正态分布\nX_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis = 0)\ny = y.reshape(-1,1) # reshape转化为vector\nprint(X_.shape)\nprint(y.shape)\n```\n\n`(506, 13)  `\n`(506, 1)`\n    \n\n## 建立激活函数\n\n\n```python\ndef sigmoid(x):\n    r = 1 / (1 + np.exp(-x))\n    return r\nnums = np.arange(-10, 10, step = 1)\nfig, ax = plt.subplots(figsize = (10, 4))\nax.plot(nums, sigmoid(nums), c='red')\n```\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125621.png)\n\n\n```python\ndef relu(x):\n    return (x > 0) * x\nfig, ax = plt.subplots(figsize = (10, 4))\nnums = np.arange(-10, 10, step = 1)\nax.plot(nums, relu(nums), c = 'blue')\n```\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125708.png)\n\n\n## 定义模型\n\n线性模型：$y = wx + b$\n\n\n```python\ndef Linear(x, w, b):\n    y_pre = x.dot(w) + b\n    return y_pre\n```\n\n**在计算损失时，需要把每个样本的损失都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数N。**\n\n\n```python\ndef MSE_loss(y_pre, y):\n    loss = np.mean(np.square(y_pre - y))\n    return loss\n```\n\n\n```python\ndef gradient(x, y_pre, y):\n    n = x.shape[0]\n    grad_w = x.T.dot(y_pre - y)/n\n    grad_b = np.mean(y_pre - y)\n    return grad_w, grad_b\n    \n```\n\n\n```python\n# 初始化网络\nn = X_.shape[0] # 样本数量506\nn_features = X_.shape[1] #特征数量13\n\n# 初始化网络参数\n# randn从标准正态分布中返回一个或多个样本值\nW = np.random.randn(n_features, 1)\nb = np.zeros(1)\n\n#设定学习率\nlearning_rate = 1e-2\n\n#训练次数\nepoch = 10000\n```\n\n## 训练(不加激活函数)\n\n\n```python\nlosses = []\n# 训练 \nfor t in range(epoch):\n    # 向前传播\n    y_pred = Linear(X_, W, b)\n    # 计算损失函数\n    loss = MSE_loss(y_pred,y)\n    losses.append(loss)\n    grad_w, grad_b = gradient(X_, y_pred, y)\n    \n    #权重更新\n    W = W - grad_w * learning_rate\n    b = b - grad_b * learning_rate\n\nfig, ax = plt.subplots(figsize = (10, 4))\nax.plot(np.arange(len(losses)), losses, c = 'r')\n```\n\n\n\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125735.png)\n\n\n```python\nn_hidden = 10 #设计隐藏神经元个数（可修改）\nW1 = np.random.randn(n_features, n_hidden)  # 维度 n_features * n_hidden\nb1 = np.zeros(n_hidden)                     # 维度 1 * n_hidden\nW2 = np.random.randn(n_hidden, 1)           # 维度 n_hidden * 1\nb2 = np.zeros(1)                            # 维度1\n```\n\n## 训练(加激活函数)\n\n\n```python\n# 训练\nlosses = []\nfor t in range(epoch):\n    #向前传播\n    y_pred1 = Linear(X_, W1, b1)     # 维度 n * n_hidden\n    y_relu = relu(y_pred1)           # 维度 n * n_hidden\n    y_pred = Linear(y_relu, W2, b2) # 维度 n * 1\n    \n    #计算损失函数\n    loss = MSE_loss(y_pred, y)\n    losses.append(loss)\n    \n    #反向传播，求梯度\n    grad_y_pred = y_pred - y                 # 维度n*1\n    grad_w2 = y_relu.T.dot(grad_y_pred) / n  # 维度n_hidden*1\n    grad_b2 = np.mean(grad_y_pred, axis = 0) # 维度1*1\n    grad_relu = grad_y_pred.dot(W2.T)        # 维度n*n_hidden\n    #注意：y_pred1与relu直接相关\n    grad_relu[y_pred1 < 0] = 0\n    grad_w1 = X_.T.dot(grad_relu) / n        # 维度n_features* n_hidden\n    grad_b1 = np.mean(grad_relu, axis = 0)   # 维度n_hidden*1\n    \n    #更新梯度\n    W1 = W1 - grad_w1 * learning_rate\n    b1 = b1 - grad_b1 * learning_rate\n    W2 = W2 - grad_w2 * learning_rate\n    b2 = b2 - grad_b2 * learning_rate\n\nfig, ax = plt.subplots(figsize = (10, 4))\nax.plot(np.arange(len(losses)), losses, c = 'r')\n```\n\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20200716125832.png)\n\n参考文档\n\n[波士顿房价数据集解读](https://blog.csdn.net/appleyuchi/article/details/84998894)\n","tags":["numpy","线性回归"],"categories":["project实战"]},{"title":"奥卡姆剃刀","url":"/2020/07/15/0/","content":"任何一件事情，都要从简单的开始做起，若无必要，勿增实体！！！\n\n![img](https://gitee.com/wxler/blogimg/raw/master/imgs/20200715105539.jpg)\n","categories":["Life"]},{"title":"git基本使用方法","url":"/2020/06/17/115900/","content":"由于平时写代码和博客常常用到git和github，每次用到都去百度，感觉太麻烦了，也大大降低了效率，索性自己整理一下常用到的git指令和使用方法，对git的使用能有一个系统的认识。这里只介绍一下基本用法，对更高级的用法如果以后用到再进行补充。\n<!-- more -->\n\n## git安装和配置\ngit的安装和配置在我的这篇[搭建个人博客](https://wxler.github.io/2020/06/01/hexoCreateAndConfig/#%E5%AE%89%E8%A3%85git)里，请自行参考配置，主要有一下几点：\n- 下载安装git程序\n- 配置github账户\n- 配置SSH KEY\n## git工作原理\nGit是分布式版本控制系统，那么它就没有中央服务器的，每个人的电脑就是一个完整的版本库，这样，工作的时候就不需要联网了，因为版本都是在自己的电脑上。既然每个人的电脑都有一个完整的版本库，那多个人如何协作呢？比如说自己在电脑上改了文件A，其他人也在电脑上改了文件A，这时，你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。Git在执行更新操作时，更像是对数据的一组快照，每次你提交更新，它主要对当时的全部文件制作一个快照并保存这个快照的索引。为了高效，如果文件没有修改，Git不再重新存储该文件，而是只保留一个链接指向之前存储的文件。  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125501.png)\n如上图所示，在version2中的B即是因为File B没有改变，所以直接存储了一个指向FileB的链接。只有修改了的文件才会产生一个新的文件，覆盖原来的文件。\ngit的工作原理/流程如下:  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125536.png)\n\n- Workspace：工作区(本地目录文件)\n- Index / Stage：暂存区/缓存区\n- Repository：仓库区（或本地仓库）\n- Remote：远程仓库\n\n## 基本操作\n\n### 初始化仓库\n仓库的初始化有两种方式：一种是直接从远程仓库克隆，另一种则是直接从当前目录初始化。远程初始化命令在[从远程仓库获取](#从远程仓库获取)，本地初始化命令的方法是首先创建一个文件夹，我命名为mygit，然后执行如下命令：  \n```bash\n$ git init\n```\n执行完毕后，当前目录下会出现一个隐藏的.git文件夹，git所需的数据和资源都放在改目录中。\n\n### 查看仓库状态\n\n通过`git status`来查看仓库状态，执行效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125612.png)\n可以看到nothing to commit，表示本地工作区没有要提交的文件，我们再创建一个one.txt的文件，然后在执行`git status`，效果如下： \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130149.png)\n从结果Untracked files可以看到，one.txt还没有被add到暂存区。\n\n### 添加文件到暂存区\n\n`git add`命令可以将一个文件添加到暂存区,执行如下命令将one.txt添加到暂存区：\n```bash\ngit add one.txt\n```\n将one.txt添加到暂存区后，再次执行`git status`,可看到如下效果：  ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125701.png)\n从Changes to be committed可以看出，one.txt已被添加到暂存区，但还未被添加到本地仓库。\n\n### 提交到本地仓库\n\n当文件提交到暂存区之后，执行`git commit`命令将当前暂存区的文件提交到本地仓库，执行如下命令：\n```bash\ngit commit -m '新增一个one.txt'\n```\n-m是指将当前暂存区的文件提交到本地仓库的时候，加上提交备注/说明，再次执行`git status`,可以看到已经没有要add或commit的文件了。这里要强调一下，如果直接执行`git commit`命令，会自动打开一个vi编辑器，在里面输入备注/说明即可。此外，当我们提交成功后，还可以通过`git commit --amend  `修改备注信息。\n### 查看更改前后的差异\n使用`git diff`命令可以查看**工作区和暂存区的区别**，在one.txt里面写入一行hello world，然后执行`git diff`命令，结果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130230.png)\n根据结果可以看到新增了一行hello world，如果我们要比较**工作区与最新本地版本库的区别**，可以执行`git diff HEAD`，结果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130230.png)\n\n### 查看提交历史\n\n使用`git log`查看提交历史，我们首先将工作区的内容提交到本地仓库，执行`git add one.txt`将更改后的one.txt添加到暂存区，执行` git commit -m '添加了一行hello world'`将暂存区的内容提交到本地仓库，然后执行`git log`，结果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125804.png)\n\n## git撤销修改\n\n### 工作区的代码撤销\n\n使用`git checkout`撤销工作区的代码。我们先向one.txt添加一行hello everyone，执行`cat one.txt`查看内容，再执行`git checkout -- one.txt`撤销之前的操作，让one,txt恢复之前的状态，然后执行`cat one.txt`再次查看内容，效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124125909.png)  \n可以看到，工作区的内容已经被修改,这时候本地文件刚刚添加的内容就被撤销了。\n\n### 暂存区的代码撤销\n\n使用`git reset HEAD`撤销暂存区的代码。首先在one.txt添加一行hello people，执行`git add one.txt`将更改的内容提交到暂存区，`git reset HEAD`来撤销暂存区的代码，如下图：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130403.png)\n撤销暂存区的代码之后，如需要将代码添加到暂存区，则需要再次执行`git add`命令\n\n### 本地仓库的代码撤销\n\n可以使用`git reset --hard <版本号>`来撤销本地仓库的代码，版本号有几种不同的写法：\n1. 可以使用HEAD^来描述版本，一个^表示前一个版本，两个^^表示前两个版本，以此类推。\n2. 也可以使用数字来代替^，比如说前100个版本可以写作HEAD~100。\n3. 也可以直接写版本号，表示跳转到某一个版本处。我们每次提交成功后，都会生成一个哈希码作为版本号，所以这里我们也可以直接填版本号，哈希码很长，但是我们不用全部输入，只需要输入前面几个字符即可，就能识别出来。执行`git log`后那一串长符号就是哈希码版本号。\n\n依次执行如下命令：\n```bash\n$ git add 'one.txt'\n$ git commit -m '添加一行hello people'\n$ git reset --hard head^\n```\n执行`git reset --hard head^`后的效果如下：\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130427.png)  \n可以从结果看出，前半部分816b208是执行撤销操作以后，当前版本的版本号前七位，后半部分是该版本的备注，可以用`git log`来查看不同版本的版本号和备注。  \n\n再次查看本地one.txt文件，发现本地目录的刚刚添加的内容已经没有了，如需要再次提交到本地仓库，则可执行`git add`和`git commit`命令。**需要注意的是，当撤销到最开始版本的时候，`git reset --hard head^`就不能再用了，否则会报如下的错误：**  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130454.png)\n\n## git分支管理\n\n### 查看分支\n通过`git branch`来查看当前仓库有哪些分支和我们处于哪一分支中，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130516.png) \n可以看到，当前本地仓库只有一个master分支，这是git默认创建出来的，master前面的\\*表示我们当前处于这一个分支中。\n\n### 分支创建和切换\n可以利用`git branch <分支名>`来创建一个分支，利用`git checkout <分支名>`来切换分支，如下所示： \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130558.png)\n\n### 分支合并\n\n由于math分支是从master分支中创建出来的，所以此时math分支的内容和master分支的内容是一致的，现在，我们在math分支向one.txt添加一行hello branch math(由于刚刚执行了[本地仓库的代码撤销](#本地仓库的代码撤销)，所以one.txt现在的内容是空白的)，此时math分支的one.txt和math分支的one.txt就不同了，具体效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130558.png)  \n执行完毕后，我们也可以在本地查看，先在math分支下，打开one.txt可以看到我们刚刚添加的内容，然后再切换到master分支，再从本地打开one.txt文件，就看不到内容了。\n**可以通过`git merge <分支名>`合并分支**，先切换到master分支，然后执行` git merge math`合并math分支到master分支上，效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130633.png)  \n可以看到再次在master分支下查看one.txt，就可以显示math分支的内容了。\n\n通常合并分支时，git一般使用”Fast forward”模式，fast-forward方式表示当条件允许时，git直接把HEAD指针指向合并分支的头，完成合并，这种方式合并速度快，但是在整个过程中没有创建commit。在这种模式下，删除分支后，会丢掉分支信息，可使用带参数 `–no-ff`来禁用”Fast forward”模式，即删除时可以实用`git merge --no-ff <分支名>`\n\n### 以图表方式查看分支\n可以用`git log --graph`命令来直观的查看分支的创建和合并等操作，合并math和master分支前的效果如下： \n    ![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130807.png)\n合并math和master分支后的效果如下：   \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130807.png)\n\n### 解决冲突\n\n我们创建一个新的分支dev,并在dev分支下给one.txt添加一行12345，然后提交，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130943.png)  \n同样，我们现在切换到master分支上来，也在one.txt添加一行内容，内容为56789，并提交，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124130943.png)  \n现在，我们将dev分支合并到master上来，如下所示：\n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131003.png)  \n从结果中可以看出，=======之前是主分支的内容，=======之后是dev分支的内容，此时我们用文本编辑器修改one.txt的冲突然后提交即可，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131140.png)\n\n**分支策略：首先master主分支应该是非常稳定的，也就是用来发布新版本，一般情况下不允许在上面干活，干活一般情况下在新建的dev分支上干活，干完后，dev分支代码可以合并到主分支master上来。**\n\n## github远程仓库\n\n### 关联远程仓库\n在此之前我相信你已经配置SSH KEY，如果没有，可以参考我的这篇[搭建个人博客](https://wxler.gitee.io/2020/06/01/hexoCreateAndConfig/#git%E9%85%8D%E7%BD%AE)里进行配置，配置完成以后在github上创建一个仓库，这里命名为test，我们可以看到仓库的地址，例如：`https://github.com/wxler/test.git`。然后将我们之前的本地仓库和这个远程仓库进行关联，使用`git remote add`命令，如下：\n```bash\n$ git remote add origin https://github.com/wxler/test.git\n```\n### 推送到远程仓库\n把本地库的内容推送到远程，使用`git push -u origin master`命令，实际上是把当前分支master推送到远程。由于远程库是空的，我们第一次推送master分支时，加上了–u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令,不用加-u了,效果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131213.png) \n推送成功后，可以立刻在github页面中看到远程库的内容已经和本地一模一样了。从现在起，只要本地作了提交，就可以通过命令：`git push origin master`把本地master分支的最新修改推送到github上了，现在你就拥有了真正的分布式版本库了。  \n**我们一般不把其它分支推送到远程仓库，master主分支是最稳定的版本，一般情况下不允许在上面干活，干活一般情况下在新建的分支上干活，干完后，把分支代码可以合并到主分支master上来。**当然，你也可以将其它分支推送到远程仓库，可以执行如下命令：\n\n```bash\n$ git checkout fa\n$ git push -u origin fa\n```\n### 从远程仓库获取\n我们可以通过git clone命令克隆一个远程仓库到本地,方式也简单，在本地创建一个空文件夹，执行如下命令：\n```bash\n$ git clone https://github.com/wxler/test.git\n```\n此时克隆的是master分支到本地仓库，我们可以通过`git branch -a`来查看本地仓库和远程仓库的信息，-a参数可以同时显示本地仓库和远程仓库的信息，如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131310.png)\n我们也可以把远程仓库其它分支的内容clone下来，可以执行如下命令：\n\n```bash\n$ git branch fa origin/dev\n$ git checkout dev\n```\n上面的指令表示根据远程仓库的dev分支创建一个本地仓库的dev分支，然后再切换到dev分支，**注意由于dev分支就是从远程仓库克隆下来的，所以这里可以不添加-u参数。**\n\n### 从远程仓库更新\n使用`git pull`获取远程仓库最新的代码和数据，例如，我们可以通过以下代码将远程主机的master分支最新内容拉下来后与当前本地分支直接合并\n```bash\ngit pull origin master\n```\n\n## git命令大全\ngit常用命令速查表，方便查阅：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124131337.png)\n\n## 遗留问题\n\n到现在，我们就可以使用git的大多数操作了，但是还有git的一些操作平常没有用到，我也就不主动去一个个试了，毕竟一口吃不成胖子，查了也记不住，就不自找苦吃了，遗留的问题主要有：\n- git分支衍合\n- git标签管理\n- bug分支&stash功能\n\n## 参考文档\n[松哥git教程](https://mp.weixin.qq.com/s?__biz=MzI1NDY0MTkzNQ==&mid=100004284&idx=1&sn=f9adbded2bac4d8efb9b07a0262a0e8a&chksm=69c343dc5eb4cacaa45d2a968ab935e85eccbcb11f9493d32ef147da5c1a1ff53cc45a1f32a2&mpshare=1&scene=1&srcid=0616SVENp6ameT2V21QsW2nZ&sharer_sharetime=1592289626242&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=1de400e48ea73360020a1786d1997d16a5b67307619595df7c6b06fc16be187dba8368d87386b6a57a40d3c3f3671fae6927462285e9b59479eb08a036e5aa45b02add3de42bb86044ef6649218a6531&ascene=1&uin=MjA2Nzc1NzU0Mg%3D%3D&devicetype=Windows+10+x64&version=6209007b&lang=zh_CN&exportkey=ASZhqnZ2JNrU1FCHtEQsA0s%3D&pass_ticket=DNBBiZIg7%2Fjg4GBZz9sSD49h8dYitJv5rAPR21lJbAICH1bIYdByWmp3fQOoDyHW)\n[Git从入门到熟练使用](https://www.jianshu.com/p/34cfe097e06a)\n[史上最简单Git入门教程](https://www.cnblogs.com/jjlee/p/10305194.html)\n[git命令大全](https://www.jianshu.com/p/46ffff059092)\n\n>特别声明：本篇博客只做个人学习交流和参考手册使用，不作任何商业目的，内容上较多参考了[松哥git教程](https://mp.weixin.qq.com/s?__biz=MzI1NDY0MTkzNQ==&mid=100004284&idx=1&sn=f9adbded2bac4d8efb9b07a0262a0e8a&chksm=69c343dc5eb4cacaa45d2a968ab935e85eccbcb11f9493d32ef147da5c1a1ff53cc45a1f32a2&mpshare=1&scene=1&srcid=0616SVENp6ameT2V21QsW2nZ&sharer_sharetime=1592289626242&sharer_shareid=18383980e942ee6dfd94ea4b7b61fcbe&key=1de400e48ea73360020a1786d1997d16a5b67307619595df7c6b06fc16be187dba8368d87386b6a57a40d3c3f3671fae6927462285e9b59479eb08a036e5aa45b02add3de42bb86044ef6649218a6531&ascene=1&uin=MjA2Nzc1NzU0Mg%3D%3D&devicetype=Windows+10+x64&version=6209007b&lang=zh_CN&exportkey=ASZhqnZ2JNrU1FCHtEQsA0s%3D&pass_ticket=DNBBiZIg7%2Fjg4GBZz9sSD49h8dYitJv5rAPR21lJbAICH1bIYdByWmp3fQOoDyHW)，根据自己实际应用进行删减，并加上了自己的理解和补充，如有侵权，请联系博主本人删除。","tags":["git"],"categories":["工具"]},{"title":"使用hexo平台从0搭建个人博客","url":"/2020/06/01/122000/","content":"搭建这个博客，花费了我不少时间，这期间我遇到各种各样的问题，这些问题本可以避免，因操作不规范、对指令代码的不理解、网络不稳定、配置上的错误，使得最后暴露出来的各种bug很不容易解决。前前后后我也重新搭建了三次，经历了心态上的各种起伏。为此，我记录下我制作的过程，让想和我一样自建博客的人少走一些弯路。\n\n我使用的hexo博客框架，stun主题，搭建环境和过程可分为几个部分:\n\n1. 安装git\n2. 安装nodejs\n3. 安装hexo\n4. hexo搭桥github\n5. hexo-admin使用\n6. npm&hexo常用命令\n<!-- more -->\n\n## 安装git\n\n### git下载\n下载[git](https://git-scm.com/download)，双击安装，然后一直next，按键Ctrl+r，然后在弹出框中出入cmd，在弹出的界面输入git，回车,出来一大串命令符就代表安装成功了。可以使用git version查看自己的git版本。\n\n### git配置\n\n1. git安装好去GitHub上注册一个账号，注册好后，桌面空白地方右键选择Git Bash，要git账户进行环境配置  \n```bash\n//usename是用户名\ngit config --global user.name \"username\"\ngit config --global user.email \"username@email.com\"\n```\n2. 当以上命令执行结束后，可用 `git config --global --list` 命令查看配置是否OK  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133328.png)\n3. 在命令框中输入命令`ssh-keygen -t rsa`，连敲三次回车键，结束后去系统盘目录下（一般在 C:\\Users\\你的用户名.ssh）(mac: /Users/用户/.ssh）查看是否有。ssh文件夹生成，此文件夹中以下两个文件  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133402.png)\n4. 将ssh文件夹中的公钥（ id_rsa.pub）添加到GitHub管理平台中，在GitHub的个人账户的设置中找到如下界面，title随便起一个，将公钥（ id_rsa.pub）文件中内容复制粘贴到key中，然后点击Ass SSH key  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133430.png)\n5、测试一下配置是否成功，在Git Bush命令框（就是刚才配置账号和邮箱的命令框）中继续输入命令`ssh -T git@github.com`，回车,出现如下界面即说明成功  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133448.png)\n\n## 安装nodejs\n\nHexo是基于nodeJS环境的静态博客，里面的npm工具很有用。下载[nodejs](https://nodejs.org/en/)，(说明：LTS为长期支持版，Current为当前最新版)，下载后一路next进行安装，在git bash下使用node -v查看版本。\n\n## 安装hexo\n\n我建议先看一下npm&hexo常用命令部分，了解命令的结构和大体含义之后，在配置的过程中可以避免很多错误，少走很多弯路。  \n执行 `npm config list`查看当前的配置，如下所示：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133516.png)\n可以看到，最初的镜像地址是官方的npm镜像，我最初使用的就是这个配置，执行起来很不稳定，导致大多数错误都是网络问题导致的,执行如下命令,切换淘宝镜像\n\n```bash\nnpm install -g cnpm --registry=https://registry.npm.taobao.org\nnpm config set registry https://registry.npm.taobao.org\n```\n接下来，就可以安装hexo了，执行`npm install -g hexo-cli`或`npm install -g hexo`，如果之前安装失败，可以先执行`npm uninstall hexo-cli -g`或`npm uninstall hexo -g` 卸载hexo，再进行安装，结果如下：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133610.png)\n查看版本信息` hexo v`  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133628.png)\n初始化hexo,执行 `hexo init myblog`，然后`cd myblog`，再次执行`hexo v`，就可以看到hexo的版本：  \n![](https://gitee.com/wxler/blogimg/raw/master/imgs/20201124133657.png)  \n打开myblog文件夹，我们可以看到hexo的结构\n\n> * node_modules：是依赖包\n> * public：存放的是生成的页面\n> * scaffolds：命令生成文章等的模板\n> * source：用命令创建的各种文章\n> * themes：主题\n> * _config.yml：整个博客的配置\n> * db.json：source解析所得到的\n> * package.json：项目所需模块项目的配置信息  \n\n\n到这里我们的hexo博客就安装完成啦，只有搭桥到github,才能进行部署。\n\n## hexo搭桥github\n\n创建一个repo，名称为yourname.github.io,其中yourname是你的github名称，按照这个规则创建才有用，这个仓库就是存放你博客的地方。\n1. 用编辑器打开你的blog项目，修改_config.yml  \n```text\ndeploy:  \n\ttype: git\n\trepo:https://github.com/YourgithubName/YourgithubName.github.io.git\n\tbranch: master\n```\n2. 回到gitbash中，进入你的blog目录，分别执行以下命令：\n```bash\nhexo clean\nhexo generate\nhexo server\n```\n需要注意的是，hexo 3.0把服务器独立成个别模块，需要单独安装：`npm i hexo-server`\n3. 打开浏览器输入：`http://localhost:4000`\n4. 先安装一波：`npm install hexo-deployer-git --save`（这样才能将你写好的文章部署到github服务器上并让别人浏览到） \n5. 执行命令\n```bash\nhexo clean\nhexo generate\nhexo deploy\n```\n6. 在浏览器中输入`http://yourgithubname.github.io`就可以看到你的个人博客啦。\n\n我使用的主题是stun，如果大家也想使用这个主题，可以到[ hexo-theme-stun](https://liuyib.github.io/hexo-theme-stun/zh-CN/)查阅配置。\n\n\n## hexo-admin使用\n\n\n用原生的方法来管理博文十分的不便，因此便有了Hexo Admin这一插件来方便我们的操作。执行`npm install --save hexo-admin`安装hexo-admin，安装成功后，在`http://localhost:4000/admin`就可以访问hexo-admin页面。\n详细情形我就不多说了，推荐大家到[hexo博客使用hexo-admin插件管理文章](https://blog.csdn.net/nineya_com/article/details/103380243?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase)，这位作者的[hexo-admin插件windows系统插入图片失败问题](https://blog.csdn.net/nineya_com/article/details/103384546)修复了windows下粘贴图片的裂图和显示功能，我使用起来非常好，推荐大家看看。\n\n除此之外，我还要强调一点，hexo-admin创建文章的时候，首先创建英文名，再在里面编辑成中文，这样你的文章显示的链接就不会带有中文了。hexo-admin的文章只有未发布状态才能删除，并且删除后在source/_discarded文件夹，未发布变成draft,发布直接到post。\n\n## npm&hexo常用命令\n### npm&cnpm介绍\nnpm（node package manager）：nodejs的包管理器，用于node插件管理（包括安装、卸载、管理依赖等），使用`npm -v`查看版本信息。\n\ncnpm:因为npm安装插件是从国外服务器下载，受网络的影响比较大，可能会出现异常，如果npm的服务器在中国就好了，所以我们乐于分享的淘宝团队干了这事。来自官网：“这是一个完整 npmjs.org 镜像，你可以用此代替官方版本(只读)，同步频率目前为 10分钟 一次以保证尽量与官方服务同步”，更多详情可以查看[淘宝 NPM 镜像](https://developer.aliyun.com/mirror/NPM?from=tnpm)，使用`cnpm -v`查看版本信息。\n\n\nnpm和cnpm安装命令一样，只不过是多了一个c。\n\n### npm命令\n使用npm命令首先要设置下载的镜像，模式是npm官网的镜像(服务器在国外)，建议设置国内的淘宝镜像,设置以后我们就可以用npm从淘宝镜像下载数据了  \n永久使用：  \n`npm config set registry https://registry.npm.taobao.org`  \n临时使用：  \n`npm install node-sass --registry=http://registry.npm.taobao.org`  \n还有个清除缓存命令，可以解决些奇怪的问题:  \n`npm cache clean --force`  \n查看已安装的npm插件，这个命令很实用，可以查看缺少哪些插件  \n`npm ls --depth 0`   \n可以通过定制的 cnpm 命令行工具代替默认的 npm  \n`npm install -g cnpm --registry=http://registry.npm.taobao.org`  \n**在使用过程中要么用npm，要么用cnpm，不能混用**  \n查看当前的配置命令`npm config list `,操作之前一定要先查看配置再进行操作。  \n**下面需要强调后缀参数的作用和区别**  \n`npm install packagename --save 或 -S`    \n--save、-S参数意思是把模块的版本信息保存到dependencies（生产环境依赖）中，即你的package.json文件的dependencies字段中。  \n`npm install packagename --save-dev 或 -D`  \n--save-dev 、 -D参数意思是吧模块版本信息保存到devDependencies（开发环境依赖）中，即你的package.json文件的devDependencies字段中。  \n`npm install packagename -g 或 --global`  \n安装全局的模块（不加参数的时候默认安装本地模块），\n**使用npm安装插件的时候一定要加上--save添加依赖，否则容易出错** ，更多关于npm详情，请点击[npm常用命令及参数详解](https://segmentfault.com/a/1190000012099112?utm_source=tag-newest)\n\n### hexo命令\n\n`hexo init`  \n初始化站点，生成一个简单网站所需的各种文件。\n\n`hexo clean == hexo c`  \n清除缓存 网页正常情况下可以忽略此条命令\n\n`hexo generate == hexo g`  \n生效新增、修改、更新的文件\n\n`hexo server == hexo s`  \n启动本地网站，可在本地观察网站效果，同时也可以输入`http://localhost:4000/admin`管理文章\n\n`hexo s --debug`  \n以调试模式启动本地网站，在此模式下，对文件的更改无需停止网站只需刷新即可看到效果，调试非常方便\n\n\n`hexo clean && hexo s`  \n一次执行两个命令\n\n`hexo deploy == hexo d`  \nhexo的一键部署功能，执行此命令即可将网站发布到配置中的仓库地址，执行此命令前需要配置站点配置文件_config.yml","tags":["hexo"],"categories":["工具"]}]