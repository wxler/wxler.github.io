<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/assets/logo.png?v=2.0.0" type="image/png" sizes="16x16"><link rel="icon" href="/assets/logo.png?v=2.0.0" type="image/png" sizes="32x32"><meta name="description" content="MapReduce框架核心原理">
<meta property="og:type" content="article">
<meta property="og:title" content="七、MapReduce框架核心原理">
<meta property="og:url" content="https://wxler.github.io/2021/03/31/214801/index.html">
<meta property="og:site_name" content="Layne&#39;s Blog">
<meta property="og:description" content="MapReduce框架核心原理">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402105741767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402110146142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402113858694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2021040214484549.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402144927250.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402145436545.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2021040214553316.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402145602114.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402141939635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402142824388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402150723649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402152431322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210402153324149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406111821682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406111901460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406115725933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406153228865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406154912654.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406155913331.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406161106715.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2021040616104256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406163016528.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406171843851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406172403280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210406172805607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406193836.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406195046.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406201739.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406201720.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202011.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202105.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202142.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202304.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202332.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202554.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406213048.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406213124.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406213152.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406214042.png">
<meta property="og:image" content="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406232017.png">
<meta property="article:published_time" content="2021-03-31T13:48:01.000Z">
<meta property="article:modified_time" content="2021-04-10T13:38:32.199Z">
<meta property="article:author" content="wxler">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="MapReduce">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20210402105741767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"><meta name="keywords" content="wxler, Layne's Blog"><meta name="description" content="博客，分享，开源，心得"><title>七、MapReduce框架核心原理 | Layne's Blog</title><link ref="canonical" href="https://wxler.github.io/2021/03/31/214801/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.0.0"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement('script');
  hm.src = 'https://hm.baidu.com/hm.js?01911aa0fc6bdb840626994292397110';
  hm.async = true;

  if (true) {
    hm.setAttribute('data-pjax', '');
  }
  var s = document.getElementsByTagName('script')[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"ocean","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: true,
  pjax: {"avoidBanner":true},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/message/"><span class="header-nav-menu-item__icon"><i class="fa fa-comment"></i></span><span class="header-nav-menu-item__text">留言板</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user"></i></span><span class="header-nav-menu-item__text">关于</span></a></div></div><div class="header-nav-search"><span class="header-nav-search__icon"><i class="fas fa-search"></i></span><span class="header-nav-search__text">搜索</span></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Layne's Blog</div><div class="header-banner-info__subtitle">一个爱好coding的男孩纸</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">七、MapReduce框架核心原理</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-31</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-04-10</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">14.4k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">118分</span></span></div></header><div class="post-body"><p>MapReduce框架核心原理</p>
<a id="more"></a>
<p><strong>文章目录</strong></p>
<p><ul class="markdownIt-TOC">
<li><a href="#1-inputformat-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5">1. InputFormat 数据输入</a>
<ul>
<li><a href="#11-%E5%88%87%E7%89%87%E4%B8%8E-maptask-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6">1.1 切片与 MapTask 并行度决定机制</a></li>
<li><a href="#12-job-%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%92%8C%E5%88%87%E7%89%87%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3">1.2 Job 提交流程源码和切片源码详解</a></li>
<li><a href="#13-fileinputformat-%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6">1.3 FileInputFormat 切片机制</a></li>
<li><a href="#14-fileinputformat%E5%AE%9E%E7%8E%B0%E7%B1%BB">1.4 FileInputFormat实现类</a></li>
<li><a href="#15-combinetextinputformat-%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6">1.5 CombineTextInputFormat 切片机制</a></li>
<li><a href="#16-combinetextinputformat-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">1.6 CombineTextInputFormat 案例实操</a></li>
<li><a href="#17-keyvaluetextinputformat%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B">1.7 KeyValueTextInputFormat使用案例</a></li>
<li><a href="#18-nlineinputformat%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B">1.8 NLineInputFormat使用案例</a></li>
</ul>
</li>
<li><a href="#2-mapreduce-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B">2. MapReduce 工作流程</a></li>
<li><a href="#3-shuffle-%E6%9C%BA%E5%88%B6">3. Shuffle 机制</a>
<ul>
<li><a href="#31-shuffle-%E6%9C%BA%E5%88%B6">3.1 Shuffle 机制</a></li>
<li><a href="#32-partition-%E5%88%86%E5%8C%BA">3.2 Partition 分区</a></li>
<li><a href="#33-partition-%E5%88%86%E5%8C%BA%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.3 Partition 分区案例实操</a></li>
<li><a href="#34-writablecomparable-%E6%8E%92%E5%BA%8F">3.4 WritableComparable 排序</a></li>
<li><a href="#35-writablecomparable-%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E5%85%A8%E6%8E%92%E5%BA%8F">3.5 WritableComparable 排序案例实操（全排序）</a></li>
<li><a href="#36-writablecomparable-%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E5%8C%BA%E5%86%85%E6%8E%92%E5%BA%8F">3.6 WritableComparable 排序案例实操（区内排序）</a></li>
<li><a href="#37-combiner-%E5%90%88%E5%B9%B6">3.7 Combiner 合并</a></li>
<li><a href="#38-combiner-%E5%90%88%E5%B9%B6%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.8 Combiner 合并案例实操</a></li>
</ul>
</li>
<li><a href="#4-outputformat-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA">4. OutputFormat 数据输出</a>
<ul>
<li><a href="#41-outputformat-%E6%8E%A5%E5%8F%A3%E5%AE%9E%E7%8E%B0%E7%B1%BB">4.1 OutputFormat 接口实现类</a></li>
<li><a href="#42-%E8%87%AA%E5%AE%9A%E4%B9%89-outputformat-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">4.2 自定义 OutputFormat 案例实操</a></li>
</ul>
</li>
<li><a href="#5-mapreduce-%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90">5. MapReduce 内核源码解析</a>
<ul>
<li><a href="#51-maptask-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">5.1 MapTask 工作机制</a></li>
<li><a href="#52-reducetask-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6">5.2 ReduceTask 并行度决定机制</a></li>
<li><a href="#53-maptask-reducetask-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90">5.3 MapTask &amp; ReduceTask 源码解析</a></li>
</ul>
</li>
<li><a href="#6-join%E5%BA%94%E7%94%A8">6. Join应用</a>
<ul>
<li><a href="#61-reduce-join">6.1 Reduce Join</a></li>
<li><a href="#62-reduce-join-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">6.2  Reduce Join 案例实操</a></li>
<li><a href="#63-map-join">6.3 Map Join</a></li>
<li><a href="#64-map-join-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">6.4 Map Join 案例实操</a></li>
</ul>
</li>
<li><a href="#7-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97etl">7. 数据清洗（ETL）</a></li>
<li><a href="#8-etl%E6%B8%85%E6%B4%97%E8%A7%84%E5%88%99">8. ETL清洗规则</a></li>
<li><a href="#9-mapreduce-%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93">9. MapReduce 开发总结</a></li>
<li><a href="#10-%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">10. 常见错误及解决方案</a></li>
</ul>
</p>
<p><strong>MapReduce 框架原理</strong></p>
<p>
        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402105741767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>

        <h2 id="1-inputformat-数据输入"   >
          <a href="#1-inputformat-数据输入" class="heading-link"><i class="fas fa-link"></i></a>1. InputFormat 数据输入</h2>
      

        <h3 id="11-切片与-maptask-并行度决定机制"   >
          <a href="#11-切片与-maptask-并行度决定机制" class="heading-link"><i class="fas fa-link"></i></a>1.1 切片与 MapTask 并行度决定机制</h3>
      
<p>（1）问题引入<br />
MapT ask 的并行度决定 Map 阶段的任务处理并发度，进而影响到整个 Job 的处理速度。</p>
<p>思考：1G 的数据，启动 8 个 MapTask，可以提高集群的并发处理能力。那么 1K 的数据，也启动 8 个 MapTask，会提高集群性能吗？MapTask 并行任务是否越多越好呢？哪些因素影响了 MapTask 并行度？</p>
<p>MapTask并不是也多越好，如果1K的数据也分为8个MapTask，则MapTask启动的时间就比任务运行的时间长，这就得不偿失了。</p>
<p>（2）MapTask 并行度决定机制</p>
<ul>
<li>数据块：Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。</li>
<li>数据切片： 数据切片只是在逻辑上对输入进行分片， 并不会在磁盘上将其切分成片进行存储。 数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</li>
</ul>
<p><strong>数据切片与MapTask并行度决定机制</strong></p>
<p>
        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402110146142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>

        <h3 id="12-job-提交流程源码和切片源码详解"   >
          <a href="#12-job-提交流程源码和切片源码详解" class="heading-link"><i class="fas fa-link"></i></a>1.2 Job 提交流程源码和切片源码详解</h3>
      
<p>一、<strong>Job提交流程源码详解</strong></p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion();</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一、建立连接</span></span><br><span class="line">	connect();</span><br><span class="line">		<span class="comment">// 创建提交 Job 的代理</span></span><br><span class="line">		<span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">			<span class="comment">// 判断是本地运行环境还是 yarn 集群运行环境</span></span><br><span class="line">			initialize(jobTrackAddr, conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二、提交 job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line">	<span class="comment">// （1）创建给集群提交数据的 Stag 路径</span></span><br><span class="line">	Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">	<span class="comment">// （2）获取 jobid  ，并创建 Job 路径</span></span><br><span class="line">	JobID jobId = submitClient.getNewJobID();</span><br><span class="line">	<span class="comment">// （3）拷贝 jar 包到集群</span></span><br><span class="line">	copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">	rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">	<span class="comment">// （4）计算切片，生成切片规划文件</span></span><br><span class="line">	writeSplits(job, submitJobDir);</span><br><span class="line">	maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">	input.getSplits(job);</span><br><span class="line">	<span class="comment">// （5）向 Stag 路径写 XML 配置文件</span></span><br><span class="line">	writeConf(conf, submitJobFile);</span><br><span class="line">	conf.writeXml(out);</span><br><span class="line">	<span class="comment">// （6）提交 Job,返回提交状态</span></span><br><span class="line">	status  =  submitClient.submitJob(jobId,  submitJobDir.toString(),job.getCredentials());</span><br></pre></td></tr></table></div></figure>
<p>
        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402113858694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>
<p>二、<strong>FileInputFormat切片源码解析</strong>（input.getSplits(job)）</p>
<p>（1）程序先找到你数据存储的目录。<br />
（2）开始遍历处理（规划切片）目录下的每一个文件<br />
（3）遍历第一个文件ss.txt</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">（a）获取文件大小fs.sizeOf(ss.txt)</span><br><span class="line">（b）计算切片大小`computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M`</span><br><span class="line">（c）默认情况下，切片大小=blocksize，如果增大切片大小，则将minSize设置大于128M，如果要减小切片大小，则将maxSize设置小于128M</span><br><span class="line">   （d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，大于1.1倍就划分一块切片，小于1.1倍就不再切了）</span><br><span class="line">（e）将切片信息写到一个切片规划文件中</span><br><span class="line">   （f）整个切片的核心过程在getSplit()方法中完成</span><br><span class="line">   （g）InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等。</span><br></pre></td></tr></table></div></figure>
<p>（4）提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数。</p>

        <h3 id="13-fileinputformat-切片机制"   >
          <a href="#13-fileinputformat-切片机制" class="heading-link"><i class="fas fa-link"></i></a>1.3 FileInputFormat 切片机制</h3>
      
<p>一、<strong>FileInputFormat切片机制</strong></p>
<p>（1）切片机制</p>
<ol>
<li>简单地按照文件的内容长度进行切片</li>
<li>切片大小，默认等于Block大小</li>
<li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li>
</ol>
<p>（2）案例分析<br />
① 输入数据有两个文件：</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file1.txt 320M</span><br><span class="line">file2.txt 10M</span><br></pre></td></tr></table></div></figure>
<p>② 经过FileInputFormat的切片机制运算后，形成的切片信息如下：</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">file1.txt.split1-- 0~128</span><br><span class="line">file1.txt.split2-- 128~256</span><br><span class="line">file1.txt.split3-- 256~320</span><br><span class="line">file2.txt.split1-- 0~10M</span><br></pre></td></tr></table></div></figure>
<p>二、<strong>FileInputFormat切片大小的参数配置</strong></p>
<p>（1）源码中计算切片大小的公式</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Math.max(minSize, Math.min(maxSize, blockSize));</span><br></pre></td></tr></table></div></figure>
<p><code>mapreduce.input.fileinputformat.split.minsize=1</code> 默认值为1<br />
<code>mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue</code> 默认值Long.MAXValue<br />
因此，默认情况下，切片大小=blocksize。</p>
<p>（2）切片大小设置</p>
<ul>
<li>maxsize（切片最大值）：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。</li>
<li>minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blockSize还大。</li>
</ul>
<p>（3）获取切片信息API</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取切片的文件名称</span></span><br><span class="line">String name = inputSplit.getPath().getName();</span><br><span class="line"><span class="comment">// 根据文件类型获取切片信息</span></span><br><span class="line">FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br></pre></td></tr></table></div></figure>

        <h3 id="14-fileinputformat实现类"   >
          <a href="#14-fileinputformat实现类" class="heading-link"><i class="fas fa-link"></i></a>1.4 FileInputFormat实现类</h3>
      
<p>（1）FileInputFormat 实现类<br />
思考：在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。 那么，针对不同的数据类型， MapReduce 是如何读取这些数据的呢？</p>
<p>FileInputFormat 常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat 和自定义 InputFormat 等。</p>
<p>（2）TextInputFormat<br />
TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。 键是存储该行在整个文件中的起始字节偏移量，  LongWritable 类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text 类型。</p>
<p>以下是一个示例，比如，一个分片包含了如下 4 条文本记录。</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br></pre></td></tr></table></div></figure>
<p>每条记录表示为以下键/值对：</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(0,Rich learning form)</span><br><span class="line">(20,Intelligent learning engine)</span><br><span class="line">(49,Learning more convenient)</span><br><span class="line">(74,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></div></figure>
<p>（3）KeyValueTextInputFormat<br />
每行均为一条记录，被分隔符分割为key，value。可以通过在驱动类中设置<code>conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, &quot;\t&quot;);</code>来设定分隔符。默认分隔符是tab（<code>\t</code>）。<br />
以下是一个示例，输入时一个包含4条记录的分片，其中<code>——&gt;</code>表示一个（水平方向的）制表符<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/2021040214484549.png"  alt="在这里插入图片描述" />
      </p>
<p>每条记录表示以下键值对：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402144927250.png"  alt="在这里插入图片描述" />
      <br />
此时的健是每行排在制表符之前的Text序列。</p>
<p>（4）NLineInputFormat<br />
如果使用NLineInputFormat指定的行数N来划分。即输入文件的总行数 除以 N =切片数，如果不整除，切片数=商+1。<br />
以下是一个示例，仍然以上面的4行输入为例。<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402145436545.png"  alt="在这里插入图片描述" />
      </p>
<p>例如，如果N是2，则每个输入分片包含两行，开启2个MapTask。<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/2021040214553316.png"  alt="在这里插入图片描述" />
      <br />
另一个mapper则收到后两行。</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402145602114.png"  alt="在这里插入图片描述" />
      </p>

        <h3 id="15-combinetextinputformat-切片机制"   >
          <a href="#15-combinetextinputformat-切片机制" class="heading-link"><i class="fas fa-link"></i></a>1.5 CombineTextInputFormat 切片机制</h3>
      
<p>框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，<strong>不管文件多小，都会是一个单独的切片，都会交给一个 MapTask</strong>，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</p>
<p>（1）应用场景：<br />
CombineTextInputFormat 用于小文件过多的场景， 它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。</p>
<p>（2）虚拟存储切片最大值设置</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);<span class="comment">// 4m</span></span><br></pre></td></tr></table></div></figure>
<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
<p>（3）切片机制<br />
生成切片过程包括：虚拟存储过程和切片过程二部分。 举个例子来说明：<br />
假如有四个文件，每个文件大小如下：</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a.txt 1.7M</span><br><span class="line">b.txt 5.1M</span><br><span class="line">c.txt 3.4M</span><br><span class="line">d.txt 6.8M</span><br></pre></td></tr></table></div></figure>
<p>处理过程如下图所示<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402141939635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>
<p><strong>对于虚拟存储过程</strong></p>
<p>将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块； 当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片）。</p>
<p>例如 setMaxInputSplitSize 值为 4M，输入文件大小为 8.02M，则先逻辑上分成一个4M。剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储文件，所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件。</p>
<p><strong>对于切片过程</strong></p>
<p>（a）判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独形成一个切片。<br />
（b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片，依次类推，直到产生大于4M的切片为止。<br />
（c） 测试举例：有 4 个小文件大小分别为 1.7M、 5.1M、 3.4M 以及 6.8M 这四个小文件，则虚拟存储之后形成 6 个文件块，大小分别为：</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.7M，（2.55M、2.55M），3.4M 以及（3.4M、3.4M）</span><br></pre></td></tr></table></div></figure>
<p>最终会形成 3 个切片，大小分别为：</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</span><br></pre></td></tr></table></div></figure>

        <h3 id="16-combinetextinputformat-案例实操"   >
          <a href="#16-combinetextinputformat-案例实操" class="heading-link"><i class="fas fa-link"></i></a>1.6 CombineTextInputFormat 案例实操</h3>
      
<p>（1）需求<br />
将输入的大量小文件合并成一个切片统一处理。<br />
① 输入数据，准备 4 个小文件<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402142824388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>
<p>② 期望一个切片处理 4 个文件</p>
<p>（2）实现过程<br />
① 不做任何处理，运行之前的 WordCount 案例程序，即采用默认的TextInputFormat处理，观察切片个数为 4。</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">number of splits:4</span><br></pre></td></tr></table></div></figure>
<p>② 在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 3。</p>
<ul>
<li>驱动类中添加代码如下：</li>
</ul>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  如果不设置 InputFormat，它默认用的是 TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置 4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br></pre></td></tr></table></div></figure>
<ul>
<li>运行如果为 3 个切片</li>
</ul>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">number of splits:3</span><br></pre></td></tr></table></div></figure>
<p>③ 在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 1。</p>
<ul>
<li>驱动中添加代码如下：</li>
</ul>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  如果不设置 InputFormat，它默认用的是 TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置 20m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">20971520</span>);</span><br></pre></td></tr></table></div></figure>
<ul>
<li>运行如果为 1 个切片</li>
</ul>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">number of splits:1</span><br></pre></td></tr></table></div></figure>

        <h3 id="17-keyvaluetextinputformat使用案例"   >
          <a href="#17-keyvaluetextinputformat使用案例" class="heading-link"><i class="fas fa-link"></i></a>1.7 KeyValueTextInputFormat使用案例</h3>
      
<p>（1）需求<br />
统计输入文件中每一行的第一个单词相同的行数。</p>
<p>① 输入数据</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">banzhang ni hao</span><br><span class="line">xihuan hadoop banzhang</span><br><span class="line">banzhang ni hao</span><br><span class="line">xihuan hadoop banzhang</span><br></pre></td></tr></table></div></figure>
<p>② 期望结果数据</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">banzhang	2</span><br><span class="line">xihuan	2</span><br></pre></td></tr></table></div></figure>
<p>（2）需求分析<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402150723649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      <br />
（3）代码实现<br />
① 编写Mapper类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.KeyValueTextInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 设置value</span></span><br><span class="line">    LongWritable v = <span class="keyword">new</span> LongWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// banzhang ni hao</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 写出</span></span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>② 编写reduce类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.KeyValueTextInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    LongWritable v = <span class="keyword">new</span> LongWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values,	Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> sum = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 汇总统计</span></span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        v.set(sum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 输出</span></span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>③ 编写driver类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.KeyValueTextInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 设置切割符</span></span><br><span class="line">        conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">" "</span>);</span><br><span class="line">        <span class="comment">// 1 获取job对象</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar包位置，关联mapper和reducer</span></span><br><span class="line">        job.setJarByClass(KVTextDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapperClass(KVTextMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(KVTextReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 设置map输出kv类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置最终输出kv类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置输入输出数据路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\kvinputformat"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入格式</span></span><br><span class="line">        job.setInputFormatClass(KeyValueTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输出数据路径</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\kvoutputformat"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h3 id="18-nlineinputformat使用案例"   >
          <a href="#18-nlineinputformat使用案例" class="heading-link"><i class="fas fa-link"></i></a>1.8 NLineInputFormat使用案例</h3>
      
<p>（1）需求<br />
对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中。</p>
<p>① 输入数据</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">banzhang ni hao</span><br><span class="line">xihuan hadoop banzhang</span><br><span class="line">banzhang ni hao</span><br><span class="line">xihuan hadoop banzhang</span><br><span class="line">banzhang ni hao</span><br><span class="line">xihuan hadoop banzhang</span><br><span class="line">banzhang ni hao</span><br><span class="line">xihuan hadoop banzhang</span><br><span class="line">banzhang ni hao</span><br><span class="line">xihuan hadoop banzhang banzhang ni hao</span><br><span class="line">xihuan hadoop banzhang</span><br></pre></td></tr></table></div></figure>
<p>② 期望输出数据</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of splits:4</span><br></pre></td></tr></table></div></figure>
<p>（2）需求分析</p>
<p>
        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402152431322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>
<p>（3）代码实现</p>
<p>① Mapper类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.nline;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLineMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> LongWritable v = <span class="keyword">new</span> LongWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] splited = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; splited.length; i++) &#123;</span><br><span class="line"></span><br><span class="line">            k.set(splited[i]);</span><br><span class="line"></span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>②Reduce类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.nline;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLineReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    LongWritable v = <span class="keyword">new</span> LongWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values,	Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> sum = <span class="number">0l</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 汇总</span></span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        v.set(sum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 输出</span></span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>③ Driver类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.nline;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLineDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">        args = <span class="keyword">new</span> String[] &#123; <span class="string">"D:\\test\\nlineinput"</span>, <span class="string">"D:\\test\\nlineoutput"</span> &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取job对象</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7设置每个切片InputSplit中划分三条记录</span></span><br><span class="line">        NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 8使用NLineInputFormat处理记录数</span></span><br><span class="line">        job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2设置jar包位置，关联mapper和reducer</span></span><br><span class="line">        job.setJarByClass(NLineDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapperClass(NLineMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(NLineReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3设置map输出kv类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4设置最终输出kv类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5设置输入输出数据路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>（4）测试<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210402153324149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>

        <h2 id="2-mapreduce-工作流程"   >
          <a href="#2-mapreduce-工作流程" class="heading-link"><i class="fas fa-link"></i></a>2. MapReduce 工作流程</h2>
      
<p>MapReduce详细工作流程<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406111821682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      <br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406111901460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      <br />
上面的流程是整个 MapReduce 最全工作流程，但是 Shuffle 过程只是从第 7 步开始到第16 步结束，具体 Shuffle 过程详解，如下：<br />
（1）MapT ask 收集我们的 map()方法输出的 kv 对，放到内存缓冲区中<br />
（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件<br />
（3）多个溢出文件会被合并成大的溢出文件<br />
（4）在溢出过程及合并的过程中，都要调用 Partitioner 进行分区和针对 key 进行排序<br />
（5）ReduceTask 根据自己的分区号，去各个 MapTask 机器上取相应的结果分区数据<br />
（6）ReduceTask 会抓取到同一个分区的来自不同 MapTask 的结果文件， ReduceTask 会将这些文件再进行合并（归并排序）<br />
（7）合并成大文件后， Shuffle的过程也就结束了，后面进入ReduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 Group，调用用户自定义的 reduce()方法）<br />
注意：<br />
（1）Shuffle 中的缓冲区大小会影响到 MapReduce 程序的执行效率，原则上说，缓冲区越大，磁盘 io 的次数越少，执行速度就越快。<br />
（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认 100M。</p>

        <h2 id="3-shuffle-机制"   >
          <a href="#3-shuffle-机制" class="heading-link"><i class="fas fa-link"></i></a>3. Shuffle 机制</h2>
      

        <h3 id="31-shuffle-机制"   >
          <a href="#31-shuffle-机制" class="heading-link"><i class="fas fa-link"></i></a>3.1 Shuffle 机制</h3>
      
<p>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406115725933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>

        <h3 id="32-partition-分区"   >
          <a href="#32-partition-分区" class="heading-link"><i class="fas fa-link"></i></a>3.2 Partition 分区</h3>
      
<p>（1）问题引出</p>
<p>要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
<p>（2）默认Partitioner分区</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。</p>
<p>（3）自定义Partitioner步骤</p>
<p>① 自定义类继承Partitioner，重写getPartition()方法</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 控制分区代码逻辑</span></span><br><span class="line">		… …</span><br><span class="line">		<span class="keyword">return</span> partition;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>② 在Job驱动中，设置自定义Partitioner</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></div></figure>
<p>③ 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></div></figure>
<p>（4）分区总结</p>
<ul>
<li>如果ReduceTask的数量 <code>&gt;</code> getPartition的结果数，则会多产生几个空的输出文件part-r-000xx</li>
<li>如果1 <code>&lt;</code> ReduceTask的数量 <code>&lt;</code> getPartition的结果数，则有一部分分区数据无处安放，会抛出Exception</li>
<li>如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000；</li>
<li>分区号必须从零开始，逐一累加。</li>
</ul>
<p>（5）案例分析<br />
例如：假设自定义分区数为5，则</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">1</span>); <span class="comment">//会正常运行，只不过会产生一个输出文件</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">2</span>); <span class="comment">//会报错</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">6</span>); <span class="comment">//大于5，程序会正常运行，会产生空文件</span></span><br><span class="line"><span class="comment">// 不写job.setNumReduceTasks，默认同job.setNumReduceTasks(1)</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>); <span class="comment">//相当于没有reduce阶段</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="33-partition-分区案例实操"   >
          <a href="#33-partition-分区案例实操" class="heading-link"><i class="fas fa-link"></i></a>3.3 Partition 分区案例实操</h3>
      
<p>（1）需求分析</p>
<p>① 需求：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
<p>② 输入数据phone_data.txt</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200</span><br><span class="line">2	13846544121	192.196.100.2			264	0	200</span><br><span class="line">3 	13956435636	192.196.100.3			132	1512	200</span><br><span class="line">4 	13966251146	192.168.100.1			240	0	404</span><br><span class="line">5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200</span><br><span class="line">6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200</span><br><span class="line">7 	13590439668	192.168.100.4			1116	954	200</span><br><span class="line">8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200</span><br><span class="line">9 	13729199489	192.168.100.6			240	0	200</span><br><span class="line">10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200</span><br><span class="line">11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200</span><br><span class="line">12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500</span><br><span class="line">13 	13560439638	192.168.100.10			918	4938	200</span><br><span class="line">14 	13470253144	192.168.100.11			180	180	200</span><br><span class="line">15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200</span><br><span class="line">16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200</span><br><span class="line">17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404</span><br><span class="line">18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200</span><br><span class="line">19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200</span><br><span class="line">20 	13768778790	192.168.100.17			120	120	200</span><br><span class="line">21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200</span><br><span class="line">22 	13568436656	192.168.100.19			1116	954	200</span><br></pre></td></tr></table></div></figure>
<p>③ 期望数据输出</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">文件1</span><br><span class="line">文件2</span><br><span class="line">文件3</span><br><span class="line">文件4</span><br><span class="line">文件5</span><br></pre></td></tr></table></div></figure>
<p>④ 增加一个ProvincePartitioner分区</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">36 分区0</span><br><span class="line">137 分区1</span><br><span class="line">138 分区2</span><br><span class="line">139 分区3</span><br><span class="line">其他 分区4</span><br></pre></td></tr></table></div></figure>
<p>⑤  Drive驱动类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定自定义数据分区</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 同时指定相应数量的reduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></div></figure>
<p>（2）实现</p>
<p>在序列化案例实操中增加一个分区类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.partitioner2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//这个输入是Map输出的K，V</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// text 是手机号</span></span><br><span class="line"></span><br><span class="line">        String phone = text.toString();</span><br><span class="line"></span><br><span class="line">        String prePhone = phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> partition ;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"136"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>在驱动函数中增加自定义数据分区设置和 ReduceTask 设置</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.partitioner2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取job</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar</span></span><br><span class="line">        job.setJarByClass(FlowDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联mapper 和Reducer</span></span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置mapper 输出的key和value类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终数据输出的key和value类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置数据的输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\inputflow"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\output666"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h3 id="34-writablecomparable-排序"   >
          <a href="#34-writablecomparable-排序" class="heading-link"><i class="fas fa-link"></i></a>3.4 WritableComparable 排序</h3>
      
<p>排序是MapReduce框架中最重要的操作之一。</p>
<p>MapTask 和 ReduceTask 均会对数据 按照 key 进行排序 。 该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。</p>
<p>默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。</p>
<p>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值（80%）后，再对缓冲区中的数据进行一次快速排序（排序的过程是在内存中完成的），并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。</p>
<p>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值（即超过ReduceTask进程内存缓冲区的大小），则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask <strong>统一对内存和磁盘上的所有数据进行一次归并排序</strong>。</p>
<p><strong>排序分类</strong></p>
<p>（1）部分排序<br />
MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。<br />
（2）全排序<br />
最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。<br />
（3）辅助排序：（GroupingComparator分组）<br />
在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（<strong>全部字段比较不相同</strong>）的key进入到同一个reduce方法时，可以采用分组排序。<br />
（4）二次排序（或自定义排序）<br />
在自定义排序过程中，如果compareTo中的<strong>判断条件为两个</strong>即为二次排序。</p>
<p><strong>自定义排序WritableComparable原理分析</strong></p>
<p><strong>bean对象</strong>做为 key 传输，需要实现 WritableComparable 接口重写 compareTo 方法， 就可以实现排序。</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean bean)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> result;</span><br><span class="line">	<span class="comment">//  按照总流量大小，倒序排列</span></span><br><span class="line">	<span class="keyword">if</span> (<span class="keyword">this</span>.sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = -<span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = <span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">		result = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h3 id="35-writablecomparable-排序案例实操全排序"   >
          <a href="#35-writablecomparable-排序案例实操全排序" class="heading-link"><i class="fas fa-link"></i></a>3.5 WritableComparable 排序案例实操（全排序）</h3>
      
<p>该案例在在序列化案例实操中基础上实现</p>
<p>（1）需求分析</p>
<p>① 需求：根据手机的总流量进行倒序排序</p>
<p>② 输入数据</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200</span><br><span class="line">2	13846544121	192.196.100.2			264	0	200</span><br><span class="line">3 	13956435636	192.196.100.3			132	1512	200</span><br><span class="line">4 	13966251146	192.168.100.1			240	0	404</span><br><span class="line">5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200</span><br><span class="line">6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200</span><br><span class="line">7 	13590439668	192.168.100.4			1116	954	200</span><br><span class="line">8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200</span><br><span class="line">9 	13729199489	192.168.100.6			240	0	200</span><br><span class="line">10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200</span><br><span class="line">11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200</span><br><span class="line">12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500</span><br><span class="line">13 	13560439638	192.168.100.10			918	4938	200</span><br><span class="line">14 	13470253144	192.168.100.11			180	180	200</span><br><span class="line">15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200</span><br><span class="line">16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200</span><br><span class="line">17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404</span><br><span class="line">18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200</span><br><span class="line">19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200</span><br><span class="line">20 	13768778790	192.168.100.17			120	120	200</span><br><span class="line">21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200</span><br><span class="line">22 	13568436656	192.168.100.19			1116	954	200</span><br></pre></td></tr></table></div></figure>
<p>③ 输出数据</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">13509468723 7335 110349  117684</span><br><span class="line">13736230513 2481 24681 27162</span><br><span class="line">13956435636 132 1512 1644</span><br><span class="line">13846544121 264 0 264</span><br><span class="line">。。。 。。。</span><br></pre></td></tr></table></div></figure>
<p>④ FlowBean实现WritableComparable接口重写compareTo方法</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 倒序排列，按照总流量从大到小</span></span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>⑤ Mapper类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.write(bean，手机号)</span><br></pre></td></tr></table></div></figure>
<p>⑥ Reducer类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 循环输出，避免总流量相同情况</span></span><br><span class="line"><span class="keyword">for</span> (Text text : values) &#123;</span><br><span class="line">	context.write(text, key);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>（2）代码实现<br />
① FlowBean 对象在在需求 1 基础上增加了比较功能</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 1、定义类实现WritableComparable接口</span></span><br><span class="line"><span class="comment"> * 2、重写序列化和反序列化方法</span></span><br><span class="line"><span class="comment"> * 3、重写空参构造</span></span><br><span class="line"><span class="comment"> * 4、toString方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> upFlow; <span class="comment">// 上行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> downFlow; <span class="comment">// 下行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> sumFlow; <span class="comment">// 总流量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 空参构造</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = <span class="keyword">this</span>.upFlow + <span class="keyword">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = in.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = in.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 总流量的倒序排序</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.sumFlow &gt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.sumFlow &lt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 按照上行流量的正序排</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.upFlow &gt; o.upFlow) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.upFlow &lt; o.upFlow) &#123;</span><br><span class="line">                <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>② 编写 Mapper 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="comment">//注意输出的Key是FlowBean，Valuer是Text</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FlowBean outK = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    <span class="keyword">private</span> Text outV = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        <span class="comment">// 1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        <span class="comment">// 1,13736230513,192.196.100.1,www.atguigu.com,2481,24681,200   7 - 3= 4</span></span><br><span class="line">        <span class="comment">// 2	13846544121	192.196.100.2			264	0	200  6 - 3 = 3</span></span><br><span class="line">        String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 抓取想要的数据</span></span><br><span class="line">        <span class="comment">// 手机号：13736230513</span></span><br><span class="line">        <span class="comment">// 上行流量和下行流量：2481,24681</span></span><br><span class="line">        String phone = split[<span class="number">1</span>];</span><br><span class="line">        String up = split[split.length - <span class="number">3</span>];</span><br><span class="line">        String down  = split[split.length - <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4封装</span></span><br><span class="line">        outV.set(phone);</span><br><span class="line">        outK.setUpFlow(Long.parseLong(up));</span><br><span class="line">        outK.setDownFlow(Long.parseLong(down));</span><br><span class="line">        outK.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>③ 编写 Reducer 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">////遍历 values 集合,循环写出,避免总流量相同的情况</span></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">////调换 KV 位置,反向写出</span></span><br><span class="line">            context.write(value,key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>④ 编写Drive类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取job</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar</span></span><br><span class="line">        job.setJarByClass(FlowDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联mapper 和Reducer</span></span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置mapper 输出的key和value类型，注意Key和Value的变化</span></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终数据输出的key和value类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置数据的输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\inputflow"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\output777"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交job</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>⑤ 输出结果</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">13509468723	7335	110349	117684</span><br><span class="line">13975057813	11058	48243	59301</span><br><span class="line">13736230513	2481	24681	27162</span><br><span class="line">13568436656	2481	24681	27162</span><br><span class="line">18390173782	9531	2412	11943</span><br><span class="line">13630577991	6960	690	7650</span><br><span class="line">15043685818	3659	3538	7197</span><br><span class="line">13992314666	3008	3720	6728</span><br><span class="line">15910133277	3156	2936	6092</span><br><span class="line">13560439638	918	4938	5856</span><br><span class="line">84188413	4116	1432	5548</span><br><span class="line">13682846555	1938	2910	4848</span><br><span class="line">18271575951	1527	2106	3633</span><br><span class="line">15959002129	1938	180	2118</span><br><span class="line">13590439668	1116	954	2070</span><br><span class="line">13568436656	1116	954	2070</span><br><span class="line">13956435636	132	1512	1644</span><br><span class="line">13470253144	180	180	360</span><br><span class="line">13846544121	264	0	264</span><br><span class="line">13768778790	120	120	240</span><br><span class="line">13729199489	240	0	240</span><br><span class="line">13966251146	240	0	240</span><br></pre></td></tr></table></div></figure>

        <h3 id="36-writablecomparable-排序案例实操区内排序"   >
          <a href="#36-writablecomparable-排序案例实操区内排序" class="heading-link"><i class="fas fa-link"></i></a>3.6 WritableComparable 排序案例实操（区内排序）</h3>
      
<p>该案例基于 WritableComparable 排序案例实操（全排序）实现</p>
<p>（1）需求分析<br />
① 需求：要求每个省份手机号输出的文件中按照总流量内部排序。即基于前一个需求，增加自定义分区类，分区按照省份手机号设置。<br />
② 输入</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200</span><br><span class="line">2	13846544121	192.196.100.2			264	0	200</span><br><span class="line">3 	13956435636	192.196.100.3			132	1512	200</span><br><span class="line">4 	13966251146	192.168.100.1			240	0	404</span><br><span class="line">5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200</span><br><span class="line">6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200</span><br><span class="line">7 	13590439668	192.168.100.4			1116	954	200</span><br><span class="line">8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200</span><br><span class="line">9 	13729199489	192.168.100.6			240	0	200</span><br><span class="line">10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200</span><br><span class="line">11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200</span><br><span class="line">12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500</span><br><span class="line">13 	13560439638	192.168.100.10			918	4938	200</span><br><span class="line">14 	13470253144	192.168.100.11			180	180	200</span><br><span class="line">15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200</span><br><span class="line">16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200</span><br><span class="line">17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404</span><br><span class="line">18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200</span><br><span class="line">19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200</span><br><span class="line">20 	13768778790	192.168.100.17			120	120	200</span><br><span class="line">21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200</span><br><span class="line">22 	13568436656	192.168.100.19			1116	954	200</span><br></pre></td></tr></table></div></figure>
<p>③ 期望输出<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406153228865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      <br />
（2）代码实现</p>
<p>① 增加自定义区类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.partitionercompable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注意K,V的变化</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner2</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(FlowBean flowBean, Text text, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        String phone = text.toString();</span><br><span class="line"></span><br><span class="line">        String prePhone = phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> partition;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"136"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition =  <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition =  <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition =  <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(prePhone))&#123;</span><br><span class="line">            partition =  <span class="number">3</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>② 在驱动类中添加分区类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  设置自定义分区器</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner2<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//  设置对应的 ReduceTask 的个数</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></div></figure>

        <h3 id="37-combiner-合并"   >
          <a href="#37-combiner-合并" class="heading-link"><i class="fas fa-link"></i></a>3.7 Combiner 合并</h3>
      
<p>
        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406154912654.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      <br />
由于溢出文件有很多，但是归并文件的个数有限，默认一次归并10个文件，所以有很多次归并排序操作。</p>
<p><strong>Combiner合并</strong><br />
（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。<br />
（2）Combiner组件的父类就是Reducer。<br />
（3）Combiner和Reducer的区别在于运行的位置</p>
<ul>
<li>Combiner是在每一个MapTask所在的节点运行；</li>
<li>Reducer是接收全局所有Mapper的输出结果；</li>
</ul>
<p>（4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。<br />
（5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟Reducer的输入kv类型要对应起来。</p>
<p>例如，我们要求平均值，就不符合要求<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406155913331.png"  alt="在这里插入图片描述" />
      <br />
但是，如果我们要求和，那么使用Combiner后，不影响最终结果。</p>
<p>（6）自定义 Combiner 实现步骤<br />
① 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span>  <span class="class"><span class="keyword">class</span>  <span class="title">WordCountCombiner</span>  <span class="keyword">extends</span>  <span class="title">Reducer</span>&lt;<span class="title">Text</span>,  <span class="title">IntWritable</span>,  <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> IntWritable outV = <span class="keyword">new</span> IntWritable();</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">			sum += value.get();</span><br><span class="line">		&#125;</span><br><span class="line">		outV.set(sum);</span><br><span class="line">		context.write(key,outV);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>② 在 Job 驱动类中设置：</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountCombiner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></div></figure>

        <h3 id="38-combiner-合并案例实操"   >
          <a href="#38-combiner-合并案例实操" class="heading-link"><i class="fas fa-link"></i></a>3.8 Combiner 合并案例实操</h3>
      
<p>（1）需求分析</p>
<p>① 需求：统计过程中对每一个 MapTask 的输出进行局部汇总，以减小网络传输量即采用Combiner 功能。</p>
<p>② 数据输入</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">banzhang ni hao</span><br><span class="line">xihuan hadoop</span><br><span class="line">banzhang</span><br><span class="line">banzhang ni hao</span><br><span class="line">xihuan hadoop</span><br><span class="line">banzhang</span><br></pre></td></tr></table></div></figure>
<p>总12个单词<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406161106715.png"  alt="在这里插入图片描述" />
      </p>
<p>③ 期望：Combine 输入数据多，输出时经过合并，输出数据降低。<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/2021040616104256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      <br />
（2）方案一<br />
① 增加一个WordcountCombiner类继承Reducer<br />
② 在WordcountCombiner中</p>
<ul>
<li>统计单词汇总</li>
<li>将结果输出</li>
</ul>
<p>（3）方案二</p>
<p>将WordcountReducer作为Combiner 在WordcountDriver驱动类中指定</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordcountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></div></figure>
<p>（4）案例实操-方案一<br />
本案例在wordCount基础上实现</p>
<p>① 增加一个 WordCountCombiner 类继承 Reducer</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>,<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outV = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sum =<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>（2）在 WordcountDriver 驱动类中指定 Combiner</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  指定需要使用 combiner，以及用哪个类作为 combiner 的逻辑</span></span><br><span class="line">job.setCombinerClass(WordCountCombiner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></div></figure>
<p>（5）案例实操-方案二<br />
① 将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  指定需要使用 Combiner，以及用哪个类作为 Combiner 的逻辑</span></span><br><span class="line">job.setCombinerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></div></figure>
<p>运行结果如下：<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406163016528.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>

        <h2 id="4-outputformat-数据输出"   >
          <a href="#4-outputformat-数据输出" class="heading-link"><i class="fas fa-link"></i></a>4. OutputFormat 数据输出</h2>
      
<p>
        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406171843851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>

        <h3 id="41-outputformat-接口实现类"   >
          <a href="#41-outputformat-接口实现类" class="heading-link"><i class="fas fa-link"></i></a>4.1 OutputFormat 接口实现类</h3>
      
<p>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。</p>
<p>其子类继承关系如下：<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406172403280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      </p>
<ol>
<li>默认输出格式为TextOutputFormat</li>
<li>自定义OutputFormat<br />
– 应用场景：<br />
例如：输出数据到MySQL/HBase/Elasticsearch等存储框架中。<br />
– 自定义OutputFormat步骤<br />
➢ 自定义一个类继承FileOutputFormat。<br />
➢ 改写RecordWriter，具体改写输出数据的方法write()。</li>
</ol>

        <h3 id="42-自定义-outputformat-案例实操"   >
          <a href="#42-自定义-outputformat-案例实操" class="heading-link"><i class="fas fa-link"></i></a>4.2 自定义 OutputFormat 案例实操</h3>
      
<p>（1）需求分析<br />
① 需求：过滤输入的 log 日志，包含 atguigu 的网站输出到 e:/atguigu.log，不包含 atguigu 的网站输出到 e:/other.log。</p>
<p>② 输入数据</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">http://www.baidu.com</span><br><span class="line">http://www.google.com</span><br><span class="line">http://cn.bing.com</span><br><span class="line">http://www.atguigu.com</span><br><span class="line">http://www.sohu.com</span><br><span class="line">http://www.sina.com</span><br><span class="line">http://www.sin2a.com</span><br><span class="line">http://www.sin2desa.com</span><br><span class="line">http://www.sindsafa.com</span><br><span class="line">http://www.atguigu.com</span><br><span class="line">http://cn.bing.com</span><br><span class="line">http://www.baidu.com</span><br><span class="line">http://www.google.com</span><br><span class="line">http://www.sin2a.com</span><br><span class="line">http://www.sin2desa.com</span><br></pre></td></tr></table></div></figure>
<p>③ 输出数据<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20210406172805607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmU=,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L215bGF5bmUpx;"  alt="在这里插入图片描述" />
      <br />
④ 自定义一个OutputFormat类<br />
创建一个类LogRecordWriter继承RecordWriter</p>
<ul>
<li>创建两个文件的输出流：atguiguOut、otherOut</li>
<li>如果输入数据包含atguigu，输出到atguiguOut流如果不包含atguigu，输出到otherOut流</li>
</ul>
<p>⑤ 驱动类Driver</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">job.setOutputFormatClass(LogOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></div></figure>
<p>（2）代码实现</p>
<p>① 编写 LogMapper 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.outputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// http://www.baidu.com</span></span><br><span class="line">        <span class="comment">//http://www.google.com</span></span><br><span class="line">        <span class="comment">// (http://www.google.com, NullWritable)</span></span><br><span class="line">        <span class="comment">// 不做任何处理</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>② 编写 LogReducer 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.outputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// http://www.baidu.com</span></span><br><span class="line">        <span class="comment">// http://www.baidu.com</span></span><br><span class="line">        <span class="comment">// 防止有相同数据，丢数据</span></span><br><span class="line">        <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>③ 自定义一个 LogOutputFormat 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.outputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="comment">//这个reduce输出的K,V</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        LogRecordWriter lrw = <span class="keyword">new</span> LogRecordWriter(job);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> lrw;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>④ 编写 LogRecordWriter 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.outputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>  FSDataOutputStream otherOut;</span><br><span class="line">    <span class="keyword">private</span>  FSDataOutputStream atguiguOut;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LogRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 创建两条流</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            FileSystem fs = FileSystem.get(job.getConfiguration());</span><br><span class="line"></span><br><span class="line">            atguiguOut = fs.create(<span class="keyword">new</span> Path(<span class="string">"D:\\test\\atguigu.log"</span>));</span><br><span class="line"></span><br><span class="line">            otherOut = fs.create(<span class="keyword">new</span> Path(<span class="string">"D:\\test\\other.log"</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String log = key.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//根据一行的 log 数据是否包含 atguigu,判断两条输出流输出的内容</span></span><br><span class="line">        <span class="keyword">if</span> (log.contains(<span class="string">"atguigu"</span>))&#123;</span><br><span class="line">            atguiguOut.writeBytes(log+<span class="string">"\n"</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            otherOut.writeBytes(log+<span class="string">"\n"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 关流</span></span><br><span class="line">        IOUtils.closeStream(atguiguOut);</span><br><span class="line">        IOUtils.closeStream(otherOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>⑤ 编写 LogDriver 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.outputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(LogDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapperClass(LogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(LogReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置自定义的outputformat</span></span><br><span class="line">        job.setOutputFormatClass(LogOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\inputoutputformat"</span>));</span><br><span class="line">        <span class="comment">//虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat</span></span><br><span class="line">        <span class="comment">//而fileoutputformat要输出一个_SUCCESS文件，所以在这还得指定一个输出目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\output1111"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h2 id="5-mapreduce-内核源码解析"   >
          <a href="#5-mapreduce-内核源码解析" class="heading-link"><i class="fas fa-link"></i></a>5. MapReduce 内核源码解析</h2>
      

        <h3 id="51-maptask-工作机制"   >
          <a href="#51-maptask-工作机制" class="heading-link"><i class="fas fa-link"></i></a>5.1 MapTask 工作机制</h3>
      
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406193836.png"  alt="" />
      </p>
<p>（1）Read 阶段：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中解析出一个个 key/value。<br />
（2）Map 阶段：该节点主要是将解析出的 key/value 交给用户编写 map()函数处理，并产生一系列新的 key/value。<br />
（3）Collect 收集阶段：在用户编写 map()函数中，当数据处理完成后，一般会调用OutputCollector.collect() 输出结果。在该函数内部，它会将生成的 key/value 分区（调用Partitioner），并写入一个环形内存缓冲区中。<br />
（4）Spill 阶段：即“溢写”， 当环形缓冲区满后， MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>溢写阶段详情：</p>
<ul>
<li>步骤 1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。</li>
<li>步骤 2： 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</li>
<li>步骤 3： 将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件 output/spillN.out.index 中。</li>
</ul>
<p>（5）Merge 阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件output/file.out 中，同时生成相应的索引文件 output/file.out.index。</p>
<p>在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区， 它将采用多轮递归合并的方式。 每轮合并 mapreduce.task.io.sort.factor （默认 10） 个文件，并将产生的文件重新加入待合并列表中，对文件排后，重复以上过程，直到最终得到一个大文件。让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>

        <h3 id="52-reducetask-并行度决定机制"   >
          <a href="#52-reducetask-并行度决定机制" class="heading-link"><i class="fas fa-link"></i></a>5.2 ReduceTask 并行度决定机制</h3>
      
<p>回顾：MapTask 并行度由切片个数决定，切片个数由输入文件和切片规则决定。</p>
<p>思考：ReduceTask 并行度由谁决定？</p>
<p>（1）设置 ReduceTask 并行度（个数）</p>
<p>ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置：</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  默认值是 1，手动设置为 4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></div></figure>
<p>（2）实验：测试 ReduceTask 多少合适</p>
<p>① 实验环境：1 个 Master 节点，16 个 Slave 节点：CPU:8GHZ，内存: 2G</p>
<p>② 实验结论：</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406195046.png"  alt="" />
      </p>
<p><strong>注意事项</strong></p>
<ul>
<li>ReduceTask=0，表示没有Reduce阶段，输出文件个数和Map个数一致。</li>
<li>ReduceTask默认值就是1，所以输出文件个数为一个。</li>
<li>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜</li>
<li>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask。</li>
<li>具体多少个ReduceTask，需要根据集群性能而定。</li>
<li>如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。</li>
</ul>

        <h3 id="53-maptask-reducetask-源码解析"   >
          <a href="#53-maptask-reducetask-源码解析" class="heading-link"><i class="fas fa-link"></i></a>5.3 MapTask &amp; ReduceTask 源码解析</h3>
      
<p>这里以 Partition 分区案例实操 为例，源码解析MapTask &amp; ReduceTask流程</p>
<p><strong>MapTask</strong></p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//=================== MapTask ===================</span></span><br><span class="line">context.write(k, NullWritable.get());    <span class="comment">//自定义的 map 方法的写出，进入</span></span><br><span class="line">	output.write(key, value);  </span><br><span class="line">		<span class="comment">//MapTask727 行，收集方法，进入两次 </span></span><br><span class="line">		collector.collect(key, value,partitioner.getPartition(key, value, partitions));</span><br><span class="line">			HashPartitioner(); <span class="comment">//默认分区器</span></span><br><span class="line">		collect()  <span class="comment">//MapTask1082 行  map 端所有的 kv 全部写出后会走下面的 close 方法</span></span><br><span class="line">			close() <span class="comment">//MapTask732 行</span></span><br><span class="line">				collector.flush() <span class="comment">//  溢出刷写方法，MapTask735 行，提前打个断点，进入</span></span><br><span class="line">					sortAndSpill() <span class="comment">//溢写排序，MapTask1505 行，进入</span></span><br><span class="line">						sorter.sort()    QuickSort <span class="comment">//溢写排序方法，MapTask1625 行，进入</span></span><br><span class="line">					mergeParts(); <span class="comment">//合并文件，MapTask1527 行，生成file.out和file.out.index</span></span><br><span class="line">				collector.close(); <span class="comment">//MapTask739 行,收集器关闭,即将进入 ReduceTask</span></span><br></pre></td></tr></table></div></figure>
<p><strong>ReduceTask</strong></p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">=================== ReduceT ask ===================</span><br><span class="line"><span class="keyword">if</span> (isMapOrReduce())   <span class="comment">//reduceTask324 行，提前打断点 </span></span><br><span class="line">	initialize()    <span class="comment">// reduceTask333 行,进入</span></span><br><span class="line">	init(shuffleContext);   <span class="comment">// reduceTask375 行,走到这需要先给下面的打断点</span></span><br><span class="line">		totalMaps = job.getNumMapTasks(); <span class="comment">// ShuffleSchedulerImpl 第 120 行，提前打断点</span></span><br><span class="line">		merger = createMergeManager(context); <span class="comment">//合并方法，Shuffle 第 80 行</span></span><br><span class="line">			<span class="comment">// MergeManagerImpl 第 232 235 行，提前打断点</span></span><br><span class="line">			<span class="keyword">this</span>.inMemoryMerger = createInMemoryMerger(); <span class="comment">//内存合并</span></span><br><span class="line">			<span class="keyword">this</span>.onDiskMerger = <span class="keyword">new</span> OnDiskMerger(<span class="keyword">this</span>); <span class="comment">//磁盘合并</span></span><br><span class="line">	rIter = shuffleConsumerPlugin.run();</span><br><span class="line">		eventFetcher.start();  <span class="comment">//开始抓取数据，Shuffle 第 107 行，提前打断点</span></span><br><span class="line">		eventFetcher.shutDown();  <span class="comment">//抓取结束，Shuffle 第 141 行，提前打断点</span></span><br><span class="line">		copyPhase.complete();   <span class="comment">//copy 阶段完成，Shuffle 第 151 行</span></span><br><span class="line">		taskStatus.setPhase(TaskStatus.Phase.SORT);  <span class="comment">//开始排序阶段，Shuffle 第 152 行</span></span><br><span class="line">	sortPhase.complete();   <span class="comment">//排序阶段完成，即将进入 reduce 阶段  reduceTask382 行</span></span><br><span class="line">reduce();   <span class="comment">//reduce 阶段调用的就是我们自定义的 reduce 方法，会被调用多次</span></span><br><span class="line">	cleanup(context); <span class="comment">//reduce 完成之前，会最后调用一次 Reducer 里面的 cleanup 方法</span></span><br></pre></td></tr></table></div></figure>

        <h2 id="6-join应用"   >
          <a href="#6-join应用" class="heading-link"><i class="fas fa-link"></i></a>6. Join应用</h2>
      

        <h3 id="61-reduce-join"   >
          <a href="#61-reduce-join" class="heading-link"><i class="fas fa-link"></i></a>6.1 Reduce Join</h3>
      
<p>Map 端的主要工作：为来自不同表或文件的 key/value 对，打标签以区别不同来源的记录。然后用连接字段作为 key，其余部分和新加的标志作为 value，最后进行输出。</p>
<p>Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进行合并就 ok 了。</p>

        <h3 id="62-reduce-join-案例实操"   >
          <a href="#62-reduce-join-案例实操" class="heading-link"><i class="fas fa-link"></i></a>6.2  Reduce Join 案例实操</h3>
      
<p>（1）需求</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406201739.png"  alt="" />
      </p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406201720.png"  alt="" />
      </p>
<p>将商品信息表中数据根据商品 pid 合并到订单数据表中。</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202011.png"  alt="" />
      </p>
<p>（2）需求分析</p>
<p>通过将关联条件作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。</p>
<p>① 输入数据</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202105.png"  alt="" />
      </p>
<p>② 预期输出数据</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202142.png"  alt="" />
      </p>
<p>③ MapTask</p>
<p>Map中处理的事情</p>
<ul>
<li>获取输入文件类型</li>
<li>获取输入数据</li>
<li>不同文件分别处理</li>
<li>封装Bean对象输出</li>
</ul>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202304.png"  alt="" />
      </p>
<p>默认产品id排序</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202332.png"  alt="" />
      </p>
<p>④ ReduceTask</p>
<p>Reduce方法缓存订单数据集合，和产品表，然后合并</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406202554.png"  alt="" />
      </p>
<p>（3）代码实现</p>
<p>① 创建商品和订单合并后的 TableBean 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id; <span class="comment">// 订单id</span></span><br><span class="line">    <span class="keyword">private</span> String pid; <span class="comment">// 商品id</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> amount; <span class="comment">// 商品数量</span></span><br><span class="line">    <span class="keyword">private</span> String pname;<span class="comment">// 商品名称</span></span><br><span class="line">    <span class="keyword">private</span> String flag; <span class="comment">// 标记是什么表 order pd</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 空参构造</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TableBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPid</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPid</span><span class="params">(String pid)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pid = pid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAmount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAmount</span><span class="params">(<span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.amount = amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPname</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pname;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPname</span><span class="params">(String pname)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pname = pname;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getFlag</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFlag</span><span class="params">(String flag)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.flag = flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeUTF(id);</span><br><span class="line">        out.writeUTF(pid);</span><br><span class="line">        out.writeInt(amount);</span><br><span class="line">        out.writeUTF(pname);</span><br><span class="line">        out.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.id = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.pid = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.amount = in.readInt();</span><br><span class="line">        <span class="keyword">this</span>.pname = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.flag = in.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// id	pname	amount</span></span><br><span class="line">        <span class="keyword">return</span>  id + <span class="string">"\t"</span> +  pname + <span class="string">"\t"</span> + amount ;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>② 编写 TableMapper 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String fileName;</span><br><span class="line">    <span class="keyword">private</span> Text outK  = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> TableBean outV = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 初始化  order.txt  pd.txt</span></span><br><span class="line">        FileSplit split = (FileSplit) context.getInputSplit();</span><br><span class="line">		<span class="comment">//获取对应文件名称</span></span><br><span class="line">        fileName = split.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 判断是哪个文件的</span></span><br><span class="line">        <span class="keyword">if</span> (fileName.contains(<span class="string">"order"</span>))&#123;<span class="comment">// 处理的是订单表</span></span><br><span class="line"></span><br><span class="line">            String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 封装k  v</span></span><br><span class="line">            outK.set(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setId(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPid(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setAmount(Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            outV.setPname(<span class="string">""</span>);</span><br><span class="line">            outV.setFlag(<span class="string">"order"</span>);</span><br><span class="line"></span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;<span class="comment">// 处理的是商品表</span></span><br><span class="line">            String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">            outK.set(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setId(<span class="string">""</span>);</span><br><span class="line">            outV.setPid(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setAmount(<span class="number">0</span>);</span><br><span class="line">            outV.setPname(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>③ 编写 TableReducer 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationTargetException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>,<span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//        01 	1001	1   order</span></span><br><span class="line"><span class="comment">//        01 	1004	4   order</span></span><br><span class="line"><span class="comment">//        01	小米   	     pd</span></span><br><span class="line">        <span class="comment">// 准备初始化集合</span></span><br><span class="line">        ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        TableBean pdBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 循环遍历</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (<span class="string">"order"</span>.equals(value.getFlag()))&#123;<span class="comment">// 订单表</span></span><br><span class="line">				<span class="comment">//创建一个临时 TableBean 对象接收 value</span></span><br><span class="line">                TableBean tmptableBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">                <span class="comment">//如果直接用orderBeans.add(value)，则只能添加一个对象</span></span><br><span class="line">                <span class="comment">//因为Hadoop处理的时候下一个value会将上一个value的地址覆盖</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">//复制对象</span></span><br><span class="line">                    BeanUtils.copyProperties(tmptableBean,value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">				<span class="comment">//将临时 TableBean 对象添加到集合 orderBeans</span></span><br><span class="line">                orderBeans.add(tmptableBean);</span><br><span class="line">            &#125;<span class="keyword">else</span> &#123;<span class="comment">// 商品表</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(pdBean,value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 循环遍历orderBeans，赋值 pdname</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean orderBean : orderBeans) &#123;</span><br><span class="line"></span><br><span class="line">            orderBean.setPname(pdBean.getPname());</span><br><span class="line"></span><br><span class="line">            context.write(orderBean,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>④ 编写 TableDriver 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(TableDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapperClass(TableMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(TableReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\inputtable"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\output2345"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>（4）测试</p>
<p>运行程序查看结果</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1004	小米	4</span><br><span class="line">1001	小米	1</span><br><span class="line">1005	华为	5</span><br><span class="line">1002	华为	2</span><br><span class="line">1006	格力	6</span><br><span class="line">1003	格力	3</span><br></pre></td></tr></table></div></figure>
<p>（5）总结</p>
<p>缺点：这种方式中，合并的操作是在 Reduce 阶段完成， Reduce 端的处理压力太大， Map节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜（毕竟在Reduce把所有数据都合并了）。</p>

        <h3 id="63-map-join"   >
          <a href="#63-map-join" class="heading-link"><i class="fas fa-link"></i></a>6.3 Map Join</h3>
      
<p>（1）使用场景<br />
Map Join 适用于一张表十分小、一张表很大的场景。</p>
<p>（2）优点<br />
思考：在 Reduce 端处理过多的表，非常容易产生数据倾斜。怎么办？<br />
在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数据的压力，尽可能的减少数据倾斜。</p>
<p>（3）具体办法：采用 DistributedCache</p>
<p>① 在 Mapper 的 setup 阶段，将文件读取到缓存集合中。</p>
<p>② 在 Driver 驱动类中加载缓存。</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到 Task 运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///e:/cache/pd.txt"</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置 HDFS 路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:8020/cache/pd.txt"</span>));</span><br></pre></td></tr></table></div></figure>

        <h3 id="64-map-join-案例实操"   >
          <a href="#64-map-join-案例实操" class="heading-link"><i class="fas fa-link"></i></a>6.4 Map Join 案例实操</h3>
      
<p>这里使用6.2节的需求分析案例</p>
<p>（1）DistributedCacheDriver 缓存文件</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 加载缓存数据</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///d:/test/cache/pd.txt"</span>));</span><br><span class="line"><span class="comment">//2 Map端join的逻辑不需要Reduce 阶段，设置ReduceTask数量为0</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">0</span>); <span class="comment">//不需要Reduce阶段</span></span><br></pre></td></tr></table></div></figure>
<p>（2）读取缓存的文件数据</p>
<figure class="highlight tex"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">setup()方法中</span><br><span class="line">// 1 获取缓存的文件</span><br><span class="line">// 2 循环读取缓存文件一行</span><br><span class="line">// 3 切割</span><br><span class="line">// 4 缓存数据到集合</span><br><span class="line">    &lt;pid, pname&gt;</span><br><span class="line">    01,小米</span><br><span class="line">    02,华为</span><br><span class="line">    03,格力</span><br><span class="line">// 5 关流</span><br><span class="line"></span><br><span class="line">map方法中</span><br><span class="line">// 1 获取一行</span><br><span class="line">// 2 截取</span><br><span class="line">// 3 获取pid</span><br><span class="line">// 4 获取订单id和商品名称</span><br><span class="line">// 5 拼接</span><br><span class="line">// 6 写出</span><br></pre></td></tr></table></div></figure>
<p>（3）代码实现</p>
<p>① 先在 MapJoinDriver 驱动类中添加缓存文件</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.mapjoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取job信息</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        <span class="comment">// 2 设置加载jar包路径</span></span><br><span class="line">        job.setJarByClass(MapJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 3 关联mapper</span></span><br><span class="line">        job.setMapperClass(MapJoinMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 4 设置Map输出KV类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 5 设置最终输出KV类型，其实这里没啥用，和map输出保持一致就行</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载缓存数据</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///D:/test/tablecache/pd.txt"</span>));</span><br><span class="line">        <span class="comment">// Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\inputtable2"</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\test\\output8888"</span>));</span><br><span class="line">        <span class="comment">// 7 提交</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>② 在 MapJoinMapper 类中的 setup 方法中读取缓存文件</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.mapjoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> HashMap&lt;String, String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取缓存的文件，并把文件内容封装到集合 pd.txt</span></span><br><span class="line">        URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line"></span><br><span class="line">        <span class="comment">////获取文件系统对象,并开流</span></span><br><span class="line">        FileSystem fs = FileSystem.get(context.getConfiguration());</span><br><span class="line">        FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(cacheFiles[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过包装流转换为 reader,方便按行读取</span></span><br><span class="line">        BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(fis, <span class="string">"UTF-8"</span>));</span><br><span class="line">		<span class="comment">//逐行读取，按行处理</span></span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotEmpty(line = reader.readLine())) &#123;</span><br><span class="line">            <span class="comment">// 切割</span></span><br><span class="line">            <span class="comment">//01  小米</span></span><br><span class="line">            String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 赋值</span></span><br><span class="line">            pdMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关流</span></span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理 order.txt</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取pid</span></span><br><span class="line">        String pname = pdMap.get(fields[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取订单id 和订单数量</span></span><br><span class="line">        <span class="comment">// 封装</span></span><br><span class="line">        outK.set(fields[<span class="number">0</span>] + <span class="string">"\t"</span> + pname + <span class="string">"\t"</span> + fields[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        context.write(outK, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h2 id="7-数据清洗etl"   >
          <a href="#7-数据清洗etl" class="heading-link"><i class="fas fa-link"></i></a>7. 数据清洗（ETL）</h2>
      
<p>“ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据仓库。</p>
<p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序。</p>
<p>（1）需求</p>
<p>去除日志中字段个数小于等于 11 的日志。</p>
<p>① 输入数据<code>inputlog\web.log</code></p>
<p>② 期望输出数据：每行字段长度都大于 11。</p>
<p>（2）需求分析</p>
<p>需要在 Map 阶段对输入的数据根据规则进行过滤清洗。</p>
<p>（3）代码实现</p>
<p>① 编写 WebLogMapper 类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.etl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WebLogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 ETL</span></span><br><span class="line">        <span class="keyword">boolean</span> result = parseLog(line, context);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!result)&#123;</span><br><span class="line">            <span class="keyword">return</span>; <span class="comment">//过滤掉</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 写出</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 切割</span></span><br><span class="line">        <span class="comment">// 1.206.126.5 - - [19/Sep/2013:05:41:41 +0000] "-" 400 0 "-" "-"</span></span><br><span class="line">        String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 判断一下日志的长度是否大于11</span></span><br><span class="line">        <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>② Drive类</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.etl;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WebLogDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">        args = <span class="keyword">new</span> String[]&#123;<span class="string">"D:/test/inputlog"</span>, <span class="string">"D:/test/output11111"</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取job信息</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 加载jar包</span></span><br><span class="line">        job.setJarByClass(WebLogDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关联map</span></span><br><span class="line">        job.setMapperClass(WebLogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置reducetask个数为0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);<span class="comment">//取消reduce阶段</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 提交</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h2 id="8-etl清洗规则"   >
          <a href="#8-etl清洗规则" class="heading-link"><i class="fas fa-link"></i></a>8. ETL清洗规则</h2>
      
<p>正则匹配全部汇总</p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406213048.png"  alt="" />
      </p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406213124.png"  alt="" />
      </p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406213152.png"  alt="" />
      </p>
<p>比如上面第15条，手机号的清洗规则</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.layne.mapreduce.etl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestETL</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//手机号的校验规则</span></span><br><span class="line">        String check = <span class="string">"^((13[0-9])|(14[0-9])|(15[0-9])|(17[0-9])|(18[0-9]))\\d&#123;8&#125;$"</span>;</span><br><span class="line"></span><br><span class="line">        String phone = <span class="string">"1352235001311"</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(phone.matches(check));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

        <h2 id="9-mapreduce-开发总结"   >
          <a href="#9-mapreduce-开发总结" class="heading-link"><i class="fas fa-link"></i></a>9. MapReduce 开发总结</h2>
      
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406214042.png"  alt="" />
      </p>
<p>（1）输入数据接口：InputFormat</p>
<ul>
<li>
<p>默认使用的实现类是：TextInputFormat</p>
</li>
<li>
<p>TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为 value 返回。</p>
</li>
<li>
<p>CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</p>
</li>
</ul>
<p>（2）逻辑处理接口：Mapper<br />
用户根据业务需求实现其中三个方法：map()    setup()    cleanup ()</p>
<ul>
<li><code>setup()</code>用来处理初始化</li>
<li><code>setup()</code>处理用户业务逻辑</li>
<li><code>cleanup ()</code>用来关闭资源</li>
</ul>
<p>（3）Partitioner 分区</p>
<ul>
<li>
<p>有默认实现  HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key.hashCode()&amp;Integer.MAXVALUE % numReduces</span><br></pre></td></tr></table></div></figure>
</li>
<li>
<p>如果业务上有特别的需求，可以自定义分区。</p>
</li>
</ul>
<p>（4）Comparable 排序</p>
<ul>
<li>当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接口，重写其中的 compareTo()方法。</li>
<li>部分排序：对最终输出的每一个文件进行内部排序。</li>
<li>全排序：对所有数据进行排序，通常只有一个 Reduce。</li>
<li>二次排序：排序的条件有两个。</li>
</ul>
<p>（5）Combiner 合并<br />
Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。</p>
<p>（6）逻辑处理接口：Reducer<br />
用户根据业务需求实现其中三个方法：reduce()    setup()    cleanup ()</p>
<ul>
<li><code>setup()</code>初始化</li>
<li><code>reduce()</code>业务逻辑</li>
<li><code>cleanup ()</code>关闭资源</li>
</ul>
<p>（7）输出数据接口：OutputFormat</p>
<ul>
<li>默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</li>
<li>用户还可以自定义 OutputFormat。</li>
</ul>

        <h2 id="10-常见错误及解决方案"   >
          <a href="#10-常见错误及解决方案" class="heading-link"><i class="fas fa-link"></i></a>10. 常见错误及解决方案</h2>
      
<ol>
<li>
<p>导包容易出错。尤其 Text 和 CombineTextInputFormat。</p>
</li>
<li>
<p>Mapper 中第一个输入的参数必须是 LongWritable 或者 NullWritable， 不可以是 IntWritable.  报的错误是类型转换异常。</p>
</li>
<li>
<p>java.lang.Exception: java.io.IOException: Illegal partition for 13926435656 (4)，说明 Partition和 ReduceT ask 个数没对上，调整 ReduceTask 个数。</p>
</li>
<li>
<p>如果分区数不是 1， 但是 reducetask 为 1， 是否执行分区过程。答案是：不执行分区过程。因为在 MapT ask 的源码中，执行分区的前提是先判断 ReduceNum 个数是否大于 1。不大于1 肯定不执行。</p>
</li>
<li>
<p>在 Windows 环境编译的 jar 包导入到 Linux 环境中运行，<code>hadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver /input /output</code><br />
报如下错误：</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Exception  <span class="keyword">in</span>  thread  <span class="string">"main"</span>  java.lang.UnsupportedClassVersionError: </span><br><span class="line">com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version 52.0</span><br></pre></td></tr></table></div></figure>
<p>原因是 Windows 环境用的 jdk1.7，Linux 环境用的 jdk1.8。</p>
</li>
<li>
<p>缓存 pd.txt 小文件案例中，报找不到 pd.txt 文件。错误原因：大部分为路径书写错误。还有就是要检查 pd.txt.txt 的问题。 还有个别电脑写相对路径找不到 pd.txt，可以修改为绝对路径。</p>
</li>
<li>
<p>报类型转换异常。通常都是在驱动函数中设置 Map 输出和最终输出时编写错误，Map 输出的 key 如果没有排序，也会报类型转换异常。</p>
</li>
<li>
<p>集群中运行 wc.jar 时出现了无法获得输入文件。原因：WordCount 案例的输入文件不能放用 HDFS 集群的根目录。</p>
</li>
<li>
<p>出现了如下相关异常</p>
<figure class="highlight bash"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Exception  <span class="keyword">in</span>  thread  <span class="string">"main"</span>  java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO<span class="variable">$Windows</span>.access0(Ljava/lang/String;I)Z</span><br><span class="line">at org.apache.hadoop.io.nativeio.NativeIO<span class="variable">$Windows</span>.access0(Native Method)</span><br><span class="line">at org.apache.hadoop.io.nativeio.NativeIO<span class="variable">$Windows</span>.access(NativeIO.java:609)</span><br><span class="line">at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)</span><br><span class="line">java.io.IOException: Could not locate executable null\bin\winutils.exe <span class="keyword">in</span> the Hadoop binaries.</span><br><span class="line">at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:356)</span><br><span class="line">at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:371)</span><br><span class="line">at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:364)</span><br></pre></td></tr></table></div></figure>
<p>解决方案：拷贝 hadoop.dll 文件到 Windows 目录 C:\Windows\System32。个别电脑还需要修改 Hadoop 源码。<br />
方案二：创建如下包名，并将 NativeIO.java 拷贝到该包名下<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://gitee.com/wxler/blogimg/raw/master/imgs/20210406232017.png"  alt="" />
      </p>
</li>
<li>
<p>自定义 Outputformat 时，注意在 RecordWirter 中的 close 方法必须关闭流资源。否则输出的文件内容中数据为空。</p>
<figure class="highlight java"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span>  <span class="keyword">void</span>  <span class="title">close</span><span class="params">(TaskAttemptContext  context)</span>  <span class="keyword">throws</span>  IOException, </span></span><br><span class="line"><span class="function">	InterruptedException </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (atguigufos != <span class="keyword">null</span>) &#123;</span><br><span class="line">		atguigufos.close();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (otherfos != <span class="keyword">null</span>) &#123;</span><br><span class="line">		otherfos.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
</li>
</ol>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://wxler.github.io">wxler</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://wxler.github.io/2021/03/31/214801/">https://wxler.github.io/2021/03/31/214801/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://wxler.github.io/tags/Hadoop/">Hadoop</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://wxler.github.io/tags/MapReduce/">MapReduce</a></span></div><div class="post-share"><div class="social-share" data-sites="qzone, qq, weibo, wechat, douban, linkedin, facebook, twitter, google">分享到：</div></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/03/31/224801/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">八、Hadoop数据压缩</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2021/03/31/204801/"><span class="paginator-prev__text">六、MapReduce之Hadoop的序列化</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="valine-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-inputformat-数据输入"><span class="toc-text">
          1. InputFormat 数据输入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-切片与-maptask-并行度决定机制"><span class="toc-text">
          1.1 切片与 MapTask 并行度决定机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-job-提交流程源码和切片源码详解"><span class="toc-text">
          1.2 Job 提交流程源码和切片源码详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-fileinputformat-切片机制"><span class="toc-text">
          1.3 FileInputFormat 切片机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-fileinputformat实现类"><span class="toc-text">
          1.4 FileInputFormat实现类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-combinetextinputformat-切片机制"><span class="toc-text">
          1.5 CombineTextInputFormat 切片机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-combinetextinputformat-案例实操"><span class="toc-text">
          1.6 CombineTextInputFormat 案例实操</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-keyvaluetextinputformat使用案例"><span class="toc-text">
          1.7 KeyValueTextInputFormat使用案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#18-nlineinputformat使用案例"><span class="toc-text">
          1.8 NLineInputFormat使用案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-mapreduce-工作流程"><span class="toc-text">
          2. MapReduce 工作流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-shuffle-机制"><span class="toc-text">
          3. Shuffle 机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-shuffle-机制"><span class="toc-text">
          3.1 Shuffle 机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-partition-分区"><span class="toc-text">
          3.2 Partition 分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-partition-分区案例实操"><span class="toc-text">
          3.3 Partition 分区案例实操</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#34-writablecomparable-排序"><span class="toc-text">
          3.4 WritableComparable 排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#35-writablecomparable-排序案例实操全排序"><span class="toc-text">
          3.5 WritableComparable 排序案例实操（全排序）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#36-writablecomparable-排序案例实操区内排序"><span class="toc-text">
          3.6 WritableComparable 排序案例实操（区内排序）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#37-combiner-合并"><span class="toc-text">
          3.7 Combiner 合并</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#38-combiner-合并案例实操"><span class="toc-text">
          3.8 Combiner 合并案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-outputformat-数据输出"><span class="toc-text">
          4. OutputFormat 数据输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-outputformat-接口实现类"><span class="toc-text">
          4.1 OutputFormat 接口实现类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-自定义-outputformat-案例实操"><span class="toc-text">
          4.2 自定义 OutputFormat 案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-mapreduce-内核源码解析"><span class="toc-text">
          5. MapReduce 内核源码解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#51-maptask-工作机制"><span class="toc-text">
          5.1 MapTask 工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#52-reducetask-并行度决定机制"><span class="toc-text">
          5.2 ReduceTask 并行度决定机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#53-maptask-reducetask-源码解析"><span class="toc-text">
          5.3 MapTask &amp; ReduceTask 源码解析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-join应用"><span class="toc-text">
          6. Join应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#61-reduce-join"><span class="toc-text">
          6.1 Reduce Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#62-reduce-join-案例实操"><span class="toc-text">
          6.2  Reduce Join 案例实操</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#63-map-join"><span class="toc-text">
          6.3 Map Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#64-map-join-案例实操"><span class="toc-text">
          6.4 Map Join 案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-数据清洗etl"><span class="toc-text">
          7. 数据清洗（ETL）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-etl清洗规则"><span class="toc-text">
          8. ETL清洗规则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-mapreduce-开发总结"><span class="toc-text">
          9. MapReduce 开发总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-常见错误及解决方案"><span class="toc-text">
          10. 常见错误及解决方案</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/assets/myhexo.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">Practical And Realistic</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/wxler" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="mailto:wangxiaolei1516@qq.com" target="_blank" rel="noopener" data-popover="邮箱" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fa fa-envelope"></i></span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">123</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">20</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">56</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2020~2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>wxler. All Rights Reserved.</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v4.2.1</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.0.0</span></div><div>托管于 <a href="https://github.com/wxler/" rel="noopener" target="_blank">Github</a></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/npm/lazyload@2.0.0-rc.2/lazyload.min.js"></script><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><script>function initSearch() {
  var isXML = true;
  var search_path = 'search.json';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;
  $.ajax({
    url: path,
    dataType: isXML ? 'xml' : 'json',
    async: true,
    success: function (res) {
      var datas = isXML ? $('entry', res).map(function () {
        // 将 XML 转为 JSON
        return {
          title: $('title', this).text(),
          content: $('content', this).text(),
          url: $('url', this).text()
        };
      }).get() : res;
      var $input = $('.search-input input');
      var $result = $('.search-results');
      // 搜索对象（标题、内容）的权重，影响显示顺序
      var WEIGHT = { title: 100, content: 1 };
      var searchPost = function () {
        var searchText = $input.val().toLowerCase().trim();
        // 根据空白字符分隔关键字
        var keywords = searchText.split(/[\s]+/);
        // 搜索结果
        var matchPosts = [];

        // 有多个关键字时，将原文字整个保存下来
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        // 防止未输入字符时搜索
        if (searchText.length > 0) {
          datas.forEach(function (data) {
            var isMatch  = false;
            // 没有标题的文章使用预设的 i18n 变量代替
            var title = (data.title && data.title.trim()) || '[ 文章无标题 ]';
            var titleLower = title && title.toLowerCase();
            // 删除 HTML 标签 和 所有空白字符
            var content = data.content && data.content.replace(/<[^>]+>/g, '');
            var contentLower = content && content.toLowerCase();
            // 删除重复的 /
            var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');
            // 标题中匹配到的关键词
            var titleHitSlice = [];
            // 内容中匹配到的关键词
            var contentHitSlice = [];

            keywords.forEach(function (keyword) {
              /**
              * 获取匹配的关键词的索引
              * @param {String} keyword 要匹配的关键字
              * @param {String} text 原文字
              * @param {Boolean} caseSensitive 是否区分大小写
              * @param {Number} weight 匹配对象的权重。权重大的优先显示
              * @return {Array}
              */
              function getIndexByword (word, text, caseSensitive, weight) {
                if (!word || !text) {
                  return [];
                };

                var startIndex = 0; // 每次匹配的开始索引
                var index = -1;     // 匹配到的索引值
                var result = [];    // 匹配结果

                if (!caseSensitive) {
                  word = word.toLowerCase();
                  text = text.toLowerCase();
                }

                while((index = text.indexOf(word, startIndex)) !== -1) {
                  var hasMatch = false;
                  // 索引位置相同的关键词，保留长度较长的
                  titleHitSlice.forEach(function (hit) {
                    if (hit.index === index && hit.word.length < word.length) {
                      hit.word = word;
                      hasMatch = true;
                    }
                  });
                  startIndex = index + word.length;
                  !hasMatch && result.push({ index: index, word: word, weight: weight });
                }
                return result;
              }
              titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
              contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
            });

            var hitTitle = titleHitSlice.length;
            var hitContent = contentHitSlice.length;

            if (hitTitle > 0 || hitContent > 0) {
              isMatch = true;
            }
            if (isMatch) {
              ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                // 按照匹配文字的索引的递增顺序排序
                hit.sort(function (left, right) {
                  return left.index - right.index;
                });
              });
              /**
              * 给文本中匹配到的关键词添加标记，从而进行高亮显示
              * @param {String} text 原文本
              * @param {Array} hitSlice 匹配项的索引信息
              * @param {Number} start 开始索引
              * @param {Number} end 结束索引
              * @return {String}
              */
              function highlightKeyword (text, hitSlice, start, end) {
                if (!text || !hitSlice || !hitSlice.length) {
                  return;
                }

                var result = '';
                var startIndex = start;
                var endIndex = end;
                hitSlice.forEach(function (hit) {
                  if (hit.index < startIndex) {
                    return;
                  }

                  var hitWordEnd = hit.index + hit.word.length;
                  result += text.slice(startIndex, hit.index);
                  result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                  startIndex = hitWordEnd;
                });
                result += text.slice(startIndex, endIndex);
                return result;
              }

              var postData = {};
              // 文章总的搜索权重
              var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
              // 标记匹配关键词后的标题
              var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
              // 标记匹配关键词后的内容
              var postContent;
              // 显示内容的长度
              var SHOW_WORD_LENGTH = 200;
              // 命中关键词前的字符显示长度
              var SHOW_WORD_FRONT_LENGTH = 20;
              var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

              // 截取匹配的第一个字符，前后共 200 个字符来显示
              if (contentHitSlice.length > 0) {
                var firstIndex = contentHitSlice[0].index;
                var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                var end = firstIndex + SHOW_WORD_END_LENGTH;
                postContent = highlightKeyword(content, contentHitSlice, start, end);
              } else { // 未匹配到内容，直接截取前 200 个字符来显示
                postContent = content.slice(0, SHOW_WORD_LENGTH);
              }
              postData.title = postTitle;
              postData.content = postContent;
              postData.url = postURL;
              postData.weight = postWeight;
              matchPosts.push(postData);
            }
          });
        }

        var resultInnerHtml = '';
        if (matchPosts.length) {
          // 按权重递增的顺序排序，使权重大的优先显示
          matchPosts.sort(function (left, right) {
            return right.weight - left.weight;
          });
          resultInnerHtml += '<ul>';
          matchPosts.forEach(function (post) {
            resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
            resultInnerHtml += post.title;
            resultInnerHtml += '</a><div class="search-results-content">';
            resultInnerHtml += post.content;
            resultInnerHtml += '</div></li>';
          });
          resultInnerHtml += '</ul>';
        } else {
          resultInnerHtml += '<div class="search-results-none"><i class="far fa-meh"></i></div>';
        }
        $result.html(resultInnerHtml);
      };
      $input.on('input', searchPost);
      $input.on('keyup', function (e) {
        if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
          searchPost();
        }
      });
    }
  });
}

function closeSearch () {
  $('body').css({ overflow: 'auto' });
  $('.search-popup').css({ display: 'none' });
  $('.search-mask').css({ display: 'none' });
}

window.addEventListener('DOMContentLoaded', function () {
  Stun.utils.pjaxReloadLocalSearch = function () {
    $('.header-nav-search').on('click', function (e) {
      e.stopPropagation();
      $('body').css('overflow', 'hidden');
      $('.search-popup')
        .velocity('stop')
        .velocity('transition.expandIn', {
          duration: 300,
          complete: function () {
            $('.search-popup input').focus();
          }
        });
      $('.search-mask')
        .velocity('stop')
        .velocity('transition.fadeIn', {
          duration: 300
        });

      initSearch();
    });
    $('.search-mask, .search-close').on('click', function () {
      closeSearch();
    });
    $(document).on('keydown', function (e) {
      // Escape <=> 27
      if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
        closeSearch();
      }
    });
  };

  Stun.utils.pjaxReloadLocalSearch();
}, false);</script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (false) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" rel="stylesheet" type="text/css"><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css"><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.min.js"></script></div><script src="https://cdn.jsdelivr.net/npm/leancloud-storage@latest/dist/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script data-pjax="">function loadValine () {
  var GUEST_INFO = ['nick', 'mail', 'link'];
  var guest_info = 'nick,mail';

  guest_info = guest_info.split(',').filter(function(item) {
    return GUEST_INFO.indexOf(item) > -1;
  });
  new Valine({
    el: '#valine-container',
    appId: 'kBUfBo302Sl5ViCNcWef6wYT-gzGzoHsz',
    appKey: 'hq4rU6YND5ziczBa2PLLUBiM',
    notify: true,
    verify: false,
    placeholder: '有什么需要和我说的，请填写昵称与邮箱(邮箱不会公开显示)，点击评论吧(支持匿名评论)！',
    avatar: 'mp',
    meta: guest_info,
    pageSize: '15' || 10,
    visitor: false,
    recordIP: false,
    lang: '' || 'zh-cn',
    path: window.location.pathname
  });
}

if (true) {
  loadValine();
} else {
  window.addEventListener('DOMContentLoaded', loadValine, false);
}</script><script src="/js/utils.js?v=2.0.0"></script><script src="/js/stun-boot.js?v=2.0.0"></script><script src="/js/scroll.js?v=2.0.0"></script><script src="/js/header.js?v=2.0.0"></script><script src="/js/sidebar.js?v=2.0.0"></script></body></html>