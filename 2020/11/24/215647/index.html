<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/assets/logo.png?v=2.0.0" type="image/png" sizes="16x16"><link rel="icon" href="/assets/logo.png?v=2.0.0" type="image/png" sizes="32x32"><meta name="description" content="决策树的含义        首先我们先来了解下什么是决策树。                  上图就是一颗带有决策的树，其中天气、温度等称为特征，后面问号的位置称为阈值，如温度&#x3D;35°，则阈值为35。">
<meta property="og:type" content="article">
<meta property="og:title" content="万字详解决策树之ID3、CART、C4.5【原理+实例+代码实现】">
<meta property="og:url" content="https://wxler.github.io/2020/11/24/215647/index.html">
<meta property="og:site_name" content="Layne&#39;s Blog">
<meta property="og:description" content="决策树的含义        首先我们先来了解下什么是决策树。                  上图就是一颗带有决策的树，其中天气、温度等称为特征，后面问号的位置称为阈值，如温度&#x3D;35°，则阈值为35。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727111117277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727112506781.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727144929893.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727114424888.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727154914395.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200725210419125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200725202826602.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727140142378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727140432832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727141600553.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200725212940738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200725214016787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727145525183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200725215757527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020072522060291.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727152553331.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727152816890.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200725231308198.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200725231336871.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020072715533054.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727155553702.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727160759867.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200725221843381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727174125995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200727185457146.png">
<meta property="article:published_time" content="2020-11-24T13:56:47.000Z">
<meta property="article:modified_time" content="2020-11-24T13:58:38.111Z">
<meta property="article:author" content="wxler">
<meta property="article:tag" content="决策树">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20200727111117277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"><meta name="keywords" content="wxler, Layne's Blog"><meta name="description" content="博客，分享，开源，心得"><title>万字详解决策树之ID3、CART、C4.5【原理+实例+代码实现】 | Layne's Blog</title><link ref="canonical" href="https://wxler.github.io/2020/11/24/215647/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.0.0"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement('script');
  hm.src = 'https://hm.baidu.com/hm.js?01911aa0fc6bdb840626994292397110';
  hm.async = true;

  if (true) {
    hm.setAttribute('data-pjax', '');
  }
  var s = document.getElementsByTagName('script')[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"ocean","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: true,
  pjax: {"avoidBanner":true},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/message/"><span class="header-nav-menu-item__icon"><i class="fa fa-comment"></i></span><span class="header-nav-menu-item__text">留言板</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user"></i></span><span class="header-nav-menu-item__text">关于</span></a></div></div><div class="header-nav-search"><span class="header-nav-search__icon"><i class="fas fa-search"></i></span><span class="header-nav-search__text">搜索</span></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Layne's Blog</div><div class="header-banner-info__subtitle">一个爱好coding的男孩纸</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">万字详解决策树之ID3、CART、C4.5【原理+实例+代码实现】</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2020-11-24</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2020-11-24</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">5.4k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">33分</span></span></div></header><div class="post-body">
        <h2 id="决策树的含义"   >
          <a href="#决策树的含义" class="heading-link"><i class="fas fa-link"></i></a>决策树的含义</h2>
      
<p>首先我们先来了解下什么是决策树。<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727111117277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      <br />
上图就是一颗带有决策的树，其中天气、温度等称为特征，后面问号的位置称为阈值，如温度=35°，则阈值为35。</p>
<a id="more"></a>
<ul>
<li>决策树是一种基本的分类与回归方法。这里主要讨论决策树用于分类。</li>
<li>决策树的结点和有向边分别表示
<ol>
<li>内部结点表示一个特征或者属性。</li>
<li>叶子结点表示一个分类。</li>
<li>有向边代表了一个划分规则。</li>
</ol>
</li>
<li>决策树从根结点到子结点的的有向边代表了一条路径。</li>
<li>决策树的路径是互斥并且是完备的。</li>
<li>用决策树分类时，是对样本的某个特征进行测试，根据测试结果将样本分配</li>
<li>如果将样本分配到了树的子结点上，每个子结点对应该特征的一个取值。</li>
<li>决策树的优点：可读性强，分类速度快。</li>
<li>决策树学习通常包括3个步骤：
<ol>
<li>数据标注</li>
<li>特征选择。</li>
<li>决策树生成。</li>
<li>分类和识别。</li>
</ol>
</li>
</ul>
<p>决策树模型可以认为是if-then规则的集合。决策树学习的算法通常是遍历选择最优特征和特征值，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类，这一过程对应着特征空间的划分，也对应着决策树的构建。</p>

        <h2 id="信息熵信息增益id3"   >
          <a href="#信息熵信息增益id3" class="heading-link"><i class="fas fa-link"></i></a>(信息熵+信息增益)&amp;ID3</h2>
      
<p><strong>信息熵(Entropy)用来度量不确定性的，当熵越大，信息的不确定性越大，对于机器学习中的分类问题，那么，当前类别的熵越大，它的不确定性就越大</strong></p>
<p>H(D)代表一个决策树的熵，熵的数学公式如下：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727112506781.png"  alt="在这里插入图片描述" />
      <br />
<em>n是分类的数目，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是当前分类发生的概率。</em></p>
<p>细想一下，如果10个硬币，分类结果是10个正面，没有反面，那么信息熵为0，如下所示：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727144929893.png"  alt="在这里插入图片描述" />
      <br />
信息熵为0，就意味着信息确定，就意味着分类完成了。</p>
<p>在原有树的熵 H(D) 增加了一个分裂节点，使得熵变成了H(D|A)，则**信息增益(Information Gain，IG)**为：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727114424888.png"  alt="在这里插入图片描述" />
      <br />
<em>A是选择作为分裂依据的特征，g(D,A)也称为条件熵</em></p>
<p><strong>即信息增益 = 分裂前的信息熵-分裂后的信息熵</strong></p>
<p>也有更加准确的定义方法：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727154914395.png"  alt="在这里插入图片描述" />
      <br />
<em>V表示根据特征a对样本集D划分(分裂)后，获得的总共类别数量(一般是二分类)；<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>D</mi><mi>v</mi></msup></mrow><annotation encoding="application/x-tex">D^v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span></span></span></span></span></span></span>表示每一个新类别中样本数量；Ent(D)和H(D)的含义相同，表示信息熵</em></p>
<p>我们以判断学生好坏的案例，计算信息熵和信息增益：<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200725210419125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      </p>
<p>（1）在初始状态下，有10个学生，7个是好学生，3个不是好学生，计算树的信息熵(此时只有一个根节点) <code>H(D)=0.88</code>，如下所示：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200725202826602.png"  alt="在这里插入图片描述" />
      </p>
<p>（2）我们根据分数这个特征对树进行分裂，设置分数的阈值为70，计算分裂后树的信息熵<code>H(D|A1)=0.4344</code>，如下所示：<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727140142378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      </p>
<p>（3）把(2)的特征有分数改为出勤率，设置出勤率阈值为75%，计算分裂后树的信息熵<code>H(D|A2)=0.79</code>，如下所示：</p>
<p>
        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727140432832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      </p>
<p>从上面的计算中，我们得到了3个熵：</p>
<ul>
<li>在初始状态下，信息熵为0.88，</li>
<li>设置分数阈值为70进行分裂，信息熵为0.4344</li>
<li>设置出勤率阈值为75%进行分裂，信息熵为0.79</li>
</ul>
<p>只要增加分裂节点后的熵比之前的熵小，那么就可以认为本次分裂是有效的，现在(2)和(3)的分裂都有效，但是哪个更好呢？<br />
在设置分数阈值为70的熵比设置出勤率阈值为75%的熵要小，即前者分裂后数据比较纯一些，整个数据的确定性大了，因此设置分数阈值为70分裂方式效果更好，所以我们以该方式为基准进行分裂。</p>
<p>现在来计算信息增益，<strong>信息增益 = 分裂前的信息熵-分裂后的信息熵</strong>，计算如下所示：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727141600553.png"  alt="在这里插入图片描述" />
      <br />
<em>A1是以分数这个特征进行分裂，A2是以出勤率这个特征进行分裂</em></p>
<p>可以看到，以设置分数阈值为70进行分裂，信息增益更大。</p>
<blockquote>
<p>信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好，该特征具有更强的分类能力，如果一个特征的信息增益为0，则表示该特征没有什么分类能力。</p>
</blockquote>
<p><strong>ID3的原理：</strong></p>
<p><strong>利用数据标注和信息增益以及遍历，可以完成一个决策树中的特征和阈值的选择(得到最大信息增益的特征和阈值)，利用这三个(标注、信息增益、遍历)就可以完成一颗树，决策的树，分类的树，这个算法就是ID3()算法的思想</strong></p>
<p><strong>ID3(Iterative Dichotomisor 3) 迭代二分类三代，用信息增益准则选择特征，判定分类器的性能，从而构建决策树</strong></p>
<p>经过上面的例子，可以得到如下结论：</p>
<ul>
<li>只要分类后的总熵为0，那么这课树就训练完了（也就是分类完了）</li>
<li>熵在决策树中的作用：判断分类后，有没有达到我们的需求和目的，也就是数据是不是更加纯了，更加确定了。如果经过分类后，信息总熵的值更加小了，那么这次分类就是有效的。</li>
<li>熵越大，不确定就越大，分类未完成时，熵不为0</li>
<li>熵越小，分得越好，分类完成时，熵为0</li>
</ul>
<p>根据上述思想，可以完成如下一颗树：<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200725212940738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      </p>

        <h2 id="纯度基尼系数cart"   >
          <a href="#纯度基尼系数cart" class="heading-link"><i class="fas fa-link"></i></a>(纯度+基尼系数)&amp;CART</h2>
      
<p>在上面的例子中，我们是用决策树做分类的，那么做回归的时候该怎么做呢？ 现在把上述的分类结果由好学生换成是否为好学生的概率，如下所示：</p>
<p>
        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200725214016787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      </p>
<p>分类的结果是概率，是个连续的变量，无法计算信息熵和信息增益了，这个时候怎么做呢？</p>
<p>为了解决这个问题，引入了纯度的概念。</p>
<p><strong>纯度的定义如下：</strong></p>
<p>
        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727145525183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      <br />
<em>𝑃_𝑙 和𝑃_r  是按照某一特征分裂后，树的两个分裂结点各自占的比例，Var是方差，就是计算左侧和右侧部分的方差，y_i是具体的值(这里是好学生的概率)</em></p>
<p><strong>纯度增益 = 分裂前的纯度 - 分裂后的纯度</strong></p>
<p>可以看到，纯度和方差的定义大致一样(这里的纯度没有除以n，方差的定义需要除以n)，在有的文献中也称为偏差，其实含义都一样，都是<strong>表示数据的离散程度</strong></p>
<p>细想一下，如果上述10个学生的是好学生的概率都一样(比如都是0.9)，那么纯度(方差)是0，说明数据完全没有离散性，非常的确定。</p>
<blockquote>
<p>可以看到，纯度和信息熵所代表的含义是一样的，只不过熵表示分类信息的不确定性，纯度表示数据的离散程度，下面即将要介绍的基尼系数，表示的也是分类(CART的分类)信息的确定性</p>
</blockquote>
<p>（1）在初始状态下，计算树的纯度(此时只有一个根节点) <code>纯度=0.4076</code>，如下所示：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = [<span class="number">0.9</span>,<span class="number">0.9</span>,<span class="number">0.8</span>,<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="number">0.8</span>,<span class="number">0.85</span>,<span class="number">0.74</span>,<span class="number">0.92</span>,<span class="number">0.99</span>]</span><br><span class="line">a = np.array(a)</span><br><span class="line">a_mean = np.mean(a)</span><br><span class="line"><span class="comment"># 不用除以n</span></span><br><span class="line">a_va = np.sum(np.square(a-a_mean))</span><br><span class="line">a_va</span><br><span class="line"><span class="comment"># 输出 0.4076000000000001</span></span><br></pre></td></tr></table></div></figure>
<p>(2) 设置分数阈值为70进行分裂，计算<code>纯度=0.2703</code>，如下所示：<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200725215757527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      </p>
<p>(3) 设置出勤率阈值为75%进行分类，计算纯度：<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/2020072522060291.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      <br />
从上面的计算中，我们得到了3个纯度：</p>
<ul>
<li>在初始状态下，纯度为0.4076，</li>
<li>设置分数阈值为70进行分裂，纯度为0.2703</li>
<li>设置出勤率阈值为75%进行分裂，纯度0.1048</li>
</ul>
<p>从这三个纯度可以得知，这两种方式的分裂均有效，按照出勤率阈值=75%计算的纯度更小，说明分裂的效果更好，信息更确定了。</p>
<p>也可以计算出纯度的增益：</p>
<ul>
<li>设置分数阈值为70进行分裂，纯度的增益为<code>0.4079 - 0.2703 = 0.1376</code></li>
<li>设置出勤率阈值为75%进行分裂，纯度的增益为<code>0.4079 - 0.1048 =0.3031</code></li>
</ul>
<p>设置出勤率阈值为75%进行分裂得到纯度增益更大，说分裂效果更好(这里和信息熵及信息增益的原理是一样的)</p>
<p><strong>思考：为什么决策树分类和回归的结果不同？</strong><br />
因为我们的标注不同，之前标注的是好学生与坏学生，现在重新标注了是好学生的概率，标注不同对我们模型训练的引导就不同，造成的结果就不同。</p>
<p><strong>思考：将模型训练完之后怎么去得到具体回归的那个值呢？</strong><br />
如果最后只有一个结点，就是指这个结点的值，如果最后有多个结点，那应该是平均值</p>
<p><strong>利用数据标注和纯度以及遍历，可以完成一个决策树中的特征和阈值的选择(得到最大信息增益的特征和阈值)，利用这三个(标注、纯度、遍历)就可以完成一颗树，决策的树，回归的树，这个算法就是CART的回归算法</strong></p>
<p>CART做回归时用的是纯度，做分类时用的是基尼系数。</p>
<p><strong>基尼系数</strong><br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727152553331.png"  alt="在这里插入图片描述" />
      </p>
<p><em>n代表n分类，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>表示不同类别的概率</em></p>
<p>按照某一特征分裂后的基尼系数为：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727152816890.png"  alt="在这里插入图片描述" />
      </p>
<p><strong>基尼增益 = 分裂前的基尼 - 分裂后的基尼</strong></p>
<p>我们以硬币来举例子解释基尼系数，假设10个硬币做二分类，10个是正面，没有反面，那么基尼系数为：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200725231308198.png"  alt="在这里插入图片描述" />
      <br />
如果10个硬币做二分类，5个是正面，5个是反面，那么基尼系数为<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200725231336871.png"  alt="在这里插入图片描述" />
      </p>
<p>可以看到，当分类完成时，基尼系数为0，因此基尼系数与信息的含义类似，基尼系数越大，信息的不确定性就越大。</p>
<p><strong>注意：基尼系数和信息熵都是用于分类的</strong></p>
<p>然后，按照某一特征进行分裂，比如按照硬币的大小(或面值)进行分裂，计算分裂后的基尼系数，最后计算基尼增益，得到的基尼增益最大的特征和阈值就是我们要找的分裂方式。</p>
<blockquote>
<p>纯度增益、基尼增益、信息增益的原理完全一样</p>
</blockquote>
<p><strong>CART（classification and regression tree）分类和回归树，分类是使用基尼系数判定分类器的性能，回归时使用纯度判定分类器的性能</strong></p>

        <h2 id="信息增益率c45"   >
          <a href="#信息增益率c45" class="heading-link"><i class="fas fa-link"></i></a>信息增益率&amp;C4.5</h2>
      
<p><strong>信息增益准则对可取值数目较多的特征有所偏好</strong>，为了减少这种偏好可能带来的不利影响，C4.5决策树算法使用了“增益率”：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/2020072715533054.png"  alt="在这里插入图片描述" />
      <br />
其中IV(a)称为特征a的“固有值”，称为特征a的分裂信息度量，其实就是特征a的信息熵(注意：这个和信息增益减去的按特征a分裂后的信息熵不一样，在下面具体例子中可看到)<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727155553702.png"  alt="在这里插入图片描述" />
      <br />
<strong>需要注意的是，信息增益率对可取值数目较少的特征所有偏好，因此，C4.5算法并不是直接选择信息增益率最大的特征进行分裂，而是使用了一个启发式：先找出信息增益高于平均水平的特征，再从中选择增益率最高的。</strong> 可以看出，使用信息增益率可以解决分裂后叶子结点过多的问题，从而解决过拟合</p>
<p>我们用下面的例子计算信息增益率(抱歉，没找到更清晰的图片)<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727160759867.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      <br />
（1）第一步：计算样本集D的信息熵</p>
<p>Ent(D) = <code>-9/14*log2(9/14) – 5/14*log2(5/14) = 0.940</code><br />
<em>Ent(D)表示熵，也可以H(D)表示</em></p>
<p>（2）第二步：依据每个特征划分样本集D，并计算每个特征（划分样本集D后）的信息熵</p>
<ul>
<li>Ent(天气) = <code>5/14*[-2/5*log2(2/5)-3/5*log2(3/5)] + 4/14*[-4/4*log2(4/4)] + 5/14*[-3/5log2(3/5) – 2/5*log2(2/5)] = 0.694</code></li>
<li>Ent(温度) = 0.911</li>
<li>Ent(湿度) = 0.789</li>
<li>Ent(风速) = 0.892</li>
</ul>
<p>（3）第三步：计算信息增益</p>
<ul>
<li>Gain(天气) = Ent(D) - Ent(天气) = 0.246</li>
<li>Gain(温度) = Ent(D) - Ent(温度) = 0.029</li>
<li>Gain(湿度) = Ent(D) - Ent(湿度) = 0.150</li>
<li>Gain(风速) = Ent(D) - Ent(风速) = 0.048</li>
</ul>
<p>（4）第四步：计算特征(属性)分裂信息度量</p>
<ul>
<li>IV(天气) = <code>-5/14*log2(5/14) – 4/14*log2(4/14) – 5/14*log2(5/14) = 1.577</code></li>
<li>IV(温度) = 1.556</li>
<li>IV(湿度) = 1.000</li>
<li>IV(风速) = 0.985</li>
</ul>
<p>（5）第五步：计算信息增益率</p>
<ul>
<li>Gain_ratio(天气) = 0.246 / 1.577 = 0.155</li>
<li>Gain_ratio(温度) = 0.0187</li>
<li>Gain_ratio(湿度) = 0.151</li>
<li>Gain_ratio(风速) = 0.048</li>
</ul>
<p>可以看到，天气的信息增益率最高，选择天气为分裂属性。发现分裂了之后，天气是“阴”的条件下，类别是”纯“的，所以把它定义为叶子节点，选择不“纯”的结点继续分裂。</p>
<p>我们在ID3和CART中的例子特征值是离散的，即特征值是一个个数值，不是本例中的类别，如果特征是类别的样本，就没有阈值的选择，直接按该特征的类别进行分裂，比如本例中按天气把决策树分裂成晴、阴、雨三个结点。</p>
<p>思考：为什么说<strong>信息增益准则对可取值数目较多的特征有所偏好</strong>？</p>
<p>就如本例中的天气特征，它的可取值数据为3个：晴、阴、雨，如果天气特征的可取值数目增加到5个，那么按照天气分裂后的信息熵就会降低的更多，它的信息增益就越大，因为天气特征的可取值数目越多，分裂的就清晰。如果把天气特征的可取值数目增加到与样本数一样，那么分裂后的信息熵就是0了，所以说信息增益准则对可取值数目较多的特征有所偏好。</p>
<p>思考：为什么说<strong>信息增益率对可取值数目较少的特征所有偏好</strong>？</p>
<p>还拿本例中的天气特征来说，它的可取值数据为3个：晴、阴、雨，如果天气特征的可取值数目减少到一个，那么天气特征(属性)分裂信息度量就为0了，此时，信息增益率就是无穷大，因此信息增益率对可取值数目较少的特征所有偏好。</p>

        <h2 id="各种决策树比较与总结"   >
          <a href="#各种决策树比较与总结" class="heading-link"><i class="fas fa-link"></i></a>各种决策树比较与总结</h2>
      
<p><strong>ID3、CART、C4.5比较</strong><br />
信息增益和信息增益率通常用于离散型的特征划分，ID3和C4.5通常情况下都是多叉树，也就是根据离散特征的取值会将数据分到多个子树中，当然采用信息增益和信息增益率的时候也可以对连续特征进行划分并计算最优点，如上面判断好坏学生的例子。CART树为二叉树，使用基尼指数作为划分准则，对于离散型特征和连续行特征都能很好的处理。<br />
<strong>离散型的特征是指：特征是分类，如天气：晴、阴、雨，不是连续的值<br />
连续型的特征是指：特征是一个个值，如分数、身高等，不是分类</strong></p>
<p><strong>决策树的参数和训练方法：</strong></p>
<ul>
<li>决策树的参数：选择的特征及其对应的阈值，还有它的拓扑结构</li>
<li>训练方法：就是遍历的方法，用纯度、基尼系数、信息熵、信息增益或信息增益率来表示</li>
<li>决策树既可以做分类，也可以做回归，既可以做二分类，也可以做多分类</li>
</ul>
<p><strong>决策树中的分类器</strong>：<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200725221843381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      </p>
<p><strong>如何求解决策树？</strong></p>
<p>求解决策树，实际上就是求解分类器(求解每个特征和阈值)，步骤如下：</p>
<ol>
<li>设定一个评价分类结果的好坏的准则(信息增益、基尼系数、纯度、信息增益率)</li>
<li>用遍历的方法求解</li>
</ol>

        <h2 id="决策树编程实战"   >
          <a href="#决策树编程实战" class="heading-link"><i class="fas fa-link"></i></a>决策树编程实战</h2>
      

        <h3 id="调用sklearn库实现回归决策树"   >
          <a href="#调用sklearn库实现回归决策树" class="heading-link"><i class="fas fa-link"></i></a>调用sklearn库实现回归决策树</h3>
      
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Data set</span></span><br><span class="line">x = np.array(list(range(<span class="number">1</span>, <span class="number">11</span>))).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">y = np.array([<span class="number">5.56</span>, <span class="number">5.70</span>, <span class="number">5.91</span>, <span class="number">6.40</span>, <span class="number">6.80</span>, <span class="number">7.05</span>, <span class="number">8.90</span>, <span class="number">8.70</span>, <span class="number">9.00</span>, <span class="number">9.05</span>]).ravel()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fit regression model</span></span><br><span class="line">model1 = DecisionTreeRegressor(max_depth=<span class="number">1</span>)</span><br><span class="line">model2 = DecisionTreeRegressor(max_depth=<span class="number">3</span>)</span><br><span class="line">model3 = linear_model.LinearRegression()</span><br><span class="line">model1.fit(x, y)</span><br><span class="line">model2.fit(x, y)</span><br><span class="line">model3.fit(x, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">X_test = np.arange(<span class="number">0.0</span>, <span class="number">10.0</span>, <span class="number">0.01</span>)[:, np.newaxis]</span><br><span class="line">y_1 = model1.predict(X_test)</span><br><span class="line">y_2 = model2.predict(X_test)</span><br><span class="line">y_3 = model3.predict(X_test)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Plot the results</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(x, y, s=<span class="number">20</span>, edgecolor=<span class="string">"black"</span>,</span><br><span class="line">            c=<span class="string">"darkorange"</span>, label=<span class="string">"data"</span>)</span><br><span class="line">plt.plot(X_test, y_1, color=<span class="string">"cornflowerblue"</span>,</span><br><span class="line">         label=<span class="string">"max_depth=1"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_test, y_2, color=<span class="string">"yellowgreen"</span>, label=<span class="string">"max_depth=3"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_test, y_3, color=<span class="string">'red'</span>, label=<span class="string">'liner regression'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">"data"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"target"</span>)</span><br><span class="line">plt.title(<span class="string">"Decision Tree Regression"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></div></figure>
<p>输出<br />

        <img     class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727174125995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTU1MDcx,size_16,color_FFFFFF,t_70"  style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fpx;"  alt="在这里插入图片描述" />
      </p>

        <h3 id="手动实现id3决策树"   >
          <a href="#手动实现id3决策树" class="heading-link"><i class="fas fa-link"></i></a>手动实现ID3决策树</h3>
      
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本的特征，每个样本包含七个特征</span></span><br><span class="line">feature_space=[[<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.</span>,<span class="number">3.</span>], </span><br><span class="line">[<span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>, <span class="number">5.</span>, <span class="number">1.</span>, <span class="number">2.</span>,<span class="number">3.</span>], </span><br><span class="line">[<span class="number">3.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">2.</span>, <span class="number">3.</span>,<span class="number">2.</span>], </span><br><span class="line">[<span class="number">4.</span>, <span class="number">0.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">2.</span>, <span class="number">0.</span>,<span class="number">1.</span>], </span><br><span class="line">[<span class="number">3.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">5.</span>, <span class="number">1.</span>,<span class="number">3.</span>], </span><br><span class="line">[<span class="number">1.</span>, <span class="number">4.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">1.</span>, <span class="number">5.</span>,<span class="number">2.</span>], </span><br><span class="line">[<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">1.</span>, <span class="number">0.</span>,<span class="number">1.</span>], </span><br><span class="line">[<span class="number">5.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">4.</span>, <span class="number">2.</span>, <span class="number">2.</span>,<span class="number">2.</span>], </span><br><span class="line">[<span class="number">6.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">2.</span>, <span class="number">3.</span>,<span class="number">0.</span>], </span><br><span class="line">[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">5.</span>, <span class="number">1.</span>,<span class="number">4.</span>]]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_label</span><span class="params">(idx)</span>:</span></span><br><span class="line">    label= idx</span><br><span class="line">    <span class="keyword">return</span> label</span><br><span class="line"></span><br><span class="line"><span class="comment">#  计算以某个特征某个阈值进行分裂时的信息熵，即条件熵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_by_node</span><span class="params">(d,value,feature_space,list_need_cut)</span>:</span></span><br><span class="line">    <span class="comment"># 分别存放以某个特征某个阈值分裂后的左右侧样本点</span></span><br><span class="line">    right_list=[]</span><br><span class="line">    left_list=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_need_cut:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> feature_space[i][d]&lt;=value:</span><br><span class="line">             right_list.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">             left_list.append(i)</span><br><span class="line"></span><br><span class="line">    left_list_t = list2label(left_list,[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    right_list_t = list2label(right_list,[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    e1=get_emtropy(left_list_t) </span><br><span class="line">    e2=get_emtropy(right_list_t) </span><br><span class="line">    n1 = float(len(left_list))</span><br><span class="line">    n2 = float(len(right_list))</span><br><span class="line">    e = e1*n1/(n2+n1) + e2*n2/(n1+n2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> e,right_list,left_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分到样本点转为one-hot各式，方便计算信息熵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">list2label</span><span class="params">(list_need_label,list_label)</span>:</span></span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> list_need_label:</span><br><span class="line">         label=get_label(i)</span><br><span class="line">         list_label[label]+=<span class="number">1</span></span><br><span class="line">     <span class="keyword">return</span> list_label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_emtropy</span><span class="params">(class_list)</span>:</span></span><br><span class="line">   E = <span class="number">0</span></span><br><span class="line">   sumv = float(sum(class_list))</span><br><span class="line">   <span class="keyword">if</span> sumv == <span class="number">0</span>:</span><br><span class="line">       sumv =<span class="number">0.000000000001</span></span><br><span class="line">   <span class="keyword">for</span> cl <span class="keyword">in</span> class_list:</span><br><span class="line">       <span class="keyword">if</span> cl==<span class="number">0</span>:</span><br><span class="line">           cl=<span class="number">0.00000000001</span></span><br><span class="line">       p = torch.tensor(float(cl/sumv))</span><br><span class="line">       <span class="comment"># log以2为底</span></span><br><span class="line">       E += <span class="number">-1.0</span> * p*torch.log(p)/torch.log(torch.tensor(<span class="number">2.</span>))</span><br><span class="line">   <span class="keyword">return</span> E.item()</span><br></pre></td></tr></table></div></figure>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_node</span><span class="params">(complate,d,list_need_cut)</span>:</span></span><br><span class="line">    <span class="comment"># 初始时的信息熵，设为最大(根节点计算之前的信息熵)</span></span><br><span class="line">    e = <span class="number">10000000</span></span><br><span class="line">    <span class="comment"># node 代表以那个维度的那个特征值进行分裂</span></span><br><span class="line">    node=[]</span><br><span class="line">    <span class="comment"># list_select:树分裂好的一个个结点序列，不包含已经分类好可以识别的结点</span></span><br><span class="line">    list_select=[]</span><br><span class="line">    <span class="comment"># 存放可以识别的结点</span></span><br><span class="line">    complate_select=[]</span><br><span class="line">    <span class="comment"># 0~8就是选择的阈值</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">8</span>):</span><br><span class="line">        complate_tmp=[]</span><br><span class="line">        etmp=<span class="number">0</span></span><br><span class="line">        list_select_tmp=[]</span><br><span class="line">        <span class="comment"># 子序列总的长度</span></span><br><span class="line">        sumv=<span class="number">0.000000001</span></span><br><span class="line">        <span class="comment"># 要进行分裂的所有结点序列</span></span><br><span class="line">        <span class="keyword">for</span> lnc <span class="keyword">in</span> list_need_cut:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算条件熵，返回熵、左侧结点的样本点，右侧结点的样本点</span></span><br><span class="line">            etmptmp,r_list,l_list=cut_by_node(d,value,feature_space,lnc)</span><br><span class="line">            <span class="comment"># 这行代码和etmp/sumv 就是求分裂后的总熵，就是那个pl*H1+pr*H2的公式</span></span><br><span class="line">            etmp+=etmptmp*len(lnc)</span><br><span class="line">            sumv+=float(len(lnc))</span><br><span class="line">            <span class="comment"># 存放可以识别的结点</span></span><br><span class="line">            <span class="keyword">if</span> len(r_list)&gt;<span class="number">1</span>:</span><br><span class="line">                list_select_tmp.append(r_list)</span><br><span class="line">            <span class="keyword">if</span> len(l_list)&gt;<span class="number">1</span>:</span><br><span class="line">                list_select_tmp.append(l_list)</span><br><span class="line">            <span class="keyword">if</span> len(r_list)==<span class="number">1</span>:</span><br><span class="line">                complate_tmp.append(r_list)</span><br><span class="line">            <span class="keyword">if</span> len(l_list)==<span class="number">1</span>:</span><br><span class="line">                complate_tmp.append(l_list)</span><br><span class="line">            <span class="comment"># print('子序列child:以每个样本的第&#123;&#125;个特征，特征值大小为&#123;&#125;划分&#123;&#125;子序列，得到的熵为&#123;&#125;'.format(d,value,lnc,etmptmp))    </span></span><br><span class="line">            </span><br><span class="line">        etmp = etmp/sumv</span><br><span class="line">        sumv=<span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># print('总序列All：以每个样本的第&#123;&#125;个特征，特征值大小为&#123;&#125;划分&#123;&#125;序列，得到的总熵为&#123;&#125;'.format(d,value,list_need_cut,etmp))    </span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 得到第n个特征以某一阈值分裂后的最小信息熵，及此时分裂后序列</span></span><br><span class="line">        <span class="keyword">if</span> etmp&lt;e:</span><br><span class="line">            e=etmp</span><br><span class="line">            node=[d,value]</span><br><span class="line">            list_select=list_select_tmp</span><br><span class="line">            complate_select=complate_tmp</span><br><span class="line">    <span class="keyword">for</span> ll <span class="keyword">in</span> complate_select:</span><br><span class="line">         complate.append(ll)</span><br><span class="line">    <span class="keyword">return</span> node,list_select,complate</span><br></pre></td></tr></table></div></figure>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tree</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment">#     pdb.set_trace()</span></span><br><span class="line">    <span class="comment"># 10个样本：0, 1, 2, 3, 4, 5, 6, 7, 8, 9，注意是这里是二维数组</span></span><br><span class="line">    <span class="comment">## 二维数组里刚开始只有一个序列，即初始时只有一个根节点</span></span><br><span class="line">    all_list=[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]] </span><br><span class="line">    <span class="comment"># complate的含义是：比如树的某个结点只有一个样本，那么存放到complate中</span></span><br><span class="line">    <span class="comment">## 即complate存放的是已经分类好的可以识别的结点</span></span><br><span class="line">    complate=[]</span><br><span class="line">    <span class="comment"># 遍历样本的7个特征，这个序列就是遍历的先后顺序</span></span><br><span class="line">    <span class="comment">## d 首先遍历每个样本的第3个特征，把分类的结果返回</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> [<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">6</span>]:</span><br><span class="line">        <span class="comment"># node 代表以那个维度的那个特征值进行分裂</span></span><br><span class="line">        <span class="comment"># all_list:树分裂好的一个个结点序列，不包含已经分类好可以识别的结点</span></span><br><span class="line">        node,all_list,complate=get_node(complate,d,all_list)</span><br><span class="line">        print(<span class="string">"node=%s,complate=%s,all_list=%s"</span>%(node,complate,all_list))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    get_tree()</span><br></pre></td></tr></table></div></figure>
<p>输出：<br />

        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="https://img-blog.csdnimg.cn/20200727185457146.png"  alt="在这里插入图片描述" />
      </p>

        <h3 id="使用sklearn库回归决策树预测boston房价"   >
          <a href="#使用sklearn库回归决策树预测boston房价" class="heading-link"><i class="fas fa-link"></i></a>使用sklearn库回归决策树预测boston房价</h3>
      
<p><span class="exturl"><a class="exturl__link"   href="https://gitee.com/wxler/AIProjectTraining/blob/master/practice/%E5%86%B3%E7%AD%96%E6%A0%91/decision_sklearn_tree_regressor_boston.ipynb"  target="_blank" rel="noopener">decision_sklearn_tree_regressor_boston</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>

        <h3 id="使用sklearn库分类决策树对iris分类"   >
          <a href="#使用sklearn库分类决策树对iris分类" class="heading-link"><i class="fas fa-link"></i></a>使用sklearn库分类决策树对iris分类</h3>
      
<p><span class="exturl"><a class="exturl__link"   href="https://gitee.com/wxler/AIProjectTraining/blob/master/practice/%E5%86%B3%E7%AD%96%E6%A0%91/decision_sklearn_tree_classify_iris.ipynb"  target="_blank" rel="noopener">decision_sklearn_tree_classify_iris</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>参考文档<br />
<span class="exturl"><a class="exturl__link"   href="https://www.cnblogs.com/rezero/p/13057584.html"  target="_blank" rel="noopener">信息增益、信息增益比、基尼指数的比较</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br />
<span class="exturl"><a class="exturl__link"   href="http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/4_decision_tree.html"  target="_blank" rel="noopener">华校专的决策树</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br />
<span class="exturl"><a class="exturl__link"   href="https://www.cnblogs.com/xiaofeiIDO/p/11947380.html"  target="_blank" rel="noopener">机器学习（二）-信息熵，条件熵，信息增益，信息增益比，基尼系数</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://wxler.github.io">wxler</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://wxler.github.io/2020/11/24/215647/">https://wxler.github.io/2020/11/24/215647/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://wxler.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a></span></div><div class="post-share"><div class="social-share" data-sites="qzone, qq, weibo, wechat, douban, linkedin, facebook, twitter, google">分享到：</div></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2020/11/24/215911/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">Anaconda与jupyter安装、操作及插件安装</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2020/11/24/215155/"><span class="paginator-prev__text">k-means及k-means++原理【python代码实现】</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="valine-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树的含义"><span class="toc-text">
          决策树的含义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信息熵信息增益id3"><span class="toc-text">
          (信息熵+信息增益)&amp;ID3</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#纯度基尼系数cart"><span class="toc-text">
          (纯度+基尼系数)&amp;CART</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信息增益率c45"><span class="toc-text">
          信息增益率&amp;C4.5</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#各种决策树比较与总结"><span class="toc-text">
          各种决策树比较与总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树编程实战"><span class="toc-text">
          决策树编程实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#调用sklearn库实现回归决策树"><span class="toc-text">
          调用sklearn库实现回归决策树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#手动实现id3决策树"><span class="toc-text">
          手动实现ID3决策树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用sklearn库回归决策树预测boston房价"><span class="toc-text">
          使用sklearn库回归决策树预测boston房价</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用sklearn库分类决策树对iris分类"><span class="toc-text">
          使用sklearn库分类决策树对iris分类</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/assets/myhexo.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">Practical And Realistic</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/wxler" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="mailto:wangxiaolei1516@qq.com" target="_blank" rel="noopener" data-popover="邮箱" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fa fa-envelope"></i></span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">125</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">21</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">58</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2020~2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>wxler. All Rights Reserved.</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v4.2.1</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.0.0</span></div><div>托管于 <a href="https://github.com/wxler/" rel="noopener" target="_blank">Github</a></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/npm/lazyload@2.0.0-rc.2/lazyload.min.js"></script><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><script>function initSearch() {
  var isXML = true;
  var search_path = 'search.json';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;
  $.ajax({
    url: path,
    dataType: isXML ? 'xml' : 'json',
    async: true,
    success: function (res) {
      var datas = isXML ? $('entry', res).map(function () {
        // 将 XML 转为 JSON
        return {
          title: $('title', this).text(),
          content: $('content', this).text(),
          url: $('url', this).text()
        };
      }).get() : res;
      var $input = $('.search-input input');
      var $result = $('.search-results');
      // 搜索对象（标题、内容）的权重，影响显示顺序
      var WEIGHT = { title: 100, content: 1 };
      var searchPost = function () {
        var searchText = $input.val().toLowerCase().trim();
        // 根据空白字符分隔关键字
        var keywords = searchText.split(/[\s]+/);
        // 搜索结果
        var matchPosts = [];

        // 有多个关键字时，将原文字整个保存下来
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        // 防止未输入字符时搜索
        if (searchText.length > 0) {
          datas.forEach(function (data) {
            var isMatch  = false;
            // 没有标题的文章使用预设的 i18n 变量代替
            var title = (data.title && data.title.trim()) || '[ 文章无标题 ]';
            var titleLower = title && title.toLowerCase();
            // 删除 HTML 标签 和 所有空白字符
            var content = data.content && data.content.replace(/<[^>]+>/g, '');
            var contentLower = content && content.toLowerCase();
            // 删除重复的 /
            var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');
            // 标题中匹配到的关键词
            var titleHitSlice = [];
            // 内容中匹配到的关键词
            var contentHitSlice = [];

            keywords.forEach(function (keyword) {
              /**
              * 获取匹配的关键词的索引
              * @param {String} keyword 要匹配的关键字
              * @param {String} text 原文字
              * @param {Boolean} caseSensitive 是否区分大小写
              * @param {Number} weight 匹配对象的权重。权重大的优先显示
              * @return {Array}
              */
              function getIndexByword (word, text, caseSensitive, weight) {
                if (!word || !text) {
                  return [];
                };

                var startIndex = 0; // 每次匹配的开始索引
                var index = -1;     // 匹配到的索引值
                var result = [];    // 匹配结果

                if (!caseSensitive) {
                  word = word.toLowerCase();
                  text = text.toLowerCase();
                }

                while((index = text.indexOf(word, startIndex)) !== -1) {
                  var hasMatch = false;
                  // 索引位置相同的关键词，保留长度较长的
                  titleHitSlice.forEach(function (hit) {
                    if (hit.index === index && hit.word.length < word.length) {
                      hit.word = word;
                      hasMatch = true;
                    }
                  });
                  startIndex = index + word.length;
                  !hasMatch && result.push({ index: index, word: word, weight: weight });
                }
                return result;
              }
              titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
              contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
            });

            var hitTitle = titleHitSlice.length;
            var hitContent = contentHitSlice.length;

            if (hitTitle > 0 || hitContent > 0) {
              isMatch = true;
            }
            if (isMatch) {
              ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                // 按照匹配文字的索引的递增顺序排序
                hit.sort(function (left, right) {
                  return left.index - right.index;
                });
              });
              /**
              * 给文本中匹配到的关键词添加标记，从而进行高亮显示
              * @param {String} text 原文本
              * @param {Array} hitSlice 匹配项的索引信息
              * @param {Number} start 开始索引
              * @param {Number} end 结束索引
              * @return {String}
              */
              function highlightKeyword (text, hitSlice, start, end) {
                if (!text || !hitSlice || !hitSlice.length) {
                  return;
                }

                var result = '';
                var startIndex = start;
                var endIndex = end;
                hitSlice.forEach(function (hit) {
                  if (hit.index < startIndex) {
                    return;
                  }

                  var hitWordEnd = hit.index + hit.word.length;
                  result += text.slice(startIndex, hit.index);
                  result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                  startIndex = hitWordEnd;
                });
                result += text.slice(startIndex, endIndex);
                return result;
              }

              var postData = {};
              // 文章总的搜索权重
              var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
              // 标记匹配关键词后的标题
              var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
              // 标记匹配关键词后的内容
              var postContent;
              // 显示内容的长度
              var SHOW_WORD_LENGTH = 200;
              // 命中关键词前的字符显示长度
              var SHOW_WORD_FRONT_LENGTH = 20;
              var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

              // 截取匹配的第一个字符，前后共 200 个字符来显示
              if (contentHitSlice.length > 0) {
                var firstIndex = contentHitSlice[0].index;
                var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                var end = firstIndex + SHOW_WORD_END_LENGTH;
                postContent = highlightKeyword(content, contentHitSlice, start, end);
              } else { // 未匹配到内容，直接截取前 200 个字符来显示
                postContent = content.slice(0, SHOW_WORD_LENGTH);
              }
              postData.title = postTitle;
              postData.content = postContent;
              postData.url = postURL;
              postData.weight = postWeight;
              matchPosts.push(postData);
            }
          });
        }

        var resultInnerHtml = '';
        if (matchPosts.length) {
          // 按权重递增的顺序排序，使权重大的优先显示
          matchPosts.sort(function (left, right) {
            return right.weight - left.weight;
          });
          resultInnerHtml += '<ul>';
          matchPosts.forEach(function (post) {
            resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
            resultInnerHtml += post.title;
            resultInnerHtml += '</a><div class="search-results-content">';
            resultInnerHtml += post.content;
            resultInnerHtml += '</div></li>';
          });
          resultInnerHtml += '</ul>';
        } else {
          resultInnerHtml += '<div class="search-results-none"><i class="far fa-meh"></i></div>';
        }
        $result.html(resultInnerHtml);
      };
      $input.on('input', searchPost);
      $input.on('keyup', function (e) {
        if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
          searchPost();
        }
      });
    }
  });
}

function closeSearch () {
  $('body').css({ overflow: 'auto' });
  $('.search-popup').css({ display: 'none' });
  $('.search-mask').css({ display: 'none' });
}

window.addEventListener('DOMContentLoaded', function () {
  Stun.utils.pjaxReloadLocalSearch = function () {
    $('.header-nav-search').on('click', function (e) {
      e.stopPropagation();
      $('body').css('overflow', 'hidden');
      $('.search-popup')
        .velocity('stop')
        .velocity('transition.expandIn', {
          duration: 300,
          complete: function () {
            $('.search-popup input').focus();
          }
        });
      $('.search-mask')
        .velocity('stop')
        .velocity('transition.fadeIn', {
          duration: 300
        });

      initSearch();
    });
    $('.search-mask, .search-close').on('click', function () {
      closeSearch();
    });
    $(document).on('keydown', function (e) {
      // Escape <=> 27
      if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
        closeSearch();
      }
    });
  };

  Stun.utils.pjaxReloadLocalSearch();
}, false);</script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (false) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" rel="stylesheet" type="text/css"><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css"><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.min.js"></script></div><script src="https://cdn.jsdelivr.net/npm/leancloud-storage@latest/dist/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script data-pjax="">function loadValine () {
  var GUEST_INFO = ['nick', 'mail', 'link'];
  var guest_info = 'nick,mail';

  guest_info = guest_info.split(',').filter(function(item) {
    return GUEST_INFO.indexOf(item) > -1;
  });
  new Valine({
    el: '#valine-container',
    appId: 'kBUfBo302Sl5ViCNcWef6wYT-gzGzoHsz',
    appKey: 'hq4rU6YND5ziczBa2PLLUBiM',
    notify: true,
    verify: false,
    placeholder: '有什么需要和我说的，请填写昵称与邮箱(邮箱不会公开显示)，点击评论吧(支持匿名评论)！',
    avatar: 'mp',
    meta: guest_info,
    pageSize: '15' || 10,
    visitor: false,
    recordIP: false,
    lang: '' || 'zh-cn',
    path: window.location.pathname
  });
}

if (true) {
  loadValine();
} else {
  window.addEventListener('DOMContentLoaded', loadValine, false);
}</script><script src="/js/utils.js?v=2.0.0"></script><script src="/js/stun-boot.js?v=2.0.0"></script><script src="/js/scroll.js?v=2.0.0"></script><script src="/js/header.js?v=2.0.0"></script><script src="/js/sidebar.js?v=2.0.0"></script></body></html>